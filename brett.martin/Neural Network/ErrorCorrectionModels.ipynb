{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import time\n",
    "import random\n",
    "import copy\n",
    "\n",
    "from scipy import interp\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.metrics import make_scorer #CHANGE (updated to be consistent with scikit_learn .24)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, ShuffleSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.layers import Dense, Input, Dropout\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import precision_score,accuracy_score, f1_score, recall_score, hamming_loss\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras import optimizers\n",
    "from keras import callbacks\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras import backend as K\n",
    "\n",
    "#%matplotlib inline\n",
    "#import mpld3\n",
    "#mpld3.enable_notebook()\n",
    "\n",
    "print (pd.__version__)\n",
    "\n",
    "######### DEFINITION OF GLOBAL VARIABLES #########\n",
    "RUN_CONFIGURATION_LOOP = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "#sys.path.append('/')\n",
    "import circuits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are super long functions to be hard coded because i dont have time to properly fix them, sorry bout it\n",
    "#[(0, 0.5, 0.5), (0, 0.5, 2.5), (0, 1.5, -0.5), (0, 1.5, 1.5)]\n",
    "def graph_with_errs_d3(df):\n",
    "    x_data = []\n",
    "    z_data = []\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        x_data.append([])\n",
    "        z_data.append([])\n",
    "        \n",
    "        if df.loc[i].at[\"X0\"] == -1:\n",
    "            x_data[i].append((0, -0.5, 0.5))\n",
    "        if df.loc[i].at[\"Z1\"] == -1:\n",
    "            z_data[i].append((0, 0.5, 0.5))\n",
    "        if df.loc[i].at[\"X2\"] == -1:\n",
    "            x_data[i].append((0, 0.5, 1.5))\n",
    "        if df.loc[i].at[\"Z3\"] == -1:\n",
    "            z_data[i].append((0, 0.5, 2.5))\n",
    "        if df.loc[i].at[\"Z4\"] == -1:\n",
    "            z_data[i].append((0, 1.5, -0.5))\n",
    "        if df.loc[i].at[\"X5\"] == -1:\n",
    "            x_data[i].append((0, 1.5, 0.5))\n",
    "        if df.loc[i].at[\"X6\"] == -1:\n",
    "            x_data[i].append((0, 2.5, 1.5))\n",
    "        if df.loc[i].at[\"Z7\"] == -1:\n",
    "            z_data[i].append((0, 1.5, 1.5))\n",
    "        x_data[i] = str(x_data[i])\n",
    "        z_data[i] = str(z_data[i])\n",
    "    #graph_df = pd.DataFrame(df[\"Labels\"], x_data, z_data, columns=[\"Labels\", \"XSyn\", \"ZSyn\"])\n",
    "    graph_df = pd.DataFrame({\"XSyn\":x_data, \"ZSyn\":z_data})\n",
    "    return graph_df\n",
    "        \n",
    "\n",
    "def graph_with_errs_d5(df):\n",
    "    x_data = []\n",
    "    z_data = []\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        x_data.append([])\n",
    "        z_data.append([])\n",
    "        if df.loc[i].at[\"X0\"] == -1:\n",
    "             x_data[i].append((0, -0.5, 0.5))\n",
    "        if df.loc[i].at[\"Z1\"] == -1:\n",
    "            z_data[i].append((0, 0.5, 0.5))\n",
    "        if df.loc[i].at[\"X2\"] == -1:\n",
    "            x_data[i].append((0, 0.5, 1.5))\n",
    "        if df.loc[i].at[\"X3\"] == -1:\n",
    "            x_data[i].append((0, -0.5, 2.5))\n",
    "        if df.loc[i].at[\"Z4\"] == -1:\n",
    "            z_data[i].append((0, 0.5, 2.5))\n",
    "        if df.loc[i].at[\"X5\"] == -1:\n",
    "            x_data[i].append((0, 0.5, 3.5))\n",
    "        if df.loc[i].at[\"Z6\"] == -1:\n",
    "            z_data[i].append((0, 0.5, 4.5))\n",
    "        if df.loc[i].at[\"Z7\"] == -1:\n",
    "            z_data[i].append((0, 1.5, -0.5))\n",
    "        if df.loc[i].at[\"X8\"] == -1:\n",
    "            x_data[i].append((0, 1.5, 0.5))\n",
    "        if df.loc[i].at[\"Z9\"] == -1:\n",
    "            z_data[i].append((0, 1.5, 1.5))\n",
    "        if df.loc[i].at[\"X10\"] == -1:\n",
    "            x_data[i].append((0, 1.5, 2.5))\n",
    "        if df.loc[i].at[\"Z11\"] == -1:\n",
    "            z_data[i].append((0, 1.5, 3.5))\n",
    "        if df.loc[i].at[\"Z12\"] == -1:\n",
    "            z_data[i].append((0, 2.5, 0.5))\n",
    "        if df.loc[i].at[\"X13\"] == -1:\n",
    "            x_data[i].append((0, 2.5, 1.5))\n",
    "        if df.loc[i].at[\"Z14\"] == -1:\n",
    "            z_data[i].append((0, 2.5, 2.5))\n",
    "        if df.loc[i].at[\"X15\"] == -1:\n",
    "            x_data[i].append((0, 2.5, 3.5))\n",
    "        if df.loc[i].at[\"Z16\"] == -1:\n",
    "            z_data[i].append((0, 2.5, 4.5))\n",
    "        if df.loc[i].at[\"Z17\"] == -1:\n",
    "            z_data[i].append((0, 3.5, -0.5))\n",
    "        if df.loc[i].at[\"X18\"] == -1:\n",
    "            x_data[i].append((0, 3.5, 0.5))\n",
    "        if df.loc[i].at[\"X19\"] == -1:\n",
    "            x_data[i].append((0, 4.5, 1.5))\n",
    "        if df.loc[i].at[\"Z20\"] == -1:\n",
    "            z_data[i].append((0, 3.5, 1.5))\n",
    "        if df.loc[i].at[\"X21\"] == -1:\n",
    "            x_data[i].append((0, 3.5, 2.5))\n",
    "        if df.loc[i].at[\"X22\"] == -1:\n",
    "            x_data[i].append((0, 4.5, 3.5))\n",
    "        if df.loc[i].at[\"Z23\"] == -1:\n",
    "            z_data[i].append((0, 3.5, 3.5))\n",
    "        x_data[i] = str(x_data[i])\n",
    "        z_data[i] = str(z_data[i])\n",
    "            \n",
    "    graph_df = pd.DataFrame({\"XSyn\":x_data, \"ZSyn\":z_data})\n",
    "    return graph_df\n",
    "    \n",
    "def graph_with_errs_d7(df):\n",
    "    x_data = []\n",
    "    z_data = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        x_data.append([])\n",
    "        z_data.append([])\n",
    "        \n",
    "        if df.loc[i].at[\"X0\"] == -1:\n",
    "            x_data[i].append((0, -0.5, 0.5))\n",
    "        if df.loc[i].at[\"Z1\"] == -1:\n",
    "            z_data[i].append((0, 0.5, 0.5))\n",
    "        if df.loc[i].at[\"X2\"] == -1:\n",
    "            x_data[i].append((0, 0.5, 1.5))\n",
    "        if df.loc[i].at[\"X3\"] == -1:\n",
    "            x_data[i].append((0, -0.5, 2.5))\n",
    "        if df.loc[i].at[\"Z4\"] == -1:\n",
    "            z_data[i].append((0, 0.5, 2.5))\n",
    "        if df.loc[i].at[\"X5\"] == -1:\n",
    "            x_data[i].append((0, 0.5, 3.5))\n",
    "        if df.loc[i].at[\"X6\"] == -1:\n",
    "            x_data[i].append((0, -0.5, 4.5))\n",
    "        if df.loc[i].at[\"Z7\"] == -1:\n",
    "            z_data[i].append((0, 0.5, 4.5))\n",
    "        if df.loc[i].at[\"X8\"] == -1:\n",
    "            x_data[i].append((0, 0.5, 5.5))\n",
    "        if df.loc[i].at[\"Z9\"] == -1:\n",
    "            z_data[i].append((0, 0.5, 6.5))\n",
    "        if df.loc[i].at[\"Z10\"] == -1:\n",
    "            z_data[i].append((0, 1.5, -0.5))\n",
    "        if df.loc[i].at[\"X11\"] == -1:\n",
    "            x_data[i].append((0, 1.5, 0.5))\n",
    "        if df.loc[i].at[\"Z12\"] == -1:\n",
    "            z_data[i].append((0, 1.5, 1.5))\n",
    "        if df.loc[i].at[\"X13\"] == -1:\n",
    "            x_data[i].append((0, 1.5, 2.5))\n",
    "        if df.loc[i].at[\"Z14\"] == -1:\n",
    "            z_data[i].append((0, 1.5, 3.5))\n",
    "        if df.loc[i].at[\"X15\"] == -1:\n",
    "            x_data[i].append((0, 1.5, 4.5))\n",
    "        if df.loc[i].at[\"Z16\"] == -1:\n",
    "            z_data[i].append((0, 1.5, 5.5))\n",
    "        if df.loc[i].at[\"Z17\"] == -1:\n",
    "            z_data[i].append((0, 2.5, 0.5))\n",
    "        if df.loc[i].at[\"X18\"] == -1:\n",
    "            x_data[i].append((0, 2.5, 1.5))\n",
    "        if df.loc[i].at[\"Z19\"] == -1:\n",
    "            z_data[i].append((0, 2.5, 2.5))\n",
    "        if df.loc[i].at[\"X20\"] == -1:\n",
    "            x_data[i].append((0, 2.5, 3.5))\n",
    "        if df.loc[i].at[\"Z21\"] == -1:\n",
    "            z_data[i].append((0, 2.5, 4.5))\n",
    "        if df.loc[i].at[\"X22\"] == -1:\n",
    "            x_data[i].append((0, 2.5, 5.5))\n",
    "        if df.loc[i].at[\"Z23\"] == -1:\n",
    "            z_data[i].append((0, 2.5, 6.5))\n",
    "        if df.loc[i].at[\"Z24\"] == -1:\n",
    "            z_data[i].append((0, 3.5, -0.5))\n",
    "        if df.loc[i].at[\"X25\"] == -1:\n",
    "            x_data[i].append((0, 3.5, 0.5))\n",
    "        if df.loc[i].at[\"Z26\"] == -1:\n",
    "            z_data[i].append((0, 3.5, 1.5))\n",
    "        if df.loc[i].at[\"X27\"] == -1:\n",
    "            x_data[i].append((0, 3.5, 2.5))\n",
    "        if df.loc[i].at[\"Z28\"] == -1:\n",
    "            z_data[i].append((0, 3.5, 3.5))\n",
    "        if df.loc[i].at[\"X29\"] == -1:\n",
    "            x_data[i].append((0, 3.5, 4.5))\n",
    "        if df.loc[i].at[\"Z30\"] == -1:\n",
    "            z_data[i].append((0, 3.5, 5.5))\n",
    "        if df.loc[i].at[\"Z31\"] == -1:\n",
    "            z_data[i].append((0, 4.5, 0.5))\n",
    "        if df.loc[i].at[\"X32\"] == -1:\n",
    "            x_data[i].append((0, 4.5, 1.5))\n",
    "        if df.loc[i].at[\"Z33\"] == -1:\n",
    "            z_data[i].append((0, 4.5, 2.5))\n",
    "        if df.loc[i].at[\"X34\"] == -1:\n",
    "            x_data[i].append((0, 4.5, 3.5))\n",
    "        if df.loc[i].at[\"Z35\"] == -1:\n",
    "            z_data[i].append((0, 4.5, 4.5))\n",
    "        if df.loc[i].at[\"X36\"] == -1:\n",
    "            x_data[i].append((0, 4.5, 5.5))\n",
    "        if df.loc[i].at[\"Z37\"] == -1:\n",
    "            z_data[i].append((0, 4.5, 6.5))\n",
    "        if df.loc[i].at[\"Z38\"] == -1:\n",
    "            z_data[i].append((0, 5.5, -0.5))\n",
    "        if df.loc[i].at[\"X39\"] == -1:\n",
    "            x_data[i].append((0, 5.5, 0.5))\n",
    "        if df.loc[i].at[\"X40\"] == -1:\n",
    "            x_data[i].append((0, 6.5, 1.5))\n",
    "        if df.loc[i].at[\"Z41\"] == -1:\n",
    "            z_data[i].append((0, 5.5, 1.5))\n",
    "        if df.loc[i].at[\"X42\"] == -1:\n",
    "            x_data[i].append((0, 5.5, 2.5))\n",
    "        if df.loc[i].at[\"X43\"] == -1:\n",
    "            x_data[i].append((0, 6.5, 3.5))\n",
    "        if df.loc[i].at[\"Z44\"] == -1:\n",
    "            z_data[i].append((0, 5.5, 3.5))\n",
    "        if df.loc[i].at[\"X45\"] == -1:\n",
    "            x_data[i].append((0, 5.5, 4.5))\n",
    "        if df.loc[i].at[\"X46\"] == -1:\n",
    "            x_data[i].append((0, 6.5, 5.5))\n",
    "        if df.loc[i].at[\"Z47\"] == -1: \n",
    "            z_data[i].append((0, 5.5, 5.5))\n",
    "        x_data[i] = str(x_data[i])\n",
    "        z_data[i] = str(z_data[i])\n",
    "    graph_df = pd.DataFrame({\"XSyn\":x_data, \"ZSyn\":z_data})\n",
    "    return graph_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These functions needed to work with the GraphDecoder/MWPM module\n",
    "import time\n",
    "\n",
    "def add_measurement_errs(curr_syn, prob_err, x_syn, depth):\n",
    "    #x_syn is True if it is x syndrome, False if it is Z syndrome\n",
    "    total_time = 0\n",
    "    new_syn = []\n",
    "    if x_syn:\n",
    "        for i in curr_syn:\n",
    "            rand = random.random()\n",
    "            if rand > prob_err:\n",
    "                new_syn.append(i)\n",
    "        return (new_syn + return_xmeasurement_errs(depth, prob_err))\n",
    "    else:\n",
    "        for i in curr_syn:\n",
    "            rand = random.random()\n",
    "            if rand > prob_err:\n",
    "                new_syn.append(i)\n",
    "        return (new_syn + return_zmeasurement_errs(depth, prob_err))\n",
    "    \n",
    "def do_new_decoding(data, depth, prob_err):\n",
    "    decoder = circuits.GraphDecoder(depth,1)\n",
    "    G = decoder.S['Z']\n",
    "    #decoder.graph_2D(G,'distance')\n",
    "    df = pd.DataFrame()\n",
    "    syn = []\n",
    "    total_time = 0\n",
    "    \n",
    "    for row in data:\n",
    "        x_input = []\n",
    "        z_input = []\n",
    "        x_type = True\n",
    "        for col in row:\n",
    "            if not col == \"[]\":\n",
    "                col = eval(col)\n",
    "                for c in col:\n",
    "                    if x_type:\n",
    "                        x_input.append(c)\n",
    "                    else:\n",
    "                        z_input.append(c)\n",
    "            x_type = not x_type  \n",
    "            \n",
    "        if prob_err > 0:\n",
    "            syndromes_x = add_measurement_errs(x_input, prob_err, True, depth)\n",
    "            syndromes_z = add_measurement_errs(z_input, prob_err, False, depth)\n",
    "        else:\n",
    "            syndromes_x = x_input\n",
    "            syndromes_z = z_input\n",
    "\n",
    "        start = time.time_ns()\n",
    "        error_graph_x, paths_x = decoder.make_error_graph(syndromes_x,'X')\n",
    "        matching_graph_x = decoder.matching_graph(error_graph_x,'X')\n",
    "        matches_x = decoder.matching(matching_graph_x,'X')\n",
    "        flips_x = decoder.calculate_qubit_flips(matches_x, paths_x,'X')\n",
    "        syn_x = (translate_errors(flips_x))\n",
    "\n",
    "        error_graph_z, paths_z = decoder.make_error_graph(syndromes_z,'Z')\n",
    "        matching_graph_z = decoder.matching_graph(error_graph_z,'Z')\n",
    "        matches_z = decoder.matching(matching_graph_z,'Z')\n",
    "        flips_z = decoder.calculate_qubit_flips(matches_z, paths_z,'Z')\n",
    "        syn_z = translate_errors(flips_z)\n",
    "        df = df.append(pd.Series([syn_x, syn_z]), ignore_index=True)\n",
    "        end = time.time_ns()\n",
    "        total_time += (end - start)/ (10 ** 9)\n",
    "    return (df, total_time) \n",
    "\n",
    "import random\n",
    "def return_xmeasurement_errs(depth, prob):\n",
    "    \n",
    "    new_errs = []\n",
    "    \n",
    "    if depth == 3:\n",
    "        errs = [(0, -0.5, 0.5), (0, 0.5, 1.5), (0, 1.5, 0.5), (0, 2.5, 1.5)]\n",
    "    elif depth == 5:\n",
    "        errs = [(0, -0.5, 0.5), (0, 0.5, 1.5), (0, -0.5, 2.5), (0, 0.5, 3.5), (0, 1.5, 0.5), (0, 1.5, 2.5),\n",
    "                        (0, 2.5, 1.5), (0, 2.5, 3.5), (0, 3.5, 0.5), (0, 4.5, 1.5), (0, 3.5, 2.5), (0, 4.5, 3.5)]\n",
    "    else:\n",
    "        errs = [(0, -0.5, 0.5), (0, 0.5, 1.5), (0, -0.5, 2.5), (0, 0.5, 3.5), (0, -0.5, 4.5), (0, 0.5, 5.5),\n",
    "                        (0, 1.5, 0.5), (0, 1.5, 2.5), (0, 1.5, 4.5), (0, 2.5, 1.5), (0, 2.5, 3.5), (0, 2.5, 5.5),\n",
    "                        (0, 3.5, 0.5),  (0, 3.5, 2.5), (0, 3.5, 4.5), (0, 4.5, 1.5), (0, 4.5, 3.5), (0, 4.5, 5.5),\n",
    "                       (0, 5.5, 0.5), (0, 6.5, 1.5), (0, 5.5, 2.5), (0, 6.5, 3.5), (0, 5.5, 4.5), (0, 6.5, 5.5)]\n",
    "    for e in errs:\n",
    "        rand = random.random()\n",
    "        if rand <= prob:\n",
    "            new_errs.append(e)\n",
    "            \n",
    "    return new_errs\n",
    "            \n",
    "\n",
    "def return_zmeasurement_errs(depth, prob):\n",
    "    \n",
    "    new_errs = []\n",
    "    \n",
    "    if depth == 3:\n",
    "        errs = [(0, 0.5, 0.5), (0, 0.5, 2.5), (0, 1.5, -0.5), (0, 1.5, 1.5)]\n",
    "    elif depth == 5:\n",
    "        errs = [(0, 0.5, 0.5), (0, 0.5, 2.5), (0, 0.5, 4.5), (0, 1.5, -0.5), (0, 1.5, 1.5), (0, 1.5, 3.5),\n",
    "                        (0, 2.5, 0.5), (0, 2.5, 2.5), (0, 2.5, 4.5), (0, 3.5, -0.5), (0, 3.5, 1.5), (0, 3.5, 3.5)]\n",
    "    else:\n",
    "        errs = [(0, 0.5, 0.5), (0, 0.5, 2.5), (0, 0.5, 4.5), (0, 0.5, 6.5), (0, 1.5, -0.5), (0, 1.5, 1.5),\n",
    "                        (0, 1.5, 3.5), (0, 1.5, 5.5), (0, 2.5, 0.5), (0, 2.5, 2.5), (0, 2.5, 4.5), (0, 2.5, 6.5),\n",
    "                        (0, 3.5, -0.5),  (0, 3.5, 1.5), (0, 3.5, 3.5), (0, 3.5, 5.5), (0, 4.5, 0.5), (0, 4.5, 2.5),\n",
    "                       (0, 4.5, 4.5), (0, 4.5, 6.5), (0, 5.5, -0.5), (0, 5.5, 1.5), (0, 5.5, 3.5), (0, 5.5, 5.5)]\n",
    "        \n",
    "    for e in errs:\n",
    "        rand = random.random()\n",
    "        if rand <= prob:\n",
    "            new_errs.append(e)\n",
    "            \n",
    "    return new_errs\n",
    "\n",
    "def translate_errors (phys_errs):\n",
    "    flipX = np.array([(0, 1),(1, 0)])\n",
    "    flipZ = np.array([(1, 0), (0, -1)])\n",
    "    errs = []\n",
    "    str2 = \"\"\n",
    "    for qubit, flip in phys_errs.items():\n",
    "        row = int(qubit[1])\n",
    "        col = int(qubit[2])\n",
    "        if str(flip) == \"X\":\n",
    "            str1 = \"X\"\n",
    "        elif str(flip) == \"Z\":\n",
    "            str1 = \"Z\"\n",
    "        else:\n",
    "            str1 = \"X\"\n",
    "            str2 = \"Z\"\n",
    "        str1 += str(row) + str(col)\n",
    "        errs.append(str1)\n",
    "        if str2 != \"\":\n",
    "            str2 += str(row) +str(col)\n",
    "            errs.append(str2)\n",
    "            str2 = \"\"\n",
    "    return errs   \n",
    "\n",
    "def translate_to_graph(df_graph, labels, mlb):\n",
    "#go through labels given\n",
    "    indices = []\n",
    "    labels = mlb.inverse_transform(labels)\n",
    "    \n",
    "    for row in labels:\n",
    "        label_str = str(row)\n",
    "        for index, r in df_graph.iterrows():\n",
    "            if label_str == \"('',)\":\n",
    "                if str(r[\"Labels\"]) == \"[' ']\":\n",
    "                    indices.append([index])\n",
    "                    break\n",
    "            if set(row) == set(r[\"Labels\"]):\n",
    "                indices.append([index])\n",
    "                break\n",
    "\n",
    "    df_syns = df_graph.drop(['Labels'], axis=1)\n",
    "    return_df = pd.DataFrame()\n",
    "    for i in indices:\n",
    "        return_df = return_df.append(df_syns.loc[i])\n",
    "    return return_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function needed for preprocessing. CSV file reads in a string, needs to be a list for labels \n",
    "#for preprocessing csv files\n",
    "def create_list_from_string(err_list):\n",
    "    newstring = err_list.replace(\"'\", \"\")\n",
    "    new_err_list = newstring.strip('][').split(', ')\n",
    "    return sorted(set(new_err_list))\n",
    "\n",
    "\n",
    "def create_string_from_list(err_list):\n",
    "    return_string = \"[\"\n",
    "    if err_list[0] == \"''\":\n",
    "        return \"[' ']\"\n",
    "    else:\n",
    "        for index, i in enumerate(err_list):\n",
    "            return_string = return_string +  \"'\" + i + \"'\"\n",
    "            if index < (len(err_list)-1):\n",
    "                return_string += \", \"\n",
    "    return return_string + \"]\"\n",
    "        \n",
    "\n",
    "#take in two 2d arrays of predicted values, true values, and threshold to determine labels. \n",
    "#calculates the partial accuracy of the predicted values, averaged out for all obersvations\n",
    "def partial_accuracy(y_pred, y_true):\n",
    "    total = 0\n",
    "    rows = y_pred.shape[0]\n",
    "    cols = y_pred.shape[1]\n",
    "    for i in range(0, rows):\n",
    "        row_correct = 0\n",
    "        for j in range(0, cols):\n",
    "            if y_pred[i,j] == y_true[i,j]:\n",
    "                row_correct += 1\n",
    "        total += row_correct/cols\n",
    "    return total/rows\n",
    "\n",
    "def partial_accuracy_and_contingency(y_pred, y_true, mlb):\n",
    "    total = 0\n",
    "    a= np.zeros(shape=y_true.shape)\n",
    "    rows = y_pred.shape[0]\n",
    "    cols = y_pred.shape[1]\n",
    "    df = pd.DataFrame(a, columns = mlb.classes_)\n",
    "    for i in range(0, rows):\n",
    "        row_correct = 0\n",
    "        for j, label in enumerate(mlb.classes_):\n",
    "            if y_pred[i,j] == y_true[i,j]:\n",
    "                row_correct += 1\n",
    "                df.at[i, label] = 1\n",
    "            else:\n",
    "                df.at[i, label] = 0\n",
    "\n",
    "        total += row_correct/cols\n",
    "\n",
    "    return (total/rows, df)\n",
    "\n",
    "def contingency_table_and_t (clf1, clf2):\n",
    "    a = 0 #clf1 pos, clf2 pos\n",
    "    b = 0 #clf1 pos, clf2 neg\n",
    "    c = 0 #clf1 neg, clf2 pos\n",
    "    d = 0 #clf1 neg, clf2 neg\n",
    "    \n",
    "    for index, value in clf1.items():\n",
    "        if value == 1 and clf2[index] == 1:\n",
    "            a+=1\n",
    "        elif value == 1 and clf2[index] == 0: #classifier 1 right, classifier 2 wrong\n",
    "            b+=1\n",
    "        elif value == 0 and clf2[index] == 1: #classifier 1 wrong, classifier 2 right\n",
    "            c+=1\n",
    "        else:\n",
    "            d+=1\n",
    "    print(\"[\"+str(a)+\", \"+str(b)+\"]\")\n",
    "    print(\"[\"+str(c)+\", \"+str(d)+\"]\")\n",
    "    if b == 0 and c ==0:\n",
    "        print(\"both b and c are zero\")\n",
    "        t=0\n",
    "    else:\n",
    "        t = (((b-c)-1)**2)/(b+c)\n",
    "    return ([[a,b],[c,d]], t)\n",
    "\n",
    "def add_noise(val, noise_level):\n",
    "    rand = random.uniform(0, 1)\n",
    "    if rand <= noise_level:\n",
    "        if val == -1:\n",
    "            val = 1\n",
    "        elif val == 1:\n",
    "            val = -1\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_d7= trainData_d7.dropna()\n",
    "#######################################################################################################\n",
    "\n",
    "trainData_d7 = pd.read_csv(\"depth7_all_combos.csv\")\n",
    "trainData_d7 = trainData_d7.applymap(lambda x: add_noise(x,.01))\n",
    "\n",
    "#These four lines remove duplicates\n",
    "trainData_d7['Labels'] = trainData_d7['Labels'].map(lambda x: create_list_from_string(x))\n",
    "trainData_d7['Labels'] = trainData_d7['Labels'].astype(str)\n",
    "trainData_d7 = trainData_d7.drop_duplicates('Labels', keep='first', ignore_index=True)\n",
    "trainData_d7['Labels'] = trainData_d7['Labels'].map(lambda x: create_list_from_string(x))\n",
    "\n",
    "testData_d7_MWPM = graph_with_errs_d7(trainData_d7)\n",
    "\n",
    "#transforms the data to encoding for ML\n",
    "mlb_d7 = MultiLabelBinarizer()\n",
    "mlb_d7.fit(trainData_d7['Labels'])\n",
    "df = pd.DataFrame(mlb_d7.transform(trainData_d7['Labels']))\n",
    "df['Labels']= df.values.tolist()\n",
    "trainData_d7 = trainData_d7.drop(['Labels'], axis=1)\n",
    "trainData_d7 = pd.concat([df[\"Labels\"],testData_d7_MWPM, trainData_d7], axis=1, ignore_index=True)\n",
    "trainData_d7.columns = [\"Labels\",\"XSyn\", \"ZSyn\",\"X0\", \"Z1\", \"X2\", \"X3\", \"Z4\", \"X5\", \"X6\", \"Z7\", \"X8\", \"Z9\", \"Z10\", \"X11\", \"Z12\", \"X13\", \"Z14\", \"X15\", \"Z16\", \"Z17\", \"X18\", \"Z19\",\"X20\", \"Z21\", \"X22\", \"Z23\", \"Z24\", \"X25\", \"Z26\", \"X27\", \"Z28\", \"X29\", \"Z30\", \"Z31\", \"X32\", \"Z33\", \"X34\", \"Z35\", \"X36\", \"Z37\", \"Z38\", \"X39\", \"X40\", \"Z41\", \"X42\", \"X43\", \"Z44\", \"X45\", \"X46\", \"Z47\"]\n",
    "\n",
    "#######################################################################################\n",
    "\n",
    "##trainData_d5 = pd.read_csv(\"depth5_all_combos.csv\") # RM FOR D7 TESTS\n",
    "##trainData_d5 = trainData_d5.applymap(lambda x: add_noise(x,.01)) #was .05  # RM FOR D7 TESTS\n",
    "#These four lines remove duplicates\n",
    "##trainData_d5['Labels'] = trainData_d5['Labels'].map(lambda x: create_list_from_string(x)) # RM FOR D7 TESTS\n",
    "##trainData_d5['Labels'] = trainData_d5['Labels'].astype(str) # RM FOR D7 TESTS\n",
    "##trainData_d5 = trainData_d5.drop_duplicates('Labels', keep='first', ignore_index=True) # RM FOR D7 TESTS\n",
    "##trainData_d5['Labels'] = trainData_d5['Labels'].map(lambda x: create_list_from_string(x)) # RM FOR D7 TESTS\n",
    "\n",
    "\n",
    "##testData_d5_MWPM = graph_with_errs_d5(trainData_d5) # RM FOR D7 TESTS\n",
    "\n",
    "\n",
    "#transforms the data to encoding for ML\n",
    "##mlb = MultiLabelBinarizer() # RM FOR D7 TESTS\n",
    "##mlb.fit(trainData_d5['Labels']) # RM FOR D7 TESTS\n",
    "##df = pd.DataFrame(mlb.transform(trainData_d5['Labels'])) # RM FOR D7 TESTS\n",
    "##df['Labels']= df.values.tolist() # RM FOR D7 TESTS\n",
    "##trainData_d5 = trainData_d5.drop(['Labels'], axis=1) # RM FOR D7 TESTS\n",
    "##trainData_d5 = pd.concat([df[\"Labels\"], testData_d5_MWPM, trainData_d5], axis=1, ignore_index=True) # RM FOR D7 TESTS\n",
    "##trainData_d5.columns = [\"Labels\",\"XSyn\", \"ZSyn\",\"X0\",\"Z1\",\"X2\",\"X3\",\"Z4\",\"X5\",\"Z6\",\"Z7\",\"X8\",\"Z9\",\"X10\",\"Z11\",\"Z12\",\"X13\",\"Z14\",\"X15\",\"Z16\",\"Z17\",\"X18\",\"X19\",\"Z20\",\"X21\",\"X22\",\"Z23\"] # RM FOR D7 TESTS\n",
    "#########################################################################################\n",
    "\n",
    "#Has no duplicates, small enough to check manually\n",
    "##trainData_d3 = pd.read_csv(\"depth3_all_combos.csv\") # RM FOR D7 TESTS\n",
    "\n",
    "##trainData_d3[\"Labels\"] = trainData_d3['Labels'].map(lambda x: create_list_from_string(x)) # RM FOR D7 TESTS\n",
    "##trainData_d3 = trainData_d3.applymap(lambda x: add_noise(x,.01)) # RM FOR D7 TESTS\n",
    "\n",
    "##testData_d3_MWPM = graph_with_errs_d3(trainData_d3) # RM FOR D7 TESTS\n",
    "\n",
    "##mlb_d3 = MultiLabelBinarizer() # RM FOR D7 TESTS\n",
    "##mlb_d3.fit(trainData_d3[\"Labels\"]) # RM FOR D7 TESTS\n",
    "##df = pd.DataFrame(mlb_d3.transform(trainData_d3['Labels'])) # RM FOR D7 TESTS\n",
    "##df['Labels']= df.values.tolist() # RM FOR D7 TESTS\n",
    "##trainData_d3 = trainData_d3.drop(['Labels'], axis=1) # RM FOR D7 TESTS\n",
    "##trainData_d3 = pd.concat([df['Labels'], testData_d3_MWPM, trainData_d3], axis=1, ignore_index=True) # RM FOR D7 TESTS\n",
    "##trainData_d3.columns = [\"Labels\",\"XSyn\", \"ZSyn\", \"X0\", \"Z1\", \"X2\", \"Z3\", \"Z4\", \"X5\", \"X6\", \"Z7\"] # RM FOR D7 TESTS\n",
    "#########################################################################################\n",
    "##y_d3 = trainData_d3[\"Labels\"] # RM FOR D7 TESTS\n",
    "##x_d3 = trainData_d3.drop([\"Labels\"], axis=1) # RM FOR D7 TESTS\n",
    "\n",
    "##y_d5 = trainData_d5[\"Labels\"]  # RM FOR D7 TESTS\n",
    "##x_d5 = trainData_d5.drop([\"Labels\"], axis=1)  # RM FOR D7 TESTS\n",
    "\n",
    "y_d7 = trainData_d7[\"Labels\"]\n",
    "x_d7 = trainData_d7.drop([\"Labels\"], axis=1)\n",
    "\n",
    "\n",
    "##x_d3 = x_d3.replace([-1], 0) # RM FOR D7 TESTS\n",
    "##x_d5 = x_d5.replace([-1], 0) # RM FOR D7 TESTS\n",
    "x_d7 = x_d7.replace([-1], 0)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brett: Not sure why this was here. It's just a duplicate. Consider removing.\n",
    "##y_d7 = trainData_d7[\"Labels\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions for creating lookup tables here:\n",
    "from collections import defaultdict\n",
    "from math import log2\n",
    "\n",
    "def generateAllBinaryStrings(n, arr, i, lookup):  \n",
    "    if i == n: \n",
    "        lookup.setBitStringArray(arr, n)  \n",
    "        return\n",
    "      \n",
    "    # First assign \"0\" at ith position  \n",
    "    # and try for all other permutations  \n",
    "    # for remaining positions  \n",
    "    arr[i] = 0\n",
    "    generateAllBinaryStrings(n, arr, i + 1, lookup)  \n",
    "  \n",
    "    # And then assign \"1\" at ith position  , \n",
    "    # and try for all other permutations  \n",
    "    # for remaining positions  \n",
    "    arr[i] = 1\n",
    "    generateAllBinaryStrings(n, arr, i + 1, lookup)\n",
    "\n",
    "class lookup_decoder:\n",
    "    \n",
    "    def __init__(self, depth):\n",
    "        #self.lookupTable = defaultdict()\n",
    "        self.lookupTable = {}\n",
    "        self.distributions = {}\n",
    "        self.depth = depth\n",
    "        #generating all possible syndrome observations\n",
    "        #arr = [None] * (depth**2 - 1)\n",
    "        #generateAllBinaryStrings((depth**2 - 1), arr, 0, self)\n",
    "        \n",
    "    def setBitStringArray(self, arr, n): \n",
    "        new_str = \"\"\n",
    "        for i in range(0, n):  \n",
    "            new_str += str(arr[i])\n",
    "        self.lookupTable.update({new_str:defaultdict()})  \n",
    "        \n",
    "    def update_table (self, syndrome, phys_errs):\n",
    "        #all the keys are made in the init, so simply update the physical error combinations for the given syndrome\n",
    "        if syndrome not in self.lookupTable:\n",
    "            self.lookupTable[syndrome] = {}\n",
    "            self.lookupTable[syndrome][phys_errs] = 1\n",
    "            return\n",
    "        \n",
    "        if phys_errs not in self.lookupTable[syndrome]:\n",
    "            self.lookupTable[syndrome].update({phys_errs: 1})\n",
    "        else:\n",
    "            self.lookupTable[syndrome][phys_errs] += 1\n",
    "     \n",
    "    def get_probable_error(self, syndrome):\n",
    "        return_key = []\n",
    "\n",
    "        if syndrome not in self.lookupTable.keys():\n",
    "            for i in range(2* int(self.depth**2) + 1):\n",
    "                return_key.append(0)\n",
    "            return return_key\n",
    "        \n",
    "        key, value = max(self.lookupTable[syndrome].items(), key=lambda x:x[1])\n",
    "        \n",
    "        for character in key:\n",
    "            if character == '0' or character == '1':\n",
    "                return_key.append(int(character))\n",
    "                \n",
    "        return return_key\n",
    "        \n",
    "    def make_distribution_graph(self, syn):\n",
    "        \n",
    "        plt.bar(list(self.lookupTable[syn].keys()), self.lookupTable[syn].values(), color='g')\n",
    "        plt.show()\n",
    "        \n",
    "    def syndrome_count_graph(self):\n",
    "        graph_dict = {}\n",
    "        for syn in self.lookupTable:\n",
    "            graph_dict.update({syn:sum(self.lookupTable[syn].values())})\n",
    "        plt.ylim((0,4))\n",
    "        plt.bar(graph_dict.keys(), graph_dict.values())\n",
    "        plt.show()\n",
    "        \n",
    "    \n",
    "    def length_of_lookup(self):\n",
    "        print(len(self.lookupTable))\n",
    "        \n",
    "            \n",
    "    def get_entropies(self):\n",
    "        entropies = {}\n",
    "        for syn in self.lookupTable:\n",
    "            total = sum(self.lookupTable[syn].values())\n",
    "            h = 0\n",
    "            for key in self.lookupTable[syn]:\n",
    "                p = self.lookupTable[syn][key]/total\n",
    "                h += p+log2(p)\n",
    "            entropies[syn] = -h\n",
    "        print(entropies)\n",
    "            \n",
    "    def get_syndromes(self):\n",
    "        return self.lookupTable.keys()\n",
    "                     \n",
    "    def print_lookup(self):\n",
    "        for syn in self.lookupTable:\n",
    "            print(self.lookupTable[syn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_plut(table, data_x, data_y):\n",
    "    i = 0\n",
    "    for index, x in enumerate(data_x):\n",
    "        syn = \"\".join([str(i) for i in x])\n",
    "        syn = syn.replace(\".0\",\"\")\n",
    "        labels = np.array2string(np.array(data_y[i]), precision=1, separator='',suppress_small=True).replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "        table.update_table(syn, labels)\n",
    "        i+=1\n",
    "    return table\n",
    "\n",
    "\n",
    "def test_plut(table, test_set):\n",
    "    predictions_lookup = []\n",
    "    for index, x in enumerate(test_set):\n",
    "        syn_x = \"\".join([str(i) for i in x])\n",
    "        syn_x = syn_x.replace(\".0\",\"\")\n",
    "        predictions_lookup.append(table.get_probable_error(syn_x))\n",
    "    return np.array(predictions_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_FFNN_model_DepthThree(depth):\n",
    "    model = Sequential()\n",
    "    layers = 2\n",
    "    #input layer\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    \n",
    "    #hidden layers go here\n",
    "    for i in range(layers):\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "    \n",
    "    model.add(Dense(19 , activation='sigmoid'))\n",
    "    model.compile(loss=keras.losses.binary_crossentropy, optimizer=tf.keras.optimizers.SGD(learning_rate=0.05),\n",
    "                  metrics=['accuracy']) #CHANGE  (added tf., changed lr to learning_rate)\n",
    "    return model\n",
    "\n",
    "#make any needed changes here\n",
    "def compile_FFNN_model_DepthFive(depth):\n",
    "    model = Sequential()\n",
    "    layers = 4\n",
    "    \n",
    "    #input layer\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    \n",
    "    #hidden layers go here\n",
    "    for i in range(layers):\n",
    "        model.add(Dense(240, activation='relu'))\n",
    "    \n",
    "    #model.add(Dense((2*depth**2) , activation='sigmoid'))\n",
    "    model.add(Dense(51, activation='sigmoid'))\n",
    "    model.compile(loss=keras.losses.binary_crossentropy, optimizer=tf.keras.optimizers.SGD(learning_rate=0.05),\n",
    "                  metrics=['accuracy']) #CHANGE (added tf.)\n",
    "    return model\n",
    "\n",
    "\n",
    "#make any needed changes here\n",
    "def compile_FFNN_model_DepthSeven(depth):\n",
    "    model = Sequential()\n",
    "    layers = 4\n",
    "    \n",
    "    #input layer\n",
    "    model.add(Dense(48, activation='relu'))\n",
    "    \n",
    "    #hidden layers go here\n",
    "    for i in range(layers):\n",
    "        model.add(Dense(400, activation='relu'))\n",
    "    \n",
    "    #model.add(Dense((2*depth**2) , activation='sigmoid'))\n",
    "    model.add(Dense(99, activation='sigmoid'))\n",
    "    model.compile(loss=keras.losses.binary_crossentropy, optimizer=tf.keras.optimizers.SGD(learning_rate=0.05),\n",
    "                  metrics=['accuracy']) #CHANGE  (added tf.)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D3 TESTING\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "inputs = x_d3.copy()\n",
    "inputs = np.array(inputs)\n",
    "inputs = np.stack(inputs)\n",
    "targets = y_d3.copy()\n",
    "targets = np.array([np.array(xi) for xi in targets])\n",
    "\n",
    "n_classes = 19\n",
    "\n",
    "acc_per_fold = []\n",
    "f1_per_fold = []\n",
    "time_per_fold = []\n",
    "\n",
    "\n",
    "acc_per_fold_plut = []\n",
    "f1_per_fold_plut = []\n",
    "time_per_fold_plut = []\n",
    "\n",
    "acc_per_fold_mwpm = []\n",
    "f1_per_fold_mwpm = []\n",
    "time_per_fold_mwpm = []\n",
    "\n",
    "\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "aucs_classes = {}\n",
    "for i in mlb_d3.classes_:\n",
    "    aucs_classes[i] = []\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "#for i in range(5):\n",
    "    \n",
    "    # K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "for i, (train, test) in enumerate(kfold.split(inputs, targets)):\n",
    "    i_train = inputs[train].copy()\n",
    "    i_test = inputs[test].copy()\n",
    "    x_test_d3 = i_test[:,:2]\n",
    "    inputs_train = i_train[:,3:]\n",
    "    inputs_test = i_test[:,3:]\n",
    "    ####################################################################################################\n",
    "    #test MWPM decoder for this fold\n",
    "    #labels = targets[train], features = inputs[train]\n",
    "    #x_test_d3 = translate_to_graph(testData_d3_MWPM, targets[test], mlb_d3)\n",
    "    decoding_d3, time_mwpm = do_new_decoding(x_test_d3, 3, .03)\n",
    "    decoding_d3['combine'] = decoding_d3[[0, 1]].values.tolist()\n",
    "    decoding_d3['combine'].apply(lambda x: x[0].extend(x[1]))\n",
    "    decoding_d3 = np.array(decoding_d3[0])\n",
    "\n",
    "    time_per_fold_mwpm.append(time_mwpm)\n",
    "\n",
    "    pred_mwpm = mlb_d3.transform(decoding_d3)\n",
    "    if fold_no < 5:\n",
    "        acc = partial_accuracy(targets[test], pred_mwpm)\n",
    "    else:\n",
    "        acc, contingency_mwpm = partial_accuracy_and_contingency(targets[test], pred_mwpm, mlb_d3)\n",
    "        \n",
    "    acc_per_fold_mwpm.append(acc)\n",
    "    f1_per_fold_mwpm.append(f1_score(targets[test], pred_mwpm, average='micro'))\n",
    "\n",
    "\n",
    "    #####################################################################################################\n",
    "    #test the plut decoder for this fold\n",
    "\n",
    "    lookup_d3 = lookup_decoder(3)\n",
    "\n",
    "    lookup_d3 = train_plut(lookup_d3, inputs_train, targets[train])\n",
    "\n",
    "    start = time.time_ns()\n",
    "    pred_plut_d3 = test_plut(lookup_d3, inputs_test)\n",
    "    end = time.time_ns() \n",
    "    time_per_fold_plut.append((end - start)/ (10 ** 9))\n",
    "    \n",
    "    if fold_no < 5:\n",
    "        acc = partial_accuracy(targets[test], pred_plut_d3)\n",
    "    else:\n",
    "        acc, contingency_plut = partial_accuracy_and_contingency(targets[test], pred_plut_d3, mlb_d3)\n",
    "\n",
    "    acc_per_fold_plut.append(acc)\n",
    "    f1_per_fold_plut.append(f1_score(targets[test], pred_plut_d3, average='micro'))\n",
    "\n",
    "    #####################################################################################################\n",
    "    #Test the NN decoder for this fold\n",
    "\n",
    "    model = compile_FFNN_model_DepthThree(3)\n",
    "    # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    # Fit data to model\n",
    "    \n",
    "    inputs_train = np.asarray(inputs_train).astype(np.int) #CHANGE (added)\n",
    "    targets[train] = np.asarray(targets[train]).astype(np.int) #CHANGE (added)\n",
    "    \n",
    "    history = model.fit(\n",
    "        inputs_train, targets[train],\n",
    "        validation_split=.2,\n",
    "        epochs=200,\n",
    "        verbose=1)\n",
    "\n",
    "   # Generate generalization metrics\n",
    "    inputs_test = np.asarray(inputs_test).astype(np.int) #CHANGE (added)\n",
    "    targets[test] = np.asarray(targets[test]).astype(np.int) #CHANGE (added)\n",
    "    \n",
    "    scores = model.evaluate(inputs_test, targets[test], verbose=0)\n",
    "\n",
    "    #get the time to predicting test\n",
    "    start = time.time_ns()\n",
    "    predictions_d3 = model.predict(inputs_test) #change here\n",
    "    end = time.time_ns()\n",
    "    time_per_fold.append((end - start)/ (10 ** 9))\n",
    "\n",
    "    #threshold based on previous tests with train_test_split in hyperparameter tuning\n",
    "    pred=predictions_d3.copy() #change here\n",
    "    pred[pred>=.1]=1 \n",
    "    pred[pred<.1]=0\n",
    "    \n",
    "    if fold_no <5:\n",
    "        acc = scores[1]\n",
    "    else:\n",
    "        acc, contingency_nn = partial_accuracy_and_contingency(targets[test], pred, mlb_d3)\n",
    "\n",
    "    acc_per_fold.append(acc)\n",
    "    f1_per_fold.append(f1_score(targets[test], pred, average='micro'))\n",
    "\n",
    "    #####################################################################################################\n",
    "    #comput ROC AUC for classes and the mircoaverage\n",
    "    fpr, tpr, thresholds = roc_curve(targets[test].ravel(), predictions_d3.ravel())\n",
    "    tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "    tprs[-1][0] = 0.0\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "    plt.plot(fpr, tpr, lw=1, alpha=0.3, label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
    "\n",
    "    #get the AUCs of each class, used to get average AUC of each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(targets[test][:, i], predictions_d3[:, i]) #change here\n",
    "        aucs_classes[mlb_d3.classes_[i]].append(auc(fpr[i], tpr[i]))\n",
    "        \n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n",
    "        \n",
    "#########################################################################################################\n",
    "#compute McNemar's statistic on results of last fold\n",
    "\n",
    "mcnemar_results_mwpm = {}\n",
    "mcnemar_results_plut = {}\n",
    "\n",
    "for class_ in mlb_d3.classes_:\n",
    "    \n",
    "    #compute the x^2 for NN and MWPM\n",
    "    mcnemar_results_mwpm[class_]=contingency_table_and_t(contingency_nn[class_], contingency_mwpm[class_])[1]    \n",
    "    #comput the x^2 for NN and PLUT\n",
    "    mcnemar_results_plut[class_] = contingency_table_and_t(contingency_nn[class_], contingency_plut[class_])[1]\n",
    "        \n",
    "    \n",
    "############print mean and stdev of AUC of each class#####################      \n",
    "    \n",
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds of MWPM:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold_mwpm)} (+- {np.std(acc_per_fold_mwpm)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold_mwpm)}(+- {np.std(f1_per_fold_mwpm)})')\n",
    "print(f'> Time: {np.mean(time_per_fold_mwpm)} (+- {np.std(time_per_fold_mwpm)})')\n",
    "print(\"#####################################################################################\")\n",
    "print('Average scores for all folds of PLUT:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold_plut)} (+- {np.std(acc_per_fold_plut)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold_plut)}(+- {np.std(f1_per_fold_plut)})')\n",
    "print(f'> Time: {np.mean(time_per_fold_plut)} (+- {np.std(time_per_fold_plut)})')\n",
    "print(\"#####################################################################################\")\n",
    "print('Average scores for all folds of NN:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold)}(+- {np.std(f1_per_fold)})')\n",
    "print(f'> Time: {np.mean(time_per_fold)} (+- {np.std(time_per_fold)})')\n",
    "print(\"#####################################################################################\")\n",
    "for key in aucs_classes:\n",
    "    cleanedList = [x for x in aucs_classes[key] if str(x) != 'nan']\n",
    "    print(f'> AUC for class {key}: {np.mean(cleanedList)} (+- {np.std(cleanedList)})') #this has to be a for loop\n",
    "    print(\"X^2 for MWPM and NN: \" + str(mcnemar_results_mwpm[key]))  \n",
    "    print(\"X^2 for PLUT and NN: \" + str(mcnemar_results_plut[key]))\n",
    "print(\"###################################################################################\")\n",
    "print(\"TOTAL F1 NN: \" + str(f1_per_fold))\n",
    "print(\"TOTAL F1 PLUT: \" + str(f1_per_fold_plut))\n",
    "print(\"TOTAL F1 MWPM: \" + str(f1_per_fold_mwpm))\n",
    "print(\"TOTAL ACC NN: \" + str(acc_per_fold))\n",
    "print(\"TOTAL ACC PLUT: \" + str(acc_per_fold_plut))\n",
    "print(\"TOTAL ACC MWPM: \" + str(acc_per_fold_mwpm))\n",
    "print(\"TOTAL TIME NN: \" + str(time_per_fold))\n",
    "print(\"TOTAL TIME PLUT: \" + str(time_per_fold_plut))\n",
    "print(\"TOTAL TIME MWPM: \" + str(time_per_fold_mwpm))\n",
    "print('------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "         label='Chance', alpha=.8)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "         label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "         lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                 label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "plt.xlim([-0.01, 1.01])\n",
    "plt.ylim([-0.01, 1.01])\n",
    "plt.xlabel('False Positive Rate',fontsize=18)\n",
    "plt.ylabel('True Positive Rate',fontsize=18)\n",
    "plt.title('Cross-Validation ROC of Depth 3 NN',fontsize=18)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "\n",
    "model = compile_FFNN_model_DepthThree(3)\n",
    "print(\"Fit model on training data\")\n",
    "history = model.fit(\n",
    "    x=x_train_d3.values,\n",
    "    y=Y_train_d3,\n",
    "    validation_split=.25,\n",
    "    epochs=200\n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Make a quick epoch vs. loss plot\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.grid()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Depth 3 Model Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (MSE)')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Make a quick epoch vs. accuracy plot\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.grid()\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Depth 3 Model Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "start = time.time()\n",
    "predictions_d3 = model.predict(x_test_d3.values)\n",
    "end = time.time()\n",
    "print(\"Time: \" + str(end - start))\n",
    "# predict\n",
    "\n",
    "thresholds=[0.1, .2, .3, .4, .5, .6, .7, .8, .9]\n",
    "for val in thresholds:\n",
    "    pred=predictions_d3.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "\n",
    "    precision = precision_score(Y_test_d3, pred, average='micro')\n",
    "    recall = recall_score(Y_test_d3, pred, average='micro')\n",
    "    f1 = f1_score(Y_test_d3, pred, average='micro')\n",
    "   \n",
    "    print(\"Micro-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "    print(\"Accuracy = \",accuracy_score(Y_test_d3, pred))\n",
    "    print(\"Partial Accuracy = \",partial_accuracy(Y_test_d3, pred))\n",
    "    print(\"\\n\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D5 TESTING\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "inputs = x_d5.copy()\n",
    "inputs = np.array(inputs)\n",
    "inputs = np.stack(inputs)\n",
    "targets = np.array(y_d5)\n",
    "#targets = np.stack(targets)\n",
    "targets = np.array([np.array(xi) for xi in targets])\n",
    "\n",
    "\n",
    "n_classes = 51\n",
    "\n",
    "acc_per_fold = []\n",
    "f1_per_fold = []\n",
    "time_per_fold = []\n",
    "\n",
    "acc_per_fold_plut = []\n",
    "f1_per_fold_plut = []\n",
    "time_per_fold_plut = []\n",
    "\n",
    "acc_per_fold_mwpm = []\n",
    "f1_per_fold_mwpm = []\n",
    "time_per_fold_mwpm = []\n",
    "\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "aucs_classes = {}\n",
    "for i in mlb.classes_:\n",
    "    aucs_classes[i] = []\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "for i, (train, test) in enumerate(kfold.split(inputs, targets)):\n",
    "    i_train = inputs[train].copy()\n",
    "    i_test = inputs[test].copy()\n",
    "    x_test_d5 = i_test[:,:2]\n",
    "    inputs_train = i_train[:,3:]\n",
    "    inputs_test = i_test[:,3:]\n",
    "    \n",
    "    ####################################################################################################\n",
    "    #test MWPM decoder for this fold\n",
    "    #labels = targets[train], features = inputs[train]\n",
    "    #x_test_d5 = translate_to_graph(testData_d5_MWPM, targets[test], mlb)\n",
    "    decoding_d5, time_mwpm = do_new_decoding(x_test_d5, 5, 0)\n",
    "    decoding_d5['combine'] = decoding_d5[[0, 1]].values.tolist()\n",
    "    decoding_d5['combine'].apply(lambda x: x[0].extend(x[1]))\n",
    "    decoding_d5 = np.array(decoding_d5[0])\n",
    "                                              \n",
    "    time_per_fold_mwpm.append(time_mwpm)\n",
    "                                              \n",
    "    pred_mwpm = mlb.transform(decoding_d5)\n",
    "    if fold_no < 5:\n",
    "        acc = partial_accuracy(targets[test], pred_mwpm)\n",
    "    else:\n",
    "        acc, contingency_mwpm = partial_accuracy_and_contingency(targets[test], pred_mwpm, mlb)\n",
    "        \n",
    "    acc_per_fold_mwpm.append(acc)\n",
    "    f1_per_fold_mwpm.append(f1_score(targets[test], pred_mwpm, average='micro'))\n",
    "    \n",
    "    #####################################################################################################\n",
    "    #test the plut decoder for this fold\n",
    "    \n",
    "    lookup_d5 = lookup_decoder(5)\n",
    "    \n",
    "    lookup_d5 = train_plut(lookup_d5, inputs_train, targets[train])\n",
    "    \n",
    "    start = time.time_ns()\n",
    "    pred_plut_d5 = test_plut(lookup_d5, inputs_test)\n",
    "    end = time.time_ns() \n",
    "    time_per_fold_plut.append((end - start)/ (10 ** 9))\n",
    "    \n",
    "    if fold_no < 5:\n",
    "        acc = partial_accuracy(targets[test], pred_plut_d5)\n",
    "    else:\n",
    "        acc, contingency_plut = partial_accuracy_and_contingency(targets[test], pred_plut_d5, mlb)\n",
    "        \n",
    "    acc_per_fold_plut.append(acc)\n",
    "    f1_per_fold_plut.append(f1_score(targets[test], pred_plut_d5, average='micro'))\n",
    "\n",
    "    #####################################################################################################\n",
    "    #Test the NN decoder for this fold\n",
    "    \n",
    "    model_d5 = compile_FFNN_model_DepthFive(5)\n",
    "    # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    # Fit data to model\n",
    "    \n",
    "    inputs_train = np.asarray(inputs_train).astype(np.int) #CHANGE (added)\n",
    "    targets[train] = np.asarray(targets[train]).astype(np.int) #CHANGE (added)\n",
    "    \n",
    "    history = model_d5.fit(\n",
    "    inputs_train,\n",
    "    targets[train],\n",
    "    validation_split=.25,\n",
    "    epochs = 800\n",
    ")\n",
    "   # Generate generalization metrics\n",
    "\n",
    "    inputs_test = np.asarray(inputs_test).astype(np.int) #CHANGE (added)\n",
    "    targets[test] = np.asarray(targets[test]).astype(np.int) #CHANGE (added)\n",
    "    scores = model_d5.evaluate(inputs_test, targets[test], verbose=0)\n",
    "    \n",
    "    start = time.time_ns()\n",
    "    predictions_d5 = model_d5.predict(inputs_test)\n",
    "    end = time.time_ns() \n",
    "    time_per_fold.append((end - start)/ (10 ** 9))\n",
    "    \n",
    "    #threshold based on previous tests with train_test_split in hyperparameter tuning\n",
    "    pred=predictions_d5.copy() #change here\n",
    "    pred[pred>=.4]=1 \n",
    "    pred[pred<.4]=0\n",
    "    \n",
    "    if fold_no < 5:\n",
    "        acc = scores[1]\n",
    "    else:\n",
    "        acc, contingency_nn = partial_accuracy_and_contingency(targets[test], pred, mlb)\n",
    "\n",
    "    acc_per_fold.append(acc)\n",
    "    f1_per_fold.append(f1_score(targets[test], pred, average='micro'))\n",
    "\n",
    "    \n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(targets[test].ravel(), predictions_d5.ravel())\n",
    "    tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "    tprs[-1][0] = 0.0\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "    plt.plot(fpr, tpr, lw=1, alpha=0.3, label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
    "    \n",
    "    #get the AUCs of each class, used to get average AUC of each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(targets[test][:, i], predictions_d5[:, i]) \n",
    "        aucs_classes[mlb.classes_[i]].append(auc(fpr[i], tpr[i]))\n",
    "        \n",
    "        \n",
    "#########################################################################################################\n",
    "#compute McNemar's statistic on results of last fold\n",
    "\n",
    "mcnemar_results_mwpm = {}\n",
    "mcnemar_results_plut = {}\n",
    "\n",
    "for class_ in mlb.classes_:\n",
    "    \n",
    "    #compute the x^2 for NN and MWPM\n",
    "    mcnemar_results_mwpm[class_]=contingency_table_and_t(contingency_nn[class_], contingency_mwpm[class_])[1]    \n",
    "    #comput the x^2 for NN and PLUT\n",
    "    mcnemar_results_plut[class_] = contingency_table_and_t(contingency_nn[class_], contingency_plut[class_])[1]\n",
    "        \n",
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds of MWPM:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold_mwpm)} (+- {np.std(acc_per_fold_mwpm)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold_mwpm)}(+- {np.std(f1_per_fold_mwpm)})')\n",
    "print(f'> Time: {np.mean(time_per_fold_mwpm)} (+- {np.std(time_per_fold_mwpm)})')\n",
    "print(\"##############################################################################\")\n",
    "print('Average scores for all folds of PLUT:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold_plut)} (+- {np.std(acc_per_fold_plut)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold_plut)}(+- {np.std(f1_per_fold_plut)})')\n",
    "print(f'> Time: {np.mean(time_per_fold_plut)} (+- {np.std(time_per_fold_plut)})')\n",
    "print(\"##############################################################################\")\n",
    "print('Average scores for all folds of NN:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold)}(+- {np.std(f1_per_fold)})')\n",
    "print(f'> Time: {np.mean(time_per_fold)} (+- {np.std(time_per_fold)})')\n",
    "print(\"##############################################################################\")\n",
    "for key in aucs_classes:\n",
    "    cleanedList = [x for x in aucs_classes[key] if str(x) != 'nan']\n",
    "    print(f'> AUC for class {key}: {np.mean(cleanedList)} (+- {np.std(cleanedList)})') #this has to be a for loop\n",
    "    print(\"X^2 for MWPM and NN: \" + str(mcnemar_results_mwpm[key]))  \n",
    "    print(\"X^2 for PLUT and NN: \" + str(mcnemar_results_plut[key]))\n",
    "    \n",
    "print(\"###################################################################################\")\n",
    "print(\"TOTAL F1 NN: \" + str(f1_per_fold))\n",
    "print(\"TOTAL F1 PLUT: \" + str(f1_per_fold_plut))\n",
    "print(\"TOTAL F1 MWPM: \" + str(f1_per_fold_mwpm))\n",
    "print(\"TOTAL ACC NN: \" + str(acc_per_fold))\n",
    "print(\"TOTAL ACC PLUT: \" + str(acc_per_fold_plut))\n",
    "print(\"TOTAL ACC MWPM: \" + str(acc_per_fold_mwpm))\n",
    "print(\"TOTAL TIME NN: \" + str(time_per_fold))\n",
    "print(\"TOTAL TIME PLUT: \" + str(time_per_fold_plut))\n",
    "print(\"TOTAL TIME MWPM: \" + str(time_per_fold_mwpm))\n",
    "print('------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "         label='Chance', alpha=.8)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "         label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "         lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                 label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "plt.xlim([-0.01, 1.01])\n",
    "plt.ylim([-0.01, 1.01])\n",
    "plt.xlabel('False Positive Rate',fontsize=18)\n",
    "plt.ylabel('True Positive Rate',fontsize=18)\n",
    "plt.title('Cross-Validation ROC of Depth 5 NN',fontsize=18)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp/ipykernel_14152/3111263836.py:103: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  inputs_train = np.asarray(inputs_train).astype(np.int) #CHANGE (added)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp/ipykernel_14152/3111263836.py:104: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  targets[train] = np.asarray(targets[train]).astype(np.int) #CHANGE (added)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "7941/7941 [==============================] - 23s 3ms/step - loss: 0.1610 - accuracy: 0.0441 - val_loss: 0.1872 - val_accuracy: 0.1020\n",
      "Epoch 2/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.1419 - accuracy: 0.1395 - val_loss: 0.1498 - val_accuracy: 0.2447\n",
      "Epoch 3/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.1113 - accuracy: 0.1976 - val_loss: 0.1195 - val_accuracy: 0.3126\n",
      "Epoch 4/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0913 - accuracy: 0.2285 - val_loss: 0.1034 - val_accuracy: 0.2949\n",
      "Epoch 5/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0793 - accuracy: 0.2323 - val_loss: 0.0932 - val_accuracy: 0.3055\n",
      "Epoch 6/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0716 - accuracy: 0.2325 - val_loss: 0.0873 - val_accuracy: 0.3113\n",
      "Epoch 7/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0665 - accuracy: 0.2304 - val_loss: 0.0833 - val_accuracy: 0.3219\n",
      "Epoch 8/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0627 - accuracy: 0.2297 - val_loss: 0.0791 - val_accuracy: 0.3213\n",
      "Epoch 9/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0597 - accuracy: 0.2290 - val_loss: 0.0766 - val_accuracy: 0.3299\n",
      "Epoch 10/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0572 - accuracy: 0.2298 - val_loss: 0.0743 - val_accuracy: 0.3375\n",
      "Epoch 11/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0552 - accuracy: 0.2296 - val_loss: 0.0711 - val_accuracy: 0.3388\n",
      "Epoch 12/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0533 - accuracy: 0.2303 - val_loss: 0.0703 - val_accuracy: 0.3485\n",
      "Epoch 13/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0518 - accuracy: 0.2302 - val_loss: 0.0682 - val_accuracy: 0.3487\n",
      "Epoch 14/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0504 - accuracy: 0.2300 - val_loss: 0.0667 - val_accuracy: 0.3483\n",
      "Epoch 15/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0492 - accuracy: 0.2306 - val_loss: 0.0645 - val_accuracy: 0.3337\n",
      "Epoch 16/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0483 - accuracy: 0.2308 - val_loss: 0.0632 - val_accuracy: 0.3463\n",
      "Epoch 17/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0474 - accuracy: 0.2295 - val_loss: 0.0624 - val_accuracy: 0.3237\n",
      "Epoch 18/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0466 - accuracy: 0.2301 - val_loss: 0.0622 - val_accuracy: 0.3253\n",
      "Epoch 19/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0459 - accuracy: 0.2299 - val_loss: 0.0608 - val_accuracy: 0.3481\n",
      "Epoch 20/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0453 - accuracy: 0.2298 - val_loss: 0.0600 - val_accuracy: 0.3364\n",
      "Epoch 21/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0448 - accuracy: 0.2298 - val_loss: 0.0595 - val_accuracy: 0.3466\n",
      "Epoch 22/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0443 - accuracy: 0.2289 - val_loss: 0.0588 - val_accuracy: 0.3314\n",
      "Epoch 23/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0438 - accuracy: 0.2286 - val_loss: 0.0580 - val_accuracy: 0.3361\n",
      "Epoch 24/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0434 - accuracy: 0.2285 - val_loss: 0.0571 - val_accuracy: 0.3165\n",
      "Epoch 25/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0429 - accuracy: 0.2282 - val_loss: 0.0572 - val_accuracy: 0.3176\n",
      "Epoch 26/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0426 - accuracy: 0.2289 - val_loss: 0.0568 - val_accuracy: 0.3219\n",
      "Epoch 27/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0422 - accuracy: 0.2287 - val_loss: 0.0565 - val_accuracy: 0.3367\n",
      "Epoch 28/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0418 - accuracy: 0.2283 - val_loss: 0.0556 - val_accuracy: 0.3193\n",
      "Epoch 29/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0415 - accuracy: 0.2284 - val_loss: 0.0548 - val_accuracy: 0.3287\n",
      "Epoch 30/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0412 - accuracy: 0.2283 - val_loss: 0.0551 - val_accuracy: 0.3313\n",
      "Epoch 31/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0409 - accuracy: 0.2282 - val_loss: 0.0542 - val_accuracy: 0.3275\n",
      "Epoch 32/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0406 - accuracy: 0.2289 - val_loss: 0.0543 - val_accuracy: 0.3265\n",
      "Epoch 33/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0403 - accuracy: 0.2291 - val_loss: 0.0540 - val_accuracy: 0.3242\n",
      "Epoch 34/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0400 - accuracy: 0.2282 - val_loss: 0.0541 - val_accuracy: 0.3334\n",
      "Epoch 35/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0398 - accuracy: 0.2295 - val_loss: 0.0529 - val_accuracy: 0.3044\n",
      "Epoch 36/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0395 - accuracy: 0.2279 - val_loss: 0.0526 - val_accuracy: 0.3345\n",
      "Epoch 37/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0393 - accuracy: 0.2277 - val_loss: 0.0520 - val_accuracy: 0.3224\n",
      "Epoch 38/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0390 - accuracy: 0.2288 - val_loss: 0.0523 - val_accuracy: 0.3313\n",
      "Epoch 39/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0388 - accuracy: 0.2271 - val_loss: 0.0514 - val_accuracy: 0.3240\n",
      "Epoch 40/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0386 - accuracy: 0.2273 - val_loss: 0.0523 - val_accuracy: 0.3324\n",
      "Epoch 41/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0384 - accuracy: 0.2276 - val_loss: 0.0511 - val_accuracy: 0.3168\n",
      "Epoch 42/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0382 - accuracy: 0.2269 - val_loss: 0.0512 - val_accuracy: 0.3323\n",
      "Epoch 43/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0380 - accuracy: 0.2274 - val_loss: 0.0507 - val_accuracy: 0.3313\n",
      "Epoch 44/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0378 - accuracy: 0.2272 - val_loss: 0.0507 - val_accuracy: 0.3311\n",
      "Epoch 45/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0377 - accuracy: 0.2270 - val_loss: 0.0507 - val_accuracy: 0.3270\n",
      "Epoch 46/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0375 - accuracy: 0.2268 - val_loss: 0.0499 - val_accuracy: 0.3159\n",
      "Epoch 47/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0373 - accuracy: 0.2268 - val_loss: 0.0499 - val_accuracy: 0.3234\n",
      "Epoch 48/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0371 - accuracy: 0.2268 - val_loss: 0.0499 - val_accuracy: 0.3291\n",
      "Epoch 49/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0370 - accuracy: 0.2261 - val_loss: 0.0497 - val_accuracy: 0.3320\n",
      "Epoch 50/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0368 - accuracy: 0.2260 - val_loss: 0.0491 - val_accuracy: 0.3191\n",
      "Epoch 51/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0367 - accuracy: 0.2264 - val_loss: 0.0490 - val_accuracy: 0.3221\n",
      "Epoch 52/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0365 - accuracy: 0.2258 - val_loss: 0.0492 - val_accuracy: 0.3030\n",
      "Epoch 53/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0364 - accuracy: 0.2247 - val_loss: 0.0489 - val_accuracy: 0.3030\n",
      "Epoch 54/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0362 - accuracy: 0.2251 - val_loss: 0.0485 - val_accuracy: 0.3160\n",
      "Epoch 55/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0361 - accuracy: 0.2249 - val_loss: 0.0483 - val_accuracy: 0.3208\n",
      "Epoch 56/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0360 - accuracy: 0.2258 - val_loss: 0.0485 - val_accuracy: 0.2965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0358 - accuracy: 0.2244 - val_loss: 0.0478 - val_accuracy: 0.3200\n",
      "Epoch 58/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0357 - accuracy: 0.2245 - val_loss: 0.0485 - val_accuracy: 0.3350\n",
      "Epoch 59/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0356 - accuracy: 0.2245 - val_loss: 0.0479 - val_accuracy: 0.3099\n",
      "Epoch 60/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0355 - accuracy: 0.2238 - val_loss: 0.0477 - val_accuracy: 0.2995\n",
      "Epoch 61/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0354 - accuracy: 0.2239 - val_loss: 0.0492 - val_accuracy: 0.3196\n",
      "Epoch 62/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0352 - accuracy: 0.2239 - val_loss: 0.0477 - val_accuracy: 0.3049\n",
      "Epoch 63/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0351 - accuracy: 0.2233 - val_loss: 0.0480 - val_accuracy: 0.3182\n",
      "Epoch 64/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0350 - accuracy: 0.2236 - val_loss: 0.0474 - val_accuracy: 0.3149\n",
      "Epoch 65/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0349 - accuracy: 0.2227 - val_loss: 0.0480 - val_accuracy: 0.3126\n",
      "Epoch 66/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0348 - accuracy: 0.2228 - val_loss: 0.0473 - val_accuracy: 0.3262\n",
      "Epoch 67/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0347 - accuracy: 0.2232 - val_loss: 0.0469 - val_accuracy: 0.3094\n",
      "Epoch 68/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0346 - accuracy: 0.2226 - val_loss: 0.0470 - val_accuracy: 0.3162\n",
      "Epoch 69/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0345 - accuracy: 0.2232 - val_loss: 0.0474 - val_accuracy: 0.3054\n",
      "Epoch 70/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0344 - accuracy: 0.2227 - val_loss: 0.0464 - val_accuracy: 0.3279\n",
      "Epoch 71/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0343 - accuracy: 0.2236 - val_loss: 0.0468 - val_accuracy: 0.3036\n",
      "Epoch 72/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0342 - accuracy: 0.2224 - val_loss: 0.0468 - val_accuracy: 0.3042\n",
      "Epoch 73/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0341 - accuracy: 0.2225 - val_loss: 0.0465 - val_accuracy: 0.3184\n",
      "Epoch 74/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0340 - accuracy: 0.2229 - val_loss: 0.0467 - val_accuracy: 0.3072\n",
      "Epoch 75/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0339 - accuracy: 0.2220 - val_loss: 0.0465 - val_accuracy: 0.3099\n",
      "Epoch 76/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0338 - accuracy: 0.2217 - val_loss: 0.0461 - val_accuracy: 0.3009\n",
      "Epoch 77/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0338 - accuracy: 0.2221 - val_loss: 0.0458 - val_accuracy: 0.2920\n",
      "Epoch 78/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0337 - accuracy: 0.2227 - val_loss: 0.0465 - val_accuracy: 0.3177\n",
      "Epoch 79/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0336 - accuracy: 0.2231 - val_loss: 0.0461 - val_accuracy: 0.3115\n",
      "Epoch 80/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0335 - accuracy: 0.2224 - val_loss: 0.0462 - val_accuracy: 0.3152\n",
      "Epoch 81/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0334 - accuracy: 0.2233 - val_loss: 0.0462 - val_accuracy: 0.3177\n",
      "Epoch 82/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0333 - accuracy: 0.2232 - val_loss: 0.0464 - val_accuracy: 0.3218\n",
      "Epoch 83/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0333 - accuracy: 0.2230 - val_loss: 0.0453 - val_accuracy: 0.3181\n",
      "Epoch 84/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0332 - accuracy: 0.2232 - val_loss: 0.0462 - val_accuracy: 0.3153\n",
      "Epoch 85/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0331 - accuracy: 0.2239 - val_loss: 0.0455 - val_accuracy: 0.2980\n",
      "Epoch 86/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0330 - accuracy: 0.2226 - val_loss: 0.0456 - val_accuracy: 0.2783\n",
      "Epoch 87/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0330 - accuracy: 0.2232 - val_loss: 0.0456 - val_accuracy: 0.3111\n",
      "Epoch 88/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0329 - accuracy: 0.2230 - val_loss: 0.0454 - val_accuracy: 0.3084\n",
      "Epoch 89/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0328 - accuracy: 0.2233 - val_loss: 0.0452 - val_accuracy: 0.3124\n",
      "Epoch 90/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0327 - accuracy: 0.2220 - val_loss: 0.0451 - val_accuracy: 0.3012\n",
      "Epoch 91/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0327 - accuracy: 0.2229 - val_loss: 0.0450 - val_accuracy: 0.3050\n",
      "Epoch 92/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0326 - accuracy: 0.2236 - val_loss: 0.0451 - val_accuracy: 0.3315\n",
      "Epoch 93/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0325 - accuracy: 0.2238 - val_loss: 0.0446 - val_accuracy: 0.2998\n",
      "Epoch 94/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0325 - accuracy: 0.2232 - val_loss: 0.0451 - val_accuracy: 0.3175\n",
      "Epoch 95/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0324 - accuracy: 0.2232 - val_loss: 0.0456 - val_accuracy: 0.2995\n",
      "Epoch 96/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0323 - accuracy: 0.2235 - val_loss: 0.0449 - val_accuracy: 0.3108\n",
      "Epoch 97/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0323 - accuracy: 0.2234 - val_loss: 0.0448 - val_accuracy: 0.3029\n",
      "Epoch 98/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0322 - accuracy: 0.2242 - val_loss: 0.0444 - val_accuracy: 0.3116\n",
      "Epoch 99/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0321 - accuracy: 0.2234 - val_loss: 0.0448 - val_accuracy: 0.3052\n",
      "Epoch 100/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0321 - accuracy: 0.2237 - val_loss: 0.0443 - val_accuracy: 0.2952\n",
      "Epoch 101/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0320 - accuracy: 0.2239 - val_loss: 0.0444 - val_accuracy: 0.3132\n",
      "Epoch 102/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0320 - accuracy: 0.2232 - val_loss: 0.0445 - val_accuracy: 0.3161\n",
      "Epoch 103/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0319 - accuracy: 0.2242 - val_loss: 0.0444 - val_accuracy: 0.3050\n",
      "Epoch 104/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0318 - accuracy: 0.2241 - val_loss: 0.0443 - val_accuracy: 0.3109\n",
      "Epoch 105/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0318 - accuracy: 0.2234 - val_loss: 0.0440 - val_accuracy: 0.3094\n",
      "Epoch 106/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0317 - accuracy: 0.2229 - val_loss: 0.0436 - val_accuracy: 0.2995\n",
      "Epoch 107/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0317 - accuracy: 0.2236 - val_loss: 0.0444 - val_accuracy: 0.3094\n",
      "Epoch 108/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0316 - accuracy: 0.2240 - val_loss: 0.0439 - val_accuracy: 0.3004\n",
      "Epoch 109/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0316 - accuracy: 0.2236 - val_loss: 0.0443 - val_accuracy: 0.3211\n",
      "Epoch 110/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0315 - accuracy: 0.2237 - val_loss: 0.0440 - val_accuracy: 0.3140\n",
      "Epoch 111/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0315 - accuracy: 0.2244 - val_loss: 0.0439 - val_accuracy: 0.2862\n",
      "Epoch 112/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0314 - accuracy: 0.2247 - val_loss: 0.0438 - val_accuracy: 0.3040\n",
      "Epoch 113/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0314 - accuracy: 0.2235 - val_loss: 0.0443 - val_accuracy: 0.3048\n",
      "Epoch 114/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0313 - accuracy: 0.2240 - val_loss: 0.0438 - val_accuracy: 0.3219\n",
      "Epoch 115/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0313 - accuracy: 0.2245 - val_loss: 0.0434 - val_accuracy: 0.3189\n",
      "Epoch 116/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0312 - accuracy: 0.2250 - val_loss: 0.0436 - val_accuracy: 0.3002\n",
      "Epoch 117/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0312 - accuracy: 0.2246 - val_loss: 0.0440 - val_accuracy: 0.3020\n",
      "Epoch 118/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0311 - accuracy: 0.2233 - val_loss: 0.0440 - val_accuracy: 0.3123\n",
      "Epoch 119/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0311 - accuracy: 0.2248 - val_loss: 0.0435 - val_accuracy: 0.3016\n",
      "Epoch 120/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0310 - accuracy: 0.2240 - val_loss: 0.0434 - val_accuracy: 0.3079\n",
      "Epoch 121/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0310 - accuracy: 0.2237 - val_loss: 0.0435 - val_accuracy: 0.3045\n",
      "Epoch 122/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0309 - accuracy: 0.2242 - val_loss: 0.0437 - val_accuracy: 0.3042\n",
      "Epoch 123/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0309 - accuracy: 0.2250 - val_loss: 0.0427 - val_accuracy: 0.3053\n",
      "Epoch 124/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0308 - accuracy: 0.2247 - val_loss: 0.0427 - val_accuracy: 0.3240\n",
      "Epoch 125/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0308 - accuracy: 0.2246 - val_loss: 0.0434 - val_accuracy: 0.2976\n",
      "Epoch 126/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0307 - accuracy: 0.2246 - val_loss: 0.0431 - val_accuracy: 0.2761\n",
      "Epoch 127/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0307 - accuracy: 0.2246 - val_loss: 0.0437 - val_accuracy: 0.3114\n",
      "Epoch 128/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0306 - accuracy: 0.2251 - val_loss: 0.0434 - val_accuracy: 0.3065\n",
      "Epoch 129/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0306 - accuracy: 0.2250 - val_loss: 0.0429 - val_accuracy: 0.3016\n",
      "Epoch 130/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0306 - accuracy: 0.2245 - val_loss: 0.0426 - val_accuracy: 0.3079\n",
      "Epoch 131/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0305 - accuracy: 0.2259 - val_loss: 0.0430 - val_accuracy: 0.3131\n",
      "Epoch 132/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0305 - accuracy: 0.2248 - val_loss: 0.0428 - val_accuracy: 0.3098\n",
      "Epoch 133/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0304 - accuracy: 0.2255 - val_loss: 0.0433 - val_accuracy: 0.2990\n",
      "Epoch 134/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0304 - accuracy: 0.2252 - val_loss: 0.0429 - val_accuracy: 0.3053\n",
      "Epoch 135/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0303 - accuracy: 0.2265 - val_loss: 0.0428 - val_accuracy: 0.3298\n",
      "Epoch 136/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0303 - accuracy: 0.2246 - val_loss: 0.0433 - val_accuracy: 0.3011\n",
      "Epoch 137/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0303 - accuracy: 0.2254 - val_loss: 0.0428 - val_accuracy: 0.3293\n",
      "Epoch 138/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0302 - accuracy: 0.2259 - val_loss: 0.0428 - val_accuracy: 0.3187\n",
      "Epoch 139/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0302 - accuracy: 0.2256 - val_loss: 0.0428 - val_accuracy: 0.3019\n",
      "Epoch 140/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0301 - accuracy: 0.2252 - val_loss: 0.0426 - val_accuracy: 0.3239\n",
      "Epoch 141/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0301 - accuracy: 0.2259 - val_loss: 0.0426 - val_accuracy: 0.3175\n",
      "Epoch 142/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0301 - accuracy: 0.2263 - val_loss: 0.0425 - val_accuracy: 0.2837\n",
      "Epoch 143/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0300 - accuracy: 0.2255 - val_loss: 0.0419 - val_accuracy: 0.2908\n",
      "Epoch 144/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0300 - accuracy: 0.2253 - val_loss: 0.0423 - val_accuracy: 0.3132\n",
      "Epoch 145/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0300 - accuracy: 0.2253 - val_loss: 0.0424 - val_accuracy: 0.3173\n",
      "Epoch 146/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0299 - accuracy: 0.2251 - val_loss: 0.0422 - val_accuracy: 0.3253\n",
      "Epoch 147/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0299 - accuracy: 0.2268 - val_loss: 0.0422 - val_accuracy: 0.2895\n",
      "Epoch 148/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0299 - accuracy: 0.2251 - val_loss: 0.0423 - val_accuracy: 0.3156\n",
      "Epoch 149/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0298 - accuracy: 0.2266 - val_loss: 0.0430 - val_accuracy: 0.3139\n",
      "Epoch 150/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0298 - accuracy: 0.2267 - val_loss: 0.0422 - val_accuracy: 0.2977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp/ipykernel_14152/3111263836.py:114: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  inputs_test = np.asarray(inputs_test).astype(np.int) #CHANGE (added)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp/ipykernel_14152/3111263836.py:115: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  targets[test] = np.asarray(targets[test]).astype(np.int) #CHANGE (added)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp/ipykernel_14152/3111263836.py:147: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "  tprs.append(interp(mean_fpr, fpr, tpr))\n",
      "C:\\Users\\User\\anaconda3\\envs\\research\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:806: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp/ipykernel_14152/3111263836.py:103: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  inputs_train = np.asarray(inputs_train).astype(np.int) #CHANGE (added)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp/ipykernel_14152/3111263836.py:104: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  targets[train] = np.asarray(targets[train]).astype(np.int) #CHANGE (added)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.1626 - accuracy: 0.0495 - val_loss: 0.1885 - val_accuracy: 0.1377\n",
      "Epoch 2/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.1434 - accuracy: 0.1546 - val_loss: 0.1522 - val_accuracy: 0.2754\n",
      "Epoch 3/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.1128 - accuracy: 0.2149 - val_loss: 0.1232 - val_accuracy: 0.3030\n",
      "Epoch 4/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0953 - accuracy: 0.2209 - val_loss: 0.1083 - val_accuracy: 0.3213\n",
      "Epoch 5/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0839 - accuracy: 0.2299 - val_loss: 0.0984 - val_accuracy: 0.3370\n",
      "Epoch 6/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0762 - accuracy: 0.2315 - val_loss: 0.0910 - val_accuracy: 0.3223\n",
      "Epoch 7/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0703 - accuracy: 0.2290 - val_loss: 0.0861 - val_accuracy: 0.3179\n",
      "Epoch 8/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0655 - accuracy: 0.2273 - val_loss: 0.0822 - val_accuracy: 0.3266\n",
      "Epoch 9/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0617 - accuracy: 0.2272 - val_loss: 0.0785 - val_accuracy: 0.3225\n",
      "Epoch 10/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0587 - accuracy: 0.2260 - val_loss: 0.0747 - val_accuracy: 0.3136\n",
      "Epoch 11/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0563 - accuracy: 0.2266 - val_loss: 0.0726 - val_accuracy: 0.3250\n",
      "Epoch 12/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0544 - accuracy: 0.2259 - val_loss: 0.0705 - val_accuracy: 0.3275\n",
      "Epoch 13/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0528 - accuracy: 0.2261 - val_loss: 0.0686 - val_accuracy: 0.3343\n",
      "Epoch 14/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0514 - accuracy: 0.2267 - val_loss: 0.0669 - val_accuracy: 0.3077\n",
      "Epoch 15/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0501 - accuracy: 0.2268 - val_loss: 0.0656 - val_accuracy: 0.3432\n",
      "Epoch 16/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0491 - accuracy: 0.2269 - val_loss: 0.0643 - val_accuracy: 0.3345\n",
      "Epoch 17/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0481 - accuracy: 0.2270 - val_loss: 0.0633 - val_accuracy: 0.3287\n",
      "Epoch 18/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0473 - accuracy: 0.2278 - val_loss: 0.0623 - val_accuracy: 0.3274\n",
      "Epoch 19/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0466 - accuracy: 0.2272 - val_loss: 0.0611 - val_accuracy: 0.3154\n",
      "Epoch 20/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0460 - accuracy: 0.2277 - val_loss: 0.0605 - val_accuracy: 0.3164\n",
      "Epoch 21/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0454 - accuracy: 0.2270 - val_loss: 0.0600 - val_accuracy: 0.3158\n",
      "Epoch 22/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0448 - accuracy: 0.2276 - val_loss: 0.0586 - val_accuracy: 0.3187\n",
      "Epoch 23/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0443 - accuracy: 0.2265 - val_loss: 0.0581 - val_accuracy: 0.3040\n",
      "Epoch 24/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0439 - accuracy: 0.2278 - val_loss: 0.0583 - val_accuracy: 0.3292\n",
      "Epoch 25/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0434 - accuracy: 0.2275 - val_loss: 0.0575 - val_accuracy: 0.3179\n",
      "Epoch 26/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0430 - accuracy: 0.2267 - val_loss: 0.0568 - val_accuracy: 0.3133\n",
      "Epoch 27/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0426 - accuracy: 0.2270 - val_loss: 0.0560 - val_accuracy: 0.3136\n",
      "Epoch 28/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0422 - accuracy: 0.2267 - val_loss: 0.0560 - val_accuracy: 0.3196\n",
      "Epoch 29/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0418 - accuracy: 0.2269 - val_loss: 0.0556 - val_accuracy: 0.3360\n",
      "Epoch 30/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0415 - accuracy: 0.2271 - val_loss: 0.0547 - val_accuracy: 0.3035\n",
      "Epoch 31/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0412 - accuracy: 0.2257 - val_loss: 0.0540 - val_accuracy: 0.3295\n",
      "Epoch 32/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0408 - accuracy: 0.2265 - val_loss: 0.0539 - val_accuracy: 0.3111\n",
      "Epoch 33/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0405 - accuracy: 0.2263 - val_loss: 0.0550 - val_accuracy: 0.3201\n",
      "Epoch 34/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0403 - accuracy: 0.2254 - val_loss: 0.0537 - val_accuracy: 0.3245\n",
      "Epoch 35/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0400 - accuracy: 0.2248 - val_loss: 0.0529 - val_accuracy: 0.3031\n",
      "Epoch 36/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0397 - accuracy: 0.2248 - val_loss: 0.0532 - val_accuracy: 0.3319\n",
      "Epoch 37/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0394 - accuracy: 0.2249 - val_loss: 0.0530 - val_accuracy: 0.3183\n",
      "Epoch 38/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0392 - accuracy: 0.2243 - val_loss: 0.0521 - val_accuracy: 0.3240\n",
      "Epoch 39/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0390 - accuracy: 0.2238 - val_loss: 0.0519 - val_accuracy: 0.3183\n",
      "Epoch 40/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0387 - accuracy: 0.2238 - val_loss: 0.0519 - val_accuracy: 0.3193\n",
      "Epoch 41/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0385 - accuracy: 0.2234 - val_loss: 0.0513 - val_accuracy: 0.3137\n",
      "Epoch 42/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0383 - accuracy: 0.2239 - val_loss: 0.0512 - val_accuracy: 0.3151\n",
      "Epoch 43/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0381 - accuracy: 0.2230 - val_loss: 0.0512 - val_accuracy: 0.3201\n",
      "Epoch 44/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0379 - accuracy: 0.2231 - val_loss: 0.0510 - val_accuracy: 0.3084\n",
      "Epoch 45/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0377 - accuracy: 0.2235 - val_loss: 0.0509 - val_accuracy: 0.3142\n",
      "Epoch 46/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0375 - accuracy: 0.2226 - val_loss: 0.0509 - val_accuracy: 0.3187\n",
      "Epoch 47/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0374 - accuracy: 0.2225 - val_loss: 0.0505 - val_accuracy: 0.2948\n",
      "Epoch 48/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0372 - accuracy: 0.2227 - val_loss: 0.0498 - val_accuracy: 0.3036\n",
      "Epoch 49/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0370 - accuracy: 0.2224 - val_loss: 0.0500 - val_accuracy: 0.3077\n",
      "Epoch 50/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0369 - accuracy: 0.2218 - val_loss: 0.0497 - val_accuracy: 0.3188\n",
      "Epoch 51/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0367 - accuracy: 0.2216 - val_loss: 0.0493 - val_accuracy: 0.2965\n",
      "Epoch 52/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0366 - accuracy: 0.2208 - val_loss: 0.0495 - val_accuracy: 0.2886\n",
      "Epoch 53/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0364 - accuracy: 0.2211 - val_loss: 0.0497 - val_accuracy: 0.3038\n",
      "Epoch 54/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0363 - accuracy: 0.2211 - val_loss: 0.0487 - val_accuracy: 0.3186\n",
      "Epoch 55/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0361 - accuracy: 0.2211 - val_loss: 0.0489 - val_accuracy: 0.3119\n",
      "Epoch 56/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0360 - accuracy: 0.2214 - val_loss: 0.0480 - val_accuracy: 0.2771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0359 - accuracy: 0.2204 - val_loss: 0.0494 - val_accuracy: 0.3029\n",
      "Epoch 58/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0357 - accuracy: 0.2208 - val_loss: 0.0488 - val_accuracy: 0.3141\n",
      "Epoch 59/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0356 - accuracy: 0.2206 - val_loss: 0.0488 - val_accuracy: 0.3019\n",
      "Epoch 60/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0355 - accuracy: 0.2194 - val_loss: 0.0486 - val_accuracy: 0.3033\n",
      "Epoch 61/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0354 - accuracy: 0.2196 - val_loss: 0.0485 - val_accuracy: 0.3061\n",
      "Epoch 62/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0352 - accuracy: 0.2202 - val_loss: 0.0484 - val_accuracy: 0.3354\n",
      "Epoch 63/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0351 - accuracy: 0.2197 - val_loss: 0.0485 - val_accuracy: 0.3201\n",
      "Epoch 64/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0350 - accuracy: 0.2200 - val_loss: 0.0477 - val_accuracy: 0.2963\n",
      "Epoch 65/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0349 - accuracy: 0.2188 - val_loss: 0.0481 - val_accuracy: 0.3007\n",
      "Epoch 66/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0348 - accuracy: 0.2192 - val_loss: 0.0480 - val_accuracy: 0.3200\n",
      "Epoch 67/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0347 - accuracy: 0.2183 - val_loss: 0.0481 - val_accuracy: 0.3148\n",
      "Epoch 68/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0346 - accuracy: 0.2193 - val_loss: 0.0483 - val_accuracy: 0.3129\n",
      "Epoch 69/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0345 - accuracy: 0.2193 - val_loss: 0.0481 - val_accuracy: 0.3317\n",
      "Epoch 70/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0344 - accuracy: 0.2187 - val_loss: 0.0478 - val_accuracy: 0.3133\n",
      "Epoch 71/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0343 - accuracy: 0.2187 - val_loss: 0.0474 - val_accuracy: 0.3086\n",
      "Epoch 72/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0342 - accuracy: 0.2184 - val_loss: 0.0472 - val_accuracy: 0.2921\n",
      "Epoch 73/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0341 - accuracy: 0.2175 - val_loss: 0.0472 - val_accuracy: 0.3158\n",
      "Epoch 74/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0340 - accuracy: 0.2177 - val_loss: 0.0472 - val_accuracy: 0.3072\n",
      "Epoch 75/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0339 - accuracy: 0.2175 - val_loss: 0.0470 - val_accuracy: 0.2959\n",
      "Epoch 76/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0339 - accuracy: 0.2171 - val_loss: 0.0469 - val_accuracy: 0.3068\n",
      "Epoch 77/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0338 - accuracy: 0.2177 - val_loss: 0.0470 - val_accuracy: 0.2923\n",
      "Epoch 78/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0337 - accuracy: 0.2165 - val_loss: 0.0466 - val_accuracy: 0.3011\n",
      "Epoch 79/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0336 - accuracy: 0.2170 - val_loss: 0.0473 - val_accuracy: 0.3029\n",
      "Epoch 80/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0335 - accuracy: 0.2176 - val_loss: 0.0465 - val_accuracy: 0.3151\n",
      "Epoch 81/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0334 - accuracy: 0.2164 - val_loss: 0.0460 - val_accuracy: 0.2859\n",
      "Epoch 82/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0334 - accuracy: 0.2174 - val_loss: 0.0462 - val_accuracy: 0.2911\n",
      "Epoch 83/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0333 - accuracy: 0.2163 - val_loss: 0.0465 - val_accuracy: 0.3158\n",
      "Epoch 84/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0332 - accuracy: 0.2178 - val_loss: 0.0462 - val_accuracy: 0.3069\n",
      "Epoch 85/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0331 - accuracy: 0.2169 - val_loss: 0.0463 - val_accuracy: 0.3039\n",
      "Epoch 86/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0331 - accuracy: 0.2165 - val_loss: 0.0461 - val_accuracy: 0.3164\n",
      "Epoch 87/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0330 - accuracy: 0.2171 - val_loss: 0.0458 - val_accuracy: 0.2888\n",
      "Epoch 88/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0329 - accuracy: 0.2164 - val_loss: 0.0462 - val_accuracy: 0.3025\n",
      "Epoch 89/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0328 - accuracy: 0.2165 - val_loss: 0.0463 - val_accuracy: 0.3004\n",
      "Epoch 90/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0328 - accuracy: 0.2165 - val_loss: 0.0460 - val_accuracy: 0.3022\n",
      "Epoch 91/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0327 - accuracy: 0.2156 - val_loss: 0.0455 - val_accuracy: 0.3162\n",
      "Epoch 92/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0326 - accuracy: 0.2163 - val_loss: 0.0461 - val_accuracy: 0.3140\n",
      "Epoch 93/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0326 - accuracy: 0.2157 - val_loss: 0.0450 - val_accuracy: 0.3100\n",
      "Epoch 94/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0325 - accuracy: 0.2152 - val_loss: 0.0458 - val_accuracy: 0.3000\n",
      "Epoch 95/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0324 - accuracy: 0.2162 - val_loss: 0.0457 - val_accuracy: 0.3102\n",
      "Epoch 96/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0324 - accuracy: 0.2162 - val_loss: 0.0461 - val_accuracy: 0.2995\n",
      "Epoch 97/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0323 - accuracy: 0.2156 - val_loss: 0.0458 - val_accuracy: 0.3111\n",
      "Epoch 98/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0322 - accuracy: 0.2152 - val_loss: 0.0458 - val_accuracy: 0.3219\n",
      "Epoch 99/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0322 - accuracy: 0.2150 - val_loss: 0.0459 - val_accuracy: 0.3159\n",
      "Epoch 100/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0321 - accuracy: 0.2160 - val_loss: 0.0454 - val_accuracy: 0.3019\n",
      "Epoch 101/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0321 - accuracy: 0.2154 - val_loss: 0.0454 - val_accuracy: 0.3268\n",
      "Epoch 102/150\n",
      "7941/7941 [==============================] - 23s 3ms/step - loss: 0.0320 - accuracy: 0.2155 - val_loss: 0.0455 - val_accuracy: 0.2912\n",
      "Epoch 103/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0320 - accuracy: 0.2154 - val_loss: 0.0454 - val_accuracy: 0.3031\n",
      "Epoch 104/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0319 - accuracy: 0.2152 - val_loss: 0.0449 - val_accuracy: 0.2965\n",
      "Epoch 105/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0318 - accuracy: 0.2150 - val_loss: 0.0451 - val_accuracy: 0.2978\n",
      "Epoch 106/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0318 - accuracy: 0.2150 - val_loss: 0.0449 - val_accuracy: 0.3135\n",
      "Epoch 107/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0317 - accuracy: 0.2157 - val_loss: 0.0450 - val_accuracy: 0.3195\n",
      "Epoch 108/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0317 - accuracy: 0.2147 - val_loss: 0.0445 - val_accuracy: 0.3029\n",
      "Epoch 109/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0316 - accuracy: 0.2160 - val_loss: 0.0443 - val_accuracy: 0.2957\n",
      "Epoch 110/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0316 - accuracy: 0.2157 - val_loss: 0.0446 - val_accuracy: 0.3049\n",
      "Epoch 111/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0315 - accuracy: 0.2154 - val_loss: 0.0446 - val_accuracy: 0.3052\n",
      "Epoch 112/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0315 - accuracy: 0.2148 - val_loss: 0.0445 - val_accuracy: 0.3112\n",
      "Epoch 113/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0314 - accuracy: 0.2153 - val_loss: 0.0444 - val_accuracy: 0.3102\n",
      "Epoch 114/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0313 - accuracy: 0.2150 - val_loss: 0.0448 - val_accuracy: 0.2995\n",
      "Epoch 115/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0313 - accuracy: 0.2157 - val_loss: 0.0447 - val_accuracy: 0.3024\n",
      "Epoch 116/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0312 - accuracy: 0.2149 - val_loss: 0.0445 - val_accuracy: 0.3000\n",
      "Epoch 117/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0312 - accuracy: 0.2152 - val_loss: 0.0443 - val_accuracy: 0.3016\n",
      "Epoch 118/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0311 - accuracy: 0.2149 - val_loss: 0.0444 - val_accuracy: 0.3165\n",
      "Epoch 119/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0311 - accuracy: 0.2161 - val_loss: 0.0444 - val_accuracy: 0.3007\n",
      "Epoch 120/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0311 - accuracy: 0.2143 - val_loss: 0.0435 - val_accuracy: 0.2898\n",
      "Epoch 121/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0310 - accuracy: 0.2149 - val_loss: 0.0439 - val_accuracy: 0.3114\n",
      "Epoch 122/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0310 - accuracy: 0.2159 - val_loss: 0.0451 - val_accuracy: 0.3090\n",
      "Epoch 123/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0309 - accuracy: 0.2150 - val_loss: 0.0440 - val_accuracy: 0.3127\n",
      "Epoch 124/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0309 - accuracy: 0.2160 - val_loss: 0.0438 - val_accuracy: 0.3137\n",
      "Epoch 125/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0308 - accuracy: 0.2149 - val_loss: 0.0439 - val_accuracy: 0.2988\n",
      "Epoch 126/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0308 - accuracy: 0.2171 - val_loss: 0.0438 - val_accuracy: 0.2986\n",
      "Epoch 127/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0307 - accuracy: 0.2154 - val_loss: 0.0445 - val_accuracy: 0.2972\n",
      "Epoch 128/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0307 - accuracy: 0.2156 - val_loss: 0.0437 - val_accuracy: 0.3052\n",
      "Epoch 129/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0306 - accuracy: 0.2149 - val_loss: 0.0441 - val_accuracy: 0.3167\n",
      "Epoch 130/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0306 - accuracy: 0.2166 - val_loss: 0.0440 - val_accuracy: 0.3055\n",
      "Epoch 131/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0306 - accuracy: 0.2149 - val_loss: 0.0440 - val_accuracy: 0.3166\n",
      "Epoch 132/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0305 - accuracy: 0.2158 - val_loss: 0.0432 - val_accuracy: 0.3060\n",
      "Epoch 133/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0305 - accuracy: 0.2161 - val_loss: 0.0439 - val_accuracy: 0.3216\n",
      "Epoch 134/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0304 - accuracy: 0.2161 - val_loss: 0.0434 - val_accuracy: 0.3167\n",
      "Epoch 135/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0304 - accuracy: 0.2167 - val_loss: 0.0433 - val_accuracy: 0.2937\n",
      "Epoch 136/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0304 - accuracy: 0.2156 - val_loss: 0.0437 - val_accuracy: 0.3171\n",
      "Epoch 137/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0303 - accuracy: 0.2161 - val_loss: 0.0433 - val_accuracy: 0.3162\n",
      "Epoch 138/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0303 - accuracy: 0.2165 - val_loss: 0.0431 - val_accuracy: 0.3034\n",
      "Epoch 139/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0302 - accuracy: 0.2159 - val_loss: 0.0437 - val_accuracy: 0.3214\n",
      "Epoch 140/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0302 - accuracy: 0.2166 - val_loss: 0.0439 - val_accuracy: 0.3241\n",
      "Epoch 141/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0301 - accuracy: 0.2150 - val_loss: 0.0437 - val_accuracy: 0.3023\n",
      "Epoch 142/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0301 - accuracy: 0.2165 - val_loss: 0.0437 - val_accuracy: 0.3036\n",
      "Epoch 143/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0301 - accuracy: 0.2169 - val_loss: 0.0435 - val_accuracy: 0.3030\n",
      "Epoch 144/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0300 - accuracy: 0.2166 - val_loss: 0.0435 - val_accuracy: 0.3340\n",
      "Epoch 145/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0300 - accuracy: 0.2166 - val_loss: 0.0429 - val_accuracy: 0.3128\n",
      "Epoch 146/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0300 - accuracy: 0.2173 - val_loss: 0.0430 - val_accuracy: 0.3217\n",
      "Epoch 147/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0299 - accuracy: 0.2172 - val_loss: 0.0427 - val_accuracy: 0.3185\n",
      "Epoch 148/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0299 - accuracy: 0.2174 - val_loss: 0.0432 - val_accuracy: 0.3144\n",
      "Epoch 149/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0298 - accuracy: 0.2175 - val_loss: 0.0428 - val_accuracy: 0.3161\n",
      "Epoch 150/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0298 - accuracy: 0.2177 - val_loss: 0.0431 - val_accuracy: 0.3084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp/ipykernel_14152/3111263836.py:114: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  inputs_test = np.asarray(inputs_test).astype(np.int) #CHANGE (added)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp/ipykernel_14152/3111263836.py:115: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  targets[test] = np.asarray(targets[test]).astype(np.int) #CHANGE (added)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp/ipykernel_14152/3111263836.py:147: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "  tprs.append(interp(mean_fpr, fpr, tpr))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp/ipykernel_14152/3111263836.py:103: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  inputs_train = np.asarray(inputs_train).astype(np.int) #CHANGE (added)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp/ipykernel_14152/3111263836.py:104: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  targets[train] = np.asarray(targets[train]).astype(np.int) #CHANGE (added)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "7941/7941 [==============================] - 23s 3ms/step - loss: 0.1609 - accuracy: 0.0536 - val_loss: 0.1869 - val_accuracy: 0.1347\n",
      "Epoch 2/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.1411 - accuracy: 0.1409 - val_loss: 0.1486 - val_accuracy: 0.2476\n",
      "Epoch 3/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.1098 - accuracy: 0.1969 - val_loss: 0.1185 - val_accuracy: 0.3099\n",
      "Epoch 4/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0907 - accuracy: 0.2221 - val_loss: 0.1027 - val_accuracy: 0.3184\n",
      "Epoch 5/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0798 - accuracy: 0.2271 - val_loss: 0.0948 - val_accuracy: 0.3233\n",
      "Epoch 6/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0731 - accuracy: 0.2313 - val_loss: 0.0889 - val_accuracy: 0.3283\n",
      "Epoch 7/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0681 - accuracy: 0.2332 - val_loss: 0.0842 - val_accuracy: 0.3330\n",
      "Epoch 8/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0641 - accuracy: 0.2322 - val_loss: 0.0800 - val_accuracy: 0.3180\n",
      "Epoch 9/150\n",
      "7941/7941 [==============================] - 23s 3ms/step - loss: 0.0606 - accuracy: 0.2310 - val_loss: 0.0772 - val_accuracy: 0.3412\n",
      "Epoch 10/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0578 - accuracy: 0.2287 - val_loss: 0.0742 - val_accuracy: 0.3441\n",
      "Epoch 11/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0557 - accuracy: 0.2272 - val_loss: 0.0720 - val_accuracy: 0.3502\n",
      "Epoch 12/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0538 - accuracy: 0.2263 - val_loss: 0.0697 - val_accuracy: 0.3258\n",
      "Epoch 13/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0523 - accuracy: 0.2259 - val_loss: 0.0677 - val_accuracy: 0.3338\n",
      "Epoch 14/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0509 - accuracy: 0.2243 - val_loss: 0.0660 - val_accuracy: 0.3234\n",
      "Epoch 15/150\n",
      "7941/7941 [==============================] - 23s 3ms/step - loss: 0.0498 - accuracy: 0.2251 - val_loss: 0.0649 - val_accuracy: 0.3224\n",
      "Epoch 16/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0488 - accuracy: 0.2246 - val_loss: 0.0636 - val_accuracy: 0.3283\n",
      "Epoch 17/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0479 - accuracy: 0.2252 - val_loss: 0.0620 - val_accuracy: 0.3153\n",
      "Epoch 18/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0471 - accuracy: 0.2247 - val_loss: 0.0617 - val_accuracy: 0.3105\n",
      "Epoch 19/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0464 - accuracy: 0.2260 - val_loss: 0.0613 - val_accuracy: 0.3209\n",
      "Epoch 20/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0458 - accuracy: 0.2254 - val_loss: 0.0606 - val_accuracy: 0.3228\n",
      "Epoch 21/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0452 - accuracy: 0.2258 - val_loss: 0.0595 - val_accuracy: 0.3169\n",
      "Epoch 22/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0447 - accuracy: 0.2254 - val_loss: 0.0587 - val_accuracy: 0.3336\n",
      "Epoch 23/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0442 - accuracy: 0.2266 - val_loss: 0.0584 - val_accuracy: 0.3140\n",
      "Epoch 24/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0437 - accuracy: 0.2265 - val_loss: 0.0578 - val_accuracy: 0.3138\n",
      "Epoch 25/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0433 - accuracy: 0.2277 - val_loss: 0.0566 - val_accuracy: 0.3069\n",
      "Epoch 26/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0429 - accuracy: 0.2270 - val_loss: 0.0564 - val_accuracy: 0.3219\n",
      "Epoch 27/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0425 - accuracy: 0.2271 - val_loss: 0.0564 - val_accuracy: 0.3164\n",
      "Epoch 28/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0421 - accuracy: 0.2268 - val_loss: 0.0557 - val_accuracy: 0.3072\n",
      "Epoch 29/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0418 - accuracy: 0.2267 - val_loss: 0.0557 - val_accuracy: 0.3201\n",
      "Epoch 30/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0415 - accuracy: 0.2276 - val_loss: 0.0551 - val_accuracy: 0.3187\n",
      "Epoch 31/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0412 - accuracy: 0.2272 - val_loss: 0.0541 - val_accuracy: 0.2976\n",
      "Epoch 32/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0409 - accuracy: 0.2281 - val_loss: 0.0540 - val_accuracy: 0.3001\n",
      "Epoch 33/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0406 - accuracy: 0.2273 - val_loss: 0.0532 - val_accuracy: 0.3144\n",
      "Epoch 34/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0403 - accuracy: 0.2290 - val_loss: 0.0534 - val_accuracy: 0.3005\n",
      "Epoch 35/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0401 - accuracy: 0.2276 - val_loss: 0.0531 - val_accuracy: 0.3127\n",
      "Epoch 36/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0398 - accuracy: 0.2289 - val_loss: 0.0531 - val_accuracy: 0.3076\n",
      "Epoch 37/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0396 - accuracy: 0.2285 - val_loss: 0.0528 - val_accuracy: 0.3193\n",
      "Epoch 38/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0394 - accuracy: 0.2292 - val_loss: 0.0522 - val_accuracy: 0.3127\n",
      "Epoch 39/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0391 - accuracy: 0.2291 - val_loss: 0.0516 - val_accuracy: 0.3013\n",
      "Epoch 40/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0389 - accuracy: 0.2297 - val_loss: 0.0516 - val_accuracy: 0.3066\n",
      "Epoch 41/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0387 - accuracy: 0.2292 - val_loss: 0.0519 - val_accuracy: 0.3128\n",
      "Epoch 42/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0385 - accuracy: 0.2283 - val_loss: 0.0516 - val_accuracy: 0.3122\n",
      "Epoch 43/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0383 - accuracy: 0.2291 - val_loss: 0.0509 - val_accuracy: 0.3193\n",
      "Epoch 44/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0381 - accuracy: 0.2294 - val_loss: 0.0512 - val_accuracy: 0.3206\n",
      "Epoch 45/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0380 - accuracy: 0.2292 - val_loss: 0.0511 - val_accuracy: 0.3211\n",
      "Epoch 46/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0378 - accuracy: 0.2300 - val_loss: 0.0509 - val_accuracy: 0.2953\n",
      "Epoch 47/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0376 - accuracy: 0.2294 - val_loss: 0.0505 - val_accuracy: 0.3191\n",
      "Epoch 48/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0375 - accuracy: 0.2296 - val_loss: 0.0503 - val_accuracy: 0.3170\n",
      "Epoch 49/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0373 - accuracy: 0.2301 - val_loss: 0.0498 - val_accuracy: 0.3078\n",
      "Epoch 50/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0371 - accuracy: 0.2308 - val_loss: 0.0492 - val_accuracy: 0.3137\n",
      "Epoch 51/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0370 - accuracy: 0.2293 - val_loss: 0.0497 - val_accuracy: 0.3114\n",
      "Epoch 52/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0369 - accuracy: 0.2305 - val_loss: 0.0494 - val_accuracy: 0.3170\n",
      "Epoch 53/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0367 - accuracy: 0.2296 - val_loss: 0.0496 - val_accuracy: 0.3010\n",
      "Epoch 54/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0366 - accuracy: 0.2292 - val_loss: 0.0497 - val_accuracy: 0.3262\n",
      "Epoch 55/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0364 - accuracy: 0.2299 - val_loss: 0.0500 - val_accuracy: 0.3167\n",
      "Epoch 56/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0363 - accuracy: 0.2298 - val_loss: 0.0495 - val_accuracy: 0.3135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0362 - accuracy: 0.2302 - val_loss: 0.0497 - val_accuracy: 0.3220\n",
      "Epoch 58/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0360 - accuracy: 0.2302 - val_loss: 0.0489 - val_accuracy: 0.3282\n",
      "Epoch 59/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0359 - accuracy: 0.2292 - val_loss: 0.0484 - val_accuracy: 0.3189\n",
      "Epoch 60/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0358 - accuracy: 0.2293 - val_loss: 0.0488 - val_accuracy: 0.3182\n",
      "Epoch 61/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0357 - accuracy: 0.2288 - val_loss: 0.0488 - val_accuracy: 0.3126\n",
      "Epoch 62/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0356 - accuracy: 0.2291 - val_loss: 0.0483 - val_accuracy: 0.3204\n",
      "Epoch 63/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0355 - accuracy: 0.2299 - val_loss: 0.0482 - val_accuracy: 0.3122\n",
      "Epoch 64/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0353 - accuracy: 0.2293 - val_loss: 0.0484 - val_accuracy: 0.3288\n",
      "Epoch 65/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0352 - accuracy: 0.2293 - val_loss: 0.0474 - val_accuracy: 0.3145\n",
      "Epoch 66/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0351 - accuracy: 0.2291 - val_loss: 0.0480 - val_accuracy: 0.3264\n",
      "Epoch 67/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0350 - accuracy: 0.2296 - val_loss: 0.0480 - val_accuracy: 0.3015\n",
      "Epoch 68/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0349 - accuracy: 0.2287 - val_loss: 0.0479 - val_accuracy: 0.3225\n",
      "Epoch 69/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0348 - accuracy: 0.2298 - val_loss: 0.0478 - val_accuracy: 0.3211\n",
      "Epoch 70/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0347 - accuracy: 0.2288 - val_loss: 0.0471 - val_accuracy: 0.2960\n",
      "Epoch 71/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0346 - accuracy: 0.2289 - val_loss: 0.0477 - val_accuracy: 0.3208\n",
      "Epoch 72/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0345 - accuracy: 0.2282 - val_loss: 0.0477 - val_accuracy: 0.3222\n",
      "Epoch 73/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0344 - accuracy: 0.2290 - val_loss: 0.0464 - val_accuracy: 0.2932\n",
      "Epoch 74/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0343 - accuracy: 0.2290 - val_loss: 0.0474 - val_accuracy: 0.3279\n",
      "Epoch 75/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0343 - accuracy: 0.2285 - val_loss: 0.0467 - val_accuracy: 0.3184\n",
      "Epoch 76/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0342 - accuracy: 0.2293 - val_loss: 0.0471 - val_accuracy: 0.3109\n",
      "Epoch 77/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0341 - accuracy: 0.2284 - val_loss: 0.0463 - val_accuracy: 0.3031\n",
      "Epoch 78/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0340 - accuracy: 0.2287 - val_loss: 0.0469 - val_accuracy: 0.3183\n",
      "Epoch 79/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0339 - accuracy: 0.2293 - val_loss: 0.0468 - val_accuracy: 0.3142\n",
      "Epoch 80/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0338 - accuracy: 0.2285 - val_loss: 0.0463 - val_accuracy: 0.2990\n",
      "Epoch 81/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0337 - accuracy: 0.2277 - val_loss: 0.0462 - val_accuracy: 0.3191\n",
      "Epoch 82/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0337 - accuracy: 0.2288 - val_loss: 0.0462 - val_accuracy: 0.3319\n",
      "Epoch 83/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0336 - accuracy: 0.2282 - val_loss: 0.0462 - val_accuracy: 0.3006\n",
      "Epoch 84/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0335 - accuracy: 0.2276 - val_loss: 0.0468 - val_accuracy: 0.3190\n",
      "Epoch 85/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0334 - accuracy: 0.2269 - val_loss: 0.0463 - val_accuracy: 0.3175\n",
      "Epoch 86/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0334 - accuracy: 0.2279 - val_loss: 0.0461 - val_accuracy: 0.3068\n",
      "Epoch 87/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0333 - accuracy: 0.2278 - val_loss: 0.0462 - val_accuracy: 0.3381\n",
      "Epoch 88/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0332 - accuracy: 0.2280 - val_loss: 0.0461 - val_accuracy: 0.3119\n",
      "Epoch 89/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0331 - accuracy: 0.2282 - val_loss: 0.0458 - val_accuracy: 0.3010\n",
      "Epoch 90/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0331 - accuracy: 0.2279 - val_loss: 0.0458 - val_accuracy: 0.3134\n",
      "Epoch 91/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0330 - accuracy: 0.2279 - val_loss: 0.0462 - val_accuracy: 0.3332\n",
      "Epoch 92/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0329 - accuracy: 0.2277 - val_loss: 0.0454 - val_accuracy: 0.3240\n",
      "Epoch 93/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0329 - accuracy: 0.2275 - val_loss: 0.0461 - val_accuracy: 0.3235\n",
      "Epoch 94/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0328 - accuracy: 0.2273 - val_loss: 0.0455 - val_accuracy: 0.3200\n",
      "Epoch 95/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0327 - accuracy: 0.2275 - val_loss: 0.0447 - val_accuracy: 0.3236\n",
      "Epoch 96/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0327 - accuracy: 0.2277 - val_loss: 0.0455 - val_accuracy: 0.3148\n",
      "Epoch 97/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0326 - accuracy: 0.2283 - val_loss: 0.0455 - val_accuracy: 0.3184\n",
      "Epoch 98/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0325 - accuracy: 0.2276 - val_loss: 0.0448 - val_accuracy: 0.3163\n",
      "Epoch 99/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0325 - accuracy: 0.2278 - val_loss: 0.0460 - val_accuracy: 0.3155\n",
      "Epoch 100/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0324 - accuracy: 0.2278 - val_loss: 0.0452 - val_accuracy: 0.3336\n",
      "Epoch 101/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0323 - accuracy: 0.2281 - val_loss: 0.0452 - val_accuracy: 0.3219\n",
      "Epoch 102/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0323 - accuracy: 0.2282 - val_loss: 0.0454 - val_accuracy: 0.3020\n",
      "Epoch 103/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0322 - accuracy: 0.2274 - val_loss: 0.0448 - val_accuracy: 0.3412\n",
      "Epoch 104/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0322 - accuracy: 0.2284 - val_loss: 0.0449 - val_accuracy: 0.3163\n",
      "Epoch 105/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0321 - accuracy: 0.2278 - val_loss: 0.0444 - val_accuracy: 0.3235\n",
      "Epoch 106/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0321 - accuracy: 0.2279 - val_loss: 0.0441 - val_accuracy: 0.3328\n",
      "Epoch 107/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0320 - accuracy: 0.2272 - val_loss: 0.0447 - val_accuracy: 0.3167\n",
      "Epoch 108/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0319 - accuracy: 0.2283 - val_loss: 0.0450 - val_accuracy: 0.3161\n",
      "Epoch 109/150\n",
      "7941/7941 [==============================] - 23s 3ms/step - loss: 0.0319 - accuracy: 0.2289 - val_loss: 0.0446 - val_accuracy: 0.3160\n",
      "Epoch 110/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0318 - accuracy: 0.2276 - val_loss: 0.0448 - val_accuracy: 0.3051\n",
      "Epoch 111/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0318 - accuracy: 0.2285 - val_loss: 0.0445 - val_accuracy: 0.3068\n",
      "Epoch 112/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0317 - accuracy: 0.2282 - val_loss: 0.0446 - val_accuracy: 0.3216\n",
      "Epoch 113/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0317 - accuracy: 0.2286 - val_loss: 0.0448 - val_accuracy: 0.3153\n",
      "Epoch 114/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0316 - accuracy: 0.2282 - val_loss: 0.0444 - val_accuracy: 0.3218\n",
      "Epoch 115/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0316 - accuracy: 0.2287 - val_loss: 0.0441 - val_accuracy: 0.3224\n",
      "Epoch 116/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0315 - accuracy: 0.2285 - val_loss: 0.0443 - val_accuracy: 0.3280\n",
      "Epoch 117/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0315 - accuracy: 0.2282 - val_loss: 0.0452 - val_accuracy: 0.3276\n",
      "Epoch 118/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0314 - accuracy: 0.2280 - val_loss: 0.0444 - val_accuracy: 0.3367\n",
      "Epoch 119/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0314 - accuracy: 0.2288 - val_loss: 0.0440 - val_accuracy: 0.3300\n",
      "Epoch 120/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0313 - accuracy: 0.2286 - val_loss: 0.0434 - val_accuracy: 0.3130\n",
      "Epoch 121/150\n",
      "7941/7941 [==============================] - 23s 3ms/step - loss: 0.0313 - accuracy: 0.2298 - val_loss: 0.0440 - val_accuracy: 0.3193\n",
      "Epoch 122/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0312 - accuracy: 0.2294 - val_loss: 0.0443 - val_accuracy: 0.3143\n",
      "Epoch 123/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0312 - accuracy: 0.2293 - val_loss: 0.0440 - val_accuracy: 0.3275\n",
      "Epoch 124/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0311 - accuracy: 0.2292 - val_loss: 0.0437 - val_accuracy: 0.3398\n",
      "Epoch 125/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0311 - accuracy: 0.2296 - val_loss: 0.0433 - val_accuracy: 0.3372\n",
      "Epoch 126/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0310 - accuracy: 0.2297 - val_loss: 0.0437 - val_accuracy: 0.3298\n",
      "Epoch 127/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0310 - accuracy: 0.2296 - val_loss: 0.0440 - val_accuracy: 0.3156\n",
      "Epoch 128/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0309 - accuracy: 0.2299 - val_loss: 0.0442 - val_accuracy: 0.3258\n",
      "Epoch 129/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0309 - accuracy: 0.2289 - val_loss: 0.0439 - val_accuracy: 0.3296\n",
      "Epoch 130/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0309 - accuracy: 0.2294 - val_loss: 0.0441 - val_accuracy: 0.3328\n",
      "Epoch 131/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0308 - accuracy: 0.2299 - val_loss: 0.0433 - val_accuracy: 0.3225\n",
      "Epoch 132/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0308 - accuracy: 0.2303 - val_loss: 0.0439 - val_accuracy: 0.3200\n",
      "Epoch 133/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0307 - accuracy: 0.2300 - val_loss: 0.0435 - val_accuracy: 0.3174\n",
      "Epoch 134/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0307 - accuracy: 0.2301 - val_loss: 0.0443 - val_accuracy: 0.3315\n",
      "Epoch 135/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0307 - accuracy: 0.2303 - val_loss: 0.0428 - val_accuracy: 0.3196\n",
      "Epoch 136/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0306 - accuracy: 0.2298 - val_loss: 0.0436 - val_accuracy: 0.3291\n",
      "Epoch 137/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0306 - accuracy: 0.2310 - val_loss: 0.0433 - val_accuracy: 0.3032\n",
      "Epoch 138/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0305 - accuracy: 0.2307 - val_loss: 0.0430 - val_accuracy: 0.3142\n",
      "Epoch 139/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0305 - accuracy: 0.2300 - val_loss: 0.0432 - val_accuracy: 0.3311\n",
      "Epoch 140/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0304 - accuracy: 0.2310 - val_loss: 0.0432 - val_accuracy: 0.3409\n",
      "Epoch 141/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0304 - accuracy: 0.2307 - val_loss: 0.0434 - val_accuracy: 0.3301\n",
      "Epoch 142/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0304 - accuracy: 0.2312 - val_loss: 0.0434 - val_accuracy: 0.3133\n",
      "Epoch 143/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0303 - accuracy: 0.2302 - val_loss: 0.0434 - val_accuracy: 0.3444\n",
      "Epoch 144/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0303 - accuracy: 0.2304 - val_loss: 0.0431 - val_accuracy: 0.3282\n",
      "Epoch 145/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0302 - accuracy: 0.2313 - val_loss: 0.0432 - val_accuracy: 0.3250\n",
      "Epoch 146/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0302 - accuracy: 0.2304 - val_loss: 0.0432 - val_accuracy: 0.3273\n",
      "Epoch 147/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0302 - accuracy: 0.2310 - val_loss: 0.0425 - val_accuracy: 0.3415\n",
      "Epoch 148/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0301 - accuracy: 0.2311 - val_loss: 0.0433 - val_accuracy: 0.3233\n",
      "Epoch 149/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0301 - accuracy: 0.2310 - val_loss: 0.0430 - val_accuracy: 0.3277\n",
      "Epoch 150/150\n",
      "7941/7941 [==============================] - 22s 3ms/step - loss: 0.0301 - accuracy: 0.2313 - val_loss: 0.0422 - val_accuracy: 0.3316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp/ipykernel_14152/3111263836.py:114: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  inputs_test = np.asarray(inputs_test).astype(np.int) #CHANGE (added)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp/ipykernel_14152/3111263836.py:115: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  targets[test] = np.asarray(targets[test]).astype(np.int) #CHANGE (added)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp/ipykernel_14152/3111263836.py:133: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  inputs_test_2 = np.asarray(inputs_test_2).astype(np.int) #CHANGE (added)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp/ipykernel_14152/3111263836.py:147: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "  tprs.append(interp(mean_fpr, fpr, tpr))\n",
      "C:\\Users\\User\\anaconda3\\envs\\research\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:806: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train complete\n",
      "------------------------------------------------------------------------\n",
      "99\n",
      "interating\n",
      "[20000, 0]\n",
      "[0, 0]\n",
      "both b and c are zero\n",
      "[20000, 0]\n",
      "[0, 0]\n",
      "both b and c are zero\n",
      "interating\n",
      "[18647, 518]\n",
      "[737, 98]\n",
      "[18801, 364]\n",
      "[91, 744]\n",
      "interating\n",
      "[19303, 325]\n",
      "[227, 145]\n",
      "[19123, 505]\n",
      "[121, 251]\n",
      "interating\n",
      "[19654, 208]\n",
      "[84, 54]\n",
      "[19227, 635]\n",
      "[39, 99]\n",
      "interating\n",
      "[19783, 112]\n",
      "[43, 62]\n",
      "[19278, 617]\n",
      "[47, 58]\n",
      "interating\n",
      "[19659, 199]\n",
      "[68, 74]\n",
      "[19227, 631]\n",
      "[80, 62]\n",
      "interating\n",
      "[19562, 292]\n",
      "[64, 82]\n",
      "[19219, 635]\n",
      "[55, 91]\n",
      "interating\n",
      "[18183, 1246]\n",
      "[375, 196]\n",
      "[18963, 466]\n",
      "[96, 475]\n",
      "interating\n",
      "[18365, 1061]\n",
      "[358, 216]\n",
      "[18946, 480]\n",
      "[82, 492]\n",
      "interating\n",
      "[19482, 284]\n",
      "[127, 107]\n",
      "[19164, 602]\n",
      "[58, 176]\n",
      "interating\n",
      "[19631, 205]\n",
      "[75, 89]\n",
      "[19218, 618]\n",
      "[53, 111]\n",
      "interating\n",
      "[19642, 201]\n",
      "[73, 84]\n",
      "[19179, 664]\n",
      "[53, 104]\n",
      "interating\n",
      "[19644, 179]\n",
      "[80, 97]\n",
      "[19185, 638]\n",
      "[89, 88]\n",
      "interating\n",
      "[19363, 450]\n",
      "[85, 102]\n",
      "[19171, 642]\n",
      "[62, 125]\n",
      "interating\n",
      "[18992, 452]\n",
      "[204, 352]\n",
      "[18943, 501]\n",
      "[148, 408]\n",
      "interating\n",
      "[19025, 393]\n",
      "[146, 436]\n",
      "[18950, 468]\n",
      "[119, 463]\n",
      "interating\n",
      "[19327, 504]\n",
      "[91, 78]\n",
      "[19200, 631]\n",
      "[60, 109]\n",
      "interating\n",
      "[19629, 224]\n",
      "[65, 82]\n",
      "[19274, 579]\n",
      "[75, 72]\n",
      "interating\n",
      "[19604, 238]\n",
      "[77, 81]\n",
      "[19224, 618]\n",
      "[65, 93]\n",
      "interating\n",
      "[19526, 332]\n",
      "[70, 72]\n",
      "[19225, 633]\n",
      "[62, 80]\n",
      "interating\n",
      "[19485, 320]\n",
      "[89, 106]\n",
      "[19172, 633]\n",
      "[76, 119]\n",
      "interating\n",
      "[18393, 1033]\n",
      "[363, 211]\n",
      "[18954, 472]\n",
      "[100, 474]\n",
      "interating\n",
      "[18521, 939]\n",
      "[244, 296]\n",
      "[18939, 521]\n",
      "[174, 366]\n",
      "interating\n",
      "[19423, 400]\n",
      "[95, 82]\n",
      "[19251, 572]\n",
      "[67, 110]\n",
      "interating\n",
      "[19572, 279]\n",
      "[76, 73]\n",
      "[19204, 647]\n",
      "[53, 96]\n",
      "interating\n",
      "[19608, 238]\n",
      "[70, 84]\n",
      "[19183, 663]\n",
      "[82, 72]\n",
      "interating\n",
      "[19586, 251]\n",
      "[69, 94]\n",
      "[19232, 605]\n",
      "[46, 117]\n",
      "interating\n",
      "[19368, 456]\n",
      "[92, 84]\n",
      "[19210, 614]\n",
      "[80, 96]\n",
      "interating\n",
      "[18982, 480]\n",
      "[180, 358]\n",
      "[18948, 514]\n",
      "[142, 396]\n",
      "interating\n",
      "[18991, 446]\n",
      "[259, 304]\n",
      "[18961, 476]\n",
      "[205, 358]\n",
      "interating\n",
      "[19324, 500]\n",
      "[86, 90]\n",
      "[19246, 578]\n",
      "[36, 140]\n",
      "interating\n",
      "[19634, 235]\n",
      "[53, 78]\n",
      "[19228, 641]\n",
      "[41, 90]\n",
      "interating\n",
      "[19569, 246]\n",
      "[89, 96]\n",
      "[19177, 638]\n",
      "[82, 103]\n",
      "interating\n",
      "[19541, 303]\n",
      "[78, 78]\n",
      "[19220, 624]\n",
      "[74, 82]\n",
      "interating\n",
      "[19479, 343]\n",
      "[93, 85]\n",
      "[19229, 593]\n",
      "[69, 109]\n",
      "interating\n",
      "[18489, 962]\n",
      "[258, 291]\n",
      "[18933, 518]\n",
      "[162, 387]\n",
      "interating\n",
      "[18557, 910]\n",
      "[294, 239]\n",
      "[19010, 457]\n",
      "[129, 404]\n",
      "interating\n",
      "[19469, 353]\n",
      "[81, 97]\n",
      "[19226, 596]\n",
      "[71, 107]\n",
      "interating\n",
      "[19566, 281]\n",
      "[66, 87]\n",
      "[19251, 596]\n",
      "[52, 101]\n",
      "interating\n",
      "[19615, 202]\n",
      "[74, 109]\n",
      "[19165, 652]\n",
      "[78, 105]\n",
      "interating\n",
      "[19624, 191]\n",
      "[88, 97]\n",
      "[19160, 655]\n",
      "[107, 78]\n",
      "interating\n",
      "[19498, 336]\n",
      "[85, 81]\n",
      "[19223, 611]\n",
      "[61, 105]\n",
      "interating\n",
      "[19031, 407]\n",
      "[149, 413]\n",
      "[18985, 453]\n",
      "[123, 439]\n",
      "interating\n",
      "[19005, 444]\n",
      "[192, 359]\n",
      "[18985, 464]\n",
      "[154, 397]\n",
      "interating\n",
      "[19669, 202]\n",
      "[41, 88]\n",
      "[19235, 636]\n",
      "[73, 56]\n",
      "interating\n",
      "[19803, 72]\n",
      "[56, 69]\n",
      "[19259, 616]\n",
      "[57, 68]\n",
      "interating\n",
      "[19762, 122]\n",
      "[40, 76]\n",
      "[19272, 612]\n",
      "[53, 63]\n",
      "interating\n",
      "[19771, 119]\n",
      "[38, 72]\n",
      "[19304, 586]\n",
      "[42, 68]\n",
      "interating\n",
      "[19729, 123]\n",
      "[65, 83]\n",
      "[19240, 612]\n",
      "[67, 81]\n",
      "interating\n",
      "[19559, 180]\n",
      "[99, 162]\n",
      "[19192, 547]\n",
      "[84, 177]\n",
      "interating\n",
      "[18228, 1104]\n",
      "[304, 364]\n",
      "[18985, 347]\n",
      "[204, 464]\n",
      "interating\n",
      "[19079, 330]\n",
      "[130, 461]\n",
      "[18982, 427]\n",
      "[130, 461]\n",
      "interating\n",
      "[18530, 940]\n",
      "[279, 251]\n",
      "[19013, 457]\n",
      "[94, 436]\n",
      "interating\n",
      "[19041, 413]\n",
      "[184, 362]\n",
      "[19018, 436]\n",
      "[145, 401]\n",
      "interating\n",
      "[18421, 1045]\n",
      "[270, 264]\n",
      "[18997, 469]\n",
      "[137, 397]\n",
      "interating\n",
      "[19027, 430]\n",
      "[166, 377]\n",
      "[18990, 467]\n",
      "[134, 409]\n",
      "interating\n",
      "[19533, 185]\n",
      "[119, 163]\n",
      "[19205, 513]\n",
      "[76, 206]\n",
      "interating\n",
      "[19550, 313]\n",
      "[89, 48]\n",
      "[19216, 647]\n",
      "[49, 88]\n",
      "interating\n",
      "[19317, 455]\n",
      "[124, 104]\n",
      "[19218, 554]\n",
      "[64, 164]\n",
      "interating\n",
      "[19479, 317]\n",
      "[94, 110]\n",
      "[19203, 593]\n",
      "[73, 131]\n",
      "interating\n",
      "[19332, 438]\n",
      "[125, 105]\n",
      "[19111, 659]\n",
      "[79, 151]\n",
      "interating\n",
      "[19428, 358]\n",
      "[105, 109]\n",
      "[19200, 586]\n",
      "[87, 127]\n",
      "interating\n",
      "[19361, 422]\n",
      "[114, 103]\n",
      "[19158, 625]\n",
      "[88, 129]\n",
      "interating\n",
      "[19696, 143]\n",
      "[65, 96]\n",
      "[19234, 605]\n",
      "[60, 101]\n",
      "interating\n",
      "[19728, 147]\n",
      "[67, 58]\n",
      "[19242, 633]\n",
      "[50, 75]\n",
      "interating\n",
      "[19635, 187]\n",
      "[89, 89]\n",
      "[19251, 571]\n",
      "[37, 141]\n",
      "interating\n",
      "[19469, 342]\n",
      "[109, 80]\n",
      "[19247, 564]\n",
      "[58, 131]\n",
      "interating\n",
      "[19456, 328]\n",
      "[133, 83]\n",
      "[19209, 575]\n",
      "[36, 180]\n",
      "interating\n",
      "[19548, 249]\n",
      "[90, 113]\n",
      "[19206, 591]\n",
      "[53, 150]\n",
      "interating\n",
      "[19622, 185]\n",
      "[87, 106]\n",
      "[19233, 574]\n",
      "[56, 137]\n",
      "interating\n",
      "[19713, 153]\n",
      "[59, 75]\n",
      "[19244, 622]\n",
      "[50, 84]\n",
      "interating\n",
      "[19717, 146]\n",
      "[67, 70]\n",
      "[19212, 651]\n",
      "[56, 81]\n",
      "interating\n",
      "[19635, 222]\n",
      "[83, 60]\n",
      "[19300, 557]\n",
      "[45, 98]\n",
      "interating\n",
      "[19636, 206]\n",
      "[69, 89]\n",
      "[19233, 609]\n",
      "[57, 101]\n",
      "interating\n",
      "[19609, 204]\n",
      "[93, 94]\n",
      "[19190, 623]\n",
      "[80, 107]\n",
      "interating\n",
      "[19582, 217]\n",
      "[100, 101]\n",
      "[19227, 572]\n",
      "[44, 157]\n",
      "interating\n",
      "[19594, 214]\n",
      "[93, 99]\n",
      "[19235, 573]\n",
      "[59, 133]\n",
      "interating\n",
      "[19747, 92]\n",
      "[73, 88]\n",
      "[19231, 608]\n",
      "[47, 114]\n",
      "interating\n",
      "[19693, 165]\n",
      "[73, 69]\n",
      "[19254, 604]\n",
      "[53, 89]\n",
      "interating\n",
      "[19618, 197]\n",
      "[103, 82]\n",
      "[19234, 581]\n",
      "[52, 133]\n",
      "interating\n",
      "[19589, 236]\n",
      "[103, 72]\n",
      "[19232, 593]\n",
      "[77, 98]\n",
      "interating\n",
      "[19508, 319]\n",
      "[97, 76]\n",
      "[19250, 577]\n",
      "[47, 126]\n",
      "interating\n",
      "[19556, 245]\n",
      "[102, 97]\n",
      "[19251, 550]\n",
      "[48, 151]\n",
      "interating\n",
      "[19611, 222]\n",
      "[85, 82]\n",
      "[19254, 579]\n",
      "[53, 114]\n",
      "interating\n",
      "[19752, 107]\n",
      "[86, 55]\n",
      "[19210, 649]\n",
      "[51, 90]\n",
      "interating\n",
      "[19459, 370]\n",
      "[102, 69]\n",
      "[19215, 614]\n",
      "[84, 87]\n",
      "interating\n",
      "[19604, 197]\n",
      "[93, 106]\n",
      "[19243, 558]\n",
      "[44, 155]\n",
      "interating\n",
      "[19383, 443]\n",
      "[102, 72]\n",
      "[19248, 578]\n",
      "[51, 123]\n",
      "interating\n",
      "[19483, 319]\n",
      "[98, 100]\n",
      "[19176, 626]\n",
      "[72, 126]\n",
      "interating\n",
      "[19397, 411]\n",
      "[96, 96]\n",
      "[19180, 628]\n",
      "[60, 132]\n",
      "interating\n",
      "[19463, 304]\n",
      "[112, 121]\n",
      "[19211, 556]\n",
      "[55, 178]\n",
      "interating\n",
      "[19676, 195]\n",
      "[62, 67]\n",
      "[19222, 649]\n",
      "[59, 70]\n",
      "interating\n",
      "[19288, 449]\n",
      "[134, 129]\n",
      "[19146, 591]\n",
      "[86, 177]\n",
      "interating\n",
      "[18462, 960]\n",
      "[326, 252]\n",
      "[18973, 449]\n",
      "[101, 477]\n",
      "interating\n",
      "[19087, 362]\n",
      "[92, 459]\n",
      "[19025, 424]\n",
      "[81, 470]\n",
      "interating\n",
      "[18497, 969]\n",
      "[280, 254]\n",
      "[19003, 463]\n",
      "[106, 428]\n",
      "interating\n",
      "[19019, 418]\n",
      "[165, 398]\n",
      "[18971, 466]\n",
      "[132, 431]\n",
      "interating\n",
      "[18520, 902]\n",
      "[292, 286]\n",
      "[18964, 458]\n",
      "[148, 430]\n",
      "interating\n",
      "[19175, 287]\n",
      "[46, 492]\n",
      "[19097, 365]\n",
      "[65, 473]\n",
      "------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds of MWPM:\n",
      "> Accuracy: 0.9741824915825119 (+- 1.6215888112777225e-05)\n",
      "> F1: 0.7001540909274414(+- 0.0004278500760738683)\n",
      "> Time: 3564.583245866663 (+- 2.1515375077950063)\n",
      "Average scores for all folds of PLUT:\n",
      "> Accuracy: 0.962119967848488 (+- 1.0883750800067886e-05)\n",
      "> F1: 0.2165126791459616(+- 0.0009293963291230573)\n",
      "> Time: 5.3837477 (+- 0.024953891823521156)\n",
      "Average scores for all folds of NN:\n",
      "> Accuracy: 0.4869119951588495 (+- 0.353119332420914)\n",
      "> F1: 0.8209540886000631(+- 0.0024403812642447105)\n",
      "> Time: 5.977881833333334 (+- 0.05871311700192946)\n",
      "> AUC for class : 0.9993653773495796 (+- 0.0)\n",
      "X^2 for MWPM and NN: 0\n",
      "X^2 for PLUT and NN: 0\n",
      "> AUC for class X00: 0.8761267965212056 (+- 0.0017767855537303714)\n",
      "X^2 for MWPM and NN: 38.56573705179283\n",
      "X^2 for PLUT and NN: 162.6021978021978\n",
      "> AUC for class X01: 0.9860592177371786 (+- 0.0006346333675435221)\n",
      "X^2 for MWPM and NN: 17.045289855072465\n",
      "X^2 for PLUT and NN: 234.32747603833866\n",
      "> AUC for class X02: 0.9983351686678016 (+- 3.6787331415063184e-05)\n",
      "X^2 for MWPM and NN: 51.81164383561644\n",
      "X^2 for PLUT and NN: 525.259643916914\n",
      "> AUC for class X03: 0.9986098677258943 (+- 0.00010285332545396269)\n",
      "X^2 for MWPM and NN: 29.83225806451613\n",
      "X^2 for PLUT and NN: 487.5918674698795\n",
      "> AUC for class X04: 0.9986933148897092 (+- 0.0001601309689970937)\n",
      "X^2 for MWPM and NN: 63.29588014981273\n",
      "X^2 for PLUT and NN: 425.45710267229254\n",
      "> AUC for class X05: 0.9981520765188746 (+- 0.0002834802206876349)\n",
      "X^2 for MWPM and NN: 144.7443820224719\n",
      "X^2 for PLUT and NN: 485.8565217391304\n",
      "> AUC for class X06: 0.9815487018952479 (+- 0.0003491183054138322)\n",
      "X^2 for MWPM and NN: 466.933991363356\n",
      "X^2 for PLUT and NN: 242.27935943060498\n",
      "> AUC for class X10: 0.9796525746211246 (+- 0.00031263198554052794)\n",
      "X^2 for MWPM and NN: 347.2896405919662\n",
      "X^2 for PLUT and NN: 280.44306049822063\n",
      "> AUC for class X11: 0.9956670923643115 (+- 0.00026440907557104916)\n",
      "X^2 for MWPM and NN: 59.21167883211679\n",
      "X^2 for PLUT and NN: 446.7409090909091\n",
      "> AUC for class X12: 0.9970450800724696 (+- 0.00021062678837384778)\n",
      "X^2 for MWPM and NN: 59.43214285714286\n",
      "X^2 for PLUT and NN: 474.06259314456037\n",
      "> AUC for class X13: 0.9980099506008543 (+- 0.0001974489230168577)\n",
      "X^2 for MWPM and NN: 58.86496350364963\n",
      "X^2 for PLUT and NN: 518.9679218967922\n",
      "> AUC for class X14: 0.9977349609577596 (+- 0.00015987009505575702)\n",
      "X^2 for MWPM and NN: 37.08108108108108\n",
      "X^2 for PLUT and NN: 413.0729023383769\n",
      "> AUC for class X15: 0.9969604901993563 (+- 0.00013901351970249066)\n",
      "X^2 for MWPM and NN: 247.65607476635515\n",
      "X^2 for PLUT and NN: 476.19460227272725\n",
      "> AUC for class X16: 0.9806322350500825 (+- 6.583633524300638e-05)\n",
      "X^2 for MWPM and NN: 93.0015243902439\n",
      "X^2 for PLUT and NN: 190.91525423728814\n",
      "> AUC for class X20: 0.97925274735305 (+- 0.00020186222506686006)\n",
      "X^2 for MWPM and NN: 112.27458256029685\n",
      "X^2 for PLUT and NN: 206.3100511073254\n",
      "> AUC for class X21: 0.9968017781391402 (+- 9.485110984162125e-05)\n",
      "X^2 for MWPM and NN: 285.2840336134454\n",
      "X^2 for PLUT and NN: 470.1881331403763\n",
      "> AUC for class X22: 0.9976735558819811 (+- 8.165161161669884e-05)\n",
      "X^2 for MWPM and NN: 86.38062283737024\n",
      "X^2 for PLUT and NN: 386.8639143730887\n",
      "> AUC for class X23: 0.9975970409022109 (+- 0.0001843043461395045)\n",
      "X^2 for MWPM and NN: 81.26984126984127\n",
      "X^2 for PLUT and NN: 446.1259150805271\n",
      "> AUC for class X24: 0.9977599745216438 (+- 0.000246550111762935)\n",
      "X^2 for MWPM and NN: 169.455223880597\n",
      "X^2 for PLUT and NN: 467.48201438848923\n",
      "> AUC for class X25: 0.996826282210913 (+- 0.00010166848097993385)\n",
      "X^2 for MWPM and NN: 129.3398533007335\n",
      "X^2 for PLUT and NN: 436.01692524682653\n",
      "> AUC for class X26: 0.980574089382167 (+- 0.0011061496530349393)\n",
      "X^2 for MWPM and NN: 320.602435530086\n",
      "X^2 for PLUT and NN: 240.63111888111888\n",
      "> AUC for class X30: 0.9801649616004259 (+- 0.0007842719993685611)\n",
      "X^2 for MWPM and NN: 407.1310228233305\n",
      "X^2 for PLUT and NN: 172.25323741007193\n",
      "> AUC for class X31: 0.9969166141422825 (+- 8.32332378323765e-05)\n",
      "X^2 for MWPM and NN: 186.6989898989899\n",
      "X^2 for PLUT and NN: 397.5211267605634\n",
      "> AUC for class X32: 0.9974962333715683 (+- 4.9772373154514846e-05)\n",
      "X^2 for MWPM and NN: 114.94084507042254\n",
      "X^2 for PLUT and NN: 502.35571428571427\n",
      "> AUC for class X33: 0.9976799908897961 (+- 0.00018123836525680743)\n",
      "X^2 for MWPM and NN: 90.5487012987013\n",
      "X^2 for PLUT and NN: 451.5436241610738\n",
      "> AUC for class X34: 0.9975647294905382 (+- 0.0002658927119158685)\n",
      "X^2 for MWPM and NN: 102.378125\n",
      "X^2 for PLUT and NN: 478.2857142857143\n",
      "> AUC for class X35: 0.9968927971640924 (+- 0.00018356800497721265)\n",
      "X^2 for MWPM and NN: 240.45437956204378\n",
      "X^2 for PLUT and NN: 409.350144092219\n",
      "> AUC for class X36: 0.9806049077359491 (+- 0.00038781435238763806)\n",
      "X^2 for MWPM and NN: 135.45606060606062\n",
      "X^2 for PLUT and NN: 209.8185975609756\n",
      "> AUC for class X40: 0.9806382670472424 (+- 0.0003533468225355318)\n",
      "X^2 for MWPM and NN: 49.07234042553191\n",
      "X^2 for PLUT and NN: 107.04845814977973\n",
      "> AUC for class X41: 0.9969199605402997 (+- 5.186856836077621e-05)\n",
      "X^2 for MWPM and NN: 291.0733788395904\n",
      "X^2 for PLUT and NN: 476.67915309446255\n",
      "> AUC for class X42: 0.9976094489826828 (+- 0.0002325598675511455)\n",
      "X^2 for MWPM and NN: 113.75347222222223\n",
      "X^2 for PLUT and NN: 526.1011730205279\n",
      "> AUC for class X43: 0.9976229991256481 (+- 0.00015181104765597713)\n",
      "X^2 for MWPM and NN: 72.64477611940299\n",
      "X^2 for PLUT and NN: 427.8125\n",
      "> AUC for class X44: 0.9977390786603499 (+- 0.00013628553622111893)\n",
      "X^2 for MWPM and NN: 131.6955380577428\n",
      "X^2 for PLUT and NN: 431.80659025787963\n",
      "> AUC for class X45: 0.9967092238120839 (+- 7.419027172084051e-05)\n",
      "X^2 for MWPM and NN: 142.204128440367\n",
      "X^2 for PLUT and NN: 413.1858006042296\n",
      "> AUC for class X46: 0.9801309285897757 (+- 8.643721368607833e-05)\n",
      "X^2 for MWPM and NN: 405.08934426229507\n",
      "X^2 for PLUT and NN: 185.3308823529412\n",
      "> AUC for class X50: 0.9809226548616296 (+- 0.0005871383562313219)\n",
      "X^2 for MWPM and NN: 314.140365448505\n",
      "X^2 for PLUT and NN: 182.4726962457338\n",
      "> AUC for class X51: 0.996994468508182 (+- 0.0002690143583141669)\n",
      "X^2 for MWPM and NN: 169.2188940092166\n",
      "X^2 for PLUT and NN: 411.6581709145427\n",
      "> AUC for class X52: 0.9978950271510515 (+- 0.0001187741730001025)\n",
      "X^2 for MWPM and NN: 131.97694524495677\n",
      "X^2 for PLUT and NN: 455.0138888888889\n",
      "> AUC for class X53: 0.9978475704319973 (+- 0.00011411306501265643)\n",
      "X^2 for MWPM and NN: 58.43840579710145\n",
      "X^2 for PLUT and NN: 449.76575342465753\n",
      "> AUC for class X54: 0.9976474406046262 (+- 6.87753241985053e-05)\n",
      "X^2 for MWPM and NN: 37.29032258064516\n",
      "X^2 for PLUT and NN: 392.66272965879267\n",
      "> AUC for class X55: 0.9971424668500615 (+- 0.00015048247280175208)\n",
      "X^2 for MWPM and NN: 148.45605700712588\n",
      "X^2 for PLUT and NN: 448.51339285714283\n",
      "> AUC for class X56: 0.9805932357116262 (+- 0.0005303854104276373)\n",
      "X^2 for MWPM and NN: 118.7931654676259\n",
      "X^2 for PLUT and NN: 187.91840277777777\n",
      "> AUC for class X60: 0.9813315289263658 (+- 0.00036654901440447954)\n",
      "X^2 for MWPM and NN: 99.05817610062893\n",
      "X^2 for PLUT and NN: 154.5\n",
      "> AUC for class X61: 0.998142614919955 (+- 0.00012867704432620119)\n",
      "X^2 for MWPM and NN: 105.34979423868313\n",
      "X^2 for PLUT and NN: 445.4781382228491\n",
      "> AUC for class X62: 0.9986226539524203 (+- 8.466611620681104e-05)\n",
      "X^2 for MWPM and NN: 1.7578125\n",
      "X^2 for PLUT and NN: 462.6508172362556\n",
      "> AUC for class X63: 0.9986440625700373 (+- 7.02941627448577e-05)\n",
      "X^2 for MWPM and NN: 40.5\n",
      "X^2 for PLUT and NN: 468.21654135338343\n",
      "> AUC for class X64: 0.9987828310278614 (+- 0.00013680224772490122)\n",
      "X^2 for MWPM and NN: 40.76433121019108\n",
      "X^2 for PLUT and NN: 469.5047770700637\n",
      "> AUC for class X65: 0.9975650880533963 (+- 0.0001835659648264078)\n",
      "X^2 for MWPM and NN: 17.28191489361702\n",
      "X^2 for PLUT and NN: 435.84094256259203\n",
      "> AUC for class X66: 0.993327819244505 (+- 0.0002318674596945348)\n",
      "X^2 for MWPM and NN: 22.939068100358423\n",
      "X^2 for PLUT and NN: 338.2630744849445\n",
      "> AUC for class Z00: 0.9731178202165357 (+- 0.00018420898482708758)\n",
      "X^2 for MWPM and NN: 453.4098011363636\n",
      "X^2 for PLUT and NN: 36.595281306715066\n",
      "> AUC for class Z01: 0.9740262314998137 (+- 0.0005001206800000596)\n",
      "X^2 for MWPM and NN: 86.0891304347826\n",
      "X^2 for PLUT and NN: 157.29982046678634\n",
      "> AUC for class Z02: 0.9763176506258917 (+- 0.0008727281080775056)\n",
      "X^2 for MWPM and NN: 357.34208367514356\n",
      "X^2 for PLUT and NN: 237.82940108892922\n",
      "> AUC for class Z03: 0.9763717098421173 (+- 0.0006528956526173908)\n",
      "X^2 for MWPM and NN: 87.07537688442211\n",
      "X^2 for PLUT and NN: 144.75043029259896\n",
      "> AUC for class Z04: 0.9770024070431119 (+- 0.0009048707751262096)\n",
      "X^2 for MWPM and NN: 455.57110266159697\n",
      "X^2 for PLUT and NN: 180.7937293729373\n",
      "> AUC for class Z05: 0.9770483995948007 (+- 0.0011476341767530766)\n",
      "X^2 for MWPM and NN: 116.05536912751678\n",
      "X^2 for PLUT and NN: 183.40099833610648\n",
      "> AUC for class Z06: 0.9921587059879534 (+- 0.00028313896461950596)\n",
      "X^2 for MWPM and NN: 13.898026315789474\n",
      "X^2 for PLUT and NN: 322.74363327674024\n",
      "> AUC for class Z10: 0.9976949716327034 (+- 0.00014350065479863325)\n",
      "X^2 for MWPM and NN: 123.70398009950249\n",
      "X^2 for PLUT and NN: 512.0818965517242\n",
      "> AUC for class Z11: 0.9953859826961883 (+- 0.0002831820983033881)\n",
      "X^2 for MWPM and NN: 188.08290155440415\n",
      "X^2 for PLUT and NN: 386.9271844660194\n",
      "> AUC for class Z12: 0.9955241939344187 (+- 0.0001163541320266582)\n",
      "X^2 for MWPM and NN: 119.91240875912409\n",
      "X^2 for PLUT and NN: 404.44594594594594\n",
      "> AUC for class Z13: 0.995556284345159 (+- 0.00018573651161286057)\n",
      "X^2 for MWPM and NN: 172.90230905861458\n",
      "X^2 for PLUT and NN: 454.2560975609756\n",
      "> AUC for class Z14: 0.9959281555574556 (+- 0.00021353868289762797)\n",
      "X^2 for MWPM and NN: 137.15766738660906\n",
      "X^2 for PLUT and NN: 368.50520059435365\n",
      "> AUC for class Z15: 0.9964683212257892 (+- 0.00023932371867490718)\n",
      "X^2 for MWPM and NN: 175.83768656716418\n",
      "X^2 for PLUT and NN: 402.93969144460027\n",
      "> AUC for class Z16: 0.9970561607120407 (+- 0.0003726796300320006)\n",
      "X^2 for MWPM and NN: 28.504807692307693\n",
      "X^2 for PLUT and NN: 445.01654135338345\n",
      "> AUC for class Z20: 0.9982722986212936 (+- 9.352247228853043e-05)\n",
      "X^2 for MWPM and NN: 29.16355140186916\n",
      "X^2 for PLUT and NN: 495.9355783308931\n",
      "> AUC for class Z21: 0.9967440519197813 (+- 0.0002512289281209671)\n",
      "X^2 for MWPM and NN: 34.09057971014493\n",
      "X^2 for PLUT and NN: 467.2516447368421\n",
      "> AUC for class Z22: 0.9963635117096107 (+- 0.00013946012147841077)\n",
      "X^2 for MWPM and NN: 119.34368070953437\n",
      "X^2 for PLUT and NN: 410.008038585209\n",
      "> AUC for class Z23: 0.9963767966968092 (+- 0.00021979186133314768)\n",
      "X^2 for MWPM and NN: 81.63991323210412\n",
      "X^2 for PLUT and NN: 473.72176759410803\n",
      "> AUC for class Z24: 0.9960818096295759 (+- 0.00016574033651525853)\n",
      "X^2 for MWPM and NN: 73.6401179941003\n",
      "X^2 for PLUT and NN: 447.777950310559\n",
      "> AUC for class Z25: 0.9965533307644333 (+- 7.796566852585906e-05)\n",
      "X^2 for MWPM and NN: 34.591911764705884\n",
      "X^2 for PLUT and NN: 424.268253968254\n",
      "> AUC for class Z26: 0.9981931056033749 (+- 0.00017613990641772513)\n",
      "X^2 for MWPM and NN: 40.79716981132076\n",
      "X^2 for PLUT and NN: 485.1800595238095\n",
      "> AUC for class Z30: 0.9984077545702682 (+- 0.00018451366913232103)\n",
      "X^2 for MWPM and NN: 28.56338028169014\n",
      "X^2 for PLUT and NN: 499.06082036775103\n",
      "> AUC for class Z31: 0.997149679339787 (+- 5.845381207225931e-05)\n",
      "X^2 for MWPM and NN: 62.43934426229508\n",
      "X^2 for PLUT and NN: 433.75581395348837\n",
      "> AUC for class Z32: 0.9966395009096516 (+- 0.00012981563602521297)\n",
      "X^2 for MWPM and NN: 67.25818181818182\n",
      "X^2 for PLUT and NN: 455.85735735735733\n",
      "> AUC for class Z33: 0.9967772345360042 (+- 0.0001552842521139535)\n",
      "X^2 for MWPM and NN: 40.74074074074074\n",
      "X^2 for PLUT and NN: 417.8719772403983\n",
      "> AUC for class Z34: 0.9966221313836773 (+- 0.00040093835712939397)\n",
      "X^2 for MWPM and NN: 42.44794952681388\n",
      "X^2 for PLUT and NN: 450.85876623376623\n",
      "> AUC for class Z35: 0.9967102189042096 (+- 0.0004494051711519776)\n",
      "X^2 for MWPM and NN: 46.90553745928339\n",
      "X^2 for PLUT and NN: 416.40664556962025\n",
      "> AUC for class Z36: 0.9982034250953865 (+- 4.411443901105386e-05)\n",
      "X^2 for MWPM and NN: 1.9636363636363636\n",
      "X^2 for PLUT and NN: 478.77862595419845\n",
      "> AUC for class Z40: 0.9982729308145659 (+- 0.0001561158572570334)\n",
      "X^2 for MWPM and NN: 34.794117647058826\n",
      "X^2 for PLUT and NN: 460.4261796042618\n",
      "> AUC for class Z41: 0.996817032387871 (+- 0.00015068205900413794)\n",
      "X^2 for MWPM and NN: 28.83\n",
      "X^2 for PLUT and NN: 440.41706161137444\n",
      "> AUC for class Z42: 0.9964686888314986 (+- 0.00036054771217930003)\n",
      "X^2 for MWPM and NN: 51.39823008849557\n",
      "X^2 for PLUT and NN: 395.85820895522386\n",
      "> AUC for class Z43: 0.9965045152269717 (+- 0.00020215473890545637)\n",
      "X^2 for MWPM and NN: 117.40625\n",
      "X^2 for PLUT and NN: 448.46314102564105\n",
      "> AUC for class Z44: 0.9963540477453261 (+- 0.00022865848467135605)\n",
      "X^2 for MWPM and NN: 58.10951008645533\n",
      "X^2 for PLUT and NN: 419.7341137123746\n",
      "> AUC for class Z45: 0.9968907820088176 (+- 0.0004520342980979682)\n",
      "X^2 for MWPM and NN: 60.24755700325733\n",
      "X^2 for PLUT and NN: 436.11550632911394\n",
      "> AUC for class Z46: 0.9982372913332022 (+- 0.00020288309517682275)\n",
      "X^2 for MWPM and NN: 2.0725388601036268\n",
      "X^2 for PLUT and NN: 509.1557142857143\n",
      "> AUC for class Z50: 0.9973397974447874 (+- 2.7644133696668718e-05)\n",
      "X^2 for MWPM and NN: 151.03601694915255\n",
      "X^2 for PLUT and NN: 400.9183381088825\n",
      "> AUC for class Z51: 0.9965537662643001 (+- 0.0003001510075515759)\n",
      "X^2 for MWPM and NN: 36.58275862068965\n",
      "X^2 for PLUT and NN: 437.1578073089701\n",
      "> AUC for class Z52: 0.9960079463726398 (+- 0.0003501456721876397)\n",
      "X^2 for MWPM and NN: 212.11009174311926\n",
      "X^2 for PLUT and NN: 439.8664546899841\n",
      "> AUC for class Z53: 0.9960234011046447 (+- 0.0003137029833736119)\n",
      "X^2 for MWPM and NN: 116.06714628297362\n",
      "X^2 for PLUT and NN: 438.121776504298\n",
      "> AUC for class Z54: 0.9955049279475082 (+- 0.00028098452692874935)\n",
      "X^2 for MWPM and NN: 194.46942800788955\n",
      "X^2 for PLUT and NN: 467.28052325581393\n",
      "> AUC for class Z55: 0.9958967316655434 (+- 0.00013285735852557984)\n",
      "X^2 for MWPM and NN: 87.69471153846153\n",
      "X^2 for PLUT and NN: 409.16530278232403\n",
      "> AUC for class Z56: 0.9978578944270099 (+- 0.0002973858522503552)\n",
      "X^2 for MWPM and NN: 67.7976653696498\n",
      "X^2 for PLUT and NN: 490.00141242937855\n",
      "> AUC for class Z60: 0.9931892599887604 (+- 0.0003322191643262471)\n",
      "X^2 for MWPM and NN: 169.11835334476845\n",
      "X^2 for PLUT and NN: 375.2082717872969\n",
      "> AUC for class Z61: 0.9776035997445912 (+- 0.0008412841940046014)\n",
      "X^2 for MWPM and NN: 311.57776049766716\n",
      "X^2 for PLUT and NN: 218.92545454545456\n",
      "> AUC for class Z62: 0.9772230308868858 (+- 0.0003776085524640159)\n",
      "X^2 for MWPM and NN: 159.38546255506608\n",
      "X^2 for PLUT and NN: 231.61188118811882\n",
      "> AUC for class Z63: 0.9770271904092076 (+- 0.000428486075463996)\n",
      "X^2 for MWPM and NN: 378.97838270616495\n",
      "X^2 for PLUT and NN: 222.73462214411248\n",
      "> AUC for class Z64: 0.9767683187322694 (+- 0.0004246235443524729)\n",
      "X^2 for MWPM and NN: 108.926243567753\n",
      "X^2 for PLUT and NN: 185.43311036789297\n",
      "> AUC for class Z65: 0.9769755806191259 (+- 0.0003344341534955925)\n",
      "X^2 for MWPM and NN: 310.6206030150754\n",
      "X^2 for PLUT and NN: 157.55940594059405\n",
      "> AUC for class Z66: 0.9779376687931025 (+- 0.00024850519925751)\n",
      "X^2 for MWPM and NN: 172.97297297297297\n",
      "X^2 for PLUT and NN: 207.9093023255814\n",
      "###################################################################################\n",
      "TOTAL F1 NN: [0.8239703804283469, 0.8208984160169279, 0.8179934693549143]\n",
      "TOTAL F1 PLUT: [0.21573756003510455, 0.21598096775228462, 0.21781950965049557]\n",
      "TOTAL F1 MWPM: [0.7005817870764759, 0.7003109051446001, 0.6995695805612483]\n",
      "TOTAL ACC NN: [0.23963068425655365, 0.23481489717960358, 0.9862904040403914]\n",
      "TOTAL ACC PLUT: [0.9621126551755909, 0.9621118948346331, 0.96213535353524]\n",
      "TOTAL ACC MWPM: [0.9741893939394216, 0.9741979797979952, 0.9741601010101187]\n",
      "TOTAL TIME NN: [6.0565148, 5.9616627, 5.915468]\n",
      "TOTAL TIME PLUT: [5.3543192, 5.4153296, 5.3815943]\n",
      "TOTAL TIME MWPM: [3567.4755684000033, 3562.318840600001, 3563.9553285999864]\n",
      "------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# D7 TESTING\n",
    "\n",
    "kfold = KFold(n_splits=3, shuffle=True)\n",
    "inputs = x_d7.copy()\n",
    "inputs = np.array(inputs)\n",
    "inputs = np.stack(inputs)\n",
    "targets = np.array(y_d7)\n",
    "targets = np.array([np.array(xi) for xi in targets])\n",
    "\n",
    "n_classes = 99\n",
    "\n",
    "acc_per_fold = []\n",
    "f1_per_fold = []\n",
    "time_per_fold = []\n",
    "\n",
    "acc_per_fold_plut = []\n",
    "f1_per_fold_plut = []\n",
    "time_per_fold_plut = []\n",
    "\n",
    "acc_per_fold_mwpm = []\n",
    "f1_per_fold_mwpm = []\n",
    "time_per_fold_mwpm = []\n",
    "\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "aucs_classes = {}\n",
    "for i in mlb_d7.classes_:\n",
    "    aucs_classes[i] = []\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "for i, (train, test) in enumerate(kfold.split(inputs, targets)):\n",
    "    i_train = inputs[train].copy()\n",
    "    i_test = inputs[test].copy()\n",
    "    t_test = targets[test].copy()\n",
    "    x_test_d7 = i_test[:,:2]\n",
    "    inputs_train = i_train[:,3:]\n",
    "    inputs_test = i_test[:,3:]\n",
    "    indices = np.random.choice(inputs[test].shape[0], 20000, replace=False)\n",
    "    x_test_d7 = x_test_d7[indices]\n",
    "    inputs_test_2 = inputs_test[indices]\n",
    "    targets_test_2 = targets[test][indices]\n",
    "    ####################################################################################################\n",
    "    #test MWPM decoder for this fold\n",
    "    #labels = targets[train], features = inputs[train]\n",
    "   # x_test_d7 = translate_to_graph(testData_d7_MWPM, targs, mlb_d7)\n",
    "    decoding_d7, time_mwpm = do_new_decoding(x_test_d7, 7, 0)\n",
    "    decoding_d7['combine'] = decoding_d7[[0, 1]].values.tolist()\n",
    "    decoding_d7['combine'].apply(lambda x: x[0].extend(x[1]))\n",
    "    decoding_d7 = np.array(decoding_d7[0])\n",
    "                                              \n",
    "    time_per_fold_mwpm.append(time_mwpm)\n",
    "                                              \n",
    "    pred_mwpm = mlb_d7.transform(decoding_d7)\n",
    "\n",
    "    if fold_no < 3:\n",
    "        acc = partial_accuracy(targets_test_2, pred_mwpm)\n",
    "    else:\n",
    "        acc, contingency_mwpm = partial_accuracy_and_contingency(targets_test_2, pred_mwpm, mlb_d7)\n",
    "        \n",
    "    acc_per_fold_mwpm.append(acc)\n",
    "    f1_per_fold_mwpm.append(f1_score(targets_test_2, pred_mwpm, average='micro'))\n",
    "\n",
    "    #####################################################################################################\n",
    "    #test the plut decoder for this fold\n",
    "    \n",
    "    lookup_d7 = lookup_decoder(7)\n",
    "    \n",
    "    lookup_d7 = train_plut(lookup_d7, inputs_train, targets[train])\n",
    "    \n",
    "    start = time.time_ns()\n",
    "    pred_plut_d7 = test_plut(lookup_d7, inputs_test)\n",
    "    end = time.time_ns() \n",
    "    time_per_fold_plut.append((end - start)/ (10 ** 9))\n",
    "    \n",
    "    if fold_no < 3:\n",
    "        acc = partial_accuracy(targets[test], pred_plut_d7)\n",
    "        f1 = f1_score(targets[test], pred_plut_d7, average='micro')\n",
    "    else:\n",
    "        pred_plut_d7 = test_plut(lookup_d7, inputs_test_2)\n",
    "        f1 = f1_score(targets_test_2, pred_plut_d7, average='micro')\n",
    "        acc, contingency_plut = partial_accuracy_and_contingency(targets_test_2, pred_plut_d7, mlb_d7)\n",
    "        \n",
    "    acc_per_fold_plut.append(acc)\n",
    "    f1_per_fold_plut.append(f1)\n",
    "    \n",
    "    #####################################################################################################\n",
    "    #Test the NN decoder for this fold\n",
    "    \n",
    "    model_d7 = compile_FFNN_model_DepthSeven(7)\n",
    "\n",
    "    # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    # Fit data to model\n",
    "    \n",
    "    inputs_train = np.asarray(inputs_train).astype(np.int) #CHANGE (added)\n",
    "    targets[train] = np.asarray(targets[train]).astype(np.int) #CHANGE (added)\n",
    "    \n",
    "    history = model_d7.fit(\n",
    "        x=inputs_train ,\n",
    "        y=targets[train],\n",
    "        validation_split=.25,\n",
    "        epochs= 150)\n",
    "    \n",
    "   # Generate generalization metrics\n",
    "    \n",
    "    inputs_test = np.asarray(inputs_test).astype(np.int) #CHANGE (added)\n",
    "    targets[test] = np.asarray(targets[test]).astype(np.int) #CHANGE (added)\n",
    "    \n",
    "    scores = model_d7.evaluate(inputs_test, targets[test], verbose=0)\n",
    "    \n",
    "    start = time.time_ns()\n",
    "    predictions_d7 = model_d7.predict(inputs_test)\n",
    "    end = time.time_ns() \n",
    "    time_per_fold.append((end - start)/ (10 ** 9))\n",
    "    \n",
    "    #threshold based on previous tests with train_test_split in hyperparameter tuning\n",
    "    pred=predictions_d7.copy() #change here\n",
    "    pred[pred>=.5]=1 \n",
    "    pred[pred<.5]=0\n",
    "     \n",
    "    if fold_no < 3:\n",
    "        acc = scores[1]\n",
    "        f1 = f1_score(targets[test], pred, average='micro')\n",
    "    else:\n",
    "        inputs_test_2 = np.asarray(inputs_test_2).astype(np.int) #CHANGE (added)\n",
    "        pred = model_d7.predict(inputs_test_2)\n",
    "        pred[pred>=.5]=1 \n",
    "        pred[pred<.5]=0\n",
    "        acc, contingency_nn = partial_accuracy_and_contingency(targets_test_2, pred, mlb_d7)\n",
    "        f1 = f1_score(targets_test_2, pred, average='micro')\n",
    " \n",
    "    acc_per_fold.append(acc)\n",
    "    f1_per_fold.append(f1)\n",
    "\n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(targets[test].ravel(), predictions_d7.ravel())\n",
    "    tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "    tprs[-1][0] = 0.0\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "    plt.plot(fpr, tpr, lw=1, alpha=0.3, label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
    "    \n",
    "    #get the AUCs of each class, used to get average AUC of each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(targets[test][:, i], predictions_d7[:, i]) \n",
    "        aucs_classes[mlb_d7.classes_[i]].append(auc(fpr[i], tpr[i]))\n",
    "\n",
    "#########################################################################################################\n",
    "#compute McNemar's statistic on results of last fold\n",
    "print(\"Train complete\")\n",
    "print('------------------------------------------------------------------------')\n",
    "\n",
    "mcnemar_results_mwpm = {}\n",
    "mcnemar_results_plut = {}\n",
    "\n",
    "print(len(mlb_d7.classes_))\n",
    "\n",
    "for class_ in mlb_d7.classes_:\n",
    "    print(\"interating\")\n",
    "    \n",
    "    #compute the x^2 for NN and MWPM\n",
    "    mcnemar_results_mwpm[class_]=contingency_table_and_t(contingency_nn[class_], contingency_mwpm[class_])[1]    \n",
    "    #comput the x^2 for NN and PLUT\n",
    "    mcnemar_results_plut[class_] = contingency_table_and_t(contingency_nn[class_], contingency_plut[class_])[1]\n",
    "        \n",
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds of MWPM:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold_mwpm)} (+- {np.std(acc_per_fold_mwpm)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold_mwpm)}(+- {np.std(f1_per_fold_mwpm)})')\n",
    "print(f'> Time: {np.mean(time_per_fold_mwpm)} (+- {np.std(time_per_fold_mwpm)})')\n",
    "print('Average scores for all folds of PLUT:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold_plut)} (+- {np.std(acc_per_fold_plut)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold_plut)}(+- {np.std(f1_per_fold_plut)})')\n",
    "print(f'> Time: {np.mean(time_per_fold_plut)} (+- {np.std(time_per_fold_plut)})')\n",
    "print('Average scores for all folds of NN:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold)}(+- {np.std(f1_per_fold)})')\n",
    "print(f'> Time: {np.mean(time_per_fold)} (+- {np.std(time_per_fold)})')\n",
    "for key in aucs_classes:\n",
    "    cleanedList = [x for x in aucs_classes[key] if str(x) != 'nan']\n",
    "    print(f'> AUC for class {key}: {np.mean(cleanedList)} (+- {np.std(cleanedList)})') #this has to be a for loop\n",
    "    print(\"X^2 for MWPM and NN: \" + str(mcnemar_results_mwpm[key]))  \n",
    "    print(\"X^2 for PLUT and NN: \" + str(mcnemar_results_plut[key]))\n",
    "print(\"###################################################################################\")\n",
    "print(\"TOTAL F1 NN: \" + str(f1_per_fold))\n",
    "print(\"TOTAL F1 PLUT: \" + str(f1_per_fold_plut))\n",
    "print(\"TOTAL F1 MWPM: \" + str(f1_per_fold_mwpm))\n",
    "print(\"TOTAL ACC NN: \" + str(acc_per_fold))\n",
    "print(\"TOTAL ACC PLUT: \" + str(acc_per_fold_plut))\n",
    "print(\"TOTAL ACC MWPM: \" + str(acc_per_fold_mwpm))\n",
    "print(\"TOTAL TIME NN: \" + str(time_per_fold))\n",
    "print(\"TOTAL TIME PLUT: \" + str(time_per_fold_plut))\n",
    "print(\"TOTAL TIME MWPM: \" + str(time_per_fold_mwpm))\n",
    "print('------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "         label='Chance', alpha=.8)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "         label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "         lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                 label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "plt.xlim([-0.01, 1.01])\n",
    "plt.ylim([-0.01, 1.01])\n",
    "plt.xlabel('False Positive Rate',fontsize=18)\n",
    "plt.ylabel('True Positive Rate',fontsize=18)\n",
    "plt.title('Cross-Validation ROC of Depth 7 NN',fontsize=18)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL F1 NN: [0.8239703804283469, 0.8208984160169279, 0.8179934693549143]\n",
      "TOTAL F1 PLUT: [0.21573756003510455, 0.21598096775228462, 0.21781950965049557]\n",
      "TOTAL F1 MWPM: [0.7005817870764759, 0.7003109051446001, 0.6995695805612483]\n",
      "TOTAL ACC NN: [0.23963068425655365, 0.23481489717960358, 0.9862904040403914]\n",
      "TOTAL ACC PLUT: [0.9621126551755909, 0.9621118948346331, 0.96213535353524]\n",
      "TOTAL ACC MWPM: [0.9741893939394216, 0.9741979797979952, 0.9741601010101187]\n",
      "TOTAL TIME NN: [6.0565148, 5.9616627, 5.915468]\n",
      "TOTAL TIME PLUT: [5.3543192, 5.4153296, 5.3815943]\n",
      "TOTAL TIME MWPM: [3567.4755684000033, 3562.318840600001, 3563.9553285999864]\n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"TOTAL F1 NN: \" + str(f1_per_fold))\n",
    "print(\"TOTAL F1 PLUT: \" + str(f1_per_fold_plut))\n",
    "print(\"TOTAL F1 MWPM: \" + str(f1_per_fold_mwpm))\n",
    "print(\"TOTAL ACC NN: \" + str(acc_per_fold))\n",
    "print(\"TOTAL ACC PLUT: \" + str(acc_per_fold_plut))\n",
    "print(\"TOTAL ACC MWPM: \" + str(acc_per_fold_mwpm))\n",
    "print(\"TOTAL TIME NN: \" + str(time_per_fold))\n",
    "print(\"TOTAL TIME PLUT: \" + str(time_per_fold_plut))\n",
    "print(\"TOTAL TIME MWPM: \" + str(time_per_fold_mwpm))\n",
    "print('-----------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp/ipykernel_14152/3579347107.py:58: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "  tprs.append(interp(mean_fpr, fpr, tpr))\n",
      "C:\\Users\\User\\anaconda3\\envs\\research\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:806: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20000, 0]\n",
      "[0, 0]\n",
      "both b and c are zero\n",
      "[20000, 0]\n",
      "[0, 0]\n",
      "both b and c are zero\n",
      "[18647, 518]\n",
      "[737, 98]\n",
      "[18801, 364]\n",
      "[91, 744]\n",
      "[19303, 325]\n",
      "[227, 145]\n",
      "[19123, 505]\n",
      "[121, 251]\n",
      "[19654, 208]\n",
      "[84, 54]\n",
      "[19227, 635]\n",
      "[39, 99]\n",
      "[19783, 112]\n",
      "[43, 62]\n",
      "[19278, 617]\n",
      "[47, 58]\n",
      "[19659, 199]\n",
      "[68, 74]\n",
      "[19227, 631]\n",
      "[80, 62]\n",
      "[19562, 292]\n",
      "[64, 82]\n",
      "[19219, 635]\n",
      "[55, 91]\n",
      "[18183, 1246]\n",
      "[375, 196]\n",
      "[18963, 466]\n",
      "[96, 475]\n",
      "[18365, 1061]\n",
      "[358, 216]\n",
      "[18946, 480]\n",
      "[82, 492]\n",
      "[19482, 284]\n",
      "[127, 107]\n",
      "[19164, 602]\n",
      "[58, 176]\n",
      "[19631, 205]\n",
      "[75, 89]\n",
      "[19218, 618]\n",
      "[53, 111]\n",
      "[19642, 201]\n",
      "[73, 84]\n",
      "[19179, 664]\n",
      "[53, 104]\n",
      "[19644, 179]\n",
      "[80, 97]\n",
      "[19185, 638]\n",
      "[89, 88]\n",
      "[19363, 450]\n",
      "[85, 102]\n",
      "[19171, 642]\n",
      "[62, 125]\n",
      "[18992, 452]\n",
      "[204, 352]\n",
      "[18943, 501]\n",
      "[148, 408]\n",
      "[19025, 393]\n",
      "[146, 436]\n",
      "[18950, 468]\n",
      "[119, 463]\n",
      "[19327, 504]\n",
      "[91, 78]\n",
      "[19200, 631]\n",
      "[60, 109]\n",
      "[19629, 224]\n",
      "[65, 82]\n",
      "[19274, 579]\n",
      "[75, 72]\n",
      "[19604, 238]\n",
      "[77, 81]\n",
      "[19224, 618]\n",
      "[65, 93]\n",
      "[19526, 332]\n",
      "[70, 72]\n",
      "[19225, 633]\n",
      "[62, 80]\n",
      "[19485, 320]\n",
      "[89, 106]\n",
      "[19172, 633]\n",
      "[76, 119]\n",
      "[18393, 1033]\n",
      "[363, 211]\n",
      "[18954, 472]\n",
      "[100, 474]\n",
      "[18521, 939]\n",
      "[244, 296]\n",
      "[18939, 521]\n",
      "[174, 366]\n",
      "[19423, 400]\n",
      "[95, 82]\n",
      "[19251, 572]\n",
      "[67, 110]\n",
      "[19572, 279]\n",
      "[76, 73]\n",
      "[19204, 647]\n",
      "[53, 96]\n",
      "[19608, 238]\n",
      "[70, 84]\n",
      "[19183, 663]\n",
      "[82, 72]\n",
      "[19586, 251]\n",
      "[69, 94]\n",
      "[19232, 605]\n",
      "[46, 117]\n",
      "[19368, 456]\n",
      "[92, 84]\n",
      "[19210, 614]\n",
      "[80, 96]\n",
      "[18982, 480]\n",
      "[180, 358]\n",
      "[18948, 514]\n",
      "[142, 396]\n",
      "[18991, 446]\n",
      "[259, 304]\n",
      "[18961, 476]\n",
      "[205, 358]\n",
      "[19324, 500]\n",
      "[86, 90]\n",
      "[19246, 578]\n",
      "[36, 140]\n",
      "[19634, 235]\n",
      "[53, 78]\n",
      "[19228, 641]\n",
      "[41, 90]\n",
      "[19569, 246]\n",
      "[89, 96]\n",
      "[19177, 638]\n",
      "[82, 103]\n",
      "[19541, 303]\n",
      "[78, 78]\n",
      "[19220, 624]\n",
      "[74, 82]\n",
      "[19479, 343]\n",
      "[93, 85]\n",
      "[19229, 593]\n",
      "[69, 109]\n",
      "[18489, 962]\n",
      "[258, 291]\n",
      "[18933, 518]\n",
      "[162, 387]\n",
      "[18557, 910]\n",
      "[294, 239]\n",
      "[19010, 457]\n",
      "[129, 404]\n",
      "[19469, 353]\n",
      "[81, 97]\n",
      "[19226, 596]\n",
      "[71, 107]\n",
      "[19566, 281]\n",
      "[66, 87]\n",
      "[19251, 596]\n",
      "[52, 101]\n",
      "[19615, 202]\n",
      "[74, 109]\n",
      "[19165, 652]\n",
      "[78, 105]\n",
      "[19624, 191]\n",
      "[88, 97]\n",
      "[19160, 655]\n",
      "[107, 78]\n",
      "[19498, 336]\n",
      "[85, 81]\n",
      "[19223, 611]\n",
      "[61, 105]\n",
      "[19031, 407]\n",
      "[149, 413]\n",
      "[18985, 453]\n",
      "[123, 439]\n",
      "[19005, 444]\n",
      "[192, 359]\n",
      "[18985, 464]\n",
      "[154, 397]\n",
      "[19669, 202]\n",
      "[41, 88]\n",
      "[19235, 636]\n",
      "[73, 56]\n",
      "[19803, 72]\n",
      "[56, 69]\n",
      "[19259, 616]\n",
      "[57, 68]\n",
      "[19762, 122]\n",
      "[40, 76]\n",
      "[19272, 612]\n",
      "[53, 63]\n",
      "[19771, 119]\n",
      "[38, 72]\n",
      "[19304, 586]\n",
      "[42, 68]\n",
      "[19729, 123]\n",
      "[65, 83]\n",
      "[19240, 612]\n",
      "[67, 81]\n",
      "[19559, 180]\n",
      "[99, 162]\n",
      "[19192, 547]\n",
      "[84, 177]\n",
      "[18228, 1104]\n",
      "[304, 364]\n",
      "[18985, 347]\n",
      "[204, 464]\n",
      "[19079, 330]\n",
      "[130, 461]\n",
      "[18982, 427]\n",
      "[130, 461]\n",
      "[18530, 940]\n",
      "[279, 251]\n",
      "[19013, 457]\n",
      "[94, 436]\n",
      "[19041, 413]\n",
      "[184, 362]\n",
      "[19018, 436]\n",
      "[145, 401]\n",
      "[18421, 1045]\n",
      "[270, 264]\n",
      "[18997, 469]\n",
      "[137, 397]\n",
      "[19027, 430]\n",
      "[166, 377]\n",
      "[18990, 467]\n",
      "[134, 409]\n",
      "[19533, 185]\n",
      "[119, 163]\n",
      "[19205, 513]\n",
      "[76, 206]\n",
      "[19550, 313]\n",
      "[89, 48]\n",
      "[19216, 647]\n",
      "[49, 88]\n",
      "[19317, 455]\n",
      "[124, 104]\n",
      "[19218, 554]\n",
      "[64, 164]\n",
      "[19479, 317]\n",
      "[94, 110]\n",
      "[19203, 593]\n",
      "[73, 131]\n",
      "[19332, 438]\n",
      "[125, 105]\n",
      "[19111, 659]\n",
      "[79, 151]\n",
      "[19428, 358]\n",
      "[105, 109]\n",
      "[19200, 586]\n",
      "[87, 127]\n",
      "[19361, 422]\n",
      "[114, 103]\n",
      "[19158, 625]\n",
      "[88, 129]\n",
      "[19696, 143]\n",
      "[65, 96]\n",
      "[19234, 605]\n",
      "[60, 101]\n",
      "[19728, 147]\n",
      "[67, 58]\n",
      "[19242, 633]\n",
      "[50, 75]\n",
      "[19635, 187]\n",
      "[89, 89]\n",
      "[19251, 571]\n",
      "[37, 141]\n",
      "[19469, 342]\n",
      "[109, 80]\n",
      "[19247, 564]\n",
      "[58, 131]\n",
      "[19456, 328]\n",
      "[133, 83]\n",
      "[19209, 575]\n",
      "[36, 180]\n",
      "[19548, 249]\n",
      "[90, 113]\n",
      "[19206, 591]\n",
      "[53, 150]\n",
      "[19622, 185]\n",
      "[87, 106]\n",
      "[19233, 574]\n",
      "[56, 137]\n",
      "[19713, 153]\n",
      "[59, 75]\n",
      "[19244, 622]\n",
      "[50, 84]\n",
      "[19717, 146]\n",
      "[67, 70]\n",
      "[19212, 651]\n",
      "[56, 81]\n",
      "[19635, 222]\n",
      "[83, 60]\n",
      "[19300, 557]\n",
      "[45, 98]\n",
      "[19636, 206]\n",
      "[69, 89]\n",
      "[19233, 609]\n",
      "[57, 101]\n",
      "[19609, 204]\n",
      "[93, 94]\n",
      "[19190, 623]\n",
      "[80, 107]\n",
      "[19582, 217]\n",
      "[100, 101]\n",
      "[19227, 572]\n",
      "[44, 157]\n",
      "[19594, 214]\n",
      "[93, 99]\n",
      "[19235, 573]\n",
      "[59, 133]\n",
      "[19747, 92]\n",
      "[73, 88]\n",
      "[19231, 608]\n",
      "[47, 114]\n",
      "[19693, 165]\n",
      "[73, 69]\n",
      "[19254, 604]\n",
      "[53, 89]\n",
      "[19618, 197]\n",
      "[103, 82]\n",
      "[19234, 581]\n",
      "[52, 133]\n",
      "[19589, 236]\n",
      "[103, 72]\n",
      "[19232, 593]\n",
      "[77, 98]\n",
      "[19508, 319]\n",
      "[97, 76]\n",
      "[19250, 577]\n",
      "[47, 126]\n",
      "[19556, 245]\n",
      "[102, 97]\n",
      "[19251, 550]\n",
      "[48, 151]\n",
      "[19611, 222]\n",
      "[85, 82]\n",
      "[19254, 579]\n",
      "[53, 114]\n",
      "[19752, 107]\n",
      "[86, 55]\n",
      "[19210, 649]\n",
      "[51, 90]\n",
      "[19459, 370]\n",
      "[102, 69]\n",
      "[19215, 614]\n",
      "[84, 87]\n",
      "[19604, 197]\n",
      "[93, 106]\n",
      "[19243, 558]\n",
      "[44, 155]\n",
      "[19383, 443]\n",
      "[102, 72]\n",
      "[19248, 578]\n",
      "[51, 123]\n",
      "[19483, 319]\n",
      "[98, 100]\n",
      "[19176, 626]\n",
      "[72, 126]\n",
      "[19397, 411]\n",
      "[96, 96]\n",
      "[19180, 628]\n",
      "[60, 132]\n",
      "[19463, 304]\n",
      "[112, 121]\n",
      "[19211, 556]\n",
      "[55, 178]\n",
      "[19676, 195]\n",
      "[62, 67]\n",
      "[19222, 649]\n",
      "[59, 70]\n",
      "[19288, 449]\n",
      "[134, 129]\n",
      "[19146, 591]\n",
      "[86, 177]\n",
      "[18462, 960]\n",
      "[326, 252]\n",
      "[18973, 449]\n",
      "[101, 477]\n",
      "[19087, 362]\n",
      "[92, 459]\n",
      "[19025, 424]\n",
      "[81, 470]\n",
      "[18497, 969]\n",
      "[280, 254]\n",
      "[19003, 463]\n",
      "[106, 428]\n",
      "[19019, 418]\n",
      "[165, 398]\n",
      "[18971, 466]\n",
      "[132, 431]\n",
      "[18520, 902]\n",
      "[292, 286]\n",
      "[18964, 458]\n",
      "[148, 430]\n",
      "[19175, 287]\n",
      "[46, 492]\n",
      "[19097, 365]\n",
      "[65, 473]\n",
      "------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds of MWPM:\n",
      "> Accuracy: 0.9741824915825119 (+- 1.6215888112777225e-05)\n",
      "> F1: 0.7001540909274414(+- 0.0004278500760738683)\n",
      "> Time: 3564.583245866663 (+- 2.1515375077950063)\n",
      "Average scores for all folds of PLUT:\n",
      "> Accuracy: 0.962119967848488 (+- 1.0883750800067886e-05)\n",
      "> F1: 0.2165126791459616(+- 0.0009293963291230573)\n",
      "> Time: 5.3837477 (+- 0.024953891823521156)\n",
      "Average scores for all folds of NN:\n",
      "> Accuracy: 0.4869119951588495 (+- 0.353119332420914)\n",
      "> F1: 0.8209540886000631(+- 0.0024403812642447105)\n",
      "> Time: 5.977881833333334 (+- 0.05871311700192946)\n",
      "> AUC for class : 0.9993653773495796 (+- 0.0)\n",
      "X^2 for MWPM and NN: 0\n",
      "X^2 for PLUT and NN: 0\n",
      "> AUC for class X00: 0.8756949757514961 (+- 0.0017108865273349429)\n",
      "X^2 for MWPM and NN: 38.56573705179283\n",
      "X^2 for PLUT and NN: 162.6021978021978\n",
      "> AUC for class X01: 0.986041104097141 (+- 0.0005505033562713036)\n",
      "X^2 for MWPM and NN: 17.045289855072465\n",
      "X^2 for PLUT and NN: 234.32747603833866\n",
      "> AUC for class X02: 0.9983225741983186 (+- 3.8611433528911914e-05)\n",
      "X^2 for MWPM and NN: 51.81164383561644\n",
      "X^2 for PLUT and NN: 525.259643916914\n",
      "> AUC for class X03: 0.9986203941687665 (+- 9.092042078832373e-05)\n",
      "X^2 for MWPM and NN: 29.83225806451613\n",
      "X^2 for PLUT and NN: 487.5918674698795\n",
      "> AUC for class X04: 0.9986985424717436 (+- 0.00013897276087687305)\n",
      "X^2 for MWPM and NN: 63.29588014981273\n",
      "X^2 for PLUT and NN: 425.45710267229254\n",
      "> AUC for class X05: 0.9980520397047898 (+- 0.00030048771881312187)\n",
      "X^2 for MWPM and NN: 144.7443820224719\n",
      "X^2 for PLUT and NN: 485.8565217391304\n",
      "> AUC for class X06: 0.9815090191533299 (+- 0.00031005943526954746)\n",
      "X^2 for MWPM and NN: 466.933991363356\n",
      "X^2 for PLUT and NN: 242.27935943060498\n",
      "> AUC for class X10: 0.9795980606617898 (+- 0.0002867392266055579)\n",
      "X^2 for MWPM and NN: 347.2896405919662\n",
      "X^2 for PLUT and NN: 280.44306049822063\n",
      "> AUC for class X11: 0.9956209286902781 (+- 0.0002425435503864235)\n",
      "X^2 for MWPM and NN: 59.21167883211679\n",
      "X^2 for PLUT and NN: 446.7409090909091\n",
      "> AUC for class X12: 0.9971192192739426 (+- 0.00022307531591524962)\n",
      "X^2 for MWPM and NN: 59.43214285714286\n",
      "X^2 for PLUT and NN: 474.06259314456037\n",
      "> AUC for class X13: 0.9979999475445238 (+- 0.00017187129285672794)\n",
      "X^2 for MWPM and NN: 58.86496350364963\n",
      "X^2 for PLUT and NN: 518.9679218967922\n",
      "> AUC for class X14: 0.9976805950651603 (+- 0.00016743890319824968)\n",
      "X^2 for MWPM and NN: 37.08108108108108\n",
      "X^2 for PLUT and NN: 413.0729023383769\n",
      "> AUC for class X15: 0.996936444399998 (+- 0.00012738983631126118)\n",
      "X^2 for MWPM and NN: 247.65607476635515\n",
      "X^2 for PLUT and NN: 476.19460227272725\n",
      "> AUC for class X16: 0.9806122088414899 (+- 6.673802791855428e-05)\n",
      "X^2 for MWPM and NN: 93.0015243902439\n",
      "X^2 for PLUT and NN: 190.91525423728814\n",
      "> AUC for class X20: 0.9792774414078339 (+- 0.00017997404661980853)\n",
      "X^2 for MWPM and NN: 112.27458256029685\n",
      "X^2 for PLUT and NN: 206.3100511073254\n",
      "> AUC for class X21: 0.9968176239479907 (+- 8.660726732220752e-05)\n",
      "X^2 for MWPM and NN: 285.2840336134454\n",
      "X^2 for PLUT and NN: 470.1881331403763\n",
      "> AUC for class X22: 0.9976751639369155 (+- 7.076720131330817e-05)\n",
      "X^2 for MWPM and NN: 86.38062283737024\n",
      "X^2 for PLUT and NN: 386.8639143730887\n",
      "> AUC for class X23: 0.9975526297974765 (+- 0.00017718100258110232)\n",
      "X^2 for MWPM and NN: 81.26984126984127\n",
      "X^2 for PLUT and NN: 446.1259150805271\n",
      "> AUC for class X24: 0.9976930308733414 (+- 0.00024297031578335748)\n",
      "X^2 for MWPM and NN: 169.455223880597\n",
      "X^2 for PLUT and NN: 467.48201438848923\n",
      "> AUC for class X25: 0.9968471385158849 (+- 9.516993427451283e-05)\n",
      "X^2 for MWPM and NN: 129.3398533007335\n",
      "X^2 for PLUT and NN: 436.01692524682653\n",
      "> AUC for class X26: 0.980283065880321 (+- 0.0010824783263706618)\n",
      "X^2 for MWPM and NN: 320.602435530086\n",
      "X^2 for PLUT and NN: 240.63111888111888\n",
      "> AUC for class X30: 0.9803941125729734 (+- 0.0007866653871903928)\n",
      "X^2 for MWPM and NN: 407.1310228233305\n",
      "X^2 for PLUT and NN: 172.25323741007193\n",
      "> AUC for class X31: 0.9969335484290869 (+- 7.782119967438922e-05)\n",
      "X^2 for MWPM and NN: 186.6989898989899\n",
      "X^2 for PLUT and NN: 397.5211267605634\n",
      "> AUC for class X32: 0.997481290412827 (+- 5.027765800948042e-05)\n",
      "X^2 for MWPM and NN: 114.94084507042254\n",
      "X^2 for PLUT and NN: 502.35571428571427\n",
      "> AUC for class X33: 0.9976307760380121 (+- 0.00017861078827746042)\n",
      "X^2 for MWPM and NN: 90.5487012987013\n",
      "X^2 for PLUT and NN: 451.5436241610738\n",
      "> AUC for class X34: 0.9975874135326951 (+- 0.0002335977268579677)\n",
      "X^2 for MWPM and NN: 102.378125\n",
      "X^2 for PLUT and NN: 478.2857142857143\n",
      "> AUC for class X35: 0.9969500614029976 (+- 0.00018737792962526898)\n",
      "X^2 for MWPM and NN: 240.45437956204378\n",
      "X^2 for PLUT and NN: 409.350144092219\n",
      "> AUC for class X36: 0.9806663205386954 (+- 0.0003522989866035039)\n",
      "X^2 for MWPM and NN: 135.45606060606062\n",
      "X^2 for PLUT and NN: 209.8185975609756\n",
      "> AUC for class X40: 0.9806688965438906 (+- 0.00031057202214899337)\n",
      "X^2 for MWPM and NN: 49.07234042553191\n",
      "X^2 for PLUT and NN: 107.04845814977973\n",
      "> AUC for class X41: 0.9969049176462655 (+- 5.1929059982023706e-05)\n",
      "X^2 for MWPM and NN: 291.0733788395904\n",
      "X^2 for PLUT and NN: 476.67915309446255\n",
      "> AUC for class X42: 0.9976793005788468 (+- 0.00023494851660852241)\n",
      "X^2 for MWPM and NN: 113.75347222222223\n",
      "X^2 for PLUT and NN: 526.1011730205279\n",
      "> AUC for class X43: 0.9975794473762107 (+- 0.00015157575756025342)\n",
      "X^2 for MWPM and NN: 72.64477611940299\n",
      "X^2 for PLUT and NN: 427.8125\n",
      "> AUC for class X44: 0.9977426949832771 (+- 0.00011819282512846598)\n",
      "X^2 for MWPM and NN: 131.6955380577428\n",
      "X^2 for PLUT and NN: 431.80659025787963\n",
      "> AUC for class X45: 0.9966843542238908 (+- 7.735396928533155e-05)\n",
      "X^2 for MWPM and NN: 142.204128440367\n",
      "X^2 for PLUT and NN: 413.1858006042296\n",
      "> AUC for class X46: 0.9801150848616096 (+- 7.972838327073358e-05)\n",
      "X^2 for MWPM and NN: 405.08934426229507\n",
      "X^2 for PLUT and NN: 185.3308823529412\n",
      "> AUC for class X50: 0.9810854845516084 (+- 0.0005814543067467114)\n",
      "X^2 for MWPM and NN: 314.140365448505\n",
      "X^2 for PLUT and NN: 182.4726962457338\n",
      "> AUC for class X51: 0.9969085682006172 (+- 0.00027642925361387634)\n",
      "X^2 for MWPM and NN: 169.2188940092166\n",
      "X^2 for PLUT and NN: 411.6581709145427\n",
      "> AUC for class X52: 0.9978974427889176 (+- 0.00010294651061494652)\n",
      "X^2 for MWPM and NN: 131.97694524495677\n",
      "X^2 for PLUT and NN: 455.0138888888889\n",
      "> AUC for class X53: 0.997855057788834 (+- 9.967208858117888e-05)\n",
      "X^2 for MWPM and NN: 58.43840579710145\n",
      "X^2 for PLUT and NN: 449.76575342465753\n",
      "> AUC for class X54: 0.9976285776938952 (+- 6.793351247859472e-05)\n",
      "X^2 for MWPM and NN: 37.29032258064516\n",
      "X^2 for PLUT and NN: 392.66272965879267\n",
      "> AUC for class X55: 0.9971423719029932 (+- 0.00013032174803245578)\n",
      "X^2 for MWPM and NN: 148.45605700712588\n",
      "X^2 for PLUT and NN: 448.51339285714283\n",
      "> AUC for class X56: 0.9804092665596332 (+- 0.0005590308214677049)\n",
      "X^2 for MWPM and NN: 118.7931654676259\n",
      "X^2 for PLUT and NN: 187.91840277777777\n",
      "> AUC for class X60: 0.9812559885222827 (+- 0.0003433477725539913)\n",
      "X^2 for MWPM and NN: 99.05817610062893\n",
      "X^2 for PLUT and NN: 154.5\n",
      "> AUC for class X61: 0.9981618367220504 (+- 0.00011630464019023477)\n",
      "X^2 for MWPM and NN: 105.34979423868313\n",
      "X^2 for PLUT and NN: 445.4781382228491\n",
      "> AUC for class X62: 0.9985930102504345 (+- 8.951262831265194e-05)\n",
      "X^2 for MWPM and NN: 1.7578125\n",
      "X^2 for PLUT and NN: 462.6508172362556\n",
      "> AUC for class X63: 0.998622684356842 (+- 7.125332262627199e-05)\n",
      "X^2 for MWPM and NN: 40.5\n",
      "X^2 for PLUT and NN: 468.21654135338343\n",
      "> AUC for class X64: 0.9987839574610047 (+- 0.00011849028564324492)\n",
      "X^2 for MWPM and NN: 40.76433121019108\n",
      "X^2 for PLUT and NN: 469.5047770700637\n",
      "> AUC for class X65: 0.9976256715323575 (+- 0.000190482076193877)\n",
      "X^2 for MWPM and NN: 17.28191489361702\n",
      "X^2 for PLUT and NN: 435.84094256259203\n",
      "> AUC for class X66: 0.9932953201917747 (+- 0.00020854365114764635)\n",
      "X^2 for MWPM and NN: 22.939068100358423\n",
      "X^2 for PLUT and NN: 338.2630744849445\n",
      "> AUC for class Z00: 0.9731829419221782 (+- 0.00019537712815386745)\n",
      "X^2 for MWPM and NN: 453.4098011363636\n",
      "X^2 for PLUT and NN: 36.595281306715066\n",
      "> AUC for class Z01: 0.9738871254238644 (+- 0.0004956228626898763)\n",
      "X^2 for MWPM and NN: 86.0891304347826\n",
      "X^2 for PLUT and NN: 157.29982046678634\n",
      "> AUC for class Z02: 0.9761103548274889 (+- 0.0008367528948726924)\n",
      "X^2 for MWPM and NN: 357.34208367514356\n",
      "X^2 for PLUT and NN: 237.82940108892922\n",
      "> AUC for class Z03: 0.9761591194197852 (+- 0.0006747507783670954)\n",
      "X^2 for MWPM and NN: 87.07537688442211\n",
      "X^2 for PLUT and NN: 144.75043029259896\n",
      "> AUC for class Z04: 0.9768026779836679 (+- 0.0008566028433978757)\n",
      "X^2 for MWPM and NN: 455.57110266159697\n",
      "X^2 for PLUT and NN: 180.7937293729373\n",
      "> AUC for class Z05: 0.9767857366227347 (+- 0.0010930570266104143)\n",
      "X^2 for MWPM and NN: 116.05536912751678\n",
      "X^2 for PLUT and NN: 183.40099833610648\n",
      "> AUC for class Z06: 0.9920629685048641 (+- 0.00029601140513362575)\n",
      "X^2 for MWPM and NN: 13.898026315789474\n",
      "X^2 for PLUT and NN: 322.74363327674024\n",
      "> AUC for class Z10: 0.9976605551423429 (+- 0.00013783255374528983)\n",
      "X^2 for MWPM and NN: 123.70398009950249\n",
      "X^2 for PLUT and NN: 512.0818965517242\n",
      "> AUC for class Z11: 0.9953816729236459 (+- 0.0002453564713182341)\n",
      "X^2 for MWPM and NN: 188.08290155440415\n",
      "X^2 for PLUT and NN: 386.9271844660194\n",
      "> AUC for class Z12: 0.9955312298475044 (+- 0.00010149987807188438)\n",
      "X^2 for MWPM and NN: 119.91240875912409\n",
      "X^2 for PLUT and NN: 404.44594594594594\n",
      "> AUC for class Z13: 0.9954906870468194 (+- 0.00019693287043532368)\n",
      "X^2 for MWPM and NN: 172.90230905861458\n",
      "X^2 for PLUT and NN: 454.2560975609756\n",
      "> AUC for class Z14: 0.9958530611639802 (+- 0.00022608976229491405)\n",
      "X^2 for MWPM and NN: 137.15766738660906\n",
      "X^2 for PLUT and NN: 368.50520059435365\n",
      "> AUC for class Z15: 0.9963879142294548 (+- 0.0002497053001416872)\n",
      "X^2 for MWPM and NN: 175.83768656716418\n",
      "X^2 for PLUT and NN: 402.93969144460027\n",
      "> AUC for class Z16: 0.9969278755763262 (+- 0.00039184028393779956)\n",
      "X^2 for MWPM and NN: 28.504807692307693\n",
      "X^2 for PLUT and NN: 445.01654135338345\n",
      "> AUC for class Z20: 0.9982439914220441 (+- 9.467698878440937e-05)\n",
      "X^2 for MWPM and NN: 29.16355140186916\n",
      "X^2 for PLUT and NN: 495.9355783308931\n",
      "> AUC for class Z21: 0.9966560106378886 (+- 0.0002656892596540376)\n",
      "X^2 for MWPM and NN: 34.09057971014493\n",
      "X^2 for PLUT and NN: 467.2516447368421\n",
      "> AUC for class Z22: 0.9963382212690961 (+- 0.0001284743680908273)\n",
      "X^2 for MWPM and NN: 119.34368070953437\n",
      "X^2 for PLUT and NN: 410.008038585209\n",
      "> AUC for class Z23: 0.9963850544378188 (+- 0.00019088194673923272)\n",
      "X^2 for MWPM and NN: 81.63991323210412\n",
      "X^2 for PLUT and NN: 473.72176759410803\n",
      "> AUC for class Z24: 0.9960958953422812 (+- 0.0001455940117875351)\n",
      "X^2 for MWPM and NN: 73.6401179941003\n",
      "X^2 for PLUT and NN: 447.777950310559\n",
      "> AUC for class Z25: 0.9965672846823496 (+- 7.171554627240257e-05)\n",
      "X^2 for MWPM and NN: 34.591911764705884\n",
      "X^2 for PLUT and NN: 424.268253968254\n",
      "> AUC for class Z26: 0.9981394158616113 (+- 0.00017865249812569348)\n",
      "X^2 for MWPM and NN: 40.79716981132076\n",
      "X^2 for PLUT and NN: 485.1800595238095\n",
      "> AUC for class Z30: 0.9983954906185823 (+- 0.00016119920628017737)\n",
      "X^2 for MWPM and NN: 28.56338028169014\n",
      "X^2 for PLUT and NN: 499.06082036775103\n",
      "> AUC for class Z31: 0.9971628767765812 (+- 5.554415471623389e-05)\n",
      "X^2 for MWPM and NN: 62.43934426229508\n",
      "X^2 for PLUT and NN: 433.75581395348837\n",
      "> AUC for class Z32: 0.9966240091487484 (+- 0.00011558139333439933)\n",
      "X^2 for MWPM and NN: 67.25818181818182\n",
      "X^2 for PLUT and NN: 455.85735735735733\n",
      "> AUC for class Z33: 0.9968318008790513 (+- 0.00016436957320792317)\n",
      "X^2 for MWPM and NN: 40.74074074074074\n",
      "X^2 for PLUT and NN: 417.8719772403983\n",
      "> AUC for class Z34: 0.996515548151348 (+- 0.00039324754405399906)\n",
      "X^2 for MWPM and NN: 42.44794952681388\n",
      "X^2 for PLUT and NN: 450.85876623376623\n",
      "> AUC for class Z35: 0.9968473290760123 (+- 0.0004559291102011286)\n",
      "X^2 for MWPM and NN: 46.90553745928339\n",
      "X^2 for PLUT and NN: 416.40664556962025\n",
      "> AUC for class Z36: 0.9981881992314916 (+- 4.6422447059346564e-05)\n",
      "X^2 for MWPM and NN: 1.9636363636363636\n",
      "X^2 for PLUT and NN: 478.77862595419845\n",
      "> AUC for class Z40: 0.9983092698193163 (+- 0.00014913313000154764)\n",
      "X^2 for MWPM and NN: 34.794117647058826\n",
      "X^2 for PLUT and NN: 460.4261796042618\n",
      "> AUC for class Z41: 0.9968017137145867 (+- 0.00013316455021088978)\n",
      "X^2 for MWPM and NN: 28.83\n",
      "X^2 for PLUT and NN: 440.41706161137444\n",
      "> AUC for class Z42: 0.9964640293811167 (+- 0.00031234775651808546)\n",
      "X^2 for MWPM and NN: 51.39823008849557\n",
      "X^2 for PLUT and NN: 395.85820895522386\n",
      "> AUC for class Z43: 0.9965653015870942 (+- 0.00020429108785573118)\n",
      "X^2 for MWPM and NN: 117.40625\n",
      "X^2 for PLUT and NN: 448.46314102564105\n",
      "> AUC for class Z44: 0.9963622215759282 (+- 0.00019852949775610282)\n",
      "X^2 for MWPM and NN: 58.10951008645533\n",
      "X^2 for PLUT and NN: 419.7341137123746\n",
      "> AUC for class Z45: 0.9969200622044654 (+- 0.00039474453075907876)\n",
      "X^2 for MWPM and NN: 60.24755700325733\n",
      "X^2 for PLUT and NN: 436.11550632911394\n",
      "> AUC for class Z46: 0.99821513381284 (+- 0.00017984446017932436)\n",
      "X^2 for MWPM and NN: 2.0725388601036268\n",
      "X^2 for PLUT and NN: 509.1557142857143\n",
      "> AUC for class Z50: 0.9973347718909997 (+- 2.5473852643431975e-05)\n",
      "X^2 for MWPM and NN: 151.03601694915255\n",
      "X^2 for PLUT and NN: 400.9183381088825\n",
      "> AUC for class Z51: 0.9965475671667516 (+- 0.0002601600602166247)\n",
      "X^2 for MWPM and NN: 36.58275862068965\n",
      "X^2 for PLUT and NN: 437.1578073089701\n",
      "> AUC for class Z52: 0.9958964441561212 (+- 0.00035951318011922986)\n",
      "X^2 for MWPM and NN: 212.11009174311926\n",
      "X^2 for PLUT and NN: 439.8664546899841\n",
      "> AUC for class Z53: 0.9959307070274799 (+- 0.00031556892615947486)\n",
      "X^2 for MWPM and NN: 116.06714628297362\n",
      "X^2 for PLUT and NN: 438.121776504298\n",
      "> AUC for class Z54: 0.9954973890867092 (+- 0.00024368982651411152)\n",
      "X^2 for MWPM and NN: 194.46942800788955\n",
      "X^2 for PLUT and NN: 467.28052325581393\n",
      "> AUC for class Z55: 0.9958525832179925 (+- 0.00013815051411138256)\n",
      "X^2 for MWPM and NN: 87.69471153846153\n",
      "X^2 for PLUT and NN: 409.16530278232403\n",
      "> AUC for class Z56: 0.9978238283045189 (+- 0.00026421631468116584)\n",
      "X^2 for MWPM and NN: 67.7976653696498\n",
      "X^2 for PLUT and NN: 490.00141242937855\n",
      "> AUC for class Z60: 0.9932777883131554 (+- 0.00032601989589562557)\n",
      "X^2 for MWPM and NN: 169.11835334476845\n",
      "X^2 for PLUT and NN: 375.2082717872969\n",
      "> AUC for class Z61: 0.9773061797223982 (+- 0.0008922977810772948)\n",
      "X^2 for MWPM and NN: 311.57776049766716\n",
      "X^2 for PLUT and NN: 218.92545454545456\n",
      "> AUC for class Z62: 0.9771464682760589 (+- 0.0003528833579249018)\n",
      "X^2 for MWPM and NN: 159.38546255506608\n",
      "X^2 for PLUT and NN: 231.61188118811882\n",
      "> AUC for class Z63: 0.9771229370881398 (+- 0.0004064511252654346)\n",
      "X^2 for MWPM and NN: 378.97838270616495\n",
      "X^2 for PLUT and NN: 222.73462214411248\n",
      "> AUC for class Z64: 0.9766658896392857 (+- 0.00040829403997889104)\n",
      "X^2 for MWPM and NN: 108.926243567753\n",
      "X^2 for PLUT and NN: 185.43311036789297\n",
      "> AUC for class Z65: 0.9768604678371193 (+- 0.0003516212593289188)\n",
      "X^2 for MWPM and NN: 310.6206030150754\n",
      "X^2 for PLUT and NN: 157.55940594059405\n",
      "> AUC for class Z66: 0.9778517516432819 (+- 0.00026165128595791577)\n",
      "X^2 for MWPM and NN: 172.97297297297297\n",
      "X^2 for PLUT and NN: 207.9093023255814\n",
      "###################################################################################\n",
      "TOTAL F1 NN: [0.8239703804283469, 0.8208984160169279, 0.8179934693549143]\n",
      "TOTAL F1 PLUT: [0.21573756003510455, 0.21598096775228462, 0.21781950965049557]\n",
      "TOTAL F1 MWPM: [0.7005817870764759, 0.7003109051446001, 0.6995695805612483]\n",
      "TOTAL ACC NN: [0.23963068425655365, 0.23481489717960358, 0.9862904040403914]\n",
      "TOTAL ACC PLUT: [0.9621126551755909, 0.9621118948346331, 0.96213535353524]\n",
      "TOTAL ACC MWPM: [0.9741893939394216, 0.9741979797979952, 0.9741601010101187]\n",
      "TOTAL TIME NN: [6.0565148, 5.9616627, 5.915468]\n",
      "TOTAL TIME PLUT: [5.3543192, 5.4153296, 5.3815943]\n",
      "TOTAL TIME MWPM: [3567.4755684000033, 3562.318840600001, 3563.9553285999864]\n",
      "------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlIAAAEiCAYAAAAyFTwvAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAABlO0lEQVR4nO3dd3xUVdrA8d+TQmihhKbSQkkIAUEhYhcFddFVQFx37V1WeFkVLFiw97qLrqhYVsC1oYgsYhfbqixFqdJEpEjvkIQkM8/7x7kDw5Ayk0wyKc/38xky995z733mZsg8c86554iqYowxxhhjIhcX6wCMMcYYY6oqS6SMMcYYY0rJEiljjDHGmFKyRMoYY4wxppQskTLGGGOMKSVLpIwxxhhjSskSKVMsETlZRFRELg9al+qtuyfMY7wqIuUyzoaI3OPFkloexzeOiBwhIp+LyLZIfvdVgfd6Xo11HMaYqqlGJlIiUldEbhCRb0Rkq4jki8gGEZkmIpeLSEKsY4yEiMwUkTwRaVZMmfoisltEllRkbNEgIgMr8wd3ULIZ/NgtInNEZHhx7ycROUlEJorI797vcKP3PhxYwjnTRWSMiCwWkT0ikiMiS0VkrIgcFeXXlwC8C6QBdwKXAJOKKX95yLXIF5Et3vV4XkSOj2Z84fAS7oHlePzQ33/o444wY1QRKRCRjEK2B95nNxVx7n8XcdwvRWR36V+dMaY4VSphiAYR6Qh8AKQDnwEPA5uB5sCpwL+ATOCWWMVYCi8DzwEXA38vosyfgXq411dWvwF1gIIoHCscA4HLgHsK2fYA8Aiwt4JiKc4bwDRAgEOAS4GngM7A4NDCIvIQcBvuer4M/OrtdyHwnohMAK5QVV/Iflfhft+53jl/wv0u0oFzgWtEpIuqLorS62rvPW5U1X9GsN/TwEzcF7aGQFdgEPBXEXkd99ryohRjSe4GxgGTy+n4lxSx/h6gA/CfCI4Vj/u7dE6EMVwgIo+r6k8R7meMKYMalUiJSB1gKu5D4VxVDf1W/aj3bb7Yb/Qikqyqu8opzNJ4A/eBfQVFJ1JXAD7ch0mZqBsOP7esx4kGVS2g4hK6ksxR1dcCCyIyBlgMXC0id6jqpqBtV+GSqM+AAaqaHbTtMVxidSmwErgraNupwFhgEfAHVf09OAARuQ34W5Rf1yHez60R7veNqr4TvEJEbsC9tguBncCQMkdXCQT/3gNEpBXQDpilqvMiONwsYKCIHKuq34e5z3xcIv0o8IcIzmWMKaOa1rR3NdAJeLKQJAoAVZ2pqmMCyyKy0qsaP1JEPhaRHcC8oO0nicinIrLDa16Z431IHkBEunhNOGtFZK+IrBeR6SLyx6Aytb3q/SUiki0i20Vkvog8XtyLUtUdwDvA4SKSVci504ATgA9VdZ2IHCYiT4rIT+L6vOSKyCIRGSki8SVdRCmij5QX/+NeM1WOiPxPRE4v4hi9xPWdWuq91l0i8l8ROSek3Je42qjQ5pPLvXWF9pHyYpwgrsl2r4j8IiIPiUjdkHKB/Tt529d45eeKyJklXYviqOoe4AdcDVWHoHPWwtWk7QYuCk6ivP0KgL8Cq4Cb5MAm20e94/0lNIkK7Kuqfw+nNiqca+Rd/6+8xX8FXf/UcK5BIfHlAJcDK3A1ZwccR0QOFZHnRGSVuKbO38U1VzYPKRf4vXURkae9/085IjJDRPqGvMZA/7zLgt9DhVyPY0XkK3FNpVtE5CURqV+a1+m5Avc39qUI97sXyAYei2CfVcAY4PTg12+MKX81qkYK+JP3c2yE+7UBvgAm4vqK1AcQkbOB94D1wJPALuB84CURaa+qd3jlmnj7AzyPa8ppCmQBR+OaGgGeBa4ExuNqmBJw/VL6hBHjK7jmhStw32iDXeH9fNn72Q3XxPIe8AuQCPTDNZG1x32Il8YbuGa4/wAf45KHSbgmq1DnABnA27jr0QSXME0SkYtU9XWv3IO4D6MTObD55LuighCRtsD/cM1JY4BlwMm4GqDjRaSvl6wEGwfkA08AtYAbgMkikq6qK0t85UULJFDBtTnH42p5/q2qGwvbSVVzReQ14HbgTGCciLQDeuBqesrUbBfBNXoQ+K8Xx1jgG+8Qm0KPGS5VzRPXbHk3rvbkBS+mNsD3uOv/Mu692RFXa3WKiGR5XxqCjcfVtD4KJOPeux+JyBmq+pkX5yXABC/2ov7vH4Grrf4X8Lp3La4C/BTSLFsSERHc/7s9uP8XkViPq1m+Q0T6q+qUMPd7EPf341EROUptIlVjKoaq1pgHsAXYEeE+KwEFrg5ZH49LALYDhwWtr4X74PEBad66/t4x/lzCubYC00r52gRY7h0jKWh9HLAG2AAkeOvqAFLIMSZ4cR8atO5kL/bLg9aleuvuCVp3urfu1ZBjDvTWa8j6eoWcvy6wBFgUsv7V0P2Dtt3jHT81aN2/vXVnhpR93Ft/VSH7Tw2+JrjmXQUeDuPaB67RXbgEuRlwOC4xVmBGSPm/eetHlHDcQV65J7zls73lp6PwfyGSa3TQe6CEY1/ulf9TGK/tyaB17wMbgVYhZbNwzbfB77fA720GUCtofStcTd/PIcc46L0Zss0PHB2y/gNccl2/FNe3r3fcf0WwT+A1ZQENcEngAiA+5PdwUyHxT/We3+4tnx+0/Utgd1nfM/awhz0Kf9S0pr0GuFqjSG3l4E7aPXE1Va9oUBOLus6zj+ESmAHe6sC36DNEpEEx59kBdBGRrpEGqKqKq5VqjEteAk4HWgLj1auFUdUcrzwiUktEUkSkKa4WKQ73hzxSgXMe0AypqpNxyVFovHsCz8XdRdkEl0h9AXQu4ToVSUTicInrj6o6LWTzw7gPzMI68Y4OXBMvvpm4D+S0CE5/L+7DbyOu+XcorkZuQEi5wGsLrV0JtdP72TBkv52FlA1bGa5RNAVeQwMvpobAWcAUIFdEmgYeuC8zy3Hv5VB/16AO66q6BpckZohI5wji+V5VZ4Ss+wJXK5wawXECrvZ+vlxsqSKo6k5c828XvKbtMP0D+B14QEQSS3NuY0xkaloitRNX/R+pXzTkzilcJ1KAhYWUD6xrD6CqX+GaIC4HNnt9ge4VkcyQ/W7AJULzvf4qL4nIAO+DDwAv6Tkk+BG0/6u4GqUrg9YFnr8SdIwEERklIktxnca34BKACV6RxoVeheK1x30ALy1k28+hK0Skudf3ZQOu+WOzF8O1XpFGpYgBXG1QfQr5vajqVmCdF2uoFYWs24JrcgzXWOA0XFPcSFwC3oqDO+aHJkhFCU24AvuV5j0crLTXKJpCk8JOuL9HV+HeB6GPTkCLQo5z0HsL1xEfInsNRf3+IbL3ACKSgktEF6vqt5HsG+I5XLP4vSJSO5wd1PW3uwfXpHxt8aWNMdFQ0xKpBUADEYn0QyK75CLFU9XLcM09d+D+QN8IzBORYUFl3sd9+70E9224L+527S+9DsrgajjWhTwC+/+Oq1U6VURaeX/Q++O+bQd/4DwF3A/MwfXjOBOXAIz0tpfr+8LrP/IJ7pv2OOAvuD5ap+H6p5R7DIUITZQDJIJjLFPVz1T1Q1V9DNcUdxSuX1ywBd7PHiUcL7B9fsh+R0YQU2XVzfsZqK0MXOfXcO+Dwh6XlmM8Rf3+g2ML10VAEqWsjQrwatruxCXj10ew6yu4u0VHiUhZk25jTAlqWmfzd4GTcNXut5fxWIFvsF0K2ZYZUgYAVV2A+zB8XEQa4fp3PCIizwaalbwagdeA17yE4xHcmFYDcJ3db6T4GqOXcYnRZbiajCSCaqM8lwBfq+r5wSvFjbFVWitwyU86B9d0hDaxdAO6A/ep6t0hMVzNwSLpNLsJ13x70O9FRBoDh+LGXSp3qvqd16n6UhF5WlUDHeS/w/VZGyAiTVV1cyGx1saNC5YLfOgd71cR+RHXGTxDVReXMrSYXiPvS8EluOTlY2/1ctzvuZa6TuLh6gzMDVlX6P+/CnQVrm/V+Cgc63Xc//lbObCmuUiq6hM3DMZ7wE0llTfGlE1Nq5F6CfcN+CYRCe23AoCI9BSRoWEcaw7uluMrgpvXvH4JN+M+FN731qUEN88BqOp2XLV9XaC2iMR7yVVwGQV+9BZTvHWzvVqPfY+QuP6D+6C8HPeHdw/wVkgZHyHfskWkHjA8jNddlPe9nzeHHHcgrlkm9PwUEkNXCu+bs9vbnlJSEKrqx12DI0WkX8jmW3Hv+fdKOk4U3Y97vfcFVqjqXlzH9Pq4hLlO8A7ihqAYA7QFHtcD7+wL1Bq+GdKsu29fcaP2hzYb7xPLa+S91ldxzW4vqOpvXkxbcIOZDhKRYwrZT6TwkfuHB9XWBsZuuhBYElILuxvv/1B5Ejf8SHfgP1rEHZmR8P4G3Ipr6r4tgv0m4xL2EbjBho0x5aRG1UiparaInIW7G2eyiHwCfIpramsGnIK7HbvE8Vu8b33DcB84M0VkLO5b/l+AY4CHVHWZV/xS3B/893DfvPOB3t653lbVHC+JWiciU3DJ00ZcP6whwDbCHBlZVfNFZDzuWyy4O5VCO9i/gxtd+i3cgJAtcEnXFkpJVT8Wkf/gxupJAT7C9dP4K64WLrgD/c+4WqtbxI1ZtARXk/VXXDNWz5DD/wAMA8aISOBOqhmqWtiwCuBqG0/D/Y7H4K75SbjfzddEYVDScKnqchF5E7hIRE5U1W+89WO9GsCbgUXe72wlbliEC3DNwK/hOrAHH+9TERmM6z+zRESCRzbviBvZvAMHXu/CVMQ1OtGrWRMOHNm8mffabggpPwT4Fvjaux4/4pK69rga2fEcPLp9AvCNdx2Scf2C6gDXhZT7AdfkPRL3BUhV9c2yv8SDBMaQi3TsqCKp6ici8jmuqT8SI3FDPnTGfaEyxpSHWN82GIsHrhZoOO6P9jbcB/MGXIJ1Cd7txl7ZlcCXxRyrNy4Z24lrhvmRoFvHvTJH4D6YluP+oO3ENUfciDdUAW7YhIdxY/tswU15shLXLJcW4evrjDfkAHBiEa//cdzwDbm4MYRuZf8t25cHlT25kHWphAx/4K2vgxtPaz2Q472W0ylk+AJcbctEXO1Ztlf2HAofziAON77TGlztzr54CivvrW+H6zy/EcjDNfM8BNQNKVfo/uH87gu5RjcVsb2zF/f0IvZ9F9fXLc+7Hh8C55Rwzk64ZGqpd/1ycQnpC8CRYb5Pwr1GB70HSjju5UHvP8Uledtw/zeeB44rZt+m3nszcCPEdlxyPRrILOT31gV4xnvP5Xrvo9MKOW4arl/ezkBcQdsKHRoh6HWcHObrruPFuwqIi+T/bMhryipkW0/czRzFDn9QyH7ve9tt+AN72KOcHqIaSfcTY4yJPXGj6t8NtNOyDZhqjDFlUtP6SBljjDHGRI0lUsYYY4wxpWSJlDHGGGNMKcWsj5SIvIKbEmKjqh50h5E3htJo3JhI2biOrnMqNkpjjDHGmKLFcviDV4F/UvSgdWfg7rRJA47G3aF0dEkHbdq0qaampkYnQmOMqSFmz569WVULG6vLGFOMmCVSqvq1iKQWU2QAbqJdBX4QkUYicqiqritmH1JTU5k1a1Y0Q62RVMHvB5/P/Qw8V92/Lnhb4BHYL1AusBy6LfR5YBkOLB+8PrhsYY+iygSvDzwvbl3wNSiqXOjz4soVdrzCjlFc5XC0yxW1TyTbyrNcuGJRoV5R56yI1oIuXYSTTnLPReS3cj+hMdVQZR6QsyWwOmh5jbfuoETKG6BwMECbNm0qJLho8/shO9s99uyBnJwDH3v3Ql7egT/37lVy9+r+5Tz3KMh36/LzvUcBFHgPXwEU+LznPu8ReO7fv84fGLEG0JAZWg74sA5+Vuj6g3cq6uPh4M+NwksWurawxKWI8xQ740y4CUF4xWLOhjcxxbnwz3GcdFJSrMMwpkqrzIlU2FR1LDAWICsrq1J8cuTkwIYNsHEjbNkCW7fCli3K1m3Ktu3K9u2wYyfs2gW7d0F2TmBwVO8nQcuol2RoUK3G/pcp+yZakQPmXJEDJmAJlDjwycFFZN+/EqfExUF8HMTFQ5y3HCcgcbjn3joRiI9XRNi37NYFnusB6wOP/euUOHEn3ncOL7i4OHdcxL3CuDh15YJfQ/CxQ+IPFucdVEQJTNoTuAaB8wXOdcBVkZDt3jEKu3qh5dxrKPQyhz7d95qCz1soYX/mGbKTFHHs0EMVe+xwyoXsUtj1KLRsEQcM5zyVRSSxVpbXJbt2U2v6dHzt2lFwuOuW2r27JVHGlFVlTqTWAq2Dllt56yqNvDxYsQJ++QVWrlRWrlJ+W6WsXeOSJL8qfr8fv1/xB0ZA9ZKB4A9bwX0WJtWGunX91K6t1KnjHrWTlKTauJ9JSlItISnJrU9MFJKSoFat/T9r1Qr8FBITITFRSEwUEhL2P4+PD17vtiUkCPHx+9e5JKmSfAIYY0pPFSZPhn/8A7L3wOYWcP1kSEyMcWDGVA+VOZGaAgzz5ik7GthRUv+o8rZ+PcyZAzNn+Zn9o/Lbb5BfoBT4fPj8iqDEiasVqlULmjf10ayZn5TGfpqmKE2aKI0aCY0bC40axdOwofvZuHE8ycnxJCTYaBTGmChaswYeeAAC/UZPOgluvdWSKGOiKGaJlDfJ6MlAUxFZg5vuIRFAVZ/HzQR/Jm5+umzgioqOURWWLoXPPlM+/szPihVKfoEfn99HnAgJ8XDYYT46tC8gta2ftm2Etm3jadcukebNa1liZIyJDb8f3ngDxoxxHSgbN4abb4bTTqs8bY3GVBOxvGvvghK2K/B/FRTOAfLyYNo0eOVVP7+s8JNX4AP1k1wfevTIo+cRPrKy4jniiLrUq1c7FiEaY0zR/H744AOXRJ15JowYAY0axToqY6qlyty0V+Hy8uCNN5RXJ/hZt95Hgc9Hk8bKicfv5dS+Su/eydSpUyfWYRpjzMHy8yE3F5KTISEB7rnH3e1ywgmxjsyYas0SKc+vv8ItI/3MX1SAz+ea6y6+IJdzz21InTp1Yx2eMcYUbeFCuPde6NABHn7YrUtPdw9jTLmq8YmUKrz7rvLQYz527S7gsEMKGH5dDn/8Y2MSE5NjHZ4xxhQtNxeee871h/L73QBxO3dCgwaxjsyYGqPGJ1JPPOnnlXE+fAUF9Dstl7vvSqJpU5slwRhTyc2aBfffD2vXuoHKLr0U/vpXSLKxoYypSDU6kfr2v8or4/wIBYy6bRcXXdSE+Pj4WIdljDFFU3XNd5MmueW0NLjzTsjMjG1cxtRQNTaR2rkTbr2jAL/Px1+vzuaSS5oVOeKyMcZUGiJuoLrERLj6arjsMte53BgTEzX2f9+d9xawaZPSvWs+Q4cmWxJljKm8tm51d+BlZLjloUNh0CBo3z62cRljqJEjRn74ofLxx0q9Oj7uvLOA2rVrxTokY4w5mCp8+CGcdx7ccoub1Rygbl1LooypJGpcjVRBATz4iB+/r4BrB++ha9cmsQ7JGGMOtmGD6wv17bduuVMnNxt6XRuOxZjKpMYlUkuXwuYtflq19HPFFY2sSc8YU7n4/fDeezB6tKuBSk6G4cPh7LNtehdjKqEal0jNmFWA3++n++EFJCbWi3U4xhhzoFGj4JNP3PNTToGRI6Fp09jGZIwpUo1LpP4320+cKJmZBbEOxRhjDvaHP7gxokaOhD59rBbKmEquxiVS8+ZBQpxw1FHWwdwYUwksXQoLFri78AB694ajjrK+UMZUETUqkVr9u49Nm4SU+n66dLE/UsaYGMrLg5dfhldfdXfndeniOpSDJVHGVCE1KpH6YZaPeKBz5wISE2vHOhxjTE01b56b3uXXX93yn/8MrVvHNiZjTKnUqERq9hw/IHTunB/rUIwxNVFODowZA2++6Wqh2rZ107sccUSsIzPGlFKNSqQWzBfigCOPrJHjkBpjYu2pp9zQBnFxcPnlcM01broXY0yVVWMSqb17YcXyOBLj/fToYbOjG2Ni4Oqr4bffYMSI/dO9GGOqtBpTNfPjXB++AqVdOx8pKZZIGWMqwJdfuqld/H633KIFjB1rSZQx1UiNqZGa/ZMCSkZGPiJ2R4wxphxt3QqPPQaffeaWP/kE+vWLbUzGmHJRYxKpuXNdItW1qy/WoRhjqitVmDYNnnwSdu6EOnXgb3+D00+PdWTGmHJSIxIpVVi4wHU0P+ooa9YzxpSDdevgoYfg++/d8rHHwu23w6GHxjYuY0y5qhGJ1Jo1sG0bNGqkdOxYJ9bhGGOqo+nTXRLVoIHrTP7HP9r0LsbUABF1NheR1iLyioisEZE8EenjrW/mrT+qfMIsm2XLwOfzk5aWT3x8jelfb4wpb3v37n9+/vlw5ZUwcSKcdZYlUcbUEGFnFSLSDpgFnAssBOID21R1E5AFXB3tAKMhLw/86ie5fqwjMcZUCwUFbmqXs8+GTZvcurg4GDoUmjSJaWjGmIoVSfXMg4Af6ApcBIR+3ZoGnBCluKJqb54fVaiVaN8QjTFltGQJXHYZ/POf7u68L7+MdUTGmBiKpI/UqcAzqrpaRAr7yvUb0Co6YUVXTp4iAgkJGutQjDFVVV4evPSSq4ny++Gww+COO+Doo2MdmTEmhiJJpBoA64rZXivC41WYnL1+4kSIs+5RxpjSWLQI7roLVq50fZ/OP98149W1MemMqekiSXxWA12K2X4MsLxs4ZSPvXmKAAmVMs0zxlQJq1ZBaqpLqLp1i3U0xphKIpI6mknAlSLSNWidAojIucB5wNtRjC1qcvf6EYH4eGvaM8aEaenS/c8zM2H0aHj9dUuijDEHiLSz+RpgBvAaLom6VUS+xyVQc4Enox5hFOR6NVLx8SUWNcbUdDt3wj33wIUXwtdf719/7LFQq1bMwjLGVE5hJ1KquhM4FngJN9SBAKcBnYAxwCmqmlseQZbV3jxFRKyzuTGmeF98AX/6E0yd6pKmzZtjHZExppKLqNeQl0xdD1wvIs1wydQmVa3UGUqgRso6mxtjCrV5s5tk+Isv3PKRR8Kdd0KbNrGNyxhT6YWdSInIXcAkVV0A+wbhDN7eBThXVe+Lbohll5cfGP4g1pEYYyqdefPg+uth1y53F95118GgQfbNyxgTlkj+UtwDFNfLsitwdyQnF5F+IrJERJaLyK2FbG8jItNF5EcRmSciZ0Zy/IC8fL/dtWeMKVzHji6BOu44ePtt17RnSZQxJkzRTC1qAwXhFhaReOBZXD+rNcBMEZmiqouCio0C3lbV50QkEzd6emokQRX4/Ph94t21F8mexphqye+HKVPgD3+AOnVcEvXqq9C0qc2PZ4yJWLGJlIg0ABoFrWoiIoV1GkjBTRuzOoJz9wKWq+oK71xvAgOA4ERKcQOBAjQEfo/g+ADk+xTUfbtMtClijKnZfv0V7r/fNeetWAEjRrj1zZrFNi5jTJVVUo3UcOAu77kC//AehRHglgjO3ZIDE681QOhcC/cAn4jI34B6uGlqDj6xyGBgMECbkM6heT6/myEQa9ozpsYqKIDx4+HFFyE/39U+9ewZ66iMMdVASanFl95PwSVU7wHzQsoosBv4QVW/i2p0cAHwqqo+KSLHAhNEpKuq+g8IQHUsMBYgKyvrgDsIC3z+fTVSlkgZUwMtXgz33bd/gM2BA13n8uTkmIZljKkeik0tVPUr4CsAEWkLPK+qM6J07rVA66DlVt66YFcB/bxYvheR2kBTYGO4J8n3Kfhdk5417RlTw6xYAZdeun+S4VGjoFevWEdljKlGwq6jUdUronzumUCaiLTDJVDnAxeGlFkF9AVeFZHOuA7tm4hAvs+Pqutlbp3Njalh2reHPn2geXMYMsR1LjfGmCiKuLHLu9suA2hMIcMnqOrXB+1UCFUtEJFhwMdAPPCKqi4UkfuAWao6BbgReFFEhuOaEC+PdPDPfJ8f9buXaTVSxlRze/bAs8+65rv0dLfuoYdsOANjTLmJKJESkZHArey/k64wYdf7qOo03JAGwevuCnq+CDg+khhD5fsU9QmgxMVV6gHYjTFl8d138OCDsGED/PwzvPKKG87AkihjTDmKZGTzq4CHcX2mPsFNYvx3IB/Xl2kFbs69SsXVSLlEymqkjKmGtm+Hp56Cad53ssxMuOMOGxPKGFMhIqmRGoK7M+8UEWmCS6Q+UNUvRGQ08BMR1EZVlHyfH7/PPbe79oypRlThs8/cHHnbtrlJhocOhQsusA6RxpgKE0mdd2dgovc80EYWD6Cq63DDD1wfvdCiww3I6b6ZJiTYN1Rjqo1t29zgmtu2QY8e8NZbcPHFlkQZYypUJHU0PmCP9zzws0nQ9pVAWhRiiqrAFDFgNVLGVHmq7hEXBykpcNNNbrDNgQOtL5QxJiYi+cuzCmgHoKp7caOSnxi0/Shga/RCi468oETK+kgZU4WtXeua7t55Z/+6/v1h0CBLoowxMRNJHc3XwB+B27zlicANIlIHl5BdDLwS3fDKRlXx+ZXAgAnWtGdMFeT3w5tvwpgxkJvrEqpBg6yK2RhTKUTyl2g0MFdE6qhqDnA3kA5c5m3/BDc0QqWR71Pi44SCAmvaM6ZKWrHCTe+yYIFb7tcPbrzR/jMbYyqNSEY2XwIsCVreA/QXkYaAT1V3l0N8ZeLzK4nxcRQUuGVr2jOmiigogFdfhZdecs+bN4fbboMTTyxxV2OMqUhl7ligqjtUdbc4l0QjqGjJ9/uJjxN83vAHlkgZU0WIwFdfuSRq0CB4+21LoowxlVKZ68dFRIALgDtxTX0TynrMaPH5lIQ42VcjZa0BxlRiubmwdy80bOiGMLj7btixA3r2jHVkxhhTpBJrpETkBBF5X0QWici3IvLXoG1/ABbgkqfDgEfLL9TIFfj1gBop62xuTCU1e7YbSPPBB/ev69jRkihjTKVXbB2NiBwPfA4kBq0+VkTqAbWBB4DtwP3AaFXdVk5xlorPryTEWR8pYyqt3bvh6adh0iS3nJQEu3ZBcnJs4zLGmDCV1Ng1EtgL/AmXUHUExgOjgGTgBeA2Vd1ejjGWmk8Dd+25ZRvw2JhK5Ntv4aGHYONG1+5+1VVw+eWQmFjirsYYU1mUlEgdDbygqv/xlueJyE24oQ7GqeqQco2ujHy+AxMpq5EyphJQdf2fApMMd+0Kd90F7dvHNi5jjCmFkhKpJsDCkHWB5clRjybKXI0UlkgZU5mIQOPGrhnv//4Pzj/fRiY3xlRZJSVScUBeyLrA8q7ohxNdPr8SJ9bZ3JiY27jRPbp2dctDhsCf/wwtW8Y2LmOMKaNwBgSoJyIpQcuB58kh6wFQ1Uoz356qEm8DchoTO34/TJ4Mo0dDvXowcaL7Wbu2JVHGmGohnETqee8RalIh6zTMY1YInyqC1UgZExOrV8MDD7ihDcANZZCX5xIpY4ypJkpKesZVSBTlxO8HEECJi4P4eEukjCl3fj+8/jo895wbYLNxY7jlFjj1VNc/yhhjqpFiEylVvaKiAikPflX8Xm2UDX1gTAW59Vb44gv3/Mwz3STDDRvGNiZjjCknlaYZrjyogt/nvgHHx2uMozGmhujfHxYuhNtvh+OPj3U0xhhTrqp1IuVXRa1GypjytWABzJ/vpngBOOEEeO89qFUrtnEZY0wFqPaJlM8vqFoiZUzU5eS4flBvvOH6Ph1xBHTu7LZZEmWMqSGqdSKlBDqcKwkJ1rRnTNTMnOnuyFu71g2mecklNjK5MaZGqt6JlIJvXx+pGAdjTHWwa5cbE2ryZLecng533rm/JspUK7Nnz26ekJDwEtAVN0CzMTWNH1hQUFBwdc+ePTcWVqCaJ1LBd+1ZjZQxZfaPf8D777uJha+5Bi691E04bKqlhISElw455JDOzZo12xYXF2d/RE2N4/f7ZdOmTZnr169/CehfWJlq/RdQYV8iZX/rjYmCa6+FTZtg+HBo1y7W0Zjy19WSKFOTxcXFabNmzXasX7++a5FlIjmgiCSLyF0i8q2ILBORY731Tb31GWUNOpoCTXuqNieqMRFThWnT4Lrr2Dc9QLNm8PTTlkTVHHGWRJmazvs/UGQWEXY9jYg0A74F2gPLvZ91AFR1s4hcBjQCRpQh3qjyq1pnc2NKY8MGeOgh+O9/3fLnn8Ppp8c2JmOMqYQiqad5ADgEOBo4ETf3SrD3gb5RiitqfN6ExdbZ3Jgw+P3wzjtw3nkuiUpOhrvvhtNOi3VkpoaKj4/vmZGRkZmWltalT58+HTdv3rzvr/msWbNqH3PMMempqald27Zt2/Xmm28+1O++PQPw9ttvN+jatWvnDh06dOncuXPmNddc0yr0+Dk5OXLcccelZ2RkZL744ouNi4qjV69enb7++uu6oeuffvrpJpdeemmb0PWbNm2KP+200zqkp6dnHn744Z1nzpxZO7Dt3nvvbd6xY8cuaWlpXc4+++x22dnZhc6ddOWVV7b+8MMP6weW161bl5CQkNDjscceaxZcrm7dukcWF9M///nPJmlpaV3S09MzO3funHnXXXe1KOp1huudd95pkJqa2rVNmzZdb7/99kMKK7N06dJaxx57bHp6enpmr169Ov3yyy+JgW1DhgxpmZaW1iUtLa1L8HU/66yz2s+fPz+prPFVpEgSqbOAMao6B9f9KNQKoHVUoooSVfD73fvT+kgZU4JVq1wfqEcegexs6NMHJk6Es8+2OfJMzCQlJfkXL168aNmyZQsbNWpU8PjjjzcD2L17t5xzzjkdb7nllvUrV65csGDBgkUzZsyo/+ijjzYDmDlzZu0bb7yxzYQJE3795ZdfFs6fP39Rx44d94Ye/7vvvqsLsHjx4kXXXHPNtmjFPWrUqEO7deuWvXTp0kXjx4//9brrrmsD8OuvvyaOHTu2xU8//bRo2bJlC30+n7z00kspofuvX78+fvbs2fXOOOOM3YF148ePb9y9e/c9EydOPKh8Ud5+++0GY8aMaf7pp58uXbp06aI5c+b83LBhQ19ZXltBQQHDhw9vM23atKVLly5d+O6776bMnj27dmi566+/vtWFF164ZenSpYtGjRr1+4033tgK4M0332w4d+7cuosWLVo4e/bsn0ePHn3I1q1b4wCGDBmy8cEHHyw0MausIkmkmuKa9IriBw66kLGkKAVejZQ17RlTgu+/hzlzICUFHnvMPZo2jXVUxuxzzDHH7Fm7dm0tgBdffLFJVlbW7kGDBu0ESE5O9j/33HOrRo8efSjAQw89dMiNN9647sgjj8wFSEhIYOTIkZuCj7d27dqEK664ot38+fPrZmRkZC5cuDDp/fffT+7cuXNmenp65nnnnZeak5Nz0LeI0aNHN0lNTe16+OGHd/7uu+/qh24HWLJkSe3TTjttF8CRRx6Zu2bNmlqrV69OAPD5fLJnz564/Px8cnJy4lq1apUfuv9rr73WuG/fvjuD102cODHliSeeWL1hw4bE4Nqd4jz22GOHPvLII2tSU1PzAerUqaM33njj5nD2LcqXX35Zr23btnszMzPzateurYMGDdr6zjvvNAott2zZsjpnnHHGToCzzjpr12effdYIYOHChbWPP/743YmJiTRo0MCfmZmZPWnSpIYA/fr12/3NN980yM8/6JJUWpEkUuuBDsVsPxJYVbZwokytac+YYu3Zs//5eefB0KGuaa9Pn9jFZEwhCgoKmD59evLAgQO3g/sw7tGjR3ZwmS5duuzNzs6O27p1a9ySJUvqHH300dmFHszTsmXLgjFjxvyWlZW1e/HixYvatWuX99e//rXdW2+99cvSpUsXFRQUEKgBC/jtt98SH3nkkcO+++67xTNnzly8dOnSOoUdu2vXrjkTJ05sDDB9+vS669atS1q5cmWtdu3a5f/f//3f+nbt2nVr3rx59+TkZF8gGQz23Xff1c/Kytr3H3T58uWJmzZtSjzllFOy+/fvv238+PFh1UotW7aszvHHH1/sdQB47rnnUjIyMjJDH/369TtopN3Vq1fXatmyZV5guVWrVnmBBDdY586ds994443GABMmTGi0Z8+euPXr18cfeeSROZ9//nnDXbt2xa1bty7hu+++a7B69epaAPHx8bRt2zb3hx9+OKgZtbKKpMFrGnCViDwD5AVvEJGjgUuBf0QvtOjw+/GmiLEaKWP2ycuDl15ySdMbb0CLFu7W1iuvjHVkphJ7/6e1DaN9zAFHtNxR3Pa9e/fGZWRkZG7YsCGxQ4cOuQMHDjwo6YiWuXPn1m7VqtXebt267QW4/PLLtzz77LPNgX0DMX799df1jjnmmF2HHXZYAcCgQYO2Ll269KDWmPvuu2/d4MGD23gJSU5GRkZ2fHy8btq0Kf6DDz5otHz58vlNmjTx/fGPf2w/ZsyYlKFDh24N3n/Dhg2JLVq0KAgsjx8/PqV///7bAC655JKtV111Veq99967oajXIiIRfegNGTJk65AhQ7aWXDJ8zzzzzJrBgwe36dy5c9NjjjlmV/PmzfMTEhIYNGjQzhkzZtQ96qijMlJSUvJ79OixOz7oQ7pp06YFq1evDqvGrTKIJJG6FzcY1Y/AFFw/qctE5BpgEPA78GgkJxeRfsBoIB54SVUfKaTMn4F7vPPNVdULwz2+Ar4CAdRqpIwJmDcP7rsPVq50fZ+++w7OOSfWUZkqoKSkpzwE+kjt2rUr7uSTT0575JFHmo8aNWpjZmZm7jfffHNAs9qiRYtq1a1b15+SkuJPT0/PnTFjRt1jjz02p6JjBkhJSfG/8847KwH8fj+tW7c+PCMjY+/kyZMbtmnTZm8gERs4cOD27777rn5oIlW7dm1/Tk7Ovlajd999N2XTpk2JkyZNSgHYuHFj4vz585MOP/zwvUlJSf7c3FypXbu2AmzdujWhadOmBQAdO3bM+e9//1u3f//+u4qL97nnnksZPXr0QX2TUlNTcz/66KMVwetat259QA3UmjVrDqihCto3/5NPPvkFYMeOHXHTpk1r3LRpUx/Ao48+uv7RRx9dD3D22We369Sp077+a3v37o2rW7euP/R4lVXYTXuquh44BpgBXIm7a+8S4M/AJ8CJqhp2Nisi8cCzwBlAJnCBiGSGlEkDbgOOV9UuwA3hHt/FzL4+UpZImRovOxueeAKuusolUW3bwosvWhJlqoTk5GT/008/vWrMmDEt8vPzGTx48JaZM2cmT548ORlc5/P/+7//a/O3v/1tPcBtt922/qmnnjp03rx5SQA+n4/Qu91Cde/ePXft2rW1FixYkAQwfvz4JieeeOIBCchJJ520Z8aMGcnr16+P37t3r7z33nuF3um3efPm+NzcXAH4+9//3rRXr167UlJS/KmpqXlz5sypv2vXrji/388XX3yR3Llz59zQ/Tt16pS7dOnSJIB58+Yl7dmzJ37jxo3z1q5dO3/t2rXzhw0btn7cuHEpAEcfffSu559/PiVwHd57773Gp5566i6AW265Zf1tt93WatWqVQkAubm58tRTTx3U+XHIkCFbFy9evCj0EZpEAfTu3XvPypUray9evLhWbm6uTJo0KeXcc8/dHlpu3bp1CT5vDLpRo0YdesEFF2wG10y7fv36eIAZM2bUWbx4cd1BgwbtS9J//fXXpB49esQkAS6NiIapVNXVqjoASMENg3AM0ExVz1bVNRGeuxewXFVXqGoe8CYwIKTMNcCzqrrNO3+h89wUGS+6bxxB62xuarS5c+H88+HNN10t1BVXuCa9I46IdWTGhO3444/PycjIyBk7dmxK/fr1ddKkScsfeuihw1JTU7tmZmZ26dGjx57bbrttI8DRRx+d8+ijj66+4IIL2rdv375Lenp6lxUrVhR7W33dunX1+eefX3neeed1SE9Pz4yLi+Omm246oIN627Zt80eOHPn7Mccc0zkrKysjPT39oCQI4KeffqqdkZHRJTU1tevHH3/ccOzYsasB+vTps+fss8/e1q1bt86dOnXq4vf7ZcSIEZtC9+/fv/+Or776Khlg3LhxKWeeeeYBdxSef/752wK1U88999zq999/v3FGRkZmz549Ow8cOHBb4G6/v/zlLzsGDx68sW/fvp06duzY5fDDD8/cuXNnmaoWEhMTefLJJ1f169cvPS0trcvAgQO3ZmVl5QLccMMNh/373/9uCPDRRx8lt2/fvmtqamrXjRs3Jjz88MPrAPLy8uT444/P6NChQ5fBgwe3HTdu3IrERNeSt3r16oSkpCRt06ZNQZEBVDKiGl6CISJNVHVL1E4s8iegn6pe7S1fAhytqsOCykwGlgLH45r/7lHVjwo51mBgMECbNm16/vbbbwB8s2wTW35O4Z67lN69cxg9Ojla4RtTtSxdChdfDB07unGhOnWKdUSmkhGR2aqaFbxu7ty5K7t3716mO7xM6fXs2bPTxx9/vDzQHFYT3Hvvvc0bNGjgHz58eKV6382dO7dp9+7dUwvbFkmN1O8iMklEBohIRY3KlACkAScDFwAvikij0EKqOlZVs1Q1q1mzZkHrITA2mzXtmRpn/vz9z9PT4fnnYfx4S6KMqSIef/zxNb/88stBd8NVZ40aNfINGzasUiVRJYkkkZoE/MH7uU5EnhaRrBL2Kc5aDhzAs5W3LtgaYIqq5qvqr7jaqbRwT6AE+kip3bVnao4tW2DkSNd8N336/vU9etjItMZUIX369Nlz9NFHV5m+QtFw/fXXbwk081UVkXQ2vwA3RcxgYBHwf8AMEVkoIjeLyGERnnsmkCYi7USkFnA+7m7AYJNxtVGISFMgHTeCergx4/e5sdSsRspUe6rwwQduPKjPP4c6dQ4cJ8oYY0zURdrZfJeqvqyqvXGTFt8DJOKGPfhNRA7qv1TMsQqAYcDHwM/A26q6UETuE5H+XrGPgS0isgiYDtwcaT+tgn2dzSPZy5gqZt06uO461/9p50449lh4+20466xYR2aMMdVaqdMLVf0NuB+4X0QuAJ4DIprZVFWn4Qb6DF53V9BzBUZ4j1IpKLABOU019+OPLonKyYEGDeDGG+HMM21+PGOMqQClTqREpD5uDKlLgRNwtVsLohRXVCgEDX8Q01CMKT+dOkHjxnDcca5vVErY85kaY4wpo4jSCxERXIfzS3FjPtUBNgP/BMap6o9Rj7CMfNbZ3FQ3BQUwcSIMGAB167rH+PHQqFGsIzMm6latWpUwdOjQNnPnzq3boEEDX9OmTfPPPvvs7R988EGj6dOnL491fMaEnUiJyBPAhUALIB+YCowHpnn9nSofG9ncVDdLlsC997pxodasgZtvdustiTLVkN/vp3///h0vvPDCLVOnTl0B8P3339eZNGlSoxiHZsw+kXQ2HwGsBv4GHKqqf1LVKZU2ifL4/a6fiDXtmSotLw/++U+45BKXRB12GJx4YqyjMqZcTZ06NTkhIUFvueWWfSN/H3vssTm9e/fevWfPnvh+/fq1b9euXZf+/fu383uDBt50002Hdu3atXNaWlqXCy64oG1gfa9evToNGTKk5eGHH945NTW160cffVQf3HQlgwcPbpWWltYlPT0988EHH2wO8M0339Q96qijOnXp0qXzCSeckPbbb79VrXvyTYWJJL3IVNXF5RZJOVB0X41UFRuWwpj9fvrJTTK8apXrQH7BBTBkiGvSM6Yide3auchtN9+8jssu2w7AuHGNePzxQ4ssu2DBz+Gcbt68eXW6d++eXdi2n3/+uc5PP/20IjU1Nb9nz54Zn376af0//OEPu2+++eaNTzzxxDqAgQMHtnvzzTcbXnjhhTsACgoKZP78+T+/9dZbDe+7777D+vXrt/TJJ59stmrVqlqLFi1amJiYyIYNG+L37t0r1113XZsPPvhg+WGHHVbw4osvNr7ppptaTpw4cWU4cZuaJexEqqolUQGBRCouooEejKkkfvkFrrnG3Xrarh3ceSd06xbrqIyJucMPP3xPhw4d8gG6dOmSHRgB/MMPP0x+6qmnDsnNzY3bvn17QmZmZg6wA+C8887bBnDcccftufnmm2sBfPHFFw2uvfbaTYFBIFu0aOGbOXNm7WXLltXp06dPOrgmxmbNmuVX/Ks0VUGRiZSIXOo9naCqGrRcLFUdH5XIosRnNVKmKuvQAc44wzXlXXkl1KpRs0WYyibMmiQuu2z7vtqpMjj88MNzJk+e3LiwbUlJSfvuIIqPj6egoECys7PlxhtvbDtjxoxFHTt2zB8xYsRhubm5+75G165dWwESEhLw+XxFjg+iqtKxY8ecn376qUpWIJiKVVw9zavAv3ADbgYvv1rM41/RDrCs9nc2tzF1TBWwY4frTP5z0OfVvffCtddaEmVqnLPPPntXXl6ePPHEE00D62bMmFHnq6++ql9Y+ezs7DiAQw45pGDHjh1x//nPfwpNwoL17dt35wsvvNA0P99VOG3YsCG+W7duuVu3bk347LPP6gHs3btXZs2aVTsqL8pUO8U17Z0CoKp5wctViaqNI2WqCFX44gt49FHYuhVWroRXXnF9omxgTVNDxcXFMWXKlF+GDh3aevTo0YckJSVpq1at9p599tnbCyvftGlT30UXXbSpc+fOXZo1a1bQvXv3EudIGj58+KalS5cmZWRkdElISNDLLrts0+23377pzTff/OW6665rs2vXrnifzydDhgzZkJWVlRv1F2mqPHGDh1cfWVlZOmvWLAA+/3kDX73RnPffz+eWW3K56KIGMY7OmEJs3uwSqMAEw0ce6fpCtWkT27hMjSIis1X1gIno586du7J79+6bYxWTMZXF3Llzm3bv3j21sG1hd8EWkVdE5OhitvcSkVdKEV+5Ca6RsnGkTKWjClOmuEmGp093d+Hdeiu88IIlUcYYU0VEci/b5UCHYra3Ay4rUzTlINCf0Dqbm0pn2zZ48knYtctN7zJxIvzpT3aLqTHGVCHR7DlUDzfieaUS6GyekGD9TEwl4A0OSFycmxNv5EjXB6pfP+sLZYwxVVCxiZSItAFSg1ZliMhJhRRNAYYAlWbeo0DfL+tsbiqNFSvggQfgtNPcoJoAZ54Z25iMMcaUSUnpxRXA3YB6jzu8RygB/F75SkOEoJHN7du+iZGCAhg3Dl56CfLzYft21y/KsntjjKnySvpLPhlYiUuUXgHGAt+HlFFgNzBTVVdHOb5SC9yMuL9GyhIpEwM//+ymd1m2zC0PHAjXX29JlDHGVBPF/jVX1bnAXAARaQu8q6oLKiKwaNnfRyq2cZgaJj8fnnsOXnvN9Ytq2RLuuAN69Yp1ZMYYY6Iokrn27i3PQMqDNe2ZmImPhx9/dM8vusiNTF6nTmxjMsYYE3VF3mctIicFdywPLJf0qJiwSxYYZnT/FDExC8XUFHv2uFHJwd2Vd/fdbnTy4cMtiTKmDESk54ABA9oFlvPz82ncuHH3U045pWN5njc+Pr5nRkZGZlpaWpc+ffp03Lx5875Pkl9++SWxb9++Hdq2bdu1devWXa+44orWubm5+76xr1q1KuGss85q37p1665dunTp3Lt3747z5s1LCj3H7t275aijjupUEPiwAiZMmNBIRHr++OOP+6alWbJkSa20tLQuwfuOGDHisLvuuqtFJOeL1DvvvNMgNTW1a5s2bbrefvvthxRW5v7772+elpbWpWPHjl3uu+++5uFuK8+YiitT2Lbc3FzJysrqFJgqKBLFDVjzJTBdRGoFLxfzCGyvVKxpz1SI//4X/vxnuP/+/R30UlOha9eYhmVMdVCnTh3/kiVL6uzevVsA3nvvvQYtWrQo9+F2kpKS/IsXL160bNmyhY0aNSp4/PHHmwH4/X4GDhzYsX///tt/++23Bb/++uuCPXv2xF1//fUtA9v79+/f8aSTTtq1evXqBQsXLvz5kUceWfv7778fNKLhM88807R///7bEoI+pN58882UHj167B4/fnxKOHFGcr5IFBQUMHz48DbTpk1bunTp0oXvvvtuyuzZsw+Yc3DmzJm1x48f32zOnDk///zzzws/+uijRgsWLEgqaVthpk6dmnzuueemljWm4soUta127drau3fvnS+99FJY1zxYcenFlbiKncCbtVLdkRcu62xuytX27fDUUzBtmltu0gR274bk5JiGZUy0de1K5/I47oIF/FxyKTj11FN3TJw4sdEVV1yx7Y033kg599xzt3733Xf1AcaMGZPy3HPPtcjPz5cePXrsGT9+/G8JCQmceuqpHdatW1dr7969cddee+2Gm266afOSJUtqnXHGGWm9evXaPWvWrPotWrTI+/jjj5fXr1+/2PnSjjnmmD3z5s2rA/Cf//wnOSkpyX/99ddvAUhISOD5559f3b59+25PPPHE79OnT6+XkJCgt9xyy6bA/scee2xOYcd9++23m7z55psrAss7duyImzlzZv3PPvtsSf/+/dP+/ve//17StZk6dWpyuOeLxJdfflmvbdu2ezMzM/MABg0atPWdd95p1LNnz/WBMvPnz69z5JFH7k5OTvYDHH/88bvefPPNRg888MCG4raVZ0zFlSlu25/+9Kftt956a8shQ4ZsjSSmImukVPVVVR2n3oBM3vMSH6W5MOXJ+kiZcqEKn3zihjGYNg1q1YIbboB//cuSKGPKwSWXXLL1rbfeapydnS0///xz3WOPPXYPwJw5c2q/8847KbNmzVq8ePHiRXFxcfr88883Afj3v/+9cuHChT//9NNPi1544YUW69evjwdYtWpV7euuu27j8uXLFzZs2NA3fvz4xsWdu6CggOnTpycPHDhwO7jkoXv37tnBZVJSUvyHHnpo3qJFi5LmzZt30PbC5ObmyurVq5M6deqUF1j3+uuvNzr55JN3dOvWbW/jxo0Lvvnmm7olHSfc8wH07NmzU0ZGRmboY/LkyQf94Vq9enWtli1b7outVatWeWvXrq0VXOaII47I+d///pe8fv36+F27dsV9+umnDVevXl2rpG3BunXrlpGRkZE5dOjQtp999lmjQEzvvvvuQRPkhhNTcWWK23bUUUflzJs3r1441zFYtW/wsqY9E3V+v5sT74sv3HLPnjBqFLRuHdu4jClH4dYclZejjz46Z82aNUkvvvhiyqmnnrojsP6jjz5KXrBgQd3u3bt3BsjNzY1r3rx5AcCjjz7a4oMPPmgEsH79+sSFCxfWbtWqVX7Lli33HnfccTkARx55ZPbKlSsLbW7au3dvXEZGRuaGDRsSO3TokDtw4MCd0XxN69evT0hOTi4IXvf222+nXHfddRsBzj333K0TJkxIOfHEE7OliJkPilpflNmzZy8pbbyF6dGjR+7111+/vm/fvul16tTxd+nSJTve65Rc3LZg8+bNWwyuZu1f//pXk3fffXdlNGMMV0JCAomJibpt27a4xo0b+8PeL9yCItIL6K6qLwatGwA8gBvZfJyq3h5J0OVNEGvaM9EXF+eSpnr13JhQAwfa/HjGVIB+/fptv/vuu1t/8sknSzZu3JgAoKpy3nnnbXn22WfXBpedOnVq8ldffZU8a9asxcnJyf5evXp1ysnJiQOoVavWvma8+Ph4DawPFegjtWvXrriTTz457ZFHHmk+atSojV27ds2ZPHnyAbVYW7dujVu3bl2tzMzMvevXr08I3V6YevXq+fPy8vade8OGDfE//PBD8pIlS+oMGzYMn88nIqJ+v39NixYtCnbs2HFAFrJ169b4du3a7W3Tpk1eOOcDVyO1Z8+eg7KZRx55ZPXAgQN3Ba9r3br1AbU9a9asOaA2J2D48OGbhw8fvhlg2LBhLVu1apUXzrbSCCem4sqUtH9+fr7UrVu32GbeUJH89b8b6B9Y8KaPeQM4BNgBjBSRStePypr2TFSsXQs//bR/efBgN8nwoEGWRBlTQYYMGbL5pptu+r1Xr177+v/069dv59SpUxuvXbs2AVwysnTp0lrbt2+Pb9iwoS85Odn/448/1p47d27ETTYBycnJ/qeffnrVmDFjWuTn59O/f/9dubm5cf/85z+bgGv6Gzp0aOvzzjtvc3Jysv/ss8/elZeXJ0888UTTwDFmzJhR56OPPqoffNxmzZr5fD6fZGdnC8CECRMan3POOVt///33+WvXrp2/fv36ea1atcr7+OOP6zds2NDfvHnz/ClTpiQHXueXX37ZsE+fPrvDPR+4GqnFixcvCn2EJlEAvXv33rNy5craixcvrpWbmyuTJk1KOffcc7eHlgtc+2XLltX64IMPGl199dVbw9kW6qyzztpVUm1UODEVV6a4bevXr49v1KhRQVJSUrklUt2Bb4OWz8eNeH6EqmYCnwCDIzl5RQjUSFkiZUrF74fXX3d35N12G+zy/tYkJUHzqNzJa4wJU4cOHfJHjRq1MXhdz549c0eNGrW2b9++6enp6Zl9+vRJX716deK55567o6CgQNq3b9/l5ptvbtm9e/c9ZTn38ccfn5ORkZEzduzYlLi4OCZPnrx80qRJjdu2bdu1Xbt2XZOSkvxPP/30WoC4uDimTJnyyxdffNGgdevWXTt27Nhl5MiRLVu2bHnQnYYnnXTSjk8++aQ+wMSJE1MGDRq0LXj7gAEDtr322mspAOPGjfv1wQcfPDQjIyOzd+/enUaOHPl7ly5d9kZyvkgkJiby5JNPrurXr196Wlpal4EDB27NysrKBejdu3fHlStXJgL079+/Q4cOHbqcddZZHf/xj3+satq0qS9wjOK2BQT6SIU+CusjFU5MxZUpbtuHH37YILjZOFwSmNy3xIIiOcAQVX3VW/4cKFDVP3jLQ4D7VbVp0Ucpf1lZWTpr1ix8fuXrpZu4b0hztm7NY/r0OFJSrKOUicAvv7jpXRYudMt/+AOMHAkNDvq/bUyVJyKzVTUreN3cuXNXdu/efXOsYqoJvv3227pPPPFEi8mTJ/8a61hqutNPP73DE088saZbt257Q7fNnTu3affu3VML2y+SzGI70AJARJKAY4CHgrYrUOlGHbSmPROx/Hx3990rr7g3UPPmrjbqxBNjHZkxppo54YQTsmfNmrWzoKCABLsrKmZyc3Olf//+2wtLokoSyW/tJ+BqEfkMOAeoDXwctL0dUOqxIcqLdTY3EbvlFvjmG/d80CC47jqof1BXA2OMiYobbrhhS6xjqOlq166tw4YNK9XvIZJE6n5cP6j/4fpGfaqqs4K2nwXMKE0Q5SHQZGk1UiZif/4zrFzphjTo2TPW0RhjjKnEIpm0+DsR6QH8AXeX3puBbSLSBJdkvRf1CMvA73cPsBurTDFmzYJFi+DSS93ysce6O/Ksmt0YY0wJIvqkUNWlwNJC1m8BhkcrqGjx+wCU+HiIi7MaKRNi9254+mmYNAlEICsLMjPdNkuijDHGhCHiTwsRaQCcCrT3Vq3ANfMdNAZFrAX6RxUykKqp6b7+Gh5+GDZtcknTVVdBWlqsozLGGFPFRJRIicjVwJNAfVw/KXB36+0WkRGq+nKEx+sHjAbigZdU9ZEiyp0LvAMcFdIvq1h+nwsxPj6isbVMdbZtGzzxBHzs3SfRtSvcdRe0b1/8fsYYY0whIpkipj8wFlcDdSfgDa5DF+BvwFgR2aiq/wnzePHAs8BpwBpgpohMUdVFIeWSgespRUd2n9/NLWs1UmafZ55xSVTt2jB0KJx/vnWgM8YYU2qR1EjdAvwMHK2qu4PWfy4i/wJ+AEYCYSVSQC9guaquABCRN4EBwKKQcvcDjwI3RxArEGjaUxISrEaqRlN1faAA/u//XN+o66+Hli1jG5cxxpgqL5JEqjtwX0gSBYCq7hKRcbiaqnC1BFYHLa8Bjg4u4N0l2FpVPxCRIhMpERmMNz1NmzZt9q3fP4ZUBFGZ6sPvh8mTXQ3UmDGuarJJE3jssVhHZkyV9Ouvv9bNycmJ2l/UOnXqFLRr1y47WscDOO+881I///zzhk2aNClYtmzZwpL3cDZv3hz/0ksvpdx6662bCts+YsSIw+rXr++77777whovMdLypuqKpE2jpNveolrtIyJxwFPAjSWVVdWxqpqlqlnNmjXbt963r49UNCMzVcKqVXDttfDQQzB7NkyfHuuIjKnycnJyEurVq1cQrUekSdnUqVOTzz333NTiylx55ZWbp0yZsizS17Zly5b4l19+2SbQNBGLJJGaC1wuIgfNoC0i9YHLvTLhWgu0Dlpu5a0LSAa6Al+KyErclDRTROSAuaCK499315417dUYPh9MmOD6Ps2ZA40bu7vz+vaNdWTGmApwxhln7G7WrFlBcWV27twZd/LJJ3fs1KlTZlpaWpcXX3yx8Y033thq9erVSRkZGZl//etfWwGMHDnykNTU1K49e/bstGzZsqSSzl1c+TFjxqQcfvjhnTMyMjIvvPDCtgUFBQwdOrTlww8/vO/b/4gRIw676667WpT2tZvYiOTbwOPAJGCOiDzN/r5Mgc7mHYFBERxvJpAmIu1wCdT5wIWBjaq6A9g3AbKIfAncFMldezb8QQ2zfLmbZHiR99Y880y48UZo2DC2cRljyqRbt24ZeXl5cdnZ2XE7duxIyMjIyAR48MEH15x77rk7Iz3epEmTGhxyyCH5X3755XJwtVEnnXTSnrPOOqvO4sWLFwF88803dd97772U+fPnL8rPz+eII47IPPLII4tshiyu/Jw5c2q/8847KbNmzVqclJSkF198cZvnn3++yUUXXbT1hhtuaHPbbbdtAnj//fcbf/zxxweN1Wgqt0hGNp8sIsNwHb+fYX9TngB7gGGq+n4ExyvwjvcxbviDV1R1oYjcB8xS1SnhHqvQ4wO+AkEV62xeU8yd65KoFi3gjjvguONiHZExJgrmzZu3GFzT3r/+9a8m77777sqyHK9Hjx45d9xxR+shQ4a0HDBgwI5+/frt3rx58wFfuadPn17/zDPP3J6cnOwHOP3007cXd8ziyn/00UfJCxYsqNu9e/fOALm5uXHNmzcvGDZs2JYtW7YkrFy5MnHdunUJDRs29HXs2DG/LK/NVLxIRzYfIyKv44YsaOetDgzIuSPSk6vqNGBayLq7iih7cqTH9wWNbG6qqR079tc4nXMO5ObCwIFQ76AWaGOMAaBbt25758yZs+jdd99teOedd7b87LPPdl5zzTXlNnGwqsp555235dlnn10buq1///7bXnvttcbr169PHDRo0NbyisGUnxL7SIlIgoicKyIjReQqIEFVJ6rqY97jndIkURXBmvaqsZwceOop6N8ffv/drYuLg4susiTKmGrqrLPO2lXW2iiAlStXJiYnJ/uHDh26dcSIEet/+umnug0bNvTt2bNn32dinz59dk+bNq3R7t27Zdu2bXGffvppo+KOWVz5fv367Zw6dWrjtWvXJgBs2LAhfunSpbUALr744q3vvvtuytSpUxtfcskl28r62kzFK7ZGSkQaA1/iOn0LrsXsMRE5XVVnl394ZWMjm1dT//sfPPCAS6Di4txdeYcdFuuojKn26tSpU7Bnz56oDn8QTrlAH6nQ9YX1kTr77LPb/fDDD8nbtm1LaNGiRbdbb7319+HDh28OLjN79uw6t912W6u4uDgSEhJ0zJgxvx1yyCG+nj177k5LS+vSp0+fHS+88MKac845Z2vXrl27NGnSJL9bt257Avv37t2747hx435LTU3d1wx3wgknZBdVvmfPnrmjRo1a27dv33S/309iYqI+/fTTq9LT0/OysrJy9+zZE9eiRYu8tm3b5hd3DlM5iWrRSYaIPImbjHgqri9TOnAtsEBVe1ZIhBHKysrSWbNmke/z8/J72xn7UEMyM3N57TWrpajydu2C0aPd2FDg5sa78879Ew0bY0pNRGar6gF3Rc+dO3dl9+7dNxe1jzE1xdy5c5t27949tbBtJX2zOBv4SFX7B1Z4QxE8ISKtVHVN1KIsB74CvM7msY7ElNmsWTBqFGzeDImJcM01cOml9ss1xhgTUyX1kWpNSGdw3BQwArQtl4iiyO93rZHWtFcNNGrkJhzu1g1efx2uvNKSKGOMMTFX0idREhB6F8G2oG2Vms9rfbfO5lWQqquFyspy8+R17AgvvQRdutgkw8ZUHL/f75e4uDj7NmpqLL+rlfEXtb0sn0iV/j+Wz3vZVnFRxaxf7yYVHjIEPv98//rDD7ckypiKtWDTpk0NvQ8SY2ocv98vmzZtaggsKKpMOCnGjSJyftByIi6JelBEQjshqqoOiDzU8uErsLv2qhS/H959F555BrKzITnZrTPGxERBQcHV69evf2n9+vVdKdsXb2OqKj+woKCg4OqiCoSTSB3pPUIdU8i6SpWx+HzW2bzKWLUK7r8ffvzRLZ9yCowcCU2bFr+fMabc9OzZcyPQv8SCxtRgxaYYqlqlv4EEKjOsRqqSmzULrrsO8vIgJQVuvRX69Il1VMYYY0yJqnVdjWvasyliKr2uXd38eN27w4gR0KBBrCMyxhhjwlK9EymbIqZyysuDf/8b/vxnN51L7dowYQLUrx/ryIwxxpiIVNtESnX/8AcJCda0V2nMmwf33QcrV7q78267za23JMoYY0wVVG0TKQCfT1C1GqlKITsbnn0W3n7bZblt28IZZ8Q6KmOMMaZMqnUi5bemvcrhhx/gwQdh3To3DtTll7spXmrVinVkxhhjTJlU60TK9ZGyKWJiatkyGDbMPU9Ph7vvhk6dYhuTMcYYEyXVOpEKDMZrNVIxlJYGAwZAq1ZwySU2qJcxxphqJeJPNRFJBU4FWgD/VtWVIlILOARYr6p50Q2x9AJ37dlndwXasgUef9wlTV26uHV33hnbmIwxxphyElGKISKPAiOAeNwo5t8DK4HawCJgFPCPqEZYBoEpYiyRqgCqMHUq/P3vsHMnbNwIL7/sJhw2xhhjqqmwRy4Xkb8CNwPPAqcD+z4hVXUnMAU4O9oBlkVgZPPERPswL1e//w5/+xvce69Loo491nUutyTKGGNMNRdJXc1Q4D1VvUFEmhSyfR4wLDphRcf+caRiG0e15ffDxInwz39CTo4bkfzGG+HMMy2JMsYYUyNEkmKkA88Vs30TUKlmmLXO5uVs+3Z4/nmXRPXt6yYZTkmJdVTGGGNMhYkkkcoF6hWzvS2wvUzRRFmgRsqa9qKooMDVNsXHu6Tp9tvdc5tk2BhjTA0Udh8p4H/AOYVtEJHawCXAf6MRVDQoanftRduSJXDppW6evIDTTrMkyhhjTI0VSSL1OHCsiEwAunnrDhGRPwBfAq2AJ6IbXtn4fNa0FxV797p+UJdcAkuXurvzAlmqMcYYU4OFXVejqp+JyBBgNHCht3qC9zMPuEZVv49yfGVinc2j4Kef3CTDq1a5Jr0LLoAhQyw7NcYYY4hwHClVHSsiU4DzgAzcEAjLgLdVdW05xFcmNvxBGeTlwT/+4SYZBmjXzg2s2a1bsbsZY4wxNUnEdTWquh54phxiibpAjZRVnpRCQoLrExUf7yYZvuoqm2TYGGOMCVGtG70CfaSsRipMO3ZAfj40bQpxcW6C4dxcN9mwMcYYYw4SdiIlIl+EUUxVtW8Z4okqv921Fx5V+PxzeOwxyMiA0aNdf6g2bWIdmTHGGFOpRZJitMfNrxe6/6G4u/82A3uiFFdU7B/+wGqkirR5MzzyCHz5pVvOzYXsbKhX3JBhxhhjjIHI7tpLLWy9iCThJjK+AugdnbCiw+e3pr0iqcJ//gNPPQW7d0PdunD99XDOOa5ZzxhjjDElKnOjl6ruBR4WkUzgKeCCMkcVJTb8QRH8frjhBvjuO7d83HFwxx3QokVMwzLGGGOqmmimGN8CD0fxeGWiur9pz2qkQsTFub5QCxfCTTdBv342ybAxxhhTCtFsw2kHRHR/vIj0E5ElIrJcRG4tZPsIEVkkIvNE5HMRaRvJ8f3WR2q/FStg5sz9y1dfDRMnwhlnWBJljDHGlFIkd+0VdQtXCnAqcB1uqphwjxcPPAucBqwBZorIFFVdFFTsRyBLVbO9UdUfA/4S7jkCwx/U6Ka9/HwYNw5efhmSk+Gdd6BBAzcmVEpKrKMzxhhjqrRIUoyVHHzXXoAAS3DJVLh6ActVdQWAiLwJDAD2JVKqOj2o/A/AxREcf18fqRrbtLdoEdx/Pyxb5pZ797aO5MYYY0wURZJI3cfBiZQCW4GlwGeq6o/geC2B1UHLa4Cjiyl/FfBhYRtEZDAwGKBN0NhHgT5SNW5k87174YUX4LXXXMfyli1h1Cg46qhYR2aMMcZUK5EMf3BPOcZRLBG5GMiiiOEVVHUsMBYgKytrX7JXY0c2v+km+P57V/t00UVw7bVQp06sozLGGGOqnbASKRGpD8wFnlHVf0Tp3GuB1kHLrbx1oec+FbgD6O0NtRC2GtvZ/JJLYONGN8lw166xjsYYY4yptsLqMKOqu4EmwO4onnsmkCYi7USkFnA+MCW4gIgcCbwA9FfVjZGeoMYMf/Dtt/Dii/uXe/WCN96wJMoYY4wpZ5H0kfoB17z2UjROrKoFIjIM+BiIB15R1YUich8wS1WnAI8D9YGJ4m7RX6Wq/cM9h88nxKHVN5Havh2efBI+9LqOHX88ZGa659ap3BhjjCl3kSRStwJfiMgM4FVVLeoOvrCp6jRgWsi6u4Ken1qW4/t9rsqt2jXtqcKnn7pJhrdvh6QkGDLEDbJpjDHGmApTbCLljR21SVVzcNO/bMPVSD0mIr8A2SG7qKr2LZdIIxQY2Twhrpo17W3c6CYZ/vprt9yzp7sjr3Xr4vczxhhjTNSVVCP1K27spjeA9rjhDlZ52yr1xGyB+jIRiI+vRonUCy+4JKpePTdf3sCBNjK5McYYEyMlJVLiPVDV1HKPJooKCgCtJmNI+f37+zwNG+ZGKx82DJo3j21cxhhjTA1XbXskFxS46rOEhDJ35Yodvx/+/W+46iqXPAE0bgz33WdJlDHGGFMJVNtZ6Aq86WGqbI3UL7+4hGnhQrf8zTfQp09sYzLGGGPMAcJJpE4UkUhGQB9fhniipsomUvn58K9/wSuvuBfRvDncdhuceGKsIzPGGGNMiHASpH3z2JVAcK1plSKR2j/PXhVq2lu0CO6919VGAQwaBNddB/XrxzYuY4wxxhQqnERqLG4wzirF1Uhp1aqRWrrUJVGtWrnpXXr2jHVExhhjjClGOInUN6r6erlHEmWBpr1K39l882Zo2tQ9HzDABX7WWVC7dmzjMsYYY0yJqu1de34flXv4g9274cEH3ThQa9a4dSLwpz9ZEmWMMcZUEdU2karUnc2//hrOOw/ee88FumBBrCMyxhhjTClU2+EPfL5KOI7Utm3w+OPwySduuWtXuOsuaN8+tnEZY4wxplSKTaRUtcrWWFW6GqkffoA77oAdO1zT3dChcP75+0csN8YYY0yVU21rpPZ3No9tHPs0bw7Z2dCrl0uoWraMdUTGGGOMKaPKkmZEnc/vfsZsHCm/H/77XzjhBNeJvH17GDcO0tJskmFjjDGmmqi27UpuaroYjSO1ahVcey0MHw4ff7x/fXq6JVHGGGNMNVJ9a6RiMY6Uz+cmGX7+ecjLcxMM21AGxhhjTLVVbROpggJAK7Av97JlcP/9bpoXgDPPhBtvhIYNKygAY4wxxlS06p1IUUGdzWfMcHPi+XzQooXrTH7ccRVwYmOMMcbEUrVNpALjSFVIZ/MjjnDz4/XqBcOGQb165X9OY4wxxsRctU2kynUcqZwcePVVuPhiSE6GpCTXN8r6QxljjDE1SrVNpHw+9zPqTXv/+x888AD8/jts3eqa8cCSKGOMMaYGqraJlKuR0ug17e3aBf/4B7z/vltOT4dBg6JzbGOMMcZUSdU2kQrUSEWlae/LL+GRR2DzZkhMhGuugUsvrUTDphtjjDEmFqptJuDzuYEvy5zrLF0KN93knnfr5iYZTk0t40GNMcYYUx1U20Rqf2fzMjbtpafDn/8MbdvCeefZJMPGGGOM2afaZgUFPgUtRY3Uhg1uapd58/avu+UW+MtfLIkyxhhjzAGsRirA74d334VnnoHsbNixA155pfwCNMYYY0yVV20TKV9BYEDOMAqvWuWmd/nxR7fcpw+MHFme4RljjDGmGqi2iVRYA3L6fPDaa/DCC26S4ZQUuPVWl0gZY4wxxpSg2idSxfaR2rkTxo1zSdRZZ8GIEdCgQYXEZ4wxxpiqr9omUkWObJ6X5zqNJyRA48Zw551uVPJjj63wGI0xxhhTtVXb29D210jJ/pXz5sGFF8KECfvXnXKKJVHGGGOMKZXqm0gF10hlZ8Pjj8NVV8HKlfDpp/urrIwxxhhjSimmiZSI9BORJSKyXERuLWR7koi85W2fISKp4R7b7+VJSb/94saAeustEIErr4RXX43S3DHGGGOMqclilkiJSDzwLHAGkAlcICKZIcWuArapakfg78Cj4R6/INdH7c0bqffaOFi3Djp1cnfoDR0KtWpF62UYY4wxpgaLZY1UL2C5qq5Q1TzgTWBASJkBwDjv+TtAXxERwlCg8cTl55OQKDBsmLs7Lz09asEbY4wxxsTyrr2WwOqg5TXA0UWVUdUCEdkBNAE2BxcSkcHAYIA2bdoA0L1bHNmDGnHIgBEwoE25vABjjDHG1GzVYvgDVR0LjAXIyspSgAsvFC68sAku7zLGGGOMib5YJlJrgdZBy628dYWVWSMiCUBDYEtxB509e/ZmEfnNW2xKSO1VDWXXwbHrYNcgwK6DE3wd2sYyEGOqqlgmUjOBNBFph0uYzgcuDCkzBbgM+B74E/CFqhY7C7GqNgs8F5FZqpoV1airILsOjl0HuwYBdh0cuw7GlF3MEimvz9Mw4GMgHnhFVReKyH3ALFWdArwMTBCR5cBWXLJljDHGGFMpxLSPlKpOA6aFrLsr6HkucF5Fx2WMMcYYE45qO7K5Z2ysA6gk7Do4dh3sGgTYdXDsOhhTRlJClyNjjDHGGFOE6l4jZYwxxhhTbiyRMsYYY4wppWqRSJXn5MdVSRjXYYSILBKReSLyuYhUu3FjSroGQeXOFREVkWp563c410FE/uy9HxaKyOsVHWNFCOP/RBsRmS4iP3r/L86MRZzlSUReEZGNIrKgiO0iIk9712ieiPSo6BiNqcqqfCJV3pMfVxVhXocfgSxV7Yabu/Cxio2yfIV5DRCRZOB6YEbFRlgxwrkOIpIG3AYcr6pdgBsqOs7yFub7YRTwtqoeiRteZUzFRlkhXgX6FbP9DCDNewwGnquAmIypNqp8IkU5T35chZR4HVR1uqpme4s/4EaTr07CeS8A3I9LpnMrMrgKFM51uAZ4VlW3AajqxgqOsSKEcx0UaOA9bwj8XoHxVQhV/Ro3Dl9RBgDj1fkBaCQih1ZMdMZUfdUhkSps8uOWRZVR1QIgMPlxdRLOdQh2FfBhuUZU8Uq8Bl6zRWtV/aAiA6tg4bwX0oF0EfmviPwgIsXVWFRV4VyHe4CLRWQNbky7v1VMaJVKpH87jDFBqsWkxSYyInIxkAX0jnUsFUlE4oCngMtjHEplkIBryjkZVzP5tYgcrqrbYxlUDFwAvKqqT4rIsbiZFLqqqj/WgRljqobqUCMVyeTHhDv5cRUUznVARE4F7gD6q+reCoqtopR0DZKBrsCXIrISOAaYUg07nIfzXlgDTFHVfFX9FViKS6yqk3Cuw1XA2wCq+j1QGzeRb00S1t8OY0zhqkMitW/yYxGpheswOiWkTGDyYwhz8uMqqMTrICJHAi/gkqjq2Cem2GugqjtUtamqpqpqKq6fWH9VnRWbcMtNOP8nJuNqoxCRprimvhUVGGNFCOc6rAL6AohIZ1witalCo4y9KcCl3t17xwA7VHVdrIMypqqo8k17NvmxE+Z1eByoD0z0+tqvUtX+MQs6ysK8BtVemNfhY+B0EVkE+ICbVbVa1dKGeR1uBF4UkeG4jueXV7cvWSLyBi5pbur1BbsbSARQ1edxfcPOBJYD2cAVsYnUmKrJpogxxhhjjCml6tC0Z4wxxhgTE5ZIGWOMMcaUkiVSxhhjjDGlZImUMcYYY0wpWSJljDHGGFNKlkiZCici94iIikhqrGOpSJG+bhG53Ct/crkGZowxptQskTIlEpGTvQ/0oh7HxDrGcIlIaiHxZ4vIAhG5W0TqVHA8J3sJVqOKPG+4ROTLkGuVLyK/i8hbItK1jMceKCL3RClUY4yJiSo/IKepUG/gBu8LtbyiA4mCT4Hx3vNmwF9wE9geB/yhnM75APAIEDw1z8m4ARJfBbaHlJ8AvAnklVM84doLXO09rwP0xA3aeKaIZKnqklIedyBuxoF7yhqgMcbEiiVSJhJzVPW1WAcRJUuDX4uIPIObUuR0ETlKVWdG+4SqWgAURFDehxt1PNYKQn7vL3ojoo8GhgF/i01YxhgTe9a0Z6JCRHqJyKsistRrKtslIv8VkXPC3D9FRP4uIr+ISK6IbBGR2SJycyFl/yIi33rnyBaRGSLyp7LE7yU5n3uLHYPOdbWIzBGRHBHZISKfiMgJhcT0RxH5SkQ2e2VXicgkEUkPKnNAHykReRVXGwXwa1Dz2T3e9gP6SInIGd7ydYW9BhH5XkQ2iUhi0Lo0EZkgIutEJE9EVorI4yJSr9QXywlcqwMmOg73fSAiX+LNfxnSdHh5UJlDReQ571rmeU2KY0WkeRljN8aYqLEaKROJuuImuA22V1V3AecAGcDbwG9AE9wH5SQRuUhVXy/h2BOBk4DngXm4JqTOuKavxwOFROQB4A7gI+BOwO+de6KIDFPVZ8vw+gJJwWbvXI8CtwD/A24HkoHBwHQRGaCq07xyvXETvy4AHsY10R0GnIpLypYWcb4XgAZe/MMD5/Vef2E+AdYDlwJPB28QkTTgGOBpVc331vUEvvDieQFYC3QHrgOOF5HegbKl0MH7uTVkfbjvgwdxX+ROBC4J2v87L/Y2wPdALdxcmb/gruUQ4BSvSXFHKWM3xpjoUVV72KPYBy6Z0SIeb3pl6hWyX11gCbAoZP093r6p3nJDb3lMCXH08Mo9VMi2ycBOILmEY6R6x3gJaOo9OuP6LynwK5AEdMIlad8CtYL2PwyXmKwE4r11T3n7Ni/h3Ae87qLWBW273Nt2ctC6x711mSFl7/fW9whaNxdYHHpNcMlOYILekn73XwK7g65Va1zfppXeMc4MKR/J++BV9yeo0PO+D2wEWoWsz8I1j94T6/8X9rCHPeyhqta0ZyIyFjgt5PEAgKruCRQSkboi0gT3AfoF0FlEGhRz3Bxch+ajpfihAS7CfXiPE5GmwQ9cjVAycGyYr+UqYJP3WISr5foaOF1V9wIDAAEeU9V9nb1V9XfgX0Bb4EhvdaBm5FwRKe9a3nHez0sDK0REgIuBBao6x1t3ONANeB1ICrlW3wJ7gNPDPGc99l+rVcB7uJqiy9SrlQso4/sgsF9D4Czc7zQ3JPaVuJsbwo3dGGPKlTXtmUgsU9XPCtvg9Vt5AJeAFNaHpRGuxuggqponIjfgOi//6nVk/gKYrKqfBxXtjEtuFhcTY4sSXkPA+8A/cYlZLrBcVTcEbW/n/VxYyL6Bde2BWd5xBgBjgEdF5Ftc0+MbqropzHjCoqoLRGQOcJGI3K6qflyTaCquGTKgs/fzXu9RmHCvVS5wtvc8BZfEnUYhfSzL8j4I0sk79lXeozArSgraGGMqgiVSpsy8GpFPcB/eo3HJxQ7cHWdXABdSwo0Nqvq8iLwP/BHoDfwJGCYib6nq+YFT4RKfMyj6brbCEp/CrCkqKYyUqm4RkaNw/X1OwyU2fwfuFZEzVfX7aJwnyHjgH0Af4DNcYuMDgu+sE+/nk7ikrjDbwjyfL/haicg7wFRgrIjMUdV53voyvw9CYn+N/TVwoXLCjN0YY8qVJVImGrrhOjHfp6p3B28QkasL3+VgqroO13fpJRGJx42jdIGIPKluOIJlQD9glar+HLXoCxeo8eiC6+gcLDOkDOqGKvjSeyAi3YDZwChcclgULUVsr+P6Sl0qIv/FJZ2fetcvYJn30xethDFAVf0icj2uSfQJ9jezRfo+KOq1L/e21Yp27MYYE23WR8pEQ6B2SIJXihv5usThD7y+NHWD13mJSeDutRTv5wTv50NeohV6nHCbqsIxBfdhfnPIcAKH4mpXfgN+9NaF3skIrvkxh/2xF2W397Okcvt4zYUfAoNw/cYacHDNzY+4uwivFZH2occQkQQRCfuchcSwDJfQnRY0HESk74Pd3vYD4lDVLbiBXwdJIaPmi9OstLEbY0w0WY2UiYafcU1qt3gJ0RIgHfgrMB83EnZx0oGvROQ93If/Nlzz0BDcXXTfAKjqTG+MpXuAn0RkIvA7cKh3jjNxnaDLTFWXiMjjuH5HX4vIW+wf/qA+cJGX7IEboLIVrlnrN9zQDX/xyo8/6OAH+sH7+aiI/BvXH2mBqi4oYb9xQH9c090O3F2LwfGriFyC62s2T0Rewf2O6uKGERgE3Ia7c660HsJ1cr8X6Evk74MfcAN6jhGRD4B8YIaq/or73X+Lu/bjcYlhHK5f2gDcdb2nDLEbY0xUWCJlykxVfSLyR1wzz2W4u7wWeM+7U3IitRp4BTgFd2t9Em7MoxeBR1U1O+hc94rILNxYSDd459rona/QgSpLS1VHishyYChuapc8YAZwoap+E1R0Am6ogstw083sxDV7/UlV3y3hHP8VkZHAtbjXm4BLTEpKpKbixnBKAV5S1dxCjv2TiByJS5j6e+fYhbvz7VX2D6pZKl6y+TZwvjcm1VcRvg/ewN35eD5wHi5RugL4VVVXe+NgjcQlThfjkszVwH9w41QZY0zMiWppumgYY4wxxhjrI2WMMcYYU0qWSBljjDHGlJIlUsYYY4wxpWSJlDHGGGNMKVkiZYwxxhhTSpZIGWOMMcaUkiVSxhhjjDGlZImUMcYYY0wpWSJljDHGGFNK/w8Rh8Oyx3TZOgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#test MWPM decoder for this fold\n",
    "#labels = targets[train], features = inputs[train]\n",
    "# x_test_d7 = translate_to_graph(testData_d7_MWPM, targs, mlb_d7)\n",
    "\"\"\"\n",
    "decoding_d7, time_mwpm = do_new_decoding(x_test_d7, 7, 0)\n",
    "decoding_d7['combine'] = decoding_d7[[0, 1]].values.tolist()\n",
    "decoding_d7['combine'].apply(lambda x: x[0].extend(x[1]))\n",
    "decoding_d7 = np.array(decoding_d7[0])\n",
    "\n",
    "time_per_fold_mwpm.append(time_mwpm)\n",
    "\n",
    "pred_mwpm = mlb_d7.transform(decoding_d7)\n",
    "\n",
    "if fold_no < 3:\n",
    "    acc = partial_accuracy(targets_test_2, pred_mwpm)\n",
    "else:\n",
    "    acc, contingency_mwpm = partial_accuracy_and_contingency(targets_test_2, pred_mwpm, mlb_d7)\"\"\"\n",
    "\n",
    "#acc_per_fold_mwpm.append(acc)\n",
    "#f1_per_fold_mwpm.append(f1_score(targets_test_2, pred_mwpm, average='micro'))\n",
    "\n",
    "#####################################################################################################\n",
    "#test the plut decoder for this fold\n",
    "\n",
    "#lookup_d7 = lookup_decoder(7)\n",
    "\n",
    "#lookup_d7 = train_plut(lookup_d7, inputs_train, targets[train])\n",
    "\n",
    "#start = time.time_ns()\n",
    "#pred_plut_d7 = test_plut(lookup_d7, inputs_test)\n",
    "#end = time.time_ns() \n",
    "#time_per_fold_plut.append((end - start)/ (10 ** 9))\n",
    "\n",
    "pred_plut_d7 = test_plut(lookup_d7, inputs_test_2)\n",
    "#f1 = f1_score(targets_test_2, pred_plut_d7, average='micro')\n",
    "acc, contingency_plut = partial_accuracy_and_contingency(targets_test_2, pred_plut_d7, mlb_d7)\n",
    "\n",
    "#acc_per_fold_plut.append(acc)\n",
    "#f1_per_fold_plut.append(f1)\n",
    "\n",
    "#####################################################################################################\n",
    "#Test the NN decoder for this fold\n",
    "\"\"\"\n",
    "model_d7 = compile_FFNN_model_DepthSeven(7)\n",
    "\n",
    "# Generate a print\n",
    "print('------------------------------------------------------------------------')\n",
    "print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "# Fit data to model\n",
    "history = model_d7.fit(\n",
    "    x=inputs_train ,\n",
    "    y=targets[train],\n",
    "    validation_split=.25,\n",
    "    epochs= 150)\"\"\"\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(targets[test].ravel(), predictions_d7.ravel())\n",
    "tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "tprs[-1][0] = 0.0\n",
    "roc_auc = auc(fpr, tpr)\n",
    "aucs.append(roc_auc)\n",
    "plt.plot(fpr, tpr, lw=1, alpha=0.3, label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
    "\n",
    "#get the AUCs of each class, used to get average AUC of each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(targets[test][:, i], predictions_d7[:, i]) \n",
    "    aucs_classes[mlb_d7.classes_[i]].append(auc(fpr[i], tpr[i]))\n",
    "\n",
    "#########################################################################################################\n",
    "#compute McNemar's statistic on results of last fold\n",
    "\n",
    "mcnemar_results_mwpm = {}\n",
    "mcnemar_results_plut = {}\n",
    "\n",
    "for class_ in mlb_d7.classes_:\n",
    "    \n",
    "    #compute the x^2 for NN and MWPM\n",
    "    mcnemar_results_mwpm[class_]=contingency_table_and_t(contingency_nn[class_], contingency_mwpm[class_])[1]    \n",
    "    #comput the x^2 for NN and PLUT\n",
    "    mcnemar_results_plut[class_] = contingency_table_and_t(contingency_nn[class_], contingency_plut[class_])[1]\n",
    "        \n",
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds of MWPM:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold_mwpm)} (+- {np.std(acc_per_fold_mwpm)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold_mwpm)}(+- {np.std(f1_per_fold_mwpm)})')\n",
    "print(f'> Time: {np.mean(time_per_fold_mwpm)} (+- {np.std(time_per_fold_mwpm)})')\n",
    "print('Average scores for all folds of PLUT:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold_plut)} (+- {np.std(acc_per_fold_plut)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold_plut)}(+- {np.std(f1_per_fold_plut)})')\n",
    "print(f'> Time: {np.mean(time_per_fold_plut)} (+- {np.std(time_per_fold_plut)})')\n",
    "print('Average scores for all folds of NN:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold)}(+- {np.std(f1_per_fold)})')\n",
    "print(f'> Time: {np.mean(time_per_fold)} (+- {np.std(time_per_fold)})')\n",
    "for key in aucs_classes:\n",
    "    cleanedList = [x for x in aucs_classes[key] if str(x) != 'nan']\n",
    "    print(f'> AUC for class {key}: {np.mean(cleanedList)} (+- {np.std(cleanedList)})') #this has to be a for loop\n",
    "    print(\"X^2 for MWPM and NN: \" + str(mcnemar_results_mwpm[key]))  \n",
    "    print(\"X^2 for PLUT and NN: \" + str(mcnemar_results_plut[key]))\n",
    "print(\"###################################################################################\")\n",
    "print(\"TOTAL F1 NN: \" + str(f1_per_fold))\n",
    "print(\"TOTAL F1 PLUT: \" + str(f1_per_fold_plut))\n",
    "print(\"TOTAL F1 MWPM: \" + str(f1_per_fold_mwpm))\n",
    "print(\"TOTAL ACC NN: \" + str(acc_per_fold))\n",
    "print(\"TOTAL ACC PLUT: \" + str(acc_per_fold_plut))\n",
    "print(\"TOTAL ACC MWPM: \" + str(acc_per_fold_mwpm))\n",
    "print(\"TOTAL TIME NN: \" + str(time_per_fold))\n",
    "print(\"TOTAL TIME PLUT: \" + str(time_per_fold_plut))\n",
    "print(\"TOTAL TIME MWPM: \" + str(time_per_fold_mwpm))\n",
    "print('------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "         label='Chance', alpha=.8)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "         label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "         lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                 label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "plt.xlim([-0.01, 1.01])\n",
    "plt.ylim([-0.01, 1.01])\n",
    "plt.xlabel('False Positive Rate',fontsize=18)\n",
    "plt.ylabel('True Positive Rate',fontsize=18)\n",
    "plt.title('Cross-Validation ROC of Depth 7 NN',fontsize=18)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp/ipykernel_14152/1456530209.py:74: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "  tprs.append(interp(mean_fpr, fpr, tpr))\n",
      "C:\\Users\\User\\anaconda3\\envs\\research\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:806: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20000, 0]\n",
      "[0, 0]\n",
      "both b and c are zero\n",
      "[20000, 0]\n",
      "[0, 0]\n",
      "both b and c are zero\n",
      "[18647, 518]\n",
      "[737, 98]\n",
      "[18801, 364]\n",
      "[91, 744]\n",
      "[19303, 325]\n",
      "[227, 145]\n",
      "[19123, 505]\n",
      "[121, 251]\n",
      "[19654, 208]\n",
      "[84, 54]\n",
      "[19227, 635]\n",
      "[39, 99]\n",
      "[19783, 112]\n",
      "[43, 62]\n",
      "[19278, 617]\n",
      "[47, 58]\n",
      "[19659, 199]\n",
      "[68, 74]\n",
      "[19227, 631]\n",
      "[80, 62]\n",
      "[19562, 292]\n",
      "[64, 82]\n",
      "[19219, 635]\n",
      "[55, 91]\n",
      "[18183, 1246]\n",
      "[375, 196]\n",
      "[18963, 466]\n",
      "[96, 475]\n",
      "[18365, 1061]\n",
      "[358, 216]\n",
      "[18946, 480]\n",
      "[82, 492]\n",
      "[19482, 284]\n",
      "[127, 107]\n",
      "[19164, 602]\n",
      "[58, 176]\n",
      "[19631, 205]\n",
      "[75, 89]\n",
      "[19218, 618]\n",
      "[53, 111]\n",
      "[19642, 201]\n",
      "[73, 84]\n",
      "[19179, 664]\n",
      "[53, 104]\n",
      "[19644, 179]\n",
      "[80, 97]\n",
      "[19185, 638]\n",
      "[89, 88]\n",
      "[19363, 450]\n",
      "[85, 102]\n",
      "[19171, 642]\n",
      "[62, 125]\n",
      "[18992, 452]\n",
      "[204, 352]\n",
      "[18943, 501]\n",
      "[148, 408]\n",
      "[19025, 393]\n",
      "[146, 436]\n",
      "[18950, 468]\n",
      "[119, 463]\n",
      "[19327, 504]\n",
      "[91, 78]\n",
      "[19200, 631]\n",
      "[60, 109]\n",
      "[19629, 224]\n",
      "[65, 82]\n",
      "[19274, 579]\n",
      "[75, 72]\n",
      "[19604, 238]\n",
      "[77, 81]\n",
      "[19224, 618]\n",
      "[65, 93]\n",
      "[19526, 332]\n",
      "[70, 72]\n",
      "[19225, 633]\n",
      "[62, 80]\n",
      "[19485, 320]\n",
      "[89, 106]\n",
      "[19172, 633]\n",
      "[76, 119]\n",
      "[18393, 1033]\n",
      "[363, 211]\n",
      "[18954, 472]\n",
      "[100, 474]\n",
      "[18521, 939]\n",
      "[244, 296]\n",
      "[18939, 521]\n",
      "[174, 366]\n",
      "[19423, 400]\n",
      "[95, 82]\n",
      "[19251, 572]\n",
      "[67, 110]\n",
      "[19572, 279]\n",
      "[76, 73]\n",
      "[19204, 647]\n",
      "[53, 96]\n",
      "[19608, 238]\n",
      "[70, 84]\n",
      "[19183, 663]\n",
      "[82, 72]\n",
      "[19586, 251]\n",
      "[69, 94]\n",
      "[19232, 605]\n",
      "[46, 117]\n",
      "[19368, 456]\n",
      "[92, 84]\n",
      "[19210, 614]\n",
      "[80, 96]\n",
      "[18982, 480]\n",
      "[180, 358]\n",
      "[18948, 514]\n",
      "[142, 396]\n",
      "[18991, 446]\n",
      "[259, 304]\n",
      "[18961, 476]\n",
      "[205, 358]\n",
      "[19324, 500]\n",
      "[86, 90]\n",
      "[19246, 578]\n",
      "[36, 140]\n",
      "[19634, 235]\n",
      "[53, 78]\n",
      "[19228, 641]\n",
      "[41, 90]\n",
      "[19569, 246]\n",
      "[89, 96]\n",
      "[19177, 638]\n",
      "[82, 103]\n",
      "[19541, 303]\n",
      "[78, 78]\n",
      "[19220, 624]\n",
      "[74, 82]\n",
      "[19479, 343]\n",
      "[93, 85]\n",
      "[19229, 593]\n",
      "[69, 109]\n",
      "[18489, 962]\n",
      "[258, 291]\n",
      "[18933, 518]\n",
      "[162, 387]\n",
      "[18557, 910]\n",
      "[294, 239]\n",
      "[19010, 457]\n",
      "[129, 404]\n",
      "[19469, 353]\n",
      "[81, 97]\n",
      "[19226, 596]\n",
      "[71, 107]\n",
      "[19566, 281]\n",
      "[66, 87]\n",
      "[19251, 596]\n",
      "[52, 101]\n",
      "[19615, 202]\n",
      "[74, 109]\n",
      "[19165, 652]\n",
      "[78, 105]\n",
      "[19624, 191]\n",
      "[88, 97]\n",
      "[19160, 655]\n",
      "[107, 78]\n",
      "[19498, 336]\n",
      "[85, 81]\n",
      "[19223, 611]\n",
      "[61, 105]\n",
      "[19031, 407]\n",
      "[149, 413]\n",
      "[18985, 453]\n",
      "[123, 439]\n",
      "[19005, 444]\n",
      "[192, 359]\n",
      "[18985, 464]\n",
      "[154, 397]\n",
      "[19669, 202]\n",
      "[41, 88]\n",
      "[19235, 636]\n",
      "[73, 56]\n",
      "[19803, 72]\n",
      "[56, 69]\n",
      "[19259, 616]\n",
      "[57, 68]\n",
      "[19762, 122]\n",
      "[40, 76]\n",
      "[19272, 612]\n",
      "[53, 63]\n",
      "[19771, 119]\n",
      "[38, 72]\n",
      "[19304, 586]\n",
      "[42, 68]\n",
      "[19729, 123]\n",
      "[65, 83]\n",
      "[19240, 612]\n",
      "[67, 81]\n",
      "[19559, 180]\n",
      "[99, 162]\n",
      "[19192, 547]\n",
      "[84, 177]\n",
      "[18228, 1104]\n",
      "[304, 364]\n",
      "[18985, 347]\n",
      "[204, 464]\n",
      "[19079, 330]\n",
      "[130, 461]\n",
      "[18982, 427]\n",
      "[130, 461]\n",
      "[18530, 940]\n",
      "[279, 251]\n",
      "[19013, 457]\n",
      "[94, 436]\n",
      "[19041, 413]\n",
      "[184, 362]\n",
      "[19018, 436]\n",
      "[145, 401]\n",
      "[18421, 1045]\n",
      "[270, 264]\n",
      "[18997, 469]\n",
      "[137, 397]\n",
      "[19027, 430]\n",
      "[166, 377]\n",
      "[18990, 467]\n",
      "[134, 409]\n",
      "[19533, 185]\n",
      "[119, 163]\n",
      "[19205, 513]\n",
      "[76, 206]\n",
      "[19550, 313]\n",
      "[89, 48]\n",
      "[19216, 647]\n",
      "[49, 88]\n",
      "[19317, 455]\n",
      "[124, 104]\n",
      "[19218, 554]\n",
      "[64, 164]\n",
      "[19479, 317]\n",
      "[94, 110]\n",
      "[19203, 593]\n",
      "[73, 131]\n",
      "[19332, 438]\n",
      "[125, 105]\n",
      "[19111, 659]\n",
      "[79, 151]\n",
      "[19428, 358]\n",
      "[105, 109]\n",
      "[19200, 586]\n",
      "[87, 127]\n",
      "[19361, 422]\n",
      "[114, 103]\n",
      "[19158, 625]\n",
      "[88, 129]\n",
      "[19696, 143]\n",
      "[65, 96]\n",
      "[19234, 605]\n",
      "[60, 101]\n",
      "[19728, 147]\n",
      "[67, 58]\n",
      "[19242, 633]\n",
      "[50, 75]\n",
      "[19635, 187]\n",
      "[89, 89]\n",
      "[19251, 571]\n",
      "[37, 141]\n",
      "[19469, 342]\n",
      "[109, 80]\n",
      "[19247, 564]\n",
      "[58, 131]\n",
      "[19456, 328]\n",
      "[133, 83]\n",
      "[19209, 575]\n",
      "[36, 180]\n",
      "[19548, 249]\n",
      "[90, 113]\n",
      "[19206, 591]\n",
      "[53, 150]\n",
      "[19622, 185]\n",
      "[87, 106]\n",
      "[19233, 574]\n",
      "[56, 137]\n",
      "[19713, 153]\n",
      "[59, 75]\n",
      "[19244, 622]\n",
      "[50, 84]\n",
      "[19717, 146]\n",
      "[67, 70]\n",
      "[19212, 651]\n",
      "[56, 81]\n",
      "[19635, 222]\n",
      "[83, 60]\n",
      "[19300, 557]\n",
      "[45, 98]\n",
      "[19636, 206]\n",
      "[69, 89]\n",
      "[19233, 609]\n",
      "[57, 101]\n",
      "[19609, 204]\n",
      "[93, 94]\n",
      "[19190, 623]\n",
      "[80, 107]\n",
      "[19582, 217]\n",
      "[100, 101]\n",
      "[19227, 572]\n",
      "[44, 157]\n",
      "[19594, 214]\n",
      "[93, 99]\n",
      "[19235, 573]\n",
      "[59, 133]\n",
      "[19747, 92]\n",
      "[73, 88]\n",
      "[19231, 608]\n",
      "[47, 114]\n",
      "[19693, 165]\n",
      "[73, 69]\n",
      "[19254, 604]\n",
      "[53, 89]\n",
      "[19618, 197]\n",
      "[103, 82]\n",
      "[19234, 581]\n",
      "[52, 133]\n",
      "[19589, 236]\n",
      "[103, 72]\n",
      "[19232, 593]\n",
      "[77, 98]\n",
      "[19508, 319]\n",
      "[97, 76]\n",
      "[19250, 577]\n",
      "[47, 126]\n",
      "[19556, 245]\n",
      "[102, 97]\n",
      "[19251, 550]\n",
      "[48, 151]\n",
      "[19611, 222]\n",
      "[85, 82]\n",
      "[19254, 579]\n",
      "[53, 114]\n",
      "[19752, 107]\n",
      "[86, 55]\n",
      "[19210, 649]\n",
      "[51, 90]\n",
      "[19459, 370]\n",
      "[102, 69]\n",
      "[19215, 614]\n",
      "[84, 87]\n",
      "[19604, 197]\n",
      "[93, 106]\n",
      "[19243, 558]\n",
      "[44, 155]\n",
      "[19383, 443]\n",
      "[102, 72]\n",
      "[19248, 578]\n",
      "[51, 123]\n",
      "[19483, 319]\n",
      "[98, 100]\n",
      "[19176, 626]\n",
      "[72, 126]\n",
      "[19397, 411]\n",
      "[96, 96]\n",
      "[19180, 628]\n",
      "[60, 132]\n",
      "[19463, 304]\n",
      "[112, 121]\n",
      "[19211, 556]\n",
      "[55, 178]\n",
      "[19676, 195]\n",
      "[62, 67]\n",
      "[19222, 649]\n",
      "[59, 70]\n",
      "[19288, 449]\n",
      "[134, 129]\n",
      "[19146, 591]\n",
      "[86, 177]\n",
      "[18462, 960]\n",
      "[326, 252]\n",
      "[18973, 449]\n",
      "[101, 477]\n",
      "[19087, 362]\n",
      "[92, 459]\n",
      "[19025, 424]\n",
      "[81, 470]\n",
      "[18497, 969]\n",
      "[280, 254]\n",
      "[19003, 463]\n",
      "[106, 428]\n",
      "[19019, 418]\n",
      "[165, 398]\n",
      "[18971, 466]\n",
      "[132, 431]\n",
      "[18520, 902]\n",
      "[292, 286]\n",
      "[18964, 458]\n",
      "[148, 430]\n",
      "[19175, 287]\n",
      "[46, 492]\n",
      "[19097, 365]\n",
      "[65, 473]\n",
      "------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds of MWPM:\n",
      "> Accuracy: 0.9741824915825119 (+- 1.6215888112777225e-05)\n",
      "> F1: 0.7001540909274414(+- 0.0004278500760738683)\n",
      "> Time: 3564.583245866663 (+- 2.1515375077950063)\n",
      "Average scores for all folds of PLUT:\n",
      "> Accuracy: 0.962119967848488 (+- 1.0883750800067886e-05)\n",
      "> F1: 0.2165126791459616(+- 0.0009293963291230573)\n",
      "> Time: 5.3837477 (+- 0.024953891823521156)\n",
      "Average scores for all folds of NN:\n",
      "> Accuracy: 0.4869119951588495 (+- 0.353119332420914)\n",
      "> F1: 0.8202139337887759(+- 0.0024718582018740747)\n",
      "> Time: 5.977881833333334 (+- 0.05871311700192946)\n",
      "> AUC for class : 0.9993653773495796 (+- 0.0)\n",
      "X^2 for MWPM and NN: 0\n",
      "X^2 for PLUT and NN: 0\n",
      "> AUC for class X00: 0.8754358832896705 (+- 0.0016156180806838547)\n",
      "X^2 for MWPM and NN: 38.56573705179283\n",
      "X^2 for PLUT and NN: 162.6021978021978\n",
      "> AUC for class X01: 0.9860302359131182 (+- 0.0004928647135965144)\n",
      "X^2 for MWPM and NN: 17.045289855072465\n",
      "X^2 for PLUT and NN: 234.32747603833866\n",
      "> AUC for class X02: 0.9983150175166287 (+- 3.769732075321397e-05)\n",
      "X^2 for MWPM and NN: 51.81164383561644\n",
      "X^2 for PLUT and NN: 525.259643916914\n",
      "> AUC for class X03: 0.9986267100344899 (+- 8.229689527800628e-05)\n",
      "X^2 for MWPM and NN: 29.83225806451613\n",
      "X^2 for PLUT and NN: 487.5918674698795\n",
      "> AUC for class X04: 0.9987016790209642 (+- 0.00012445920768125812)\n",
      "X^2 for MWPM and NN: 63.29588014981273\n",
      "X^2 for PLUT and NN: 425.45710267229254\n",
      "> AUC for class X05: 0.9979920176163389 (+- 0.00029435505725911693)\n",
      "X^2 for MWPM and NN: 144.7443820224719\n",
      "X^2 for PLUT and NN: 485.8565217391304\n",
      "> AUC for class X06: 0.9814852095081792 (+- 0.00028138422046832906)\n",
      "X^2 for MWPM and NN: 466.933991363356\n",
      "X^2 for PLUT and NN: 242.27935943060498\n",
      "> AUC for class X10: 0.9795653522861889 (+- 0.0002646787838064997)\n",
      "X^2 for MWPM and NN: 347.2896405919662\n",
      "X^2 for PLUT and NN: 280.44306049822063\n",
      "> AUC for class X11: 0.995593230485858 (+- 0.00022389877440402557)\n",
      "X^2 for MWPM and NN: 59.21167883211679\n",
      "X^2 for PLUT and NN: 446.7409090909091\n",
      "> AUC for class X12: 0.9971637027948266 (+- 0.00021846100745312832)\n",
      "X^2 for MWPM and NN: 59.43214285714286\n",
      "X^2 for PLUT and NN: 474.06259314456037\n",
      "> AUC for class X13: 0.9979939457107255 (+- 0.0001541942965299466)\n",
      "X^2 for MWPM and NN: 58.86496350364963\n",
      "X^2 for PLUT and NN: 518.9679218967922\n",
      "> AUC for class X14: 0.9976479755296006 (+- 0.0001633547227474271)\n",
      "X^2 for MWPM and NN: 37.08108108108108\n",
      "X^2 for PLUT and NN: 413.0729023383769\n",
      "> AUC for class X15: 0.996922016920383 (+- 0.00011753784492025442)\n",
      "X^2 for MWPM and NN: 247.65607476635515\n",
      "X^2 for PLUT and NN: 476.19460227272725\n",
      "> AUC for class X16: 0.9806001931163344 (+- 6.434813206630996e-05)\n",
      "X^2 for MWPM and NN: 93.0015243902439\n",
      "X^2 for PLUT and NN: 190.91525423728814\n",
      "> AUC for class X20: 0.9792922578407042 (+- 0.0001636784429830875)\n",
      "X^2 for MWPM and NN: 112.27458256029685\n",
      "X^2 for PLUT and NN: 206.3100511073254\n",
      "> AUC for class X21: 0.9968271314333009 (+- 7.976355126320903e-05)\n",
      "X^2 for MWPM and NN: 285.2840336134454\n",
      "X^2 for PLUT and NN: 470.1881331403763\n",
      "> AUC for class X22: 0.9976761287698761 (+- 6.332551646802326e-05)\n",
      "X^2 for MWPM and NN: 86.38062283737024\n",
      "X^2 for PLUT and NN: 386.8639143730887\n",
      "> AUC for class X23: 0.9975259831346358 (+- 0.00016719648531801547)\n",
      "X^2 for MWPM and NN: 81.26984126984127\n",
      "X^2 for PLUT and NN: 446.1259150805271\n",
      "> AUC for class X24: 0.9976528646843599 (+- 0.00023169149839964533)\n",
      "X^2 for MWPM and NN: 169.455223880597\n",
      "X^2 for PLUT and NN: 467.48201438848923\n",
      "> AUC for class X25: 0.996859652298868 (+- 8.87256004208883e-05)\n",
      "X^2 for MWPM and NN: 129.3398533007335\n",
      "X^2 for PLUT and NN: 436.01692524682653\n",
      "> AUC for class X26: 0.9801084517792134 (+- 0.0010292559442977725)\n",
      "X^2 for MWPM and NN: 320.602435530086\n",
      "X^2 for PLUT and NN: 240.63111888111888\n",
      "> AUC for class X30: 0.9805316031565019 (+- 0.0007554393340028593)\n",
      "X^2 for MWPM and NN: 407.1310228233305\n",
      "X^2 for PLUT and NN: 172.25323741007193\n",
      "> AUC for class X31: 0.9969437090011695 (+- 7.25111039441018e-05)\n",
      "X^2 for MWPM and NN: 186.6989898989899\n",
      "X^2 for PLUT and NN: 397.5211267605634\n",
      "> AUC for class X32: 0.9974723246375821 (+- 4.8412961269682414e-05)\n",
      "X^2 for MWPM and NN: 114.94084507042254\n",
      "X^2 for PLUT and NN: 502.35571428571427\n",
      "> AUC for class X33: 0.9976012471269418 (+- 0.00017032110059325307)\n",
      "X^2 for MWPM and NN: 90.5487012987013\n",
      "X^2 for PLUT and NN: 451.5436241610738\n",
      "> AUC for class X34: 0.9976010239579892 (+- 0.00021070190578473036)\n",
      "X^2 for MWPM and NN: 102.378125\n",
      "X^2 for PLUT and NN: 478.2857142857143\n",
      "> AUC for class X35: 0.9969844199463406 (+- 0.00018113649221281134)\n",
      "X^2 for MWPM and NN: 240.45437956204378\n",
      "X^2 for PLUT and NN: 409.350144092219\n",
      "> AUC for class X36: 0.9807031682203432 (+- 0.0003236088183915141)\n",
      "X^2 for MWPM and NN: 135.45606060606062\n",
      "X^2 for PLUT and NN: 209.8185975609756\n",
      "> AUC for class X40: 0.9806872742418793 (+- 0.00028020518176302635)\n",
      "X^2 for MWPM and NN: 49.07234042553191\n",
      "X^2 for PLUT and NN: 107.04845814977973\n",
      "> AUC for class X41: 0.9968958919098452 (+- 4.9831290252436e-05)\n",
      "X^2 for MWPM and NN: 291.0733788395904\n",
      "X^2 for PLUT and NN: 476.67915309446255\n",
      "> AUC for class X42: 0.9977212115365452 (+- 0.0002262449068289306)\n",
      "X^2 for MWPM and NN: 113.75347222222223\n",
      "X^2 for PLUT and NN: 526.1011730205279\n",
      "> AUC for class X43: 0.9975533163265483 (+- 0.00014529795335726142)\n",
      "X^2 for MWPM and NN: 72.64477611940299\n",
      "X^2 for PLUT and NN: 427.8125\n",
      "> AUC for class X44: 0.9977448647770334 (+- 0.00010580390895074515)\n",
      "X^2 for MWPM and NN: 131.6955380577428\n",
      "X^2 for PLUT and NN: 431.80659025787963\n",
      "> AUC for class X45: 0.996669432470975 (+- 7.534947970415277e-05)\n",
      "X^2 for MWPM and NN: 142.204128440367\n",
      "X^2 for PLUT and NN: 413.1858006042296\n",
      "> AUC for class X46: 0.9801055786247099 (+- 7.38022102591961e-05)\n",
      "X^2 for MWPM and NN: 405.08934426229507\n",
      "X^2 for PLUT and NN: 185.3308823529412\n",
      "> AUC for class X50: 0.9811831823655958 (+- 0.00055556344381089)\n",
      "X^2 for MWPM and NN: 314.140365448505\n",
      "X^2 for PLUT and NN: 182.4726962457338\n",
      "> AUC for class X51: 0.9968570280160783 (+- 0.0002678732317571239)\n",
      "X^2 for MWPM and NN: 169.2188940092166\n",
      "X^2 for PLUT and NN: 411.6581709145427\n",
      "> AUC for class X52: 0.9978988921716372 (+- 9.212377586327235e-05)\n",
      "X^2 for MWPM and NN: 131.97694524495677\n",
      "X^2 for PLUT and NN: 455.0138888888889\n",
      "> AUC for class X53: 0.997859550202936 (+- 8.9601045370923e-05)\n",
      "X^2 for MWPM and NN: 58.43840579710145\n",
      "X^2 for PLUT and NN: 449.76575342465753\n",
      "> AUC for class X54: 0.9976172599474568 (+- 6.484084539802197e-05)\n",
      "X^2 for MWPM and NN: 37.29032258064516\n",
      "X^2 for PLUT and NN: 392.66272965879267\n",
      "> AUC for class X55: 0.9971423149347522 (+- 0.00011656337070327813)\n",
      "X^2 for MWPM and NN: 148.45605700712588\n",
      "X^2 for PLUT and NN: 448.51339285714283\n",
      "> AUC for class X56: 0.9802988850684375 (+- 0.0005465790536373301)\n",
      "X^2 for MWPM and NN: 118.7931654676259\n",
      "X^2 for PLUT and NN: 187.91840277777777\n",
      "> AUC for class X60: 0.9812106642798328 (+- 0.00032019884782560584)\n",
      "X^2 for MWPM and NN: 99.05817610062893\n",
      "X^2 for PLUT and NN: 154.5\n",
      "> AUC for class X61: 0.9981733698033077 (+- 0.00010655263167547101)\n",
      "X^2 for MWPM and NN: 105.34979423868313\n",
      "X^2 for PLUT and NN: 445.4781382228491\n",
      "> AUC for class X62: 0.9985752240292429 (+- 8.76094010882714e-05)\n",
      "X^2 for MWPM and NN: 1.7578125\n",
      "X^2 for PLUT and NN: 462.6508172362556\n",
      "> AUC for class X63: 0.9986098574289247 (+- 6.870043018353538e-05)\n",
      "X^2 for MWPM and NN: 40.5\n",
      "X^2 for PLUT and NN: 468.21654135338343\n",
      "> AUC for class X64: 0.9987846333208907 (+- 0.00010598955316348899)\n",
      "X^2 for MWPM and NN: 40.76433121019108\n",
      "X^2 for PLUT and NN: 469.5047770700637\n",
      "> AUC for class X65: 0.9976620216197343 (+- 0.00018523512758138603)\n",
      "X^2 for MWPM and NN: 17.28191489361702\n",
      "X^2 for PLUT and NN: 435.84094256259203\n",
      "> AUC for class X66: 0.9932758207601365 (+- 0.00019056042318392856)\n",
      "X^2 for MWPM and NN: 22.939068100358423\n",
      "X^2 for PLUT and NN: 338.2630744849445\n",
      "> AUC for class Z00: 0.9732220149455637 (+- 0.00019142774718014648)\n",
      "X^2 for MWPM and NN: 453.4098011363636\n",
      "X^2 for PLUT and NN: 36.595281306715066\n",
      "> AUC for class Z01: 0.9738036617782948 (+- 0.00047368590663415594)\n",
      "X^2 for MWPM and NN: 86.0891304347826\n",
      "X^2 for PLUT and NN: 157.29982046678634\n",
      "> AUC for class Z02: 0.9759859773484472 (+- 0.0007886718930159415)\n",
      "X^2 for MWPM and NN: 357.34208367514356\n",
      "X^2 for PLUT and NN: 237.82940108892922\n",
      "> AUC for class Z03: 0.9760315651663859 (+- 0.0006552184678155473)\n",
      "X^2 for MWPM and NN: 87.07537688442211\n",
      "X^2 for PLUT and NN: 144.75043029259896\n",
      "> AUC for class Z04: 0.9766828405480016 (+- 0.0008027819062499312)\n",
      "X^2 for MWPM and NN: 455.57110266159697\n",
      "X^2 for PLUT and NN: 180.7937293729373\n",
      "> AUC for class Z05: 0.9766281388394951 (+- 0.0010272133059233879)\n",
      "X^2 for MWPM and NN: 116.05536912751678\n",
      "X^2 for PLUT and NN: 183.40099833610648\n",
      "> AUC for class Z06: 0.9920055260150106 (+- 0.00028861178100982786)\n",
      "X^2 for MWPM and NN: 13.898026315789474\n",
      "X^2 for PLUT and NN: 322.74363327674024\n",
      "> AUC for class Z10: 0.9976399052481264 (+- 0.0001300150869020383)\n",
      "X^2 for MWPM and NN: 123.70398009950249\n",
      "X^2 for PLUT and NN: 512.0818965517242\n",
      "> AUC for class Z11: 0.9953790870601203 (+- 0.000219514430447927)\n",
      "X^2 for MWPM and NN: 188.08290155440415\n",
      "X^2 for PLUT and NN: 386.9271844660194\n",
      "> AUC for class Z12: 0.9955354513953558 (+- 9.117601693391603e-05)\n",
      "X^2 for MWPM and NN: 119.91240875912409\n",
      "X^2 for PLUT and NN: 404.44594594594594\n",
      "> AUC for class Z13: 0.9954513286678157 (+- 0.00019293100413750433)\n",
      "X^2 for MWPM and NN: 172.90230905861458\n",
      "X^2 for PLUT and NN: 454.2560975609756\n",
      "> AUC for class Z14: 0.995808004527895 (+- 0.00022139030311405964)\n",
      "X^2 for MWPM and NN: 137.15766738660906\n",
      "X^2 for PLUT and NN: 368.50520059435365\n",
      "> AUC for class Z15: 0.9963396700316542 (+- 0.00024329447182537688)\n",
      "X^2 for MWPM and NN: 175.83768656716418\n",
      "X^2 for PLUT and NN: 402.93969144460027\n",
      "> AUC for class Z16: 0.9968509044948975 (+- 0.0003827913739863778)\n",
      "X^2 for MWPM and NN: 28.504807692307693\n",
      "X^2 for PLUT and NN: 445.01654135338345\n",
      "> AUC for class Z20: 0.9982270071024943 (+- 9.124063900750175e-05)\n",
      "X^2 for MWPM and NN: 29.16355140186916\n",
      "X^2 for PLUT and NN: 495.9355783308931\n",
      "> AUC for class Z21: 0.996603185868753 (+- 0.0002600662436636519)\n",
      "X^2 for MWPM and NN: 34.09057971014493\n",
      "X^2 for PLUT and NN: 467.2516447368421\n",
      "> AUC for class Z22: 0.9963230470047874 (+- 0.00011885101511688297)\n",
      "X^2 for MWPM and NN: 119.34368070953437\n",
      "X^2 for PLUT and NN: 410.008038585209\n",
      "> AUC for class Z23: 0.9963900090824245 (+- 0.00017101733270473963)\n",
      "X^2 for MWPM and NN: 81.63991323210412\n",
      "X^2 for PLUT and NN: 473.72176759410803\n",
      "> AUC for class Z24: 0.9961043467699042 (+- 0.00013131564845892887)\n",
      "X^2 for MWPM and NN: 73.6401179941003\n",
      "X^2 for PLUT and NN: 447.777950310559\n",
      "> AUC for class Z25: 0.9965756570330992 (+- 6.629389632556363e-05)\n",
      "X^2 for MWPM and NN: 34.591911764705884\n",
      "X^2 for PLUT and NN: 424.268253968254\n",
      "> AUC for class Z26: 0.9981072020165531 (+- 0.0001722913210900682)\n",
      "X^2 for MWPM and NN: 40.79716981132076\n",
      "X^2 for PLUT and NN: 485.1800595238095\n",
      "> AUC for class Z30: 0.9983881322475708 (+- 0.00014493008583471532)\n",
      "X^2 for MWPM and NN: 28.56338028169014\n",
      "X^2 for PLUT and NN: 499.06082036775103\n",
      "> AUC for class Z31: 0.9971707952386577 (+- 5.214336645384478e-05)\n",
      "X^2 for MWPM and NN: 62.43934426229508\n",
      "X^2 for PLUT and NN: 433.75581395348837\n",
      "> AUC for class Z32: 0.9966147140922065 (+- 0.00010503732237908601)\n",
      "X^2 for MWPM and NN: 67.25818181818182\n",
      "X^2 for PLUT and NN: 455.85735735735733\n",
      "> AUC for class Z33: 0.9968645406848797 (+- 0.0001609393203040755)\n",
      "X^2 for MWPM and NN: 40.74074074074074\n",
      "X^2 for PLUT and NN: 417.8719772403983\n",
      "> AUC for class Z34: 0.9964515982119504 (+- 0.0003742636553546446)\n",
      "X^2 for MWPM and NN: 42.44794952681388\n",
      "X^2 for PLUT and NN: 450.85876623376623\n",
      "> AUC for class Z35: 0.9969295951790939 (+- 0.00043973620465902636)\n",
      "X^2 for MWPM and NN: 46.90553745928339\n",
      "X^2 for PLUT and NN: 416.40664556962025\n",
      "> AUC for class Z36: 0.9981790637131548 (+- 4.5363704146795944e-05)\n",
      "X^2 for MWPM and NN: 1.9636363636363636\n",
      "X^2 for PLUT and NN: 478.77862595419845\n",
      "> AUC for class Z40: 0.9983310732221666 (+- 0.00014033568995319563)\n",
      "X^2 for MWPM and NN: 34.794117647058826\n",
      "X^2 for PLUT and NN: 460.4261796042618\n",
      "> AUC for class Z41: 0.996792522510616 (+- 0.00012051618508735764)\n",
      "X^2 for MWPM and NN: 28.83\n",
      "X^2 for PLUT and NN: 440.41706161137444\n",
      "> AUC for class Z42: 0.9964612337108875 (+- 0.0002794282732466886)\n",
      "X^2 for MWPM and NN: 51.39823008849557\n",
      "X^2 for PLUT and NN: 395.85820895522386\n",
      "> AUC for class Z43: 0.9966017734031677 (+- 0.0001967451456392761)\n",
      "X^2 for MWPM and NN: 117.40625\n",
      "X^2 for PLUT and NN: 448.46314102564105\n",
      "> AUC for class Z44: 0.9963671258742895 (+- 0.00017784087762125862)\n",
      "X^2 for MWPM and NN: 58.10951008645533\n",
      "X^2 for PLUT and NN: 419.7341137123746\n",
      "> AUC for class Z45: 0.996937630321854 (+- 0.00035481424808720763)\n",
      "X^2 for MWPM and NN: 60.24755700325733\n",
      "X^2 for PLUT and NN: 436.11550632911394\n",
      "> AUC for class Z46: 0.9982018393006227 (+- 0.00016304048609513077)\n",
      "X^2 for MWPM and NN: 2.0725388601036268\n",
      "X^2 for PLUT and NN: 509.1557142857143\n",
      "> AUC for class Z50: 0.9973317565587271 (+- 2.3569103709240625e-05)\n",
      "X^2 for MWPM and NN: 151.03601694915255\n",
      "X^2 for PLUT and NN: 400.9183381088825\n",
      "> AUC for class Z51: 0.9965438477082225 (+- 0.00023281310751874205)\n",
      "X^2 for MWPM and NN: 36.58275862068965\n",
      "X^2 for PLUT and NN: 437.1578073089701\n",
      "> AUC for class Z52: 0.9958295428262101 (+- 0.0003482857061652993)\n",
      "X^2 for MWPM and NN: 212.11009174311926\n",
      "X^2 for PLUT and NN: 439.8664546899841\n",
      "> AUC for class Z53: 0.995875090581181 (+- 0.0003033805434130982)\n",
      "X^2 for MWPM and NN: 116.06714628297362\n",
      "X^2 for PLUT and NN: 438.121776504298\n",
      "> AUC for class Z54: 0.9954928657702297 (+- 0.00021815046826692178)\n",
      "X^2 for MWPM and NN: 194.46942800788955\n",
      "X^2 for PLUT and NN: 467.28052325581393\n",
      "> AUC for class Z55: 0.995826094149462 (+- 0.00013444379734983802)\n",
      "X^2 for MWPM and NN: 87.69471153846153\n",
      "X^2 for PLUT and NN: 409.16530278232403\n",
      "> AUC for class Z56: 0.9978033886310242 (+- 0.00023983187812551257)\n",
      "X^2 for MWPM and NN: 67.7976653696498\n",
      "X^2 for PLUT and NN: 490.00141242937855\n",
      "> AUC for class Z60: 0.9933309053077923 (+- 0.0003103495424403107)\n",
      "X^2 for MWPM and NN: 169.11835334476845\n",
      "X^2 for PLUT and NN: 375.2082717872969\n",
      "> AUC for class Z61: 0.9771277277090823 (+- 0.0008742635462595268)\n",
      "X^2 for MWPM and NN: 311.57776049766716\n",
      "X^2 for PLUT and NN: 218.92545454545456\n",
      "> AUC for class Z62: 0.9771005307095628 (+- 0.0003287284160254854)\n",
      "X^2 for MWPM and NN: 159.38546255506608\n",
      "X^2 for PLUT and NN: 231.61188118811882\n",
      "> AUC for class Z63: 0.9771803850954992 (+- 0.00038126514131487976)\n",
      "X^2 for MWPM and NN: 378.97838270616495\n",
      "X^2 for PLUT and NN: 222.73462214411248\n",
      "> AUC for class Z64: 0.9766044321834955 (+- 0.000385319729516514)\n",
      "X^2 for MWPM and NN: 108.926243567753\n",
      "X^2 for PLUT and NN: 185.43311036789297\n",
      "> AUC for class Z65: 0.9767914001679154 (+- 0.0003434987332242685)\n",
      "X^2 for MWPM and NN: 310.6206030150754\n",
      "X^2 for PLUT and NN: 157.55940594059405\n",
      "> AUC for class Z66: 0.9778002013533895 (+- 0.00025573198060998626)\n",
      "X^2 for MWPM and NN: 172.97297297297297\n",
      "X^2 for PLUT and NN: 207.9093023255814\n",
      "###################################################################################\n",
      "TOTAL F1 NN: [0.8239703804283469, 0.8208984160169279, 0.8179934693549143, 0.8179934693549143]\n",
      "TOTAL F1 PLUT: [0.21573756003510455, 0.21598096775228462, 0.21781950965049557]\n",
      "TOTAL F1 MWPM: [0.7005817870764759, 0.7003109051446001, 0.6995695805612483]\n",
      "TOTAL ACC NN: [0.23963068425655365, 0.23481489717960358, 0.9862904040403914]\n",
      "TOTAL ACC PLUT: [0.9621126551755909, 0.9621118948346331, 0.96213535353524]\n",
      "TOTAL ACC MWPM: [0.9741893939394216, 0.9741979797979952, 0.9741601010101187]\n",
      "TOTAL TIME NN: [6.0565148, 5.9616627, 5.915468]\n",
      "TOTAL TIME PLUT: [5.3543192, 5.4153296, 5.3815943]\n",
      "TOTAL TIME MWPM: [3567.4755684000033, 3562.318840600001, 3563.9553285999864]\n",
      "------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlIAAAEiCAYAAAAyFTwvAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAABlNklEQVR4nO3dd3xUVdrA8d+TQmihhKbSQkkIAUElYhcFddFXAVFce5cVllUBFQv2Xld0RcUKujYUkUXsgmVVFlBBQJqKFOkdkpBk5nn/OHdgGFJmkkkm5fl+PkPm3nvuvc/cDJlnzjn3HFFVjDHGGGNM5OJiHYAxxhhjTFVliZQxxhhjTClZImWMMcYYU0qWSBljjDHGlJIlUsYYY4wxpWSJlDHGGGNMKVkiZYolIieIiIrIpUHrUr11d4Z5jFdEpFzG2RCRO71YUsvj+MYRkUNE5HMR2RLJ774q8F7PK7GOwxhTNdXIREpE6orIdSLytYhsFpF8EVknItNE5FIRSYh1jJEQkVkikicizYopU19EdorI4oqMLRpEZEBl/uAOSjaDHztF5AcRGV7c+0lEjheRiSLyp/c7XO+9DweUcM50ERkrIotEZJeI5IjIEhEZJyKHR/n1JQDvAmnAbcBFwKRiyl8aci3yRWSTdz2eFZFjohlfOLyEe0A5Hj/09x/6uDXMGFVECkQko5DtgffZ9UWc+99FHHeGiOws/aszxhSnSiUM0SAiHYEPgHTgM+ABYCPQHDgJeBnIBG6MVYyl8CLwDHAh8M8iypwD1MO9vrL6A6gDFEThWOEYAFwC3FnItnuBB4HdFRRLcd4ApgECHABcDDwOdAYGhxYWkfuBm3HX80Xgd2+/84H3RORV4DJV9YXsdwXu953rnfMn3O8iHTgLuEpEuqjqwii9rvbeY6Sq/iuC/Z4EZuG+sDUEugIDgb+JyOu415YXpRhLcgcwHphcTse/qIj1dwIdgP9EcKx43N+lMyOM4TwReURVf4pwP2NMGdSoREpE6gBTcR8KZ6lq6Lfqh7xv88V+oxeRZFXdUU5hlsYbuA/syyg6kboM8OE+TMpE3XD4uWU9TjSoagEVl9CV5AdVfS2wICJjgUXAlSJyq6puCNp2BS6J+gzor6rZQdsexiVWFwPLgduDtp0EjAMWAn9R1T+DAxCRm4F/RPl1HeD93Bzhfl+r6jvBK0TkOtxrOx/YDgwpc3SVQPDvPUBEWgHtgNmqOi+Cw80GBojIUar6XZj7/IxLpB8C/hLBuYwxZVTTmvauBDoBjxWSRAGgqrNUdWxgWUSWe1Xjh4rIxyKyDZgXtP14EflURLZ5zSs/eB+S+xCRLl4TzmoR2S0ia0Vkuoj8X1CZ2l71/mIRyRaRrSLys4g8UtyLUtVtwDvAwSKSVci504BjgQ9VdY2IHCQij4nIT+L6vOSKyEIRGSUi8SVdRCmij5QX/yNeM1WOiPxPRE4p4hg9xfWdWuK91h0i8l8ROTOk3AxcbVRo88ml3rpC+0h5Mb4qrsl2t4j8KiL3i0jdkHKB/Tt521d55eeKyGklXYviqOou4HtcDVWHoHPWwtWk7QQuCE6ivP0KgL8BK4DrZd8m24e84/01NIkK7Kuq/wynNiqca+Rd/y+9xZeDrn9qONegkPhygEuB33A1Z/scR0QOFJFnRGSFuKbOP8U1VzYPKRf4vXURkSe9/085IjJTRPqEvMZA/7xLgt9DhVyPo0TkS3FNpZtE5AURqV+a1+m5DPc39oUI97sLyAYejmCfFcBY4JTg12+MKX81qkYKONv7OS7C/doAXwATcX1F6gOIyBnAe8Ba4DFgB3Au8IKItFfVW71yTbz9AZ7FNeU0BbKAI3BNjQBPA5cDE3A1TAm4fim9w4jxJVzzwmW4b7TBLvN+vuj97IZrYnkP+BVIBPrimsja4z7ES+MNXDPcf4CPccnDJFyTVagzgQzgbdz1aIJLmCaJyAWq+rpX7j7ch9Fx7Nt88m1RQYhIW+B/uOakscBS4ARcDdAxItLHS1aCjQfygUeBWsB1wGQRSVfV5SW+8qIFEqjg2pxjcLU8/1bV9YXtpKq5IvIacAtwGjBeRNoBh+FqesrUbBfBNboP+K8Xxzjga+8QG0KPGS5VzRPXbHkHrvbkOS+mNsB3uOv/Iu692RFXa3WiiGR5XxqCTcDVtD4EJOPeux+JyKmq+pkX50XAq17sRf3fPwRXW/0y8Lp3La4A/BTSLFsSERHc/7tduP8XkViLq1m+VUT6qeqUMPe7D/f34yEROVxtIlVjKoaq1pgHsAnYFuE+ywEFrgxZH49LALYCBwWtr4X74PEBad66ft4xzinhXJuBaaV8bQIs846RFLQ+DlgFrAMSvHV1ACnkGK96cR8YtO4EL/ZLg9aleuvuDFp3irfulZBjDvDWa8j6eoWcvy6wGFgYsv6V0P2Dtt3pHT81aN2/vXWnhZR9xFt/RSH7Tw2+JrjmXQUeCOPaB67R7bgEuRlwMC4xVmBmSPl/eOtHlHDcgV65R73lM7zlJ6PwfyGSa7Tfe6CEY1/qlT87jNf2WNC694H1QKuQslm45tvg91vg9zYTqBW0vhWupu+XkGPs994M2eYHjghZ/wEuua5fiuvbxzvuyxHsE3hNWUADXBI4H4gP+T1cX0j8U73nt3jL5wZtnwHsLOt7xh72sEfhj5rWtNcAV2sUqc3s30m7B66m6iUNamJR13n2YVwC099bHfgWfaqINCjmPNuALiLSNdIAVVVxtVKNcclLwClAS2CCerUwqprjlUdEaolIiog0xdUixeH+kEcqcM59miFVdTIuOQqNd1fgubi7KJvgEqkvgM4lXKciiUgcLnH9UVWnhWx+APeBWVgn3jGBa+LFNwv3gZwWwenvwn34rcc1/w7F1cj1DykXeG2htSuhtns/G4bst72QsmErwzWKpsBraODF1BA4HZgC5IpI08AD92VmGe69HOqfGtRhXVVX4ZLEDBHpHEE836nqzJB1X+BqhVMjOE7Ald7PF4stVQRV3Y5r/u2C17QdpieAP4F7RSSxNOc2xkSmpiVS23HV/5H6VUPunMJ1IgVYUEj5wLr2AKr6Ja4J4lJgo9cX6C4RyQzZ7zpcIvSz11/lBRHp733wAeAlPQcEP4L2fwVXo3R50LrA85eCjpEgIqNFZAmu0/gmXALwqlekcaFXoXjtcR/ASwrZ9kvoChFp7vV9WYdr/tjoxXC1V6RRKWIAVxtUn0J+L6q6GVjjxRrqt0LWbcI1OYZrHHAyriluFC4Bb8X+HfNDE6SihCZcgf1K8x4OVtprFE2hSWEn3N+jK3Dvg9BHJ6BFIcfZ772F64gPkb2Gon7/ENl7ABFJwSWii1T1m0j2DfEMrln8LhGpHc4O6vrb3YlrUr66+NLGmGioaYnUfKCBiET6IZFdcpHiqeoluOaeW3F/oEcC80RkWFCZ93Hffi/CfRvug7tde4bXQRlcDceakEdg/z9xtUoniUgr7w96P9y37eAPnMeBe4AfcP04TsMlAKO87eX6vvD6j3yC+6Y9Hvgrro/Wybj+KeUeQyFCE+UAieAYS1X1M1X9UFUfxjXFHY7rFxdsvvfzsBKOF9j+c8h+h0YQU2XVzfsZqK0MXOfXcO+Dwh4Xl2M8Rf3+g2ML1wVAEqWsjQrwatpuwyXj10aw60u4u0VHi0hZk25jTAlqWmfzd4HjcdXut5TxWIFvsF0K2ZYZUgYAVZ2P+zB8REQa4fp3PCgiTwealbwagdeA17yE40HcmFb9cZ3dR1J8jdGLuMToElxNRhJBtVGei4CvVPXc4JXixtgqrd9wyU86+9d0hDaxdAO6A3er6h0hMVzJ/iLpNLsB13y73+9FRBoDB+LGXSp3qvqt16n6YhF5UlUDHeS/xfVZ6y8iTVV1YyGx1saNC5YLfOgd73cR+RHXGTxDVReVMrSYXiPvS8FFuOTlY2/1MtzvuZa6TuLh6gzMDVlX6P+/CnQFrm/VhCgc63Xc//mb2LemuUiq6hM3DMZ7wPUllTfGlE1Nq5F6AfcN+HoRCe23AoCI9BCRoWEc6wfcLceXBTevef0SbsB9KLzvrUsJbp4DUNWtuGr7ukBtEYn3kqvgMgr86C2meOvmeLUeex4hcf0H90F5Ke4P7y7grZAyPkK+ZYtIPWB4GK+7KO97P28IOe4AXLNM6PkpJIauFN43Z6e3PaWkIFTVj7sGh4pI35DNN+He8++VdJwougf3eu8OrFDV3biO6fVxCXOd4B3EDUExFmgLPKL73tkXqDV8M6RZd8++4kbtD2023iOW18h7ra/gmt2eU9U/vJg24QYzHSgiRxayn0jhI/cPD6qtDYzddD6wOKQWdife/6HyJG74ke7Af7SIOzIj4f0NuAnX1H1zBPtNxiXsI3CDDRtjykmNqpFS1WwROR13N85kEfkE+BTX1NYMOBF3O3aJ47d43/qG4T5wZonIONy3/L8CRwL3q+pSr/jFuD/47+G+eecDvbxzva2qOV4StUZEpuCSp/W4flhDgC2EOTKyquaLyATct1hwdyqFdrB/Bze69Fu4ASFb4JKuTZSSqn4sIv/BjdWTAnyE66fxN1wtXHAH+l9wtVY3ihuzaDGuJutvuGasHiGH/x4YBowVkcCdVDNVtbBhFcDVNp6M+x2PxV3z43G/m6+IwqCk4VLVZSLyJnCBiBynql9768d5NYA3AAu939ly3LAI5+GagV/DdWAPPt6nIjIY139msYgEj2zeETeyeQf2vd6FqYhrdJxXsybsO7J5M++1XRdSfgjwDfCVdz1+xCV17XE1shPYf3T7BOBr7zok4/oF1QGuCSn3Pa7JexTuC5Cq6ptlf4n7CYwhF+nYUUVS1U9E5HNcU38kRuGGfOiM+0JljCkPsb5tMBYPXC3QcNwf7S24D+Z1uATrIrzbjb2yy4EZxRyrFy4Z245rhvmRoFvHvTKH4D6YluH+oG3HNUeMxBuqADdswgO4sX024aY8WY5rlkuL8PV1xhtyADiuiNf/CG74hlzcGEI3sfeW7UuDyp5QyLpUQoY/8NbXwY2ntRbI8V7LKRQyfAGutmUirvYs2yt7JoUPZxCHG99pFa52Z088hZX31rfDdZ5fD+ThmnnuB+qGlCt0/3B+94Vco+uL2N7Zi3t6Efu+i+vrluddjw+BM0s4ZydcMrXEu365uIT0OeDQMN8n4V6j/d4DJRz30qD3n+KSvC24/xvPAkcXs29T770ZuBFiKy65HgNkFvJ76wI85b3ncr330cmFHDcN1y9veyCuoG2FDo0Q9DpOCPN11/HiXQHERfJ/NuQ1ZRWyrQfuZo5ihz8oZL/3ve02/IE97FFOD1GNpPuJMcbEnrhR9e8A2mnZBkw1xpgyqWl9pIwxxhhjosYSKWOMMcaYUrJEyhhjjDGmlGLWR0pEXsJNCbFeVfe7w8gbQ2kMbkykbFxH1x8qNkpjjDHGmKLFcviDV4B/UfSgdafi7rRJA47A3aF0REkHbdq0qaampkYnQmOMqSHmzJmzUVULG6vLGFOMmCVSqvqViKQWU6Q/bqJdBb4XkUYicqCqrilmH1JTU5k9e3Y0Q62RVMHvB5/P/Qw8V927Lnhb4BHYL1AusBy6LfR58HJh6wMxRfII7BO8bzjrgq9BUeVCnxdXrrDjFXaM4iqHw90W7vHKep5ItpWmXLhiUaFeUeesiNaCLl2E4493z0Xkj3I/oTHVUGUekLMlsDJoeZW3br9EyhugcDBAmzZtKiS4aPP7ITvbPXbtgpycfR+7d0NeXuhPJSdX9yzvzoO8fMjPU/LyoSDfW86HggL38BVAgc977vMegef+vev8gRFrAA2ZoWWfD+vgZ4Wu33/Hoj4e9v/cKLxk4ccOs1wJW8KdjKaqDBpiw5uY4px/ThzHH58U6zCMqdIqcyIVNlUdB4wDyMrKqhSfHLm5sG4drF8PmzbB5s2wcaOyeYuyZauydSts2w47dsDOHZCdExgc1ftJ0DLqJRkaVKux92XKnolWZJ85V2SfCVgCJfZ9sn8R2fOvxClxcRAfBxIHcXFKfLw7blxc4OHKiEB8vO7ZJhJYF3iu+6wPPPau23d7nOyNPy7OHRdxrzAuTsErsydiLx7vKcjeWILFeQcVUQKT9gSuQeB8gXPtc1UkZLt3jMKuXmg59xoKvcyhT/e8puDzligkXini2KGHKvLYxZyzqH3c6w7vv56E9aLCfO0xEklsleV1yI6d1Jo+HV+7dhQc7Lqldu9uSZQxZVWZE6nVQOug5VbeukojLw9+/x1+/RV+/11ZvkJZsVJZtQq2bQO/Kn6/H79f8QdGQJXAh07gA90txwkk1YY6dZQ6dfzeT6hdW0lKUmonuZ9JtWTPcmKikJQEtWrt/VmrVuCnkJgIiYlCYqKQkLD/84SEwAMSEoT4eNmz3iVJleQTwBhTeqoweTI88QRk74KNLeDayZCYGOPAjKkeKnMiNQUY5s1TdgSwraT+UeVt/XqYMwdmzfYz50dl+XLIL1AKfD78fjcbRpy4WqHERGjezEezZn5SGvtp0lhJSVFSUoTGjYVGjeJp2ND9bNw4nuTkeBISbDQKY0wUrVoF994LgX6jxx8PN91kSZQxURSzRMqbZPQEoKmIrMJN95AIoKrP4maCPw03P102cFlFx6gKy5bBZ58pH3/mZ9kyJb/Aj8/vI06EhHg48EAf7dv5aNfWR9u2QmpqPG3bJnLAAbUsMTLGxIbfD2+8AWPHug6UjRvDDTfAySdXnrZGY6qJWN61d14J2xX4ewWFs4/8fPjoI3jxZR9Llyl5Be72s3p1lUO659Pj0AIOPzyB7t1rk5xcOxYhGmNM0fx++OADl0SddhqMGAGNGsU6KmOqpcrctFfh8vNh4kTlpfF+Vq32UeDz0bihcsoxeZzU288JJ9SnXr06sQ7TGGP2l5/v7nJJToaEBLjzTtcf4dhjYx2ZMdWaJVKeFSvgxpv8/DTPh89XQLu2Pi44L5dBgxpQt27dWIdnjDFFW7AA7roLOnSABx5w69LT3cMYU65qfCKlCu+/r9z7gI9tOwo4oLmPa/6+i/79U6hVq36swzPGmKLl5sIzz7j+UH6/GyBu+3Zo0CDWkRlTY9T4ROqJMX7GvejD5/Nx0gm53HF7Igcc0DzWYRljTPFmz4Z77oHVq91AZRdfDH/7GyTZ2FDGVKQanUh9+50y7kU/aAE337CDiy5qQkJCfKzDMsaYoqm65rtJk9xyWhrcdhtkZsY2LmNqqBqbSG3fDqNuLcDv83HV5TlcemmzsEdcNsaYmBGBWrXcWFBXXgmXXOI6lxtjYqLG/u+7894C1q9TDs7M5+9/r29JlDGm8tq82d2Bl5HhlocOhYEDoX372MZljKFGjhj5ySfKtGlK3do+brutgDp1asU6JGOM2Z8qfPghDBoEN97oZjUHqFvXkihjKokaVyNVUAD3PuDH7/cx+MpddOvWJNYhGWPM/tatc32hvvnGLXfqBDk5LokyxlQaNS6RWroU1m/0c9ABPq68spE16RljKhe/H957D8aMcTVQyckwfDiccYZN72JMJVTjEqn/zSnA7/NzSLcCEhPrxTocY4zZ1+jR8Mkn7vmJJ8KoUdC0aWxjMsYUqcYlUjNn+4kTpUuXgliHYowx+/vLX9wYUaNGQe/eVgtlTCVX4xKpefMgIU7IyrIO5saYSmDJEpg/392FB9CrFxx+uPWFMqaKqFGJ1Mo/fWxYLzSq56drV/sjZYyJobw8ePFFeOUVd3dely6uQzlYEmVMFVKjEqmZc3zEARkZBSQm1o51OMaYmmrePDe9y++/u+VzzoHWrWMbkzGmVGpUIjV7jh8QMjPzYx2KMaYmysmBsWPhzTddLVTbtm56l0MOiXVkxphSqlGJ1MIFQhxwyCE1chxSY0ysPf64G9ogLg4uvRSuuspN92KMqbJqTCKVlwfLlsaREO+nRw+bHd0YEwNXXgl//AEjRuyd7sUYU6XVmKqZn+b58OUrqW19NGliiZQxpgLMmOGmdvH73XKLFjBunCVRxlQjNaZGavaPCiidO+cjYnfEGGPK0ebN8PDD8NlnbvmTT6Bv39jGZIwpFzUmkZo71yVSXbv6Yh2KMaa6UoVp0+Cxx2D7dqhTB/7xDzjllFhHZowpJzUikVKFBfNdR/MePaxjpzGmHKxZA/ffD99955aPOgpuuQUOPDC2cRljylWNSKT+/NPVtDdsqHTqZM16xphyMH26S6IaNHCdyf/v/2x6F2NqgIg6m4tIaxF5SURWiUieiPT21jfz1h9ePmGWzZIl4PP5SUvLJz6+xvSvN8aUt9279z4/91y4/HKYOBFOP92SKGNqiLCzChFpB8wGzgIWAPGBbaq6AcgCrox2gNGwezf41U+D5FhHYoypFgoK3NQuZ5wBGza4dXFxMHQoNGkS09CMMRUrkuqZ+wA/0BW4AAj9ujUNODZKcUXV7jw/qlAr0b4hGmPKaPFiuOQS+Ne/XJ+BGTNiHZExJoYi6SN1EvCUqq4UkcK+cv0BtIpOWNGVk6eIQEKCxjoUY0xVlZcHL7zgaqL8fjjoILj1VjjiiFhHZoyJoUgSqQbAmmK214rweBUmd7efOBHirHuUMaY0Fi6E22+H5ctd36dzz3XNeHXt5hVjarpIEp+VQJdith8JLCtbOOUjN08RIKFSpnnGmCphxQpITXUJVbdusY7GGFNJRFJHMwm4XES6Bq1TABE5CxgEvB3F2KImd7cfEYiPt6Y9Y0yYlizZ+zwzE8aMgddftyTKGLOPSDubrwJmAq/hkqibROQ7XAI1F3gs6hFGQaBGKj6+xKLGmJpu+3a48044/3z46qu96486CmrZgL7GmH2FnUip6nbgKOAF3FAHApwMdALGAieqam55BFlWu/MUEbHO5saY4n3xBZx9Nkyd6pKmjRtjHZExppKLqNeQl0xdC1wrIs1wydQGVa3UGcrufFcjZZ3NjTGF2rjRTTL8xRdu+dBD4bbboE2b2MZljKn0wk6kROR2YJKqzoc9g3AGb+8CnKWqd0c3xLLbvWf4g1hHYoypdObNg2uvhR073F1411wDAwfaNy9jTFgi+UtxJ1BcL8uuwB2RnFxE+orIYhFZJiI3FbK9jYhMF5EfRWSeiJwWyfED8vL91kfKGFO4jh1dAnX00fD2265pz5IoY0yYollHUxsoCLewiMQDT+P6Wa0CZonIFFVdGFRsNPC2qj4jIpm40dNTIwmqwOfH7xOrkTLGOH4/TJkCf/kL1KnjkqhXXoGmTW1+PGNMxIpNLUSkAdAoaFUTESms00AKbtqYlRGcuyewTFV/8871JtAfCE6kFDcQKEBD4M8Ijg9Avk9B3bfLRJsixpia7fff4Z57XHPeb7/BiBFufbNmsY3LGFNllVRHMxy43XuuwBPeozAC3BjBuVuyb+K1Cgida+FO4BMR+QdQDzdNzf4nFhkMDAZoE9I5NM/ndzMEYjVSxtRYBQUwYQI8/zzk57vapx49Yh2VMaYaKCm1mOH9FFxC9R4wL6SMAjuB71X126hGB+cBr6jqYyJyFPCqiHRVVf8+AaiOA8YBZGVl7XMHYYHPv6dGyhIpY2qgRYvg7rv3DrA5YIDrXJ6cHNOwjDHVQ7Gphap+CXwJICJtgWdVdWaUzr0aaB203MpbF+wKoK8Xy3ciUhtoCqwP9yT5PgW/a9Kzpj1japjffoOLL947yfDo0dCzZ6yjMsZUI2HX0ajqZVE+9ywgTUTa4RKoc4HzQ8qsAPoAr4hIZ1yH9g1EIN/nR9Xdrmd37RlTw7RvD717Q/PmMGSI61xujDFRFHFjl3e3XQbQmEKGT1DVr/bbqRCqWiAiw4CPgXjgJVVdICJ3A7NVdQowEnheRIbjmhAvjXTwz3yfH/W7l2k1UsZUc7t2wdNPu+a79HS37v77bTgDY0y5iSiREpFRwE3svZOuMGHX+6jqNNyQBsHrbg96vhA4JpIYQ+X7FPUJoMTFVeoB2I0xZfHtt3DffbBuHfzyC7z0khvOwJIoY0w5imRk8yuAB3B9pj7BTWL8TyAf15fpN9yce5WKq5FyiZTVSBlTDW3dCo8/DtO872SZmXDrrTYmlDGmQkRSIzUEd2feiSLSBJdIfaCqX4jIGOAnIqiNqij5Pj9+n3tud+0ZU42owmefuTnytmxxkwwPHQrnnWcdIo0xFSaSOu/OwETveaCNLB5AVdfghh+4NnqhRYcbkNN9M01IsG+oxlQbW7a4wTW3bIHDDoO33oILL7QkyhhToSKpo/EBu7zngZ9NgrYvB9KiEFNUBaaIAauRMqbKU3WPuDhISYHrr3eDbQ4YYH2hjDExEclfnhVAOwBV3Y0blfy4oO2HA5ujF1p05O2TSFmNlDFV1urVrununXf2ruvXDwYOtCTKGBMzkdTRfAX8H3CztzwRuE5E6uASsguBl6IbXtmoKj6/EhgwwTqbG1MF+f3w5pswdizk5rqEauBAq2I2xlQKkfwlGgPMFZE6qpoD3AGkA5d42z/BDY1QaeT7lPg4oaDAmvaMqZJ++81N7zJ/vlvu2xdGjrT/zMaYSiOSkc0XA4uDlncB/USkIeBT1Z3lEF+Z+PxKYnwcPu+uPauRMqaKKCiAV16BF15wz5s3h5tvhuOOK3FXY4ypSGXuWKCq21R1pzgXRSOoaMn3+70aKbdsiZQxVYQIfPmlS6IGDoS337YkyhhTKZW5flxEBDgPuA3X1PdqWY8ZLT6fkhCUSNld0cZUYrm5sHs3NGzo/rPecQds2wY9esQ6MmOMKVKJNVIicqyIvC8iC0XkGxH5W9C2vwDzccnTQcBD5Rdq5Ar8ro+UNe0ZU8nNmeMG0rzvvr3rOna0JMoYU+kVWyMlIscAnwOJQauPEpF6QG3gXmArcA8wRlW3lFOcpeLzKwlxcda0Z0xltXMnPPkkTJrklpOSYMcOSE6ObVzGGBOmkpr2RgG7gbNxCVVHYAIwGkgGngNuVtWt5RhjqflU9+kjZU17xlQi33wD998P69e7u/CuuAIuvRQSE0vc1RhjKouSEqkjgOdU9T/e8jwRuR431MF4VR1SrtGVkc+n1tncmMpG1fV/Ckwy3LUr3H47tG8f27iMMaYUSkqkmgALQtYFlidHPZooczVS7EmkbGRzYyoBEWjc2DXj/f3vcO65NjK5MabKKimRigPyQtYFlndEP5zo8vmVOLHO5sbE3Pr17tG1q1seMgTOOQdatoxtXMYYU0bhDH9QT0RSgpYDz5ND1gOgqpVmvj1VJT7eOpsbEzN+P0yeDGPGQL16MHGi+1m7tiVRxphqIZxE6lnvEWpSIes0zGNWCJ8qwt4aKWvaM6YCrVwJ997rhjYAN5RBXp5LpIwxppooKekZXyFRlBO/H0AAJS4O4uMtkTKm3Pn98Prr8MwzboDNxo3hxhvhpJNc/yhjjKlGik2kVPWyigqkPPhV8Xu1UTb0gTEV5Kab4Isv3PPTTnOTDDdsGNuYjDGmnFSaZrjyoAp+n/sGHB+vMY7GmBqiXz9YsABuuQWOOSbW0RhjTLmq1omUXxW1Giljytf8+fDzz26KF4Bjj4X33oNatWIblzHGVIBqn0j5/IKqJVLGRF1OjusH9cYbru/TIYdA585umyVRxpgaolonUkqgw7mSkGBNe8ZEzaxZ7o681avdYJoXXWQjkxtjaqTqnUgp+Pb0kYpxMMZUBzt2uDGhJk92y+npcNtte2uiTLUyZ86c5gkJCS8AXXEDNBtT0/iB+QUFBVf26NFjfWEFqnkiFXzXntVIGVNmTzwB77/vJha+6iq4+GI34bCplhISEl444IADOjdr1mxLXFyc/RE1NY7f75cNGzZkrl279gWgX2FlqvVfQIU9iZT9rTcmCq6+GjZsgOHDoV27WEdjyl9XS6JMTRYXF6fNmjXbtnbt2q5FlonkgCKSLCK3i8g3IrJURI7y1jf11meUNehoCjTtqdqcqMZETBWmTYNrrmHP9ADNmsGTT1oSVXPEWRJlajrv/0CRWUTY9TQi0gz4BmgPLPN+1gFQ1Y0icgnQCBhRhnijyq9qnc2NKY116+D+++G//3XLn38Op5wS25iMMaYSiqSe5l7gAOAI4Djc3CvB3gf6RCmuqPF5ExZbZ3NjwuD3wzvvwKBBLolKToY77oCTT451ZKaGio+P75GRkZGZlpbWpXfv3h03bty456/57Nmzax955JHpqampXdu2bdv1hhtuONDvvj0D8Pbbbzfo2rVr5w4dOnTp3Llz5lVXXdUq9Pg5OTly9NFHp2dkZGQ+//zzjYuKo2fPnp2++uqruqHrn3zyySYXX3xxm9D1GzZsiD/55JM7pKenZx588MGdZ82aVTuw7a677mresWPHLmlpaV3OOOOMdtnZ2YXOnXT55Ze3/vDDD+sHltesWZOQkJBw2MMPP9wsuFzdunUPLS6mf/3rX03S0tK6pKenZ3bu3Dnz9ttvb1HU6wzXO++80yA1NbVrmzZtut5yyy0HFFZmyZIltY466qj09PT0zJ49e3b69ddfEwPbhgwZ0jItLa1LWlpal+Drfvrpp7f/+eefk8oaX0WKJJE6HRirqj/guh+F+g1oHZWookQV/H73/rQ+UsaUYMUK1wfqwQchOxt694aJE+GMM2yOPBMzSUlJ/kWLFi1cunTpgkaNGhU88sgjzQB27twpZ555Zscbb7xx7fLly+fPnz9/4cyZM+s/9NBDzQBmzZpVe+TIkW1effXV33/99dcFP//888KOHTvuDj3+t99+Wxdg0aJFC6+66qot0Yp79OjRB3br1i17yZIlCydMmPD7Nddc0wbg999/Txw3blyLn376aeHSpUsX+Hw+eeGFF1JC91+7dm38nDlz6p166qk7A+smTJjQuHv37rsmTpy4X/mivP322w3Gjh3b/NNPP12yZMmShT/88MMvDRs29JXltRUUFDB8+PA206ZNW7JkyZIF7777bsqcOXNqh5a79tprW51//vmblixZsnD06NF/jhw5shXAm2++2XDu3Ll1Fy5cuGDOnDm/jBkz5oDNmzfHAQwZMmT9fffdV2hiVllFkkg1xTXpFcUP7HchY0lRCrwaKWvaM6YE330HP/wAKSnw8MPu0bRprKMyZo8jjzxy1+rVq2sBPP/8802ysrJ2Dhw4cDtAcnKy/5lnnlkxZsyYAwHuv//+A0aOHLnm0EMPzQVISEhg1KhRG4KPt3r16oTLLrus3c8//1w3IyMjc8GCBUnvv/9+cufOnTPT09MzBw0alJqTk7Pft4gxY8Y0SU1N7XrwwQd3/vbbb+uHbgdYvHhx7ZNPPnkHwKGHHpq7atWqWitXrkwA8Pl8smvXrrj8/HxycnLiWrVqlR+6/2uvvda4T58+24PXTZw4MeXRRx9duW7dusTg2p3iPPzwwwc++OCDq1JTU/MB6tSpoyNHjtwYzr5FmTFjRr22bdvuzszMzKtdu7YOHDhw8zvvvNMotNzSpUvrnHrqqdsBTj/99B2fffZZI4AFCxbUPuaYY3YmJibSoEEDf2ZmZvakSZMaAvTt23fn119/3SA/f79LUmlFkkitBToUs/1QYEXZwoky3du0Z53NjSnErl17nw8aBEOHuqa93r1jF5MxhSgoKGD69OnJAwYM2Aruw/iwww7LDi7TpUuX3dnZ2XGbN2+OW7x4cZ0jjjgiu9CDeVq2bFkwduzYP7KysnYuWrRoYbt27fL+9re/tXvrrbd+XbJkycKCggICNWABf/zxR+KDDz540Lfffrto1qxZi5YsWVKnsGN37do1Z+LEiY0Bpk+fXnfNmjVJy5cvr9WuXbv8v//972vbtWvXrXnz5t2Tk5N9gWQw2Lfffls/Kytrz3/QZcuWJW7YsCHxxBNPzO7Xr9+WCRMmhFUrtXTp0jrHHHNMsdcB4JlnnknJyMjIDH307dt3v5F2V65cWatly5Z5geVWrVrlBRLcYJ07d85+4403GgO8+uqrjXbt2hW3du3a+EMPPTTn888/b7hjx464NWvWJHz77bcNVq5cWQsgPj6etm3b5n7//ff7NaNWVpE0eE0DrhCRp4C84A0icgRwMfBE9EKLDr/fNfFZjZQxQfLy4IUXXNL0xhvQooX7tnH55bGOzFRi7/+0umG0j9n/kJbbitu+e/fuuIyMjMx169YldujQIXfAgAH7JR3RMnfu3NqtWrXa3a1bt90Al1566aann366ObBnIMavvvqq3pFHHrnjoIMOKgAYOHDg5iVLluzXGnP33XevGTx4cBsvIcnJyMjIjo+P1w0bNsR/8MEHjZYtW/ZzkyZNfP/3f//XfuzYsSlDhw7dHLz/unXrElu0aFEQWJ4wYUJKv379tgBcdNFFm6+44orUu+66a11Rr0VEIvrQGzJkyOYhQ4ZsLrlk+J566qlVgwcPbtO5c+emRx555I7mzZvnJyQkMHDgwO0zZ86se/jhh2ekpKTkH3bYYTvjgwZ7bNq0acHKlSvDqnGrDCJJpO7CDUb1IzAF10/qEhG5ChgI/Ak8FMnJRaQvMAaIB15Q1QcLKXMOcKd3vrmqen64x1fAVyCAWmdzYwLmzYO774bly13fp2+/hTPPjHVUpgooKekpD4E+Ujt27Ig74YQT0h588MHmo0ePXp+ZmZn79ddf79OstnDhwlp169b1p6Sk+NPT03NnzpxZ96ijjsqp6JgBUlJS/O+8885yAL/fT+vWrQ/OyMjYPXny5IZt2rTZHUjEBgwYsPXbb7+tH5pI1a5d25+Tk7OnLeXdd99N2bBhQ+KkSZNSANavX5/4888/Jx188MG7k5KS/Lm5uVK7dm0F2Lx5c0LTpk0LADp27Jjz3//+t26/fv12FBfvM888kzJmzJj9+ialpqbmfvTRR78Fr2vduvU+NVCrVq3ap4YqaN/8Tz755FeAbdu2xU2bNq1x06ZNfQAPPfTQ2oceemgtwBlnnNGuU6dOe/qv7d69O65u3br+0ONVVmE3eKnqWuBIYCZwOe6uvYuAc4BPgONUNexsVkTigaeBU4FM4DwRyQwpkwbcDByjql2A68I9vouZPX2kLJEyNV52Njz6KFxxhUui2raF55+3JMpUCcnJyf4nn3xyxdixY1vk5+czePDgTbNmzUqePHlyMrjO53//+9/b/OMf/1gLcPPNN699/PHHD5w3b14SgM/nI/Rut1Ddu3fPXb16da358+cnAUyYMKHJcccdt08Ccvzxx++aOXNm8tq1a+N3794t7733XqF3+m3cuDE+NzdXAP75z3827dmz546UlBR/ampq3g8//FB/x44dcX6/ny+++CK5c+fOuaH7d+rUKXfJkiVJAPPmzUvatWtX/Pr16+etXr3659WrV/88bNiwtePHj08BOOKII3Y8++yzKYHr8N577zU+6aSTdgDceOONa2+++eZWK1asSADIzc2Vxx9/fL/Oj0OGDNm8aNGihaGP0CQKoFevXruWL19ee9GiRbVyc3Nl0qRJKWedddbW0HJr1qxJ8Hlj0I0ePfrA8847byO4Ztq1a9fGA8ycObPOokWL6g4cOHBPkv77778nHXbYYTFJgEsjop5DqrpSVfsDKbhhEI4EmqnqGaq6KsJz9wSWqepvqpoHvAn0DylzFfC0qm7xzl/oPDdFxovuGUfQmvZMjTZ3Lpx7Lrz5pquFuuwy16R3yCGxjsyYsB1zzDE5GRkZOePGjUupX7++Tpo0adn9999/UGpqatfMzMwuhx122K6bb755PcARRxyR89BDD60877zz2rdv375Lenp6l99++63Y2+rr1q2rzz777PJBgwZ1SE9Pz4yLi+P666/fp4N627Zt80eNGvXnkUce2TkrKysjPT19vyQI4KeffqqdkZHRJTU1tevHH3/ccNy4cSsBevfuveuMM87Y0q1bt86dOnXq4vf7ZcSIERtC9+/Xr9+2L7/8Mhlg/PjxKaeddto+dxSee+65WwK1U88888zK999/v3FGRkZmjx49Og8YMGBL4G6/v/71r9sGDx68vk+fPp06duzY5eCDD87cvn17maoWEhMTeeyxx1b07ds3PS0trcuAAQM2Z2Vl5QJcd911B/373/9uCPDRRx8lt2/fvmtqamrX9evXJzzwwANrAPLy8uSYY47J6NChQ5fBgwe3HT9+/G+Jia4lb+XKlQlJSUnapk2bgiIDqGRENbwEQ0SaqOqmqJ1Y5Gygr6pe6S1fBByhqsOCykwGlgDH4Jr/7lTVjwo51mBgMECbNm16/PHHHwB8vXQDm35J4c7bleOPz+HJJ5OjFb4xVcuSJXDhhdCxoxsXqlOnWEdkKhkRmaOqWcHr5s6du7x79+5lusPLlF6PHj06ffzxx8sCzWE1wV133dW8QYMG/uHDh1eq993cuXObdu/ePbWwbZHUSP0pIpNEpL+IVNSoTAlAGnACcB7wvIg0Ci2kquNUNUtVs5o1axa0HgJjs9k4UqbG+fnnvc/T0+HZZ2HCBEuijKkiHnnkkVW//vrrfnfDVWeNGjXyDRs2rFIlUSWJJJGaBPzF+7lGRJ4UkawS9inOavYdwLOVty7YKmCKquar6u+42qm0cE+gBPpIKUE3BBhTvW3aBKNGuea76dP3rj/sMPtGYUwV0rt3711HHHFElekrFA3XXnvtpkAzX1URSWfz83BTxAwGFgJ/B2aKyAIRuUFEDorw3LOANBFpJyK1gHNxdwMGm4yrjUJEmgLpuBHUw40Zv8+NpWadzU21pwoffODGg/r8c6hTZ99xoowxxkRdpJ3Nd6jqi6raCzdp8Z1AIm7Ygz9EZL/+S8UcqwAYBnwM/AK8raoLRORuEennFfsY2CQiC4HpwA2R9tMq2NPZPJK9jKli1qyBa65x/Z+2b4ejjoK334bTT491ZMYYU62VOr1Q1T+Ae4B7ROQ84BkgoplNVXUabqDP4HW3Bz1XYIT3KJWCAvdF3Zr2TLX1448uicrJgQYNYORIOO00mx/PGGMqQKkTKRGpjxtD6mLgWFzt1vwoxRUVCkHDH8Q0FGPKT6dO0LgxHH206xuVEvZ8psYYY8ooovRCRATX4fxi3JhPdYCNwL+A8ar6Y9QjLCPfngE5rUbKVBMFBTBxIvTvD3XruseECdCoUawjMybqVqxYkTB06NA2c+fOrdugQQNf06ZN888444ytH3zwQaPp06cvi3V8xoSdSInIo8D5QAsgH5gKTACmef2dKh8N1EjZFDGmmli8GO66y40LtWoV3HCDW29JlKmG/H4//fr163j++edvmjp16m8A3333XZ1JkyY1inFoxuwRSWfzEcBK4B/Agap6tqpOqbRJlMfn3bVnTXumSsvLg3/9Cy66yCVRBx0Exx0X66iMKVdTp05NTkhI0BtvvHHPyN9HHXVUTq9evXbu2rUrvm/fvu3btWvXpV+/fu383qCB119//YFdu3btnJaW1uW8885rG1jfs2fPTkOGDGl58MEHd05NTe360Ucf1Qc3XcngwYNbpaWldUlPT8+87777mgN8/fXXdQ8//PBOXbp06Xzsscem/fHHH1XrnnxTYSJJLzJVdVG5RVIOFN0z114VG5bCmL1++slNMrxihetAft55MGSIa9IzpiJ17dq5yG033LCGSy7ZCsD48Y145JEDiyw7f/4v4Zxu3rx5dbp3755d2LZffvmlzk8//fRbampqfo8ePTI+/fTT+n/5y1923nDDDesfffTRNQADBgxo9+abbzY8//zztwEUFBTIzz///Mtbb73V8O677z6ob9++Sx577LFmK1asqLVw4cIFiYmJrFu3Ln737t1yzTXXtPnggw+WHXTQQQXPP/984+uvv77lxIkTl4cTt6lZwk6kqloSFRBIpOIiGujBmEri11/hqqvcraft2sFtt0G3brGOypiYO/jgg3d16NAhH6BLly7ZgRHAP/zww+THH3/8gNzc3LitW7cmZGZm5gDbAAYNGrQF4Oijj951ww031AL44osvGlx99dUbAoNAtmjRwjdr1qzaS5curdO7d+90cE2MzZo1y6/4V2mqgiITKRG52Hv6qqpq0HKxVHVCVCKLEp/VSJmqrEMHOPVU15R3+eVQq0bNFmEqmzBrkrjkkq17aqfK4OCDD86ZPHly48K2JSUl7bmDKD4+noKCAsnOzpaRI0e2nTlz5sKOHTvmjxgx4qDc3Nw9X6Nr166tAAkJCfgC/T4KoarSsWPHnJ9++qlKViCYilVcPc0rwMu4ATeDl18p5vFytAMsq8DwB/HxNqaOqQK2bXOdyX8J+ry66y64+mpLokyNc8YZZ+zIy8uTRx99tGlg3cyZM+t8+eWX9Qsrn52dHQdwwAEHFGzbti3uP//5T6FJWLA+ffpsf+6555rm57sKp3Xr1sV369Ytd/PmzQmfffZZPYDdu3fL7Nmza0flRZlqp7imvRMBVDUveLkqUd3btGedzU2lpgpffAEPPQSbN8Py5fDSS65PlA2saWqouLg4pkyZ8uvQoUNbjxkz5oCkpCRt1arV7jPOOGNrYeWbNm3qu+CCCzZ07ty5S7NmzQq6d+9e4hxJw4cP37BkyZKkjIyMLgkJCXrJJZdsuOWWWza8+eabv15zzTVtduzYEe/z+WTIkCHrsrKycqP+Ik2VJ27w8OojKytLZ8+eDcDnv6zjyzea8/77+dx4Yy4XXNAgxtEZU4iNG10CFZhg+NBDXV+oNm1iG5epUURkjqruMxH93Llzl3fv3n1jrGIyprKYO3du0+7du6cWti3sLtgi8pKIHFHM9p4i8lIp4is3qsFNe7GNxZj9qMKUKW6S4enT3V14N90Ezz1nSZQxxlQRkdzLdinQoZjt7YBLyhRNOQj0J7TO5qbS2bIFHnsMduxw07tMnAhnn223mBpjTBUSzZ5D9XAjnlcqe+fas34mphLwBgckLs7NiTdqlOsD1bev9YUyxpgqqNhESkTaAKlBqzJE5PhCiqYAQ4BKM+9RoO+XdTY3lcZvv8G998LJJ7tBNQFOOy22MRljjCmTktKLy4A7APUet3qPUAL4vfKVhghBI5vbt30TIwUFMH48vPAC5OfD1q2uX5Rl98YYU+WV9Jd8MrAclyi9BIwDvgspo8BOYJaqroxyfKUWuBnRmvZMTP3yi5veZelStzxgAFx7rSVRxhhTTRT711xV5wJzAUSkLfCuqs6viMCixZr2TEzk58Mzz8Brr7l+US1bwq23Qs+esY7MGGNMFEUy195d5RlIeQhu2rMaKVOh4uPhxx/d8wsucCOT16kT25iMMcZEXZH3WYvI8cEdywPLJT0qJuySBYYZtRopU2F27XKjkoO7K++OO9zo5MOHWxJlTBmISI/+/fu3Cyzn5+fTuHHj7ieeeGLH8jxvfHx8j4yMjMy0tLQuvXv37rhx48Y9IxL++uuviX369OnQtm3brq1bt+562WWXtc7Nzd3zjX3FihUJp59+evvWrVt37dKlS+devXp1nDdvXlLoOXbu3CmHH354p4LAhxXw6quvNhKRHj/++OOeaWkWL15cKy0trUvwviNGjDjo9ttvbxHJ+SL1zjvvNEhNTe3apk2brrfccssBhZW55557mqelpXXp2LFjl7vvvrt5uNvKM6biyhS2LTc3V7KysjoFpgqKRHED1swApotIreDlYh6B7ZWKJVKmQvz3v3DOOXDPPXs76KWmQteuMQ3LmOqgTp06/sWLF9fZuXOnALz33nsNWrRoUe7D7SQlJfkXLVq0cOnSpQsaNWpU8MgjjzQD8Pv9DBgwoGO/fv22/vHHH/N///33+bt27Yq79tprWwa29+vXr+Pxxx+/Y+XKlfMXLFjwy4MPPrj6zz//3G9Ew6eeeqppv379tiQEfUi9+eabKYcddtjOCRMmpIQTZyTni0RBQQHDhw9vM23atCVLlixZ8O6776bMmTNnnzkHZ82aVXvChAnNfvjhh19++eWXBR999FGj+fPnJ5W0rTBTp05NPuuss1LLGlNxZYraVrt2be3Vq9f2F154IaxrHqy49OJyXMVO4M1aqe7IC5d1NjflautWePxxmDbNLTdpAjt3QnJyTMMyJtq6dqVzeRx3/nx+KbkUnHTSSdsmTpzY6LLLLtvyxhtvpJx11lmbv/322/oAY8eOTXnmmWda5Ofny2GHHbZrwoQJfyQkJHDSSSd1WLNmTa3du3fHXX311euuv/76jYsXL6516qmnpvXs2XPn7Nmz67do0SLv448/Xla/fv1i50s78sgjd82bN68OwH/+85/kpKQk/7XXXrsJICEhgWeffXZl+/btuz366KN/Tp8+vV5CQoLeeOONGwL7H3XUUTmFHfftt99u8uabb/4WWN62bVvcrFmz6n/22WeL+/Xrl/bPf/7zz5KuzdSpU5PDPV8kZsyYUa9t27a7MzMz8wAGDhy4+Z133mnUo0ePtYEyP//8c51DDz10Z3Jysh/gmGOO2fHmm282uvfee9cVt608YyquTHHbzj777K033XRTyyFDhmyOJKYia6RU9RVVHa/egEze8xIfpbkw5cmGPzDlQhU++cQNYzBtGtSqBdddBy+/bEmUMeXgoosu2vzWW281zs7Oll9++aXuUUcdtQvghx9+qP3OO++kzJ49e9GiRYsWxsXF6bPPPtsE4N///vfyBQsW/PLTTz8tfO6551qsXbs2HmDFihW1r7nmmvXLli1b0LBhQ9+ECRMaF3fugoICpk+fnjxgwICt4JKH7t27ZweXSUlJ8R944IF5CxcuTJo3b95+2wuTm5srK1euTOrUqVNeYN3rr7/e6IQTTtjWrVu33Y0bNy74+uuv65Z0nHDPB9CjR49OGRkZmaGPyZMn7/eHa+XKlbVatmy5J7ZWrVrlrV69ulZwmUMOOSTnf//7X/LatWvjd+zYEffpp582XLlyZa2StgXr1q1bRkZGRubQoUPbfvbZZ40CMb377rv7TZAbTkzFlSlu2+GHH54zb968euFcx2DVvsErkEjZXHsmavx+NyfeF1+45R49YPRoaN06tnEZU47CrTkqL0cccUTOqlWrkp5//vmUk046aVtg/UcffZQ8f/78ut27d+8MkJubG9e8efMCgIceeqjFBx980Ahg7dq1iQsWLKjdqlWr/JYtW+4++uijcwAOPfTQ7OXLlxfa3LR79+64jIyMzHXr1iV26NAhd8CAAduj+ZrWrl2bkJycXBC87u2330655ppr1gOcddZZm1999dWU4447LluKmPmgqPVFmTNnzuLSxluYww47LPfaa69d26dPn/Q6der4u3Tpkh3vfeAWty3YvHnzFoGrWXv55ZebvPvuu8ujGWO4EhISSExM1C1btsQ1btzYH/Z+4RYUkZ5Ad1V9Pmhdf+Be3Mjm41X1lkiCLm+C7GnasxopEzVxcS5pqlfPjQk1YIDNj2dMBejbt+/WO+64o/Unn3yyeP369QkAqiqDBg3a9PTTT68OLjt16tTkL7/8Mnn27NmLkpOT/T179uyUk5MTB1CrVq09zXjx8fEaWB8q0Edqx44dcSeccELagw8+2Hz06NHru3btmjN58uR9arE2b94ct2bNmlqZmZm7165dmxC6vTD16tXz5+Xl7Tn3unXr4r///vvkxYsX1xk2bBg+n09ERP1+/6oWLVoUbNu2bZ8sZPPmzfHt2rXb3aZNm7xwzgeuRmrXrl37ZTMPPvjgygEDBuwIXte6det9antWrVq1T21OwPDhwzcOHz58I8CwYcNatmrVKi+cbaURTkzFlSlp//z8fKlbt26xzbyhIvnrfwfQL7DgTR/zBnAAsA0YJSKVrh+VNe2ZqFi9Gn76ae/y4MFukuGBAy2JMqaCDBkyZOP111//Z8+ePff0/+nbt+/2qVOnNl69enUCuGRkyZIltbZu3RrfsGFDX3Jysv/HH3+sPXfu3IibbAKSk5P9Tz755IqxY8e2yM/Pp1+/fjtyc3Pj/vWvfzUB1/Q3dOjQ1oMGDdqYnJzsP+OMM3bk5eXJo48+2jRwjJkzZ9b56KOP6gcft1mzZj6fzyfZ2dkC8OqrrzY+88wzN//5558/r169+ue1a9fOa9WqVd7HH39cv2HDhv7mzZvnT5kyJTnwOmfMmNGwd+/eO8M9H7gaqUWLFi0MfYQmUQC9evXatXz58tqLFi2qlZubK5MmTUo566yztoaWC1z7pUuX1vrggw8aXXnllZvD2Rbq9NNP31FSbVQ4MRVXprhta9eujW/UqFFBUlJSuSVS3YFvgpbPxY14foiqZgKfAIMjOXlFsBopUyZ+P7z+ursj7+abYYf3tyYpCZpH5U5eY0yYOnTokD969Oj1wet69OiRO3r06NV9+vRJT09Pz+zdu3f6ypUrE88666xtBQUF0r59+y433HBDy+7du+8qy7mPOeaYnIyMjJxx48alxMXFMXny5GWTJk1q3LZt267t2rXrmpSU5H/yySdXA8TFxTFlypRfv/jiiwatW7fu2rFjxy6jRo1q2bJly/3uNDz++OO3ffLJJ/UBJk6cmDJw4MAtwdv79++/5bXXXksBGD9+/O/33XffgRkZGZm9evXqNGrUqD+7dOmyO5LzRSIxMZHHHntsRd++fdPT0tK6DBgwYHNWVlYuQK9evTouX748EaBfv34dOnTo0OX000/v+MQTT6xo2rSpL3CM4rYFBPpIhT4K6yMVTkzFlSlu24cfftgguNk4XBKY3LfEgiI5wBBVfcVb/hwoUNW/eMtDgHtUtWnRRyl/WVlZOnv2bHx+5aslG7h7SHM2b85j+vQ4UlKqfZcwE02//uqmd1mwwC3/5S8wahQ02O//tjFVnojMUdWs4HVz585d3r17942xiqkm+Oabb+o++uijLSZPnvx7rGOp6U455ZQOjz766Kpu3brtDt02d+7cpt27d08tbL9IMoutQAsAEUkCjgTuD9quQKUbddCa9kzE8vPd3XcvveTeQM2bu9qo446LdWTGmGrm2GOPzZ49e/b2goICEmzAw5jJzc2Vfv36bS0siSpJJL+1n4ArReQz4EygNvBx0PZ2QKnHhigvNo6UidiNN8LXX7vnAwfCNddA/f26GhhjTFRcd911m2IdQ01Xu3ZtHTZsWKl+D5EkUvfg+kH9D9c36lNVnR20/XRgZmmCKA+BJkurkTIRO+ccWL7cDWnQo0esozHGGFOJRTJp8bcichjwF9xdem8GtolIE1yS9V7UIywDv989wG6sMsWYPRsWLoSLL3bLRx3l7sizanZjjDEliOiTQlWXAEsKWb8JGB6toKLF7wNQ4uMhLs5qpEyInTvhySdh0iQQgawsyMx02yyJMsYYE4aIPy1EpAFwEtDeW/UbrplvvzEoYi3QP8pGNTf7+eoreOAB2LDBJU1XXAFpabGOyhhjTBUTUSIlIlcCjwH1cf2kwN2tt1NERqjqixEery8wBogHXlDVB4sodxbwDnB4SL+sYvl9LsT4+IjG1jLV2ZYt8Oij8LF3n0TXrnD77dC+ffH7GWOMMYWIZIqYfsA4XA3UbYA3uA5dgH8A40Rkvar+J8zjxQNPAycDq4BZIjJFVReGlEsGrqUUHdl9fje3rNVImT2eesolUbVrw9ChcO651oHOGGNMqUVSI3Uj8AtwhKruDFr/uYi8DHwPjALCSqSAnsAyVf0NQETeBPoDC0PK3QM8BNwQQaxAoGlPSUiwGqkaTdX1gQL4+99d36hrr4WWLWMblzHGmCovkkSqO3B3SBIFgKruEJHxuJqqcLUEVgYtrwKOCC7g3SXYWlU/EJEiEykRGYw3PU2bNm32rN87hlQEUZnqw++HyZNdDdTYsa5qskkTePjhWEdmTJX0+++/183JyYnaX9Q6deoUtGvXLjtaxwMYNGhQ6ueff96wSZMmBUuXLl1Q8h7Oxo0b41944YWUm266aUNh20eMGHFQ/fr1fXfffXdY4yVGWt5UXZG0aZR021tUq31EJA54HBhZUllVHaeqWaqa1axZsz3rfXv6SEUzMlMlrFgBV18N998Pc+bA9OmxjsiYKi8nJyehXr16BdF6RJqUTZ06Nfmss85KLa7M5ZdfvnHKlClLI31tmzZtin/xxRdtAk0TsUgSqbnApSKy3wzaIlIfuNQrE67VQOug5VbeuoBkoCswQ0SW46akmSIi+8wFVRz/nrv2rGmvxvD54NVXXd+nH36Axo3d3Xl9+sQ6MmNMBTj11FN3NmvWrKC4Mtu3b4874YQTOnbq1CkzLS2ty/PPP9945MiRrVauXJmUkZGR+be//a0VwKhRow5ITU3t2qNHj05Lly5NKuncxZUfO3ZsysEHH9w5IyMj8/zzz29bUFDA0KFDWz7wwAN7vv2PGDHioNtvv71FaV+7iY1Ivg08AkwCfhCRJ9nblynQ2bwjMDCC480C0kSkHS6BOhc4P7BRVbcBeyZAFpEZwPWR3LVnwx/UMMuWuUmGF3pvzdNOg5EjoWHD2MZljCmTbt26ZeTl5cVlZ2fHbdu2LSEjIyMT4L777lt11llnbY/0eJMmTWpwwAEH5M+YMWMZuNqo448/ftfpp59eZ9GiRQsBvv7667rvvfdeys8//7wwPz+fQw45JPPQQw8tshmyuPI//PBD7XfeeSdl9uzZi5KSkvTCCy9s8+yzzza54IILNl933XVtbr755g0A77//fuOPP/54v7EaTeUWycjmk0VkGK7j91PsbcoTYBcwTFXfj+B4Bd7xPsYNf/CSqi4QkbuB2ao6JdxjFXp8wFcgdtdeTTJ3rkuiWrSAW2+Fo4+OdUTGmCiYN2/eInBNey+//HKTd999d3lZjnfYYYfl3Hrrra2HDBnSsn///tv69u27c+PGjft8UkyfPr3+aaedtjU5OdkPcMopp2wt7pjFlf/oo4+S58+fX7d79+6dAXJzc+OaN29eMGzYsE2bNm1KWL58eeKaNWsSGjZs6OvYsWN+WV6bqXiRjmw+VkRexw1Z0M5bHRiQc1ukJ1fVacC0kHW3F1H2hEiPb3ft1QDbtu2tcTrzTMjNhQEDoN5+LdDGGANAt27ddv/www8L33333Ya33XZby88++2z7VVddVW4TB6uqDBo0aNPTTz+9OnRbv379trz22muN165dmzhw4MDN5RWDKT8l9pESkQQROUtERonIFUCCqk5U1Ye9xzulSaIqgjXtVWM5OfD449CvH/z5p1sXFwcXXGBJlDHV1Omnn76jrLVRAMuXL09MTk72Dx06dPOIESPW/vTTT3UbNmzo27Vr157PxN69e++cNm1ao507d8qWLVviPv3000bFHbO48n379t0+derUxqtXr04AWLduXfySJUtqAVx44YWb33333ZSpU6c2vuiii7aU9bWZildsjZSINAZm4Dp9C67F7GEROUVV55R/eGVjI5tXU//7H9x7r0ug4uLcXXkHHRTrqIyp9urUqVOwa9euqA5/EE65QB+p0PWF9ZE644wz2n3//ffJW7ZsSWjRokW3m2666c/hw4dvDC4zZ86cOjfffHOruLg4EhISdOzYsX8ccMABvh49euxMS0vr0rt3723PPffcqjPPPHNz165duzRp0iS/W7duuwL79+rVq+P48eP/SE1N3dMMd+yxx2YXVb5Hjx65o0ePXt2nT590v99PYmKiPvnkkyvS09PzsrKycnft2hXXokWLvLZt2+YXdw5TOYlq0UmGiDyGm4x4Kq4vUzpwNTBfVXtUSIQRysrK0tmzZ5Pv8/Pie1sZd39DMjNzee01q6Wo8nbsgDFj3NhQ4ObGu+22vRMNG2NKTUTmqOo+d0XPnTt3effu3TcWtY8xNcXcuXObdu/ePbWwbSV9szgD+EhV+wVWeEMRPCoirVR1VdSiLAd+n00RU23Mng2jR8PGjZCYCFddBRdfbKOtGmOMiamS+ki1JqQzOG4KGAHalktEUeQG5LTO5tVCo0ZuwuFu3eD11+Hyyy2JMsYYE3MlfRIlAaF3EWwJ2lap+bzWd6uRqoJUXS1UVpabJ69jR3jhBejSxSYZNqbi+P1+v8TFxdm3UVNj+f1+AfxFbS/LJ1Kl/4/l8162VVxUMWvXukmFhwyBzz/fu/7ggy2JMqZizd+wYUND74PEmBrH7/fLhg0bGgLziyoTTooxUkTODVpOxCVR94lIaCdEVdX+kYdaPnwFdtdeleL3w7vvwlNPQXY2JCe7dcaYmCgoKLhy7dq1L6xdu7YrZfvibUxV5QfmFxQUXFlUgXASqUO9R6gjC1lXqTIWv9+1EFmNVBWwYgXccw/8+KNbPvFEGDUKmjYtfj9jTLnp0aPHeqBfiQWNqcGKTTFUtUp/A/HZpMVVw+zZcM01kJcHKSlw003Qu3esozLGGGNKVK3ralzTnlpn88qua1c3P1737jBiBDRoEOuIjDHGmLBU70TKpoipnPLy4N//hnPOcdO51K4Nr74K9evHOjJjjDEmItU2kVLdO/yBjSNVicybB3ffDcuXu7vzbr7ZrbckyhhjTBVUbRMpcANy2sjmlUR2Njz9NLz9tsty27aFU0+NdVTGGGNMmVTrRMpvTXuVw/ffw333wZo1bhyoSy91U7zUqhXryIwxxpgyqdaJlOsjpXbXXiwtXQrDhrnn6elwxx3QqVNsYzLGGGOipFonUoHBeK1GKobS0qB/f2jVCi66yAb1MsYYU61E/KkmIqnASUAL4N+qulxEagEHAGtVNS+6IZZe4K49++yuQJs2wSOPuKSpSxe37rbbYhuTMcYYU04iSjFE5CFgBBCPG8X8O2A5UBtYCIwGnohqhGUQmCLGEqkKoApTp8I//wnbt8P69fDii27CYWOMMaaaCnvkchH5G3AD8DRwCrDnE1JVtwNTgDOiHWBZBKZpS0y0D/Ny9eef8I9/wF13uSTqqKNc53JLoowxxlRzkdTVDAXeU9XrRKRJIdvnAcOiE1Z07B1HKrZxVFt+P0ycCP/6F+TkuBHJR46E006zJMoYY0yNEEmKkQ48U8z2DUClmmHWOpuXs61b4dlnXRLVp4+bZDglJdZRGWOMMRUmkkQqF6hXzPa2wNYyRRNlgRopa9qLooICV9sUH++Spltucc9tkmFjjDE1UNh9pID/AWcWtkFEagMXAf+NRlDRoKjdtRdtixfDxRe7efICTj7ZkihjjDE1ViSJ1CPAUSLyKtDNW3eAiPwFmAG0Ah6Nbnhl4/NZ015U7N7t+kFddBEsWeLuzgtkqcYYY0wNFnZdjap+JiJDgDHA+d7qV72fecBVqvpdlOMrE+tsHgU//eQmGV6xwjXpnXceDBli2akxxhhDhONIqeo4EZkCDAIycEMgLAXeVtXV5RBfmQSGP0hIsD5SEcvLgyeecJMMA7Rr5wbW7Nat2N2MMcaYmiTiuhpVXQs8VQ6xRJ3VSJVBQoLrExUf7yYZvuIKm2TYGGOMCVGtU4xAHym7ay9M27ZBfj40bQpxcW6C4dxcN9mwMcYYY/YTdiIlIl+EUUxVtU8Z4okqv921Fx5V+PxzePhhyMiAMWNcf6g2bWIdmTHGGFOpRZJitMfNrxe6/4G4u/82AruiFFdU7B3+wGqkirRxIzz4IMyY4ZZzcyE7G+oVN2SYMcYYYyCyu/ZSC1svIkm4iYwvA3pFJ6zo8Pmtaa9IqvCf/8Djj8POnVC3Llx7LZx5pmvWM8YYY0yJytzopaq7gQdEJBN4HDivzFFFiXU2L4LfD9ddB99+65aPPhpuvRVatIhpWMYYY0xVE80U4xvggSger0xU9zbtWY1UiLg41xdqwQK4/nro29cmGTbGGGNKIZptOO2AiO6PF5G+IrJYRJaJyE2FbB8hIgtFZJ6IfC4ibSM5vt/6SO31228wa9be5SuvhIkT4dRTLYkyxhhjSimSu/aKuoUrBTgJuAY3VUy4x4sHngZOBlYBs0RkiqouDCr2I5ClqtneqOoPA38N9xyB4Q9qdNNefj6MHw8vvgjJyfDOO9CggRsTKiUl1tEZY4wxVVokKcZy9r9rL0CAxbhkKlw9gWWq+huAiLwJ9Af2JFKqOj2o/PfAhREcf08fqRrbtLdwIdxzDyxd6pZ79bKO5MYYY0wURZJI3c3+iZQCm4ElwGeq6o/geC2BlUHLq4Ajiil/BfBhYRtEZDAwGKBN0NhHgT5SNW5auN274bnn4LXXXMfyli1h9Gg4/PBYR2aMMcZUK5EMf3BnOcZRLBG5EMiiiOEVVHUcMA4gKytrT7JXY0c2v/56+O47V/t0wQVw9dVQp06sozLGGGOqnbASKRGpD8wFnlLVJ6J07tVA66DlVt660HOfBNwK9PKGWghbje1sftFFsH69m2S4a9dYR2OMMcZUW2F1mFHVnUATYGcUzz0LSBORdiJSCzgXmBJcQEQOBZ4D+qnq+khPUGOGP/jmG3j++b3LPXvCG29YEmWMMcaUs0j6SH2Pa157IRonVtUCERkGfAzEAy+p6gIRuRuYrapTgEeA+sBEcbfor1DVfuGew+8TBK2+idTWrfDYY/Ch13XsmGMgM9M9t07lxhhjTLmLJJG6CfhCRGYCr6hqUXfwhU1VpwHTQtbdHvT8pLIc3+dzL7DaNe2pwqefukmGt26FpCQYMsQNsmmMMcaYClNsIuWNHbVBVXNw079swdVIPSwivwLZIbuoqvYpl0gjFBjZPCGumjXtrV/vJhn+6iu33KOHuyOvdevi9zPGGGNM1JVUI/U7buymN4D2uOEOVnjbKvXEbIH6MhGIj69GidRzz7kkql49N1/egAE2MrkxxhgTIyUlUuI9UNXUco8migoKAK0mY0j5/Xv7PA0b5kYrHzYMmjePbVzGGGNMDVdteyQXFLjqs4SEMnflih2/H/79b7jiCpc8ATRuDHffbUmUMcYYUwlU21noCrzpYapsjdSvv7qEacECt/z119C7d2xjMsYYY8w+wkmkjhORSEZAn1CGeKKmyiZS+fnw8svw0kvuRTRvDjffDMcdF+vIjDHGGBMinARpzzx2JRBca1qlSKT2zrNXhZr2Fi6Eu+5ytVEAAwfCNddA/fqxjcsYY4wxhQonkRqHG4yzSnE1Ulq1aqSWLHFJVKtWbnqXHj1iHZExxhhjihFOIvW1qr5e7pFEWaBpr9J3Nt+4EZo2dc/793eBn3461K4d27iMMcYYU6Jqe9ee30flHv5g50647z43DtSqVW6dCJx9tiVRxhhjTBVRbROpSt3Z/KuvYNAgeO89F+j8+bGOyBhjjDGlUG2HP/D5KuE4Ulu2wCOPwCefuOWuXeH226F9+9jGZYwxxphSKTaRUtUqW2NV6Wqkvv8ebr0Vtm1zTXdDh8K55+4dsdwYY4wxVU61rZHa29k8tnHs0bw5ZGdDz54uoWrZMtYRGWOMMaaMKkuaEXU+v/sZs3Gk/H7473/h2GNdJ/L27WH8eEhLs0mGjTHGmGqi2rYruanpYjSO1IoVcPXVMHw4fPzx3vXp6ZZEGWOMMdVI9a2RisU4Uj6fm2T42WchL89NMGxDGRhjjDHVVrVNpAoKAK3AvtxLl8I997hpXgBOOw1GjoSGDSsoAGOMMcZUtOqdSFFBnc1nznRz4vl80KKF60x+9NEVcGJjjDHGxFK1TaQC40hVSGfzQw5x8+P17AnDhkG9euV/TmOMMcbEXLVNpMp1HKmcHHjlFbjwQkhOhqQk1zfK+kMZY4wxNUq1TaR8Pvcz6k17//sf3Hsv/PknbN7smvHAkihjjDGmBqq2iZSrkdLoNe3t2AFPPAHvv++W09Nh4MDoHNsYY4wxVVK1TaQCNVJRadqbMQMefBA2boTERLjqKrj44ko0bLoxxhhjYqHaZgI+nxv4ssy5zpIlcP317nm3bm6S4dTUMh7UGGOMMdVBtU2k9nY2L2PTXno6nHMOtG0LgwbZJMPGGGOM2aPaZgUFPgUtRY3UunVuapd58/auu/FG+OtfLYkyxhhjzD6sRirA74d334WnnoLsbNi2DV56qfwCNMYYY0yVV20TKV9BYEDOMAqvWOGmd/nxR7fcuzeMGlWe4RljjDGmGqi2iVRYA3L6fPDaa/Dcc26S4ZQUuOkml0gZY4wxxpSg2idSxfaR2r4dxo93SdTpp8OIEdCgQYXEZ4wxxpiqr9omUkWObJ6X5zqNJyRA48Zw221uVPKjjqrwGI0xxhhTtVXb29D21kjJ3pXz5sH558Orr+5dd+KJlkQZY4wxplSqbyIVXCOVnQ2PPAJXXAHLl8Onn+6tsjLGGGOMKaWYJlIi0ldEFovIMhG5qZDtSSLylrd9poikhntsv5cnJf3xqxsD6q23QAQuvxxeeSVKc8cYY4wxpiaLWSIlIvHA08CpQCZwnohkhhS7Atiiqh2BfwIPhXv8glwftTeup95r42HNGujUyd2hN3Qo1KoVrZdhjDHGmBosljVSPYFlqvqbquYBbwL9Q8r0B8Z7z98B+oiIEIYCjScuP5+ERIFhw9zdeenpUQveGGOMMSaWd+21BFYGLa8CjiiqjKoWiMg2oAmwMbiQiAwGBgO0adMGgO7d4sge2IgD+o+A/m3K5QUYY4wxpmarFsMfqOo4YBxAVlaWApx/vnD++U1weZcxxhhjTPTFMpFaDbQOWm7lrSuszCoRSQAaApuKO+icOXM2isgf3mJTQmqvaii7Do5dB7sGAXYdnODr0DaWgRhTVcUykZoFpIlIO1zCdC5wfkiZKcAlwHfA2cAXqlrsLMSq2izwXERmq2pWVKOuguw6OHYd7BoE2HVw7DoYU3YxS6S8Pk/DgI+BeOAlVV0gIncDs1V1CvAi8KqILAM245ItY4wxxphKIaZ9pFR1GjAtZN3tQc9zgUEVHZcxxhhjTDiq7cjmnnGxDqCSsOvg2HWwaxBg18Gx62BMGUkJXY6MMcYYY0wRqnuNlDHGGGNMubFEyhhjjDGmlKpFIlWekx9XJWFchxEislBE5onI5yJS7caNKekaBJU7S0RURKrlrd/hXAcROcd7PywQkdcrOsaKEMb/iTYiMl1EfvT+X5wWizjLk4i8JCLrRWR+EdtFRJ70rtE8ETmsomM0piqr8olUeU9+XFWEeR1+BLJUtRtu7sKHKzbK8hXmNUBEkoFrgZkVG2HFCOc6iEgacDNwjKp2Aa6r6DjLW5jvh9HA26p6KG54lbEVG2WFeAXoW8z2U4E07zEYeKYCYjKm2qjyiRTlPPlxFVLidVDV6aqa7S1+jxtNvjoJ570AcA8umc6tyOAqUDjX4SrgaVXdAqCq6ys4xooQznVQoIH3vCHwZwXGVyFU9SvcOHxF6Q9MUOd7oJGIHFgx0RlT9VWHRKqwyY9bFlVGVQuAwOTH1Uk41yHYFcCH5RpRxSvxGnjNFq1V9YOKDKyChfNeSAfSReS/IvK9iBRXY1FVhXMd7gQuFJFVuDHt/lExoVUqkf7tMMYEqRaTFpvIiMiFQBbQK9axVCQRiQMeBy6NcSiVQQKuKecEXM3kVyJysKpujWVQMXAe8IqqPiYiR+FmUuiqqv5YB2aMqRqqQ41UJJMfE+7kx1VQONcBETkJuBXop6q7Kyi2ilLSNUgGugIzRGQ5cCQwpRp2OA/nvbAKmKKq+ar6O7AEl1hVJ+FchyuAtwFU9TugNm4i35okrL8dxpjCVYdEas/kxyJSC9dhdEpImcDkxxDm5MdVUInXQUQOBZ7DJVHVsU9MsddAVbepalNVTVXVVFw/sX6qOjs24ZabcP5PTMbVRiEiTXFNfb9VYIwVIZzrsALoAyAinXGJ1IYKjTL2pgAXe3fvHQlsU9U1sQ7KmKqiyjft2eTHTpjX4RGgPjDR62u/QlX7xSzoKAvzGlR7YV6Hj4FTRGQh4ANuUNVqVUsb5nUYCTwvIsNxHc8vrW5fskTkDVzS3NTrC3YHkAigqs/i+oadBiwDsoHLYhOpMVWTTRFjjDHGGFNK1aFpzxhjjDEmJiyRMsYYY4wpJUukjDHGGGNKyRIpY4wxxphSskTKGGOMMaaULJEyFU5E7hQRFZHUWMdSkSJ93SJyqVf+hHINzBhjTKlZImVKJCIneB/oRT2OjHWM4RKR1ELizxaR+SJyh4jUqeB4TvASrEYVed5wiciMkGuVLyJ/ishbItK1jMceICJ3RilUY4yJiSo/IKepUG/gBu8LtayiA4mCT4EJ3vNmwF9xE9geDfylnM55L/AgEDw1zwm4ARJfAbaGlH8VeBPIK6d4wrUbuNJ7XgfogRu08TQRyVLVxaU87gDcjAN3ljVAY4yJFUukTCR+UNXXYh1ElCwJfi0i8hRuSpFTRORwVZ0V7ROqagFQEEF5H27U8VgrCPm9P++NiD4GGAb8IzZhGWNM7FnTnokKEekpIq+IyBKvqWyHiPxXRM4Mc/8UEfmniPwqIrkisklE5ojIDYWU/auIfOOdI1tEZorI2WWJ30tyPvcWOwad60oR+UFEckRkm4h8IiLHFhLT/4nIlyKy0Su7QkQmiUh6UJl9+kiJyCu42iiA34Oaz+70tu/TR0pETvWWrynsNYjIdyKyQUQSg9alicirIrJGRPJEZLmIPCIi9Up9sZzAtdpnouNw3wciMgNv/suQpsNLg8ocKCLPeNcyz2tSHCcizcsYuzHGRI3VSJlI1BU3wW2w3aq6AzgTyADeBv4AmuA+KCeJyAWq+noJx54IHA88C8zDNSF1xjV9PRIoJCL3ArcCHwG3AX7v3BNFZJiqPl2G1xdICjZ653oIuBH4H3ALkAwMBqaLSH9VneaV64Wb+HU+8ACuie4g4CRcUrakiPM9BzTw4h8eOK/3+gvzCbAWuBh4MniDiKQBRwJPqmq+t64H8IUXz3PAaqA7cA1wjIj0CpQthQ7ez80h68N9H9yH+yJ3HHBR0P7ferG3Ab4DauHmyvwVdy2HACd6TYrbShm7McZEj6rawx7FPnDJjBbxeNMrU6+Q/eoCi4GFIevv9PZN9ZYbestjS4jjMK/c/YVsmwxsB5JLOEaqd4wXgKbeozOu/5ICvwNJQCdckvYNUCto/4NwiclyIN5b97i3b/MSzr3P6y5qXdC2S71tJwSte8RblxlS9h5v/WFB6+YCi0KvCS7ZCUzQW9LvfgawM+hatcb1bVruHeO0kPKRvA9ecX+CCj3v+8B6oFXI+ixc8+idsf5/YQ972MMeqmpNeyYi44CTQx73AqjqrkAhEakrIk1wH6BfAJ1FpEExx83BdWg+QoofGuAC3If3eBFpGvzA1QglA0eF+VquADZ4j4W4Wq6vgFNUdTfQHxDgYVXd09lbVf8EXgbaAod6qwM1I2eJSHnX8o73fl4cWCEiAlwIzFfVH7x1BwPdgNeBpJBr9Q2wCzglzHPWY++1WgG8h6spukS9WrmAMr4PAvs1BE7H/U5zQ2Jfjru5IdzYjTGmXFnTnonEUlX9rLANXr+Ve3EJSGF9WBrhaoz2o6p5InIdrvPy715H5i+Ayar6eVDRzrjkZlExMbYo4TUEvA/8C5eY5QLLVHVd0PZ23s8FhewbWNcemO0dpz8wFnhIRL7BNT2+oaobwownLKo6X0R+AC4QkVtU1Y9rEk3FNUMGdPZ+3uU9ChPutcoFzvCep+CSuJMppI9lWd4HQTp5x77CexTmt5KCNsaYimCJlCkzr0bkE9yH9xhccrENd8fZZcD5lHBjg6o+KyLvA/8H9ALOBoaJyFuqem7gVLjE51SKvputsMSnMKuKSgojpaqbRORwXH+fk3GJzT+Bu0TkNFX9LhrnCTIBeALoDXyGS2x8QPCddeL9fAyX1BVmS5jn8wVfKxF5B5gKjBORH1R1nre+zO+DkNhfY28NXKicMGM3xphyZYmUiYZuuE7Md6vqHcEbROTKwnfZn6quwfVdekFE4nHjKJ0nIo+pG45gKdAXWKGqv0Qt+sIFajy64Do6B8sMKYO6oQpmeA9EpBswBxiNSw6LoqWI7XVcX6mLReS/uKTzU+/6BSz1fvqilTAGqKpfRK7FNYk+yt5mtkjfB0W99mXetlrRjt0YY6LN+kiZaAjUDknwSnEjX5c4/IHXl6Zu8DovMQncvZbi/XzV+3m/l2iFHifcpqpwTMF9mN8QMpzAgbjalT+AH711oXcygmt+zGFv7EXZ6f0sqdweXnPhh8BAXL+xBuxfc/Mj7i7Cq0WkfegxRCRBRMI+ZyExLMUldCcHDQcR6ftgp7d9nzhUdRNu4NeBUsio+eI0K23sxhgTTVYjZaLhF1yT2o1eQrQYSAf+BvyMGwm7OOnAlyLyHu7DfwuueWgI7i66rwFUdZY3xtKdwE8iMhH4EzjQO8dpuE7QZaaqi0XkEVy/o69E5C32Dn9QH7jAS/bADVDZCtes9Qdu6Ia/euUn7HfwfX3v/XxIRP6N6480X1Xnl7DfeKAfruluG+6uxeD4VUQuwvU1myciL+F+R3VxwwgMBG7G3TlXWvfjOrnfBfQh8vfB97gBPceKyAdAPjBTVX/H/e6/wV37CbjEMA7XL60/7rreWYbYjTEmKiyRMmWmqj4R+T9cM88luLu85nvPu1NyIrUSeAk4EXdrfRJuzKPngYdUNTvoXHeJyGzcWEjXeeda752v0IEqS0tVR4nIMmAobmqXPGAmcL6qfh1U9FXcUAWX4Kab2Y5r9jpbVd8t4Rz/FZFRwNW415uAS0xKSqSm4sZwSgFeUNXcQo79k4gcikuY+nnn2IG78+0V9g6qWSpesvk2cK43JtWXEb4P3sDd+XguMAiXKF0G/K6qK71xsEbhEqcLcUnmSuA/uHGqjDEm5kS1NF00jDHGGGOM9ZEyxhhjjCklS6SMMcYYY0rJEiljjDHGmFKyRMoYY4wxppQskTLGGGOMKSVLpIwxxhhjSskSKWOMMcaYUrJEyhhjjDGmlCyRMsYYY4wppf8HEgrFse/o05AAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#test MWPM decoder for this fold\n",
    "#labels = targets[train], features = inputs[train]\n",
    "# x_test_d7 = translate_to_graph(testData_d7_MWPM, targs, mlb_d7)\n",
    "\"\"\"\n",
    "decoding_d7, time_mwpm = do_new_decoding(x_test_d7, 7, 0)\n",
    "decoding_d7['combine'] = decoding_d7[[0, 1]].values.tolist()\n",
    "decoding_d7['combine'].apply(lambda x: x[0].extend(x[1]))\n",
    "decoding_d7 = np.array(decoding_d7[0])\n",
    "\n",
    "time_per_fold_mwpm.append(time_mwpm)\n",
    "\n",
    "pred_mwpm = mlb_d7.transform(decoding_d7)\n",
    "\n",
    "if fold_no < 3:\n",
    "    acc = partial_accuracy(targets_test_2, pred_mwpm)\n",
    "else:\n",
    "    acc, contingency_mwpm = partial_accuracy_and_contingency(targets_test_2, pred_mwpm, mlb_d7)\"\"\"\n",
    "\n",
    "#acc_per_fold_mwpm.append(acc)\n",
    "#f1_per_fold_mwpm.append(f1_score(targets_test_2, pred_mwpm, average='micro'))\n",
    "\n",
    "#####################################################################################################\n",
    "#test the plut decoder for this fold\n",
    "\n",
    "#lookup_d7 = lookup_decoder(7)\n",
    "\n",
    "#lookup_d7 = train_plut(lookup_d7, inputs_train, targets[train])\n",
    "\n",
    "#start = time.time_ns()\n",
    "#pred_plut_d7 = test_plut(lookup_d7, inputs_test)\n",
    "#end = time.time_ns() \n",
    "#time_per_fold_plut.append((end - start)/ (10 ** 9))\n",
    "\n",
    "if fold_no < 3:\n",
    "    acc = partial_accuracy(targets[test], pred_plut_d7)\n",
    "    f1 = f1_score(targets[test], pred_plut_d7, average='micro')\n",
    "else:\n",
    "    pred_plut_d7 = test_plut(lookup_d7, inputs_test_2)\n",
    "    #f1 = f1_score(targets_test_2, pred_plut_d7, average='micro')\n",
    "    acc, contingency_plut = partial_accuracy_and_contingency(targets_test_2, pred_plut_d7, mlb_d7)\n",
    "\n",
    "#acc_per_fold_plut.append(acc)\n",
    "#f1_per_fold_plut.append(f1)\n",
    "\n",
    "#####################################################################################################\n",
    "#Test the NN decoder for this fold\n",
    "\"\"\"\n",
    "model_d7 = compile_FFNN_model_DepthSeven(7)\n",
    "\n",
    "# Generate a print\n",
    "print('------------------------------------------------------------------------')\n",
    "print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "# Fit data to model\n",
    "history = model_d7.fit(\n",
    "    x=inputs_train ,\n",
    "    y=targets[train],\n",
    "    validation_split=.25,\n",
    "    epochs= 150)\"\"\"\n",
    "\n",
    "pred = model_d7.predict(inputs_test_2)\n",
    "pred[pred>=.5]=1 \n",
    "pred[pred<.5]=0\n",
    "acc, contingency_nn = partial_accuracy_and_contingency(targets_test_2, pred, mlb_d7)\n",
    "f1 = f1_score(targets_test_2, pred, average='micro')\n",
    "\n",
    "#acc_per_fold.append(acc)\n",
    "f1_per_fold.append(f1)\n",
    "\n",
    "# Increase fold number\n",
    "fold_no = fold_no + 1\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(targets[test].ravel(), predictions_d7.ravel())\n",
    "tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "tprs[-1][0] = 0.0\n",
    "roc_auc = auc(fpr, tpr)\n",
    "aucs.append(roc_auc)\n",
    "plt.plot(fpr, tpr, lw=1, alpha=0.3, label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
    "\n",
    "#get the AUCs of each class, used to get average AUC of each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(targets[test][:, i], predictions_d7[:, i]) \n",
    "    aucs_classes[mlb_d7.classes_[i]].append(auc(fpr[i], tpr[i]))\n",
    "\n",
    "#########################################################################################################\n",
    "#compute McNemar's statistic on results of last fold\n",
    "\n",
    "mcnemar_results_mwpm = {}\n",
    "mcnemar_results_plut = {}\n",
    "\n",
    "for class_ in mlb_d7.classes_:\n",
    "    \n",
    "    #compute the x^2 for NN and MWPM\n",
    "    mcnemar_results_mwpm[class_]=contingency_table_and_t(contingency_nn[class_], contingency_mwpm[class_])[1]    \n",
    "    #comput the x^2 for NN and PLUT\n",
    "    mcnemar_results_plut[class_] = contingency_table_and_t(contingency_nn[class_], contingency_plut[class_])[1]\n",
    "        \n",
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds of MWPM:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold_mwpm)} (+- {np.std(acc_per_fold_mwpm)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold_mwpm)}(+- {np.std(f1_per_fold_mwpm)})')\n",
    "print(f'> Time: {np.mean(time_per_fold_mwpm)} (+- {np.std(time_per_fold_mwpm)})')\n",
    "print('Average scores for all folds of PLUT:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold_plut)} (+- {np.std(acc_per_fold_plut)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold_plut)}(+- {np.std(f1_per_fold_plut)})')\n",
    "print(f'> Time: {np.mean(time_per_fold_plut)} (+- {np.std(time_per_fold_plut)})')\n",
    "print('Average scores for all folds of NN:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold)}(+- {np.std(f1_per_fold)})')\n",
    "print(f'> Time: {np.mean(time_per_fold)} (+- {np.std(time_per_fold)})')\n",
    "for key in aucs_classes:\n",
    "    cleanedList = [x for x in aucs_classes[key] if str(x) != 'nan']\n",
    "    print(f'> AUC for class {key}: {np.mean(cleanedList)} (+- {np.std(cleanedList)})') #this has to be a for loop\n",
    "    print(\"X^2 for MWPM and NN: \" + str(mcnemar_results_mwpm[key]))  \n",
    "    print(\"X^2 for PLUT and NN: \" + str(mcnemar_results_plut[key]))\n",
    "print(\"###################################################################################\")\n",
    "print(\"TOTAL F1 NN: \" + str(f1_per_fold))\n",
    "print(\"TOTAL F1 PLUT: \" + str(f1_per_fold_plut))\n",
    "print(\"TOTAL F1 MWPM: \" + str(f1_per_fold_mwpm))\n",
    "print(\"TOTAL ACC NN: \" + str(acc_per_fold))\n",
    "print(\"TOTAL ACC PLUT: \" + str(acc_per_fold_plut))\n",
    "print(\"TOTAL ACC MWPM: \" + str(acc_per_fold_mwpm))\n",
    "print(\"TOTAL TIME NN: \" + str(time_per_fold))\n",
    "print(\"TOTAL TIME PLUT: \" + str(time_per_fold_plut))\n",
    "print(\"TOTAL TIME MWPM: \" + str(time_per_fold_mwpm))\n",
    "print('------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "         label='Chance', alpha=.8)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "         label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "         lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                 label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "plt.xlim([-0.01, 1.01])\n",
    "plt.ylim([-0.01, 1.01])\n",
    "plt.xlabel('False Positive Rate',fontsize=18)\n",
    "plt.ylabel('True Positive Rate',fontsize=18)\n",
    "plt.title('Cross-Validation ROC of Depth 7 NN',fontsize=18)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "model_d5 = compile_FFNN_model_DepthFive(5)\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "print(\"Fit model on training data\")\n",
    "history = model_d5.fit(\n",
    "    x=x_train_d5.values,\n",
    "    y=Y_train_d5,\n",
    "    validation_split=.25,\n",
    "    epochs = 500\n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Make a quick epoch vs. loss plot\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.grid()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Depth 5 Model Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (MSE)')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Make a quick epoch vs. accuracy plot\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.grid()\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Depth 5 Model Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# predict\n",
    "from sklearn.metrics import precision_score,accuracy_score, f1_score, recall_score, hamming_loss\n",
    "\n",
    "start = time.time()\n",
    "predictions_d5 = model_d5.predict(x_test_d5.values)\n",
    "end = time.time()\n",
    "print(\"Time: \" + str(end - start))\n",
    "\n",
    "pred=predictions_d5.copy()\n",
    "\n",
    "thresholds=[0.1, 0.2, 0.3, .4, .5, .6, .7, .8, .9]\n",
    "for val in thresholds:\n",
    "    pred=predictions_d5.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "    \n",
    "  \n",
    "    precision = precision_score(Y_test_d5, pred, average='micro')\n",
    "    recall = recall_score(Y_test_d5, pred, average='micro')\n",
    "    f1 = f1_score(Y_test_d5, pred, average='micro')\n",
    "   \n",
    "    print(\"Micro-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "    print(\"Accuracy = \",accuracy_score(Y_test_d5, pred))\n",
    "    print(\"Accuracy = \",partial_accuracy(Y_test_d5, pred))\n",
    "    print(\"\\n\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "pred=predictions_d5.copy()\n",
    "from sklearn.metrics import multilabel_confusion_matrix, classification_report\n",
    "#look at confusion matrix to see what got misclassified    \n",
    "pred[pred>=.5]=1\n",
    "pred[pred<.5]=0\n",
    "multilabel_confusion_matrix(Y_test_d5, pred)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#look at classifcation report to see what got mislabeled\n",
    "print(classification_report(Y_test_d5, pred))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit model on training data\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type int).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14152/3379054140.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel_d7\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompile_FFNN_model_DepthSeven\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Fit model on training data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m history = model_d7.fit(\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx_d7\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;31m#x=x_train_d7.values,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_d7\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;31m#y=Y_train_d7,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\research\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1131\u001b[0m          \u001b[0mtraining_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRespectCompiledTrainableState\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m       \u001b[1;31m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1133\u001b[1;33m       data_handler = data_adapter.get_data_handler(\n\u001b[0m\u001b[0;32m   1134\u001b[0m           \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m           \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\research\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1362\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"model\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_cluster_coordinator\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1363\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1364\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\research\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[0;32m   1152\u001b[0m     \u001b[0madapter_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1153\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_verify_data_adapter_compatibility\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madapter_cls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1154\u001b[1;33m     self._adapter = adapter_cls(\n\u001b[0m\u001b[0;32m   1155\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1156\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\research\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    245\u001b[0m                **kwargs):\n\u001b[0;32m    246\u001b[0m     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTensorLikeDataAdapter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 247\u001b[1;33m     \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_tensorlike\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    248\u001b[0m     sample_weight_modes = broadcast_sample_weight_modes(\n\u001b[0;32m    249\u001b[0m         sample_weights, sample_weight_modes)\n",
      "\u001b[1;32m~\\anaconda3\\envs\\research\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m_process_tensorlike\u001b[1;34m(inputs)\u001b[0m\n\u001b[0;32m   1044\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1045\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1046\u001b[1;33m   \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_convert_numpy_and_scipy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1047\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlist_to_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1048\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\research\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    866\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 867\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    868\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    869\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\research\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    866\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 867\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    868\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    869\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\research\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m_convert_numpy_and_scipy\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   1039\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloating\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1040\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloatx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1041\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor_v2_with_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1042\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0m_is_scipy_sparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1043\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_scipy_sparse_to_sparse_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\research\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\research\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2_with_dispatch\u001b[1;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[0;32m   1428\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtensor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mof\u001b[0m \u001b[0mgiven\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1429\u001b[0m   \"\"\"\n\u001b[1;32m-> 1430\u001b[1;33m   return convert_to_tensor_v2(\n\u001b[0m\u001b[0;32m   1431\u001b[0m       value, dtype=dtype, dtype_hint=dtype_hint, name=name)\n\u001b[0;32m   1432\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\research\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2\u001b[1;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[0;32m   1434\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mconvert_to_tensor_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype_hint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1435\u001b[0m   \u001b[1;34m\"\"\"Converts the given `value` to a `Tensor`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1436\u001b[1;33m   return convert_to_tensor(\n\u001b[0m\u001b[0;32m   1437\u001b[0m       \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1438\u001b[0m       \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\research\\lib\\site-packages\\tensorflow\\python\\profiler\\trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m           \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\research\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[0;32m   1564\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1565\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1566\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1567\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1568\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\research\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m   \u001b[1;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[1;31m# Unused.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\research\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    262\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcalled\u001b[0m \u001b[0mon\u001b[0m \u001b[0ma\u001b[0m \u001b[0msymbolic\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m   \"\"\"\n\u001b[1;32m--> 264\u001b[1;33m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0m\u001b[0;32m    265\u001b[0m                         allow_broadcast=True)\n\u001b[0;32m    266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\research\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    274\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tf.constant\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 276\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    277\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m   \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\research\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[1;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m   \u001b[1;34m\"\"\"Implementation of eager constant.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 301\u001b[1;33m   \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    302\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\research\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m     96\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m   \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type int)."
     ]
    }
   ],
   "source": [
    "model_d7 = compile_FFNN_model_DepthSeven(7)\n",
    "print(\"Fit model on training data\")\n",
    "history = model_d7.fit(\n",
    "    x=x_d7.values,#x=x_train_d7.values,\n",
    "    y=y_d7,#y=Y_train_d7,\n",
    "    validation_split=.25,\n",
    "    epochs= 150\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a quick epoch vs. loss plot\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.grid()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Depth 7 Model Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (MSE)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a quick epoch vs. accuracy plot\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.grid()\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Depth 7 Model Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\"\"\n",
    "# predict\n",
    "from sklearn.metrics import precision_score,accuracy_score, f1_score, recall_score, hamming_loss\n",
    "\n",
    "start = time.time()\n",
    "predictions_d7 = model_d7.predict(x_test_d7.values)\n",
    "end = time.time()\n",
    "print(\"Time: \" + str(end - start))\n",
    "\n",
    "pred=predictions_d7.copy()\n",
    "\n",
    "\n",
    "thresholds=[0.1, 0.2, 0.3, .4, .5, .6, .7, .8, .9]\n",
    "for val in thresholds:\n",
    "    pred=predictions_d7.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "    \n",
    "  \n",
    "    precision = precision_score(Y_test_d7, pred, average='micro')\n",
    "    recall = recall_score(Y_test_d7, pred, average='micro')\n",
    "    f1 = f1_score(Y_test_d7, pred, average='micro')\n",
    "   \n",
    "    print(\"Micro-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "    print(\"Accuracy = \",accuracy_score(Y_test_d7, pred))\n",
    "    print(\"Accuracy = \",partial_accuracy(Y_test_d7, pred))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "predictions_d7 = model_d7.predict(x_test_d7.values)\n",
    "pred=predictions_d7.copy()\n",
    "\n",
    "n_classes = 99\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(Y_test_d7[:, i], pred[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(Y_test_d7.ravel(), pred.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "    \n",
    "# Plot of a ROC curve for a specific class\n",
    "for i in range(n_classes):\n",
    "    plt.figure()\n",
    "    plt.plot(fpr[i], tpr[i], label='ROC curve (area = %0.2f)' % roc_auc[i])\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure()\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Some extension of Receiver operating characteristic to multi-class')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "x_train_d3.to_csv(\"x_train_d3_01.csv\")\n",
    "x_test_d3.to_csv(\"x_test_d3_01.csv\")\n",
    "pd.DataFrame(Y_train_d3).to_csv(\"Y_train_d3_01.csv\")\n",
    "pd.DataFrame(Y_test_d3).to_csv(\"Y_test_d3_01.csv\")\n",
    "\n",
    "x_train_d5.to_csv(\"x_train_d5_01.csv\")\n",
    "x_test_d5.to_csv(\"x_test_d5_01.csv\")\n",
    "pd.DataFrame(Y_train_d5).to_csv(\"Y_train_d5_01.csv\")\n",
    "pd.DataFrame(Y_test_d5).to_csv(\"Y_test_d5_01.csv\")\n",
    "\n",
    "x_train_d7.to_csv(\"x_train_d7_01.csv\")\n",
    "x_test_d7.to_csv(\"x_test_d7_01.csv\")\n",
    "pd.DataFrame(Y_train_d7).to_csv(\"Y_train_d7_01.csv\")\n",
    "pd.DataFrame(Y_test_d7).to_csv(\"Y_test_d7_01.csv\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "model_d7.save(\"model_d7_01.h5\")\n",
    "model_d5.save(\"model_d5_01.h5\")\n",
    "model.save(\"model_d3_01.h5\")\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
