{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\clair\\Anaconda3\\envs\\CNNs\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.metrics.scorer module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import time\n",
    "import random\n",
    "import copy\n",
    "\n",
    "from scipy import interp\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, ShuffleSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.layers import Dense, Input, Dropout\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import precision_score,accuracy_score, f1_score, recall_score, hamming_loss\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras import optimizers\n",
    "from keras import callbacks\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras import backend as K\n",
    "\n",
    "#%matplotlib inline\n",
    "#import mpld3\n",
    "#mpld3.enable_notebook()\n",
    "\n",
    "print (pd.__version__)\n",
    "\n",
    "######### DEFINITION OF GLOBAL VARIABLES #########\n",
    "RUN_CONFIGURATION_LOOP = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "#sys.path.append('/')\n",
    "import circuits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are super long functions to be hard coded because i dont have time to properly fix them, sorry bout it\n",
    "#[(0, 0.5, 0.5), (0, 0.5, 2.5), (0, 1.5, -0.5), (0, 1.5, 1.5)]\n",
    "def graph_with_errs_d3(df):\n",
    "    x_data = []\n",
    "    z_data = []\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        x_data.append([])\n",
    "        z_data.append([])\n",
    "        \n",
    "        if df.loc[i].at[\"X0\"] == -1:\n",
    "            x_data[i].append((0, -0.5, 0.5))\n",
    "        if df.loc[i].at[\"Z1\"] == -1:\n",
    "            z_data[i].append((0, 0.5, 0.5))\n",
    "        if df.loc[i].at[\"X2\"] == -1:\n",
    "            x_data[i].append((0, 0.5, 1.5))\n",
    "        if df.loc[i].at[\"Z3\"] == -1:\n",
    "            z_data[i].append((0, 0.5, 2.5))\n",
    "        if df.loc[i].at[\"Z4\"] == -1:\n",
    "            z_data[i].append((0, 1.5, -0.5))\n",
    "        if df.loc[i].at[\"X5\"] == -1:\n",
    "            x_data[i].append((0, 1.5, 0.5))\n",
    "        if df.loc[i].at[\"X6\"] == -1:\n",
    "            x_data[i].append((0, 2.5, 1.5))\n",
    "        if df.loc[i].at[\"Z7\"] == -1:\n",
    "            z_data[i].append((0, 1.5, 1.5))\n",
    "        x_data[i] = str(x_data[i])\n",
    "        z_data[i] = str(z_data[i])\n",
    "    #graph_df = pd.DataFrame(df[\"Labels\"], x_data, z_data, columns=[\"Labels\", \"XSyn\", \"ZSyn\"])\n",
    "    graph_df = pd.DataFrame({\"XSyn\":x_data, \"ZSyn\":z_data})\n",
    "    return graph_df\n",
    "        \n",
    "\n",
    "def graph_with_errs_d5(df):\n",
    "    x_data = []\n",
    "    z_data = []\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        x_data.append([])\n",
    "        z_data.append([])\n",
    "        if df.loc[i].at[\"X0\"] == -1:\n",
    "             x_data[i].append((0, -0.5, 0.5))\n",
    "        if df.loc[i].at[\"Z1\"] == -1:\n",
    "            z_data[i].append((0, 0.5, 0.5))\n",
    "        if df.loc[i].at[\"X2\"] == -1:\n",
    "            x_data[i].append((0, 0.5, 1.5))\n",
    "        if df.loc[i].at[\"X3\"] == -1:\n",
    "            x_data[i].append((0, -0.5, 2.5))\n",
    "        if df.loc[i].at[\"Z4\"] == -1:\n",
    "            z_data[i].append((0, 0.5, 2.5))\n",
    "        if df.loc[i].at[\"X5\"] == -1:\n",
    "            x_data[i].append((0, 0.5, 3.5))\n",
    "        if df.loc[i].at[\"Z6\"] == -1:\n",
    "            z_data[i].append((0, 0.5, 4.5))\n",
    "        if df.loc[i].at[\"Z7\"] == -1:\n",
    "            z_data[i].append((0, 1.5, -0.5))\n",
    "        if df.loc[i].at[\"X8\"] == -1:\n",
    "            x_data[i].append((0, 1.5, 0.5))\n",
    "        if df.loc[i].at[\"Z9\"] == -1:\n",
    "            z_data[i].append((0, 1.5, 1.5))\n",
    "        if df.loc[i].at[\"X10\"] == -1:\n",
    "            x_data[i].append((0, 1.5, 2.5))\n",
    "        if df.loc[i].at[\"Z11\"] == -1:\n",
    "            z_data[i].append((0, 1.5, 3.5))\n",
    "        if df.loc[i].at[\"Z12\"] == -1:\n",
    "            z_data[i].append((0, 2.5, 0.5))\n",
    "        if df.loc[i].at[\"X13\"] == -1:\n",
    "            x_data[i].append((0, 2.5, 1.5))\n",
    "        if df.loc[i].at[\"Z14\"] == -1:\n",
    "            z_data[i].append((0, 2.5, 2.5))\n",
    "        if df.loc[i].at[\"X15\"] == -1:\n",
    "            x_data[i].append((0, 2.5, 3.5))\n",
    "        if df.loc[i].at[\"Z16\"] == -1:\n",
    "            z_data[i].append((0, 2.5, 4.5))\n",
    "        if df.loc[i].at[\"Z17\"] == -1:\n",
    "            z_data[i].append((0, 3.5, -0.5))\n",
    "        if df.loc[i].at[\"X18\"] == -1:\n",
    "            x_data[i].append((0, 3.5, 0.5))\n",
    "        if df.loc[i].at[\"X19\"] == -1:\n",
    "            x_data[i].append((0, 4.5, 1.5))\n",
    "        if df.loc[i].at[\"Z20\"] == -1:\n",
    "            z_data[i].append((0, 3.5, 1.5))\n",
    "        if df.loc[i].at[\"X21\"] == -1:\n",
    "            x_data[i].append((0, 3.5, 2.5))\n",
    "        if df.loc[i].at[\"X22\"] == -1:\n",
    "            x_data[i].append((0, 4.5, 3.5))\n",
    "        if df.loc[i].at[\"Z23\"] == -1:\n",
    "            z_data[i].append((0, 3.5, 3.5))\n",
    "        x_data[i] = str(x_data[i])\n",
    "        z_data[i] = str(z_data[i])\n",
    "            \n",
    "    graph_df = pd.DataFrame({\"XSyn\":x_data, \"ZSyn\":z_data})\n",
    "    return graph_df\n",
    "    \n",
    "def graph_with_errs_d7(df):\n",
    "    x_data = []\n",
    "    z_data = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        x_data.append([])\n",
    "        z_data.append([])\n",
    "        \n",
    "        if df.loc[i].at[\"X0\"] == -1:\n",
    "            x_data[i].append((0, -0.5, 0.5))\n",
    "        if df.loc[i].at[\"Z1\"] == -1:\n",
    "            z_data[i].append((0, 0.5, 0.5))\n",
    "        if df.loc[i].at[\"X2\"] == -1:\n",
    "            x_data[i].append((0, 0.5, 1.5))\n",
    "        if df.loc[i].at[\"X3\"] == -1:\n",
    "            x_data[i].append((0, -0.5, 2.5))\n",
    "        if df.loc[i].at[\"Z4\"] == -1:\n",
    "            z_data[i].append((0, 0.5, 2.5))\n",
    "        if df.loc[i].at[\"X5\"] == -1:\n",
    "            x_data[i].append((0, 0.5, 3.5))\n",
    "        if df.loc[i].at[\"X6\"] == -1:\n",
    "            x_data[i].append((0, -0.5, 4.5))\n",
    "        if df.loc[i].at[\"Z7\"] == -1:\n",
    "            z_data[i].append((0, 0.5, 4.5))\n",
    "        if df.loc[i].at[\"X8\"] == -1:\n",
    "            x_data[i].append((0, 0.5, 5.5))\n",
    "        if df.loc[i].at[\"Z9\"] == -1:\n",
    "            z_data[i].append((0, 0.5, 6.5))\n",
    "        if df.loc[i].at[\"Z10\"] == -1:\n",
    "            z_data[i].append((0, 1.5, -0.5))\n",
    "        if df.loc[i].at[\"X11\"] == -1:\n",
    "            x_data[i].append((0, 1.5, 0.5))\n",
    "        if df.loc[i].at[\"Z12\"] == -1:\n",
    "            z_data[i].append((0, 1.5, 1.5))\n",
    "        if df.loc[i].at[\"X13\"] == -1:\n",
    "            x_data[i].append((0, 1.5, 2.5))\n",
    "        if df.loc[i].at[\"Z14\"] == -1:\n",
    "            z_data[i].append((0, 1.5, 3.5))\n",
    "        if df.loc[i].at[\"X15\"] == -1:\n",
    "            x_data[i].append((0, 1.5, 4.5))\n",
    "        if df.loc[i].at[\"Z16\"] == -1:\n",
    "            z_data[i].append((0, 1.5, 5.5))\n",
    "        if df.loc[i].at[\"Z17\"] == -1:\n",
    "            z_data[i].append((0, 2.5, 0.5))\n",
    "        if df.loc[i].at[\"X18\"] == -1:\n",
    "            x_data[i].append((0, 2.5, 1.5))\n",
    "        if df.loc[i].at[\"Z19\"] == -1:\n",
    "            z_data[i].append((0, 2.5, 2.5))\n",
    "        if df.loc[i].at[\"X20\"] == -1:\n",
    "            x_data[i].append((0, 2.5, 3.5))\n",
    "        if df.loc[i].at[\"Z21\"] == -1:\n",
    "            z_data[i].append((0, 2.5, 4.5))\n",
    "        if df.loc[i].at[\"X22\"] == -1:\n",
    "            x_data[i].append((0, 2.5, 5.5))\n",
    "        if df.loc[i].at[\"Z23\"] == -1:\n",
    "            z_data[i].append((0, 2.5, 6.5))\n",
    "        if df.loc[i].at[\"Z24\"] == -1:\n",
    "            z_data[i].append((0, 3.5, -0.5))\n",
    "        if df.loc[i].at[\"X25\"] == -1:\n",
    "            x_data[i].append((0, 3.5, 0.5))\n",
    "        if df.loc[i].at[\"Z26\"] == -1:\n",
    "            z_data[i].append((0, 3.5, 1.5))\n",
    "        if df.loc[i].at[\"X27\"] == -1:\n",
    "            x_data[i].append((0, 3.5, 2.5))\n",
    "        if df.loc[i].at[\"Z28\"] == -1:\n",
    "            z_data[i].append((0, 3.5, 3.5))\n",
    "        if df.loc[i].at[\"X29\"] == -1:\n",
    "            x_data[i].append((0, 3.5, 4.5))\n",
    "        if df.loc[i].at[\"Z30\"] == -1:\n",
    "            z_data[i].append((0, 3.5, 5.5))\n",
    "        if df.loc[i].at[\"Z31\"] == -1:\n",
    "            z_data[i].append((0, 4.5, 0.5))\n",
    "        if df.loc[i].at[\"X32\"] == -1:\n",
    "            x_data[i].append((0, 4.5, 1.5))\n",
    "        if df.loc[i].at[\"Z33\"] == -1:\n",
    "            z_data[i].append((0, 4.5, 2.5))\n",
    "        if df.loc[i].at[\"X34\"] == -1:\n",
    "            x_data[i].append((0, 4.5, 3.5))\n",
    "        if df.loc[i].at[\"Z35\"] == -1:\n",
    "            z_data[i].append((0, 4.5, 4.5))\n",
    "        if df.loc[i].at[\"X36\"] == -1:\n",
    "            x_data[i].append((0, 4.5, 5.5))\n",
    "        if df.loc[i].at[\"Z37\"] == -1:\n",
    "            z_data[i].append((0, 4.5, 6.5))\n",
    "        if df.loc[i].at[\"Z38\"] == -1:\n",
    "            z_data[i].append((0, 5.5, -0.5))\n",
    "        if df.loc[i].at[\"X39\"] == -1:\n",
    "            x_data[i].append((0, 5.5, 0.5))\n",
    "        if df.loc[i].at[\"X40\"] == -1:\n",
    "            x_data[i].append((0, 6.5, 1.5))\n",
    "        if df.loc[i].at[\"Z41\"] == -1:\n",
    "            z_data[i].append((0, 5.5, 1.5))\n",
    "        if df.loc[i].at[\"X42\"] == -1:\n",
    "            x_data[i].append((0, 5.5, 2.5))\n",
    "        if df.loc[i].at[\"X43\"] == -1:\n",
    "            x_data[i].append((0, 6.5, 3.5))\n",
    "        if df.loc[i].at[\"Z44\"] == -1:\n",
    "            z_data[i].append((0, 5.5, 3.5))\n",
    "        if df.loc[i].at[\"X45\"] == -1:\n",
    "            x_data[i].append((0, 5.5, 4.5))\n",
    "        if df.loc[i].at[\"X46\"] == -1:\n",
    "            x_data[i].append((0, 6.5, 5.5))\n",
    "        if df.loc[i].at[\"Z47\"] == -1: \n",
    "            z_data[i].append((0, 5.5, 5.5))\n",
    "        x_data[i] = str(x_data[i])\n",
    "        z_data[i] = str(z_data[i])\n",
    "    graph_df = pd.DataFrame({\"XSyn\":x_data, \"ZSyn\":z_data})\n",
    "    return graph_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These functions needed to work with the GraphDecoder/MWPM module\n",
    "import time\n",
    "\n",
    "def add_measurement_errs(curr_syn, prob_err, x_syn, depth):\n",
    "    #x_syn is True if it is x syndrome, False if it is Z syndrome\n",
    "    total_time = 0\n",
    "    new_syn = []\n",
    "    if x_syn:\n",
    "        for i in curr_syn:\n",
    "            rand = random.random()\n",
    "            if rand > prob_err:\n",
    "                new_syn.append(i)\n",
    "        return (new_syn + return_xmeasurement_errs(depth, prob_err))\n",
    "    else:\n",
    "        for i in curr_syn:\n",
    "            rand = random.random()\n",
    "            if rand > prob_err:\n",
    "                new_syn.append(i)\n",
    "        return (new_syn + return_zmeasurement_errs(depth, prob_err))\n",
    "    \n",
    "def do_new_decoding(data, depth, prob_err):\n",
    "    decoder = circuits.GraphDecoder(depth,1)\n",
    "    G = decoder.S['Z']\n",
    "    #decoder.graph_2D(G,'distance')\n",
    "    df = pd.DataFrame()\n",
    "    syn = []\n",
    "    total_time = 0\n",
    "    \n",
    "    for row in data:\n",
    "        x_input = []\n",
    "        z_input = []\n",
    "        x_type = True\n",
    "        for col in row:\n",
    "            if not col == \"[]\":\n",
    "                col = eval(col)\n",
    "                for c in col:\n",
    "                    if x_type:\n",
    "                        x_input.append(c)\n",
    "                    else:\n",
    "                        z_input.append(c)\n",
    "            x_type = not x_type  \n",
    "            \n",
    "        if prob_err > 0:\n",
    "            syndromes_x = add_measurement_errs(x_input, prob_err, True, depth)\n",
    "            syndromes_z = add_measurement_errs(z_input, prob_err, False, depth)\n",
    "        else:\n",
    "            syndromes_x = x_input\n",
    "            syndromes_z = z_input\n",
    "\n",
    "        start = time.time_ns()\n",
    "        error_graph_x, paths_x = decoder.make_error_graph(syndromes_x,'X')\n",
    "        matching_graph_x = decoder.matching_graph(error_graph_x,'X')\n",
    "        matches_x = decoder.matching(matching_graph_x,'X')\n",
    "        flips_x = decoder.calculate_qubit_flips(matches_x, paths_x,'X')\n",
    "        syn_x = (translate_errors(flips_x))\n",
    "\n",
    "        error_graph_z, paths_z = decoder.make_error_graph(syndromes_z,'Z')\n",
    "        matching_graph_z = decoder.matching_graph(error_graph_z,'Z')\n",
    "        matches_z = decoder.matching(matching_graph_z,'Z')\n",
    "        flips_z = decoder.calculate_qubit_flips(matches_z, paths_z,'Z')\n",
    "        syn_z = translate_errors(flips_z)\n",
    "        df = df.append(pd.Series([syn_x, syn_z]), ignore_index=True)\n",
    "        end = time.time_ns()\n",
    "        total_time += (end - start)/ (10 ** 9)\n",
    "    return (df, total_time) \n",
    "\n",
    "import random\n",
    "def return_xmeasurement_errs(depth, prob):\n",
    "    \n",
    "    new_errs = []\n",
    "    \n",
    "    if depth == 3:\n",
    "        errs = [(0, -0.5, 0.5), (0, 0.5, 1.5), (0, 1.5, 0.5), (0, 2.5, 1.5)]\n",
    "    elif depth == 5:\n",
    "        errs = [(0, -0.5, 0.5), (0, 0.5, 1.5), (0, -0.5, 2.5), (0, 0.5, 3.5), (0, 1.5, 0.5), (0, 1.5, 2.5),\n",
    "                        (0, 2.5, 1.5), (0, 2.5, 3.5), (0, 3.5, 0.5), (0, 4.5, 1.5), (0, 3.5, 2.5), (0, 4.5, 3.5)]\n",
    "    else:\n",
    "        errs = [(0, -0.5, 0.5), (0, 0.5, 1.5), (0, -0.5, 2.5), (0, 0.5, 3.5), (0, -0.5, 4.5), (0, 0.5, 5.5),\n",
    "                        (0, 1.5, 0.5), (0, 1.5, 2.5), (0, 1.5, 4.5), (0, 2.5, 1.5), (0, 2.5, 3.5), (0, 2.5, 5.5),\n",
    "                        (0, 3.5, 0.5),  (0, 3.5, 2.5), (0, 3.5, 4.5), (0, 4.5, 1.5), (0, 4.5, 3.5), (0, 4.5, 5.5),\n",
    "                       (0, 5.5, 0.5), (0, 6.5, 1.5), (0, 5.5, 2.5), (0, 6.5, 3.5), (0, 5.5, 4.5), (0, 6.5, 5.5)]\n",
    "    for e in errs:\n",
    "        rand = random.random()\n",
    "        if rand <= prob:\n",
    "            new_errs.append(e)\n",
    "            \n",
    "    return new_errs\n",
    "            \n",
    "\n",
    "def return_zmeasurement_errs(depth, prob):\n",
    "    \n",
    "    new_errs = []\n",
    "    \n",
    "    if depth == 3:\n",
    "        errs = [(0, 0.5, 0.5), (0, 0.5, 2.5), (0, 1.5, -0.5), (0, 1.5, 1.5)]\n",
    "    elif depth == 5:\n",
    "        errs = [(0, 0.5, 0.5), (0, 0.5, 2.5), (0, 0.5, 4.5), (0, 1.5, -0.5), (0, 1.5, 1.5), (0, 1.5, 3.5),\n",
    "                        (0, 2.5, 0.5), (0, 2.5, 2.5), (0, 2.5, 4.5), (0, 3.5, -0.5), (0, 3.5, 1.5), (0, 3.5, 3.5)]\n",
    "    else:\n",
    "        errs = [(0, 0.5, 0.5), (0, 0.5, 2.5), (0, 0.5, 4.5), (0, 0.5, 6.5), (0, 1.5, -0.5), (0, 1.5, 1.5),\n",
    "                        (0, 1.5, 3.5), (0, 1.5, 5.5), (0, 2.5, 0.5), (0, 2.5, 2.5), (0, 2.5, 4.5), (0, 2.5, 6.5),\n",
    "                        (0, 3.5, -0.5),  (0, 3.5, 1.5), (0, 3.5, 3.5), (0, 3.5, 5.5), (0, 4.5, 0.5), (0, 4.5, 2.5),\n",
    "                       (0, 4.5, 4.5), (0, 4.5, 6.5), (0, 5.5, -0.5), (0, 5.5, 1.5), (0, 5.5, 3.5), (0, 5.5, 5.5)]\n",
    "        \n",
    "    for e in errs:\n",
    "        rand = random.random()\n",
    "        if rand <= prob:\n",
    "            new_errs.append(e)\n",
    "            \n",
    "    return new_errs\n",
    "\n",
    "def translate_errors (phys_errs):\n",
    "    flipX = np.array([(0, 1),(1, 0)])\n",
    "    flipZ = np.array([(1, 0), (0, -1)])\n",
    "    errs = []\n",
    "    str2 = \"\"\n",
    "    for qubit, flip in phys_errs.items():\n",
    "        row = int(qubit[1])\n",
    "        col = int(qubit[2])\n",
    "        if str(flip) == \"X\":\n",
    "            str1 = \"X\"\n",
    "        elif str(flip) == \"Z\":\n",
    "            str1 = \"Z\"\n",
    "        else:\n",
    "            str1 = \"X\"\n",
    "            str2 = \"Z\"\n",
    "        str1 += str(row) + str(col)\n",
    "        errs.append(str1)\n",
    "        if str2 != \"\":\n",
    "            str2 += str(row) +str(col)\n",
    "            errs.append(str2)\n",
    "            str2 = \"\"\n",
    "    return errs   \n",
    "\n",
    "def translate_to_graph(df_graph, labels, mlb):\n",
    "#go through labels given\n",
    "    indices = []\n",
    "    labels = mlb.inverse_transform(labels)\n",
    "    \n",
    "    for row in labels:\n",
    "        label_str = str(row)\n",
    "        for index, r in df_graph.iterrows():\n",
    "            if label_str == \"('',)\":\n",
    "                if str(r[\"Labels\"]) == \"[' ']\":\n",
    "                    indices.append([index])\n",
    "                    break\n",
    "            if set(row) == set(r[\"Labels\"]):\n",
    "                indices.append([index])\n",
    "                break\n",
    "\n",
    "    df_syns = df_graph.drop(['Labels'], axis=1)\n",
    "    return_df = pd.DataFrame()\n",
    "    for i in indices:\n",
    "        return_df = return_df.append(df_syns.loc[i])\n",
    "    return return_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function needed for preprocessing. CSV file reads in a string, needs to be a list for labels \n",
    "#for preprocessing csv files\n",
    "def create_list_from_string(err_list):\n",
    "    newstring = err_list.replace(\"'\", \"\")\n",
    "    new_err_list = newstring.strip('][').split(', ')\n",
    "    return sorted(set(new_err_list))\n",
    "\n",
    "\n",
    "def create_string_from_list(err_list):\n",
    "    return_string = \"[\"\n",
    "    if err_list[0] == \"''\":\n",
    "        return \"[' ']\"\n",
    "    else:\n",
    "        for index, i in enumerate(err_list):\n",
    "            return_string = return_string +  \"'\" + i + \"'\"\n",
    "            if index < (len(err_list)-1):\n",
    "                return_string += \", \"\n",
    "    return return_string + \"]\"\n",
    "        \n",
    "\n",
    "#take in two 2d arrays of predicted values, true values, and threshold to determine labels. \n",
    "#calculates the partial accuracy of the predicted values, averaged out for all obersvations\n",
    "def partial_accuracy(y_pred, y_true):\n",
    "    total = 0\n",
    "    rows = y_pred.shape[0]\n",
    "    cols = y_pred.shape[1]\n",
    "    for i in range(0, rows):\n",
    "        row_correct = 0\n",
    "        for j in range(0, cols):\n",
    "            if y_pred[i,j] == y_true[i,j]:\n",
    "                row_correct += 1\n",
    "        total += row_correct/cols\n",
    "    return total/rows\n",
    "\n",
    "def partial_accuracy_and_contingency(y_pred, y_true, mlb):\n",
    "    total = 0\n",
    "    a= np.zeros(shape=y_true.shape)\n",
    "    rows = y_pred.shape[0]\n",
    "    cols = y_pred.shape[1]\n",
    "    df = pd.DataFrame(a, columns = mlb.classes_)\n",
    "    for i in range(0, rows):\n",
    "        row_correct = 0\n",
    "        for j, label in enumerate(mlb.classes_):\n",
    "            if y_pred[i,j] == y_true[i,j]:\n",
    "                row_correct += 1\n",
    "                df.at[i, label] = 1\n",
    "            else:\n",
    "                df.at[i, label] = 0\n",
    "\n",
    "        total += row_correct/cols\n",
    "\n",
    "    return (total/rows, df)\n",
    "\n",
    "def contingency_table_and_t (clf1, clf2):\n",
    "    a = 0 #clf1 pos, clf2 pos\n",
    "    b = 0 #clf1 pos, clf2 neg\n",
    "    c = 0 #clf1 neg, clf2 pos\n",
    "    d = 0 #clf1 neg, clf2 neg\n",
    "    \n",
    "    for index, value in clf1.items():\n",
    "        if value == 1 and clf2[index] == 1:\n",
    "            a+=1\n",
    "        elif value == 1 and clf2[index] == 0: #classifier 1 right, classifier 2 wrong\n",
    "            b+=1\n",
    "        elif value == 0 and clf2[index] == 1: #classifier 1 wrong, classifier 2 right\n",
    "            c+=1\n",
    "        else:\n",
    "            d+=1\n",
    "    print(\"[\"+str(a)+\", \"+str(b)+\"]\")\n",
    "    print(\"[\"+str(c)+\", \"+str(d)+\"]\")\n",
    "    if b == 0 and c ==0:\n",
    "        print(\"both b and c are zero\")\n",
    "        t=0\n",
    "    else:\n",
    "        t = (((b-c)-1)**2)/(b+c)\n",
    "    return ([[a,b],[c,d]], t)\n",
    "\n",
    "def add_noise(val, noise_level):\n",
    "    rand = random.uniform(0, 1)\n",
    "    if rand <= noise_level:\n",
    "        if val == -1:\n",
    "            val = 1\n",
    "        elif val == 1:\n",
    "            val = -1\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_d7= trainData_d7.dropna()\n",
    "#######################################################################################################\n",
    "\n",
    "trainData_d7 = pd.read_csv(\"depth7_all_combos.csv\")\n",
    "trainData_d7 = trainData_d7.applymap(lambda x: add_noise(x,.01))\n",
    "\n",
    "#These four lines remove duplicates\n",
    "trainData_d7['Labels'] = trainData_d7['Labels'].map(lambda x: create_list_from_string(x))\n",
    "trainData_d7['Labels'] = trainData_d7['Labels'].astype(str)\n",
    "trainData_d7 = trainData_d7.drop_duplicates('Labels', keep='first', ignore_index=True)\n",
    "trainData_d7['Labels'] = trainData_d7['Labels'].map(lambda x: create_list_from_string(x))\n",
    "\n",
    "testData_d7_MWPM = graph_with_errs_d7(trainData_d7)\n",
    "\n",
    "#transforms the data to encoding for ML\n",
    "mlb_d7 = MultiLabelBinarizer()\n",
    "mlb_d7.fit(trainData_d7['Labels'])\n",
    "df = pd.DataFrame(mlb_d7.transform(trainData_d7['Labels']))\n",
    "df['Labels']= df.values.tolist()\n",
    "trainData_d7 = trainData_d7.drop(['Labels'], axis=1)\n",
    "trainData_d7 = pd.concat([df[\"Labels\"],testData_d7_MWPM, trainData_d7], axis=1, ignore_index=True)\n",
    "trainData_d7.columns = [\"Labels\",\"XSyn\", \"ZSyn\",\"X0\", \"Z1\", \"X2\", \"X3\", \"Z4\", \"X5\", \"X6\", \"Z7\", \"X8\", \"Z9\", \"Z10\", \"X11\", \"Z12\", \"X13\", \"Z14\", \"X15\", \"Z16\", \"Z17\", \"X18\", \"Z19\",\"X20\", \"Z21\", \"X22\", \"Z23\", \"Z24\", \"X25\", \"Z26\", \"X27\", \"Z28\", \"X29\", \"Z30\", \"Z31\", \"X32\", \"Z33\", \"X34\", \"Z35\", \"X36\", \"Z37\", \"Z38\", \"X39\", \"X40\", \"Z41\", \"X42\", \"X43\", \"Z44\", \"X45\", \"X46\", \"Z47\"]\n",
    "\n",
    "#######################################################################################\n",
    "\n",
    "trainData_d5 = pd.read_csv(\"depth5_all_combos.csv\")\n",
    "trainData_d5 = trainData_d5.applymap(lambda x: add_noise(x,.01)) #was .05\n",
    "#These four lines remove duplicates\n",
    "trainData_d5['Labels'] = trainData_d5['Labels'].map(lambda x: create_list_from_string(x))\n",
    "trainData_d5['Labels'] = trainData_d5['Labels'].astype(str)\n",
    "trainData_d5 = trainData_d5.drop_duplicates('Labels', keep='first', ignore_index=True)\n",
    "trainData_d5['Labels'] = trainData_d5['Labels'].map(lambda x: create_list_from_string(x))\n",
    "\n",
    "\n",
    "testData_d5_MWPM = graph_with_errs_d5(trainData_d5)\n",
    "\n",
    "\n",
    "#transforms the data to encoding for ML\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(trainData_d5['Labels'])\n",
    "df = pd.DataFrame(mlb.transform(trainData_d5['Labels']))\n",
    "df['Labels']= df.values.tolist()\n",
    "trainData_d5 = trainData_d5.drop(['Labels'], axis=1)\n",
    "trainData_d5 = pd.concat([df[\"Labels\"], testData_d5_MWPM, trainData_d5], axis=1, ignore_index=True)\n",
    "trainData_d5.columns = [\"Labels\",\"XSyn\", \"ZSyn\",\"X0\",\"Z1\",\"X2\",\"X3\",\"Z4\",\"X5\",\"Z6\",\"Z7\",\"X8\",\"Z9\",\"X10\",\"Z11\",\"Z12\",\"X13\",\"Z14\",\"X15\",\"Z16\",\"Z17\",\"X18\",\"X19\",\"Z20\",\"X21\",\"X22\",\"Z23\"]\n",
    "#########################################################################################\n",
    "\n",
    "#Has no duplicates, small enough to check manually\n",
    "trainData_d3 = pd.read_csv(\"depth3_all_combos.csv\")\n",
    "\n",
    "trainData_d3[\"Labels\"] = trainData_d3['Labels'].map(lambda x: create_list_from_string(x))\n",
    "trainData_d3 = trainData_d3.applymap(lambda x: add_noise(x,.01))\n",
    "\n",
    "testData_d3_MWPM = graph_with_errs_d3(trainData_d3)\n",
    "\n",
    "mlb_d3 = MultiLabelBinarizer()\n",
    "mlb_d3.fit(trainData_d3[\"Labels\"])\n",
    "df = pd.DataFrame(mlb_d3.transform(trainData_d3['Labels']))\n",
    "df['Labels']= df.values.tolist()\n",
    "trainData_d3 = trainData_d3.drop(['Labels'], axis=1)\n",
    "trainData_d3 = pd.concat([df['Labels'], testData_d3_MWPM, trainData_d3], axis=1, ignore_index=True)\n",
    "trainData_d3.columns = [\"Labels\",\"XSyn\", \"ZSyn\", \"X0\", \"Z1\", \"X2\", \"Z3\", \"Z4\", \"X5\", \"X6\", \"Z7\"]\n",
    "#########################################################################################\n",
    "y_d3 = trainData_d3[\"Labels\"]\n",
    "x_d3 = trainData_d3.drop([\"Labels\"], axis=1)\n",
    "\n",
    "y_d5 = trainData_d5[\"Labels\"] \n",
    "x_d5 = trainData_d5.drop([\"Labels\"], axis=1) \n",
    "\n",
    "y_d7 = trainData_d7[\"Labels\"]\n",
    "x_d7 = trainData_d7.drop([\"Labels\"], axis=1)\n",
    "\n",
    "\n",
    "x_d3 = x_d3.replace([-1], 0)\n",
    "x_d5 = x_d5.replace([-1], 0)\n",
    "x_d7 = x_d7.replace([-1], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_d7 = trainData_d7[\"Labels\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions for creating lookup tables here:\n",
    "from collections import defaultdict\n",
    "from math import log2\n",
    "\n",
    "def generateAllBinaryStrings(n, arr, i, lookup):  \n",
    "    if i == n: \n",
    "        lookup.setBitStringArray(arr, n)  \n",
    "        return\n",
    "      \n",
    "    # First assign \"0\" at ith position  \n",
    "    # and try for all other permutations  \n",
    "    # for remaining positions  \n",
    "    arr[i] = 0\n",
    "    generateAllBinaryStrings(n, arr, i + 1, lookup)  \n",
    "  \n",
    "    # And then assign \"1\" at ith position  , \n",
    "    # and try for all other permutations  \n",
    "    # for remaining positions  \n",
    "    arr[i] = 1\n",
    "    generateAllBinaryStrings(n, arr, i + 1, lookup)\n",
    "\n",
    "class lookup_decoder:\n",
    "    \n",
    "    def __init__(self, depth):\n",
    "        #self.lookupTable = defaultdict()\n",
    "        self.lookupTable = {}\n",
    "        self.distributions = {}\n",
    "        self.depth = depth\n",
    "        #generating all possible syndrome observations\n",
    "        #arr = [None] * (depth**2 - 1)\n",
    "        #generateAllBinaryStrings((depth**2 - 1), arr, 0, self)\n",
    "        \n",
    "    def setBitStringArray(self, arr, n): \n",
    "        new_str = \"\"\n",
    "        for i in range(0, n):  \n",
    "            new_str += str(arr[i])\n",
    "        self.lookupTable.update({new_str:defaultdict()})  \n",
    "        \n",
    "    def update_table (self, syndrome, phys_errs):\n",
    "        #all the keys are made in the init, so simply update the physical error combinations for the given syndrome\n",
    "        if syndrome not in self.lookupTable:\n",
    "            self.lookupTable[syndrome] = {}\n",
    "            self.lookupTable[syndrome][phys_errs] = 1\n",
    "            return\n",
    "        \n",
    "        if phys_errs not in self.lookupTable[syndrome]:\n",
    "            self.lookupTable[syndrome].update({phys_errs: 1})\n",
    "        else:\n",
    "            self.lookupTable[syndrome][phys_errs] += 1\n",
    "     \n",
    "    def get_probable_error(self, syndrome):\n",
    "        return_key = []\n",
    "\n",
    "        if syndrome not in self.lookupTable.keys():\n",
    "            for i in range(2* int(self.depth**2) + 1):\n",
    "                return_key.append(0)\n",
    "            return return_key\n",
    "        \n",
    "        key, value = max(self.lookupTable[syndrome].items(), key=lambda x:x[1])\n",
    "        \n",
    "        for character in key:\n",
    "            if character == '0' or character == '1':\n",
    "                return_key.append(int(character))\n",
    "                \n",
    "        return return_key\n",
    "        \n",
    "    def make_distribution_graph(self, syn):\n",
    "        \n",
    "        plt.bar(list(self.lookupTable[syn].keys()), self.lookupTable[syn].values(), color='g')\n",
    "        plt.show()\n",
    "        \n",
    "    def syndrome_count_graph(self):\n",
    "        graph_dict = {}\n",
    "        for syn in self.lookupTable:\n",
    "            graph_dict.update({syn:sum(self.lookupTable[syn].values())})\n",
    "        plt.ylim((0,4))\n",
    "        plt.bar(graph_dict.keys(), graph_dict.values())\n",
    "        plt.show()\n",
    "        \n",
    "    \n",
    "    def length_of_lookup(self):\n",
    "        print(len(self.lookupTable))\n",
    "        \n",
    "            \n",
    "    def get_entropies(self):\n",
    "        entropies = {}\n",
    "        for syn in self.lookupTable:\n",
    "            total = sum(self.lookupTable[syn].values())\n",
    "            h = 0\n",
    "            for key in self.lookupTable[syn]:\n",
    "                p = self.lookupTable[syn][key]/total\n",
    "                h += p+log2(p)\n",
    "            entropies[syn] = -h\n",
    "        print(entropies)\n",
    "            \n",
    "    def get_syndromes(self):\n",
    "        return self.lookupTable.keys()\n",
    "                     \n",
    "    def print_lookup(self):\n",
    "        for syn in self.lookupTable:\n",
    "            print(self.lookupTable[syn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_plut(table, data_x, data_y):\n",
    "    i = 0\n",
    "    for index, x in enumerate(data_x):\n",
    "        syn = \"\".join([str(i) for i in x])\n",
    "        syn = syn.replace(\".0\",\"\")\n",
    "        labels = np.array2string(np.array(data_y[i]), precision=1, separator='',suppress_small=True).replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "        table.update_table(syn, labels)\n",
    "        i+=1\n",
    "    return table\n",
    "\n",
    "\n",
    "def test_plut(table, test_set):\n",
    "    predictions_lookup = []\n",
    "    for index, x in enumerate(test_set):\n",
    "        syn_x = \"\".join([str(i) for i in x])\n",
    "        syn_x = syn_x.replace(\".0\",\"\")\n",
    "        predictions_lookup.append(table.get_probable_error(syn_x))\n",
    "    return np.array(predictions_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_FFNN_model_DepthThree(depth):\n",
    "    model = Sequential()\n",
    "    layers = 2\n",
    "    #input layer\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    \n",
    "    #hidden layers go here\n",
    "    for i in range(layers):\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "    \n",
    "    model.add(Dense(19 , activation='sigmoid'))\n",
    "    model.compile(loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.SGD(lr=0.05),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "#make any needed changes here\n",
    "def compile_FFNN_model_DepthFive(depth):\n",
    "    model = Sequential()\n",
    "    layers = 4\n",
    "    \n",
    "    #input layer\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    \n",
    "    #hidden layers go here\n",
    "    for i in range(layers):\n",
    "        model.add(Dense(240, activation='relu'))\n",
    "    \n",
    "    #model.add(Dense((2*depth**2) , activation='sigmoid'))\n",
    "    model.add(Dense(51, activation='sigmoid'))\n",
    "    model.compile(loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.SGD(learning_rate=0.05),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "#make any needed changes here\n",
    "def compile_FFNN_model_DepthSeven(depth):\n",
    "    model = Sequential()\n",
    "    layers = 4\n",
    "    \n",
    "    #input layer\n",
    "    model.add(Dense(48, activation='relu'))\n",
    "    \n",
    "    #hidden layers go here\n",
    "    for i in range(layers):\n",
    "        model.add(Dense(400, activation='relu'))\n",
    "    \n",
    "    #model.add(Dense((2*depth**2) , activation='sigmoid'))\n",
    "    model.add(Dense(99, activation='sigmoid'))\n",
    "    model.compile(loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.SGD(learning_rate=0.05),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Train on 17 samples, validate on 5 samples\n",
      "Epoch 1/200\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.7432 - accuracy: 0.4861 - val_loss: 0.7206 - val_accuracy: 0.5158\n",
      "Epoch 2/200\n",
      "17/17 [==============================] - 0s 287us/step - loss: 0.7379 - accuracy: 0.4861 - val_loss: 0.7171 - val_accuracy: 0.5158\n",
      "Epoch 3/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.7328 - accuracy: 0.4954 - val_loss: 0.7136 - val_accuracy: 0.5263\n",
      "Epoch 4/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.7280 - accuracy: 0.5077 - val_loss: 0.7103 - val_accuracy: 0.5158\n",
      "Epoch 5/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.7232 - accuracy: 0.5232 - val_loss: 0.7070 - val_accuracy: 0.5263\n",
      "Epoch 6/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.7186 - accuracy: 0.5263 - val_loss: 0.7038 - val_accuracy: 0.5263\n",
      "Epoch 7/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.7141 - accuracy: 0.5418 - val_loss: 0.7007 - val_accuracy: 0.5263\n",
      "Epoch 8/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.7098 - accuracy: 0.5418 - val_loss: 0.6977 - val_accuracy: 0.5474\n",
      "Epoch 9/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.7055 - accuracy: 0.5573 - val_loss: 0.6948 - val_accuracy: 0.5579\n",
      "Epoch 10/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.7014 - accuracy: 0.5666 - val_loss: 0.6919 - val_accuracy: 0.5579\n",
      "Epoch 11/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.6975 - accuracy: 0.5789 - val_loss: 0.6891 - val_accuracy: 0.5684\n",
      "Epoch 12/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.6936 - accuracy: 0.5820 - val_loss: 0.6864 - val_accuracy: 0.5684\n",
      "Epoch 13/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.6898 - accuracy: 0.5913 - val_loss: 0.6837 - val_accuracy: 0.5684\n",
      "Epoch 14/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6861 - accuracy: 0.5944 - val_loss: 0.6811 - val_accuracy: 0.5789\n",
      "Epoch 15/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6826 - accuracy: 0.5944 - val_loss: 0.6785 - val_accuracy: 0.5895\n",
      "Epoch 16/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.6790 - accuracy: 0.5975 - val_loss: 0.6759 - val_accuracy: 0.5895\n",
      "Epoch 17/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.6756 - accuracy: 0.6099 - val_loss: 0.6734 - val_accuracy: 0.6105\n",
      "Epoch 18/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.6721 - accuracy: 0.6099 - val_loss: 0.6709 - val_accuracy: 0.6105\n",
      "Epoch 19/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6688 - accuracy: 0.6161 - val_loss: 0.6684 - val_accuracy: 0.6105\n",
      "Epoch 20/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6654 - accuracy: 0.6285 - val_loss: 0.6659 - val_accuracy: 0.6316\n",
      "Epoch 21/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6622 - accuracy: 0.6347 - val_loss: 0.6635 - val_accuracy: 0.6316\n",
      "Epoch 22/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6589 - accuracy: 0.6378 - val_loss: 0.6611 - val_accuracy: 0.6526\n",
      "Epoch 23/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6556 - accuracy: 0.6409 - val_loss: 0.6587 - val_accuracy: 0.6632\n",
      "Epoch 24/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6524 - accuracy: 0.6440 - val_loss: 0.6563 - val_accuracy: 0.6632\n",
      "Epoch 25/200\n",
      "17/17 [==============================] - 0s 115us/step - loss: 0.6492 - accuracy: 0.6502 - val_loss: 0.6539 - val_accuracy: 0.6526\n",
      "Epoch 26/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6460 - accuracy: 0.6625 - val_loss: 0.6516 - val_accuracy: 0.6526\n",
      "Epoch 27/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6428 - accuracy: 0.6656 - val_loss: 0.6492 - val_accuracy: 0.6632\n",
      "Epoch 28/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6397 - accuracy: 0.6842 - val_loss: 0.6469 - val_accuracy: 0.6737\n",
      "Epoch 29/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.6366 - accuracy: 0.6935 - val_loss: 0.6445 - val_accuracy: 0.6737\n",
      "Epoch 30/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6334 - accuracy: 0.7121 - val_loss: 0.6421 - val_accuracy: 0.6947\n",
      "Epoch 31/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.6302 - accuracy: 0.7152 - val_loss: 0.6397 - val_accuracy: 0.7053\n",
      "Epoch 32/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.6271 - accuracy: 0.7183 - val_loss: 0.6374 - val_accuracy: 0.7158\n",
      "Epoch 33/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6239 - accuracy: 0.7307 - val_loss: 0.6350 - val_accuracy: 0.7263\n",
      "Epoch 34/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6207 - accuracy: 0.7337 - val_loss: 0.6326 - val_accuracy: 0.7368\n",
      "Epoch 35/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.6175 - accuracy: 0.7399 - val_loss: 0.6302 - val_accuracy: 0.7368\n",
      "Epoch 36/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6143 - accuracy: 0.7399 - val_loss: 0.6278 - val_accuracy: 0.7474\n",
      "Epoch 37/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6111 - accuracy: 0.7492 - val_loss: 0.6254 - val_accuracy: 0.7579\n",
      "Epoch 38/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6079 - accuracy: 0.7523 - val_loss: 0.6229 - val_accuracy: 0.7789\n",
      "Epoch 39/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6046 - accuracy: 0.7616 - val_loss: 0.6205 - val_accuracy: 0.7789\n",
      "Epoch 40/200\n",
      "17/17 [==============================] - 0s 115us/step - loss: 0.6013 - accuracy: 0.7616 - val_loss: 0.6181 - val_accuracy: 0.7789\n",
      "Epoch 41/200\n",
      "17/17 [==============================] - 0s 243us/step - loss: 0.5980 - accuracy: 0.7678 - val_loss: 0.6156 - val_accuracy: 0.7789\n",
      "Epoch 42/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.5947 - accuracy: 0.7802 - val_loss: 0.6131 - val_accuracy: 0.7789\n",
      "Epoch 43/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.5913 - accuracy: 0.7833 - val_loss: 0.6106 - val_accuracy: 0.7789\n",
      "Epoch 44/200\n",
      "17/17 [==============================] - 0s 115us/step - loss: 0.5879 - accuracy: 0.7926 - val_loss: 0.6081 - val_accuracy: 0.7789\n",
      "Epoch 45/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.5845 - accuracy: 0.7988 - val_loss: 0.6055 - val_accuracy: 0.7789\n",
      "Epoch 46/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.5811 - accuracy: 0.8050 - val_loss: 0.6029 - val_accuracy: 0.7895\n",
      "Epoch 47/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.5776 - accuracy: 0.8173 - val_loss: 0.6003 - val_accuracy: 0.7895\n",
      "Epoch 48/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.5741 - accuracy: 0.8173 - val_loss: 0.5976 - val_accuracy: 0.7895\n",
      "Epoch 49/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.5705 - accuracy: 0.8204 - val_loss: 0.5949 - val_accuracy: 0.8000\n",
      "Epoch 50/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.5669 - accuracy: 0.8266 - val_loss: 0.5923 - val_accuracy: 0.8105\n",
      "Epoch 51/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.5633 - accuracy: 0.8359 - val_loss: 0.5896 - val_accuracy: 0.8105\n",
      "Epoch 52/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.5596 - accuracy: 0.8359 - val_loss: 0.5868 - val_accuracy: 0.8105\n",
      "Epoch 53/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.5558 - accuracy: 0.8390 - val_loss: 0.5840 - val_accuracy: 0.8211\n",
      "Epoch 54/200\n",
      "17/17 [==============================] - 0s 229us/step - loss: 0.5520 - accuracy: 0.8421 - val_loss: 0.5812 - val_accuracy: 0.8211\n",
      "Epoch 55/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.5482 - accuracy: 0.8421 - val_loss: 0.5784 - val_accuracy: 0.8211\n",
      "Epoch 56/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.5443 - accuracy: 0.8452 - val_loss: 0.5755 - val_accuracy: 0.8211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.5404 - accuracy: 0.8514 - val_loss: 0.5725 - val_accuracy: 0.8211\n",
      "Epoch 58/200\n",
      "17/17 [==============================] - 0s 115us/step - loss: 0.5364 - accuracy: 0.8514 - val_loss: 0.5696 - val_accuracy: 0.8211\n",
      "Epoch 59/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.5323 - accuracy: 0.8483 - val_loss: 0.5666 - val_accuracy: 0.8211\n",
      "Epoch 60/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.5283 - accuracy: 0.8483 - val_loss: 0.5636 - val_accuracy: 0.8316\n",
      "Epoch 61/200\n",
      "17/17 [==============================] - 0s 115us/step - loss: 0.5242 - accuracy: 0.8514 - val_loss: 0.5605 - val_accuracy: 0.8316\n",
      "Epoch 62/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.5200 - accuracy: 0.8514 - val_loss: 0.5574 - val_accuracy: 0.8316\n",
      "Epoch 63/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.5158 - accuracy: 0.8545 - val_loss: 0.5542 - val_accuracy: 0.8316\n",
      "Epoch 64/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.5115 - accuracy: 0.8514 - val_loss: 0.5510 - val_accuracy: 0.8421\n",
      "Epoch 65/200\n",
      "17/17 [==============================] - 0s 115us/step - loss: 0.5072 - accuracy: 0.8607 - val_loss: 0.5478 - val_accuracy: 0.8421\n",
      "Epoch 66/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.5028 - accuracy: 0.8607 - val_loss: 0.5445 - val_accuracy: 0.8421\n",
      "Epoch 67/200\n",
      "17/17 [==============================] - 0s 115us/step - loss: 0.4984 - accuracy: 0.8607 - val_loss: 0.5412 - val_accuracy: 0.8421\n",
      "Epoch 68/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.4939 - accuracy: 0.8638 - val_loss: 0.5379 - val_accuracy: 0.8421\n",
      "Epoch 69/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.4893 - accuracy: 0.8669 - val_loss: 0.5345 - val_accuracy: 0.8421\n",
      "Epoch 70/200\n",
      "17/17 [==============================] - 0s 115us/step - loss: 0.4847 - accuracy: 0.8669 - val_loss: 0.5311 - val_accuracy: 0.8526\n",
      "Epoch 71/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.4801 - accuracy: 0.8700 - val_loss: 0.5276 - val_accuracy: 0.8526\n",
      "Epoch 72/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.4754 - accuracy: 0.8700 - val_loss: 0.5242 - val_accuracy: 0.8526\n",
      "Epoch 73/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.4707 - accuracy: 0.8700 - val_loss: 0.5207 - val_accuracy: 0.8526\n",
      "Epoch 74/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.4659 - accuracy: 0.8700 - val_loss: 0.5171 - val_accuracy: 0.8526\n",
      "Epoch 75/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.4611 - accuracy: 0.8762 - val_loss: 0.5136 - val_accuracy: 0.8526\n",
      "Epoch 76/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.4563 - accuracy: 0.8824 - val_loss: 0.5100 - val_accuracy: 0.8526\n",
      "Epoch 77/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.4514 - accuracy: 0.8885 - val_loss: 0.5064 - val_accuracy: 0.8632\n",
      "Epoch 78/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.4465 - accuracy: 0.8885 - val_loss: 0.5028 - val_accuracy: 0.8632\n",
      "Epoch 79/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.4416 - accuracy: 0.8885 - val_loss: 0.4992 - val_accuracy: 0.8632\n",
      "Epoch 80/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.4367 - accuracy: 0.8885 - val_loss: 0.4956 - val_accuracy: 0.8632\n",
      "Epoch 81/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.4318 - accuracy: 0.8916 - val_loss: 0.4920 - val_accuracy: 0.8632\n",
      "Epoch 82/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.4268 - accuracy: 0.8916 - val_loss: 0.4884 - val_accuracy: 0.8632\n",
      "Epoch 83/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.4219 - accuracy: 0.8916 - val_loss: 0.4848 - val_accuracy: 0.8632\n",
      "Epoch 84/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.4170 - accuracy: 0.8916 - val_loss: 0.4812 - val_accuracy: 0.8632\n",
      "Epoch 85/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.4121 - accuracy: 0.8916 - val_loss: 0.4776 - val_accuracy: 0.8632\n",
      "Epoch 86/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.4071 - accuracy: 0.8916 - val_loss: 0.4740 - val_accuracy: 0.8632\n",
      "Epoch 87/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.4022 - accuracy: 0.8916 - val_loss: 0.4704 - val_accuracy: 0.8632\n",
      "Epoch 88/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.3973 - accuracy: 0.8916 - val_loss: 0.4669 - val_accuracy: 0.8632\n",
      "Epoch 89/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.3925 - accuracy: 0.8916 - val_loss: 0.4633 - val_accuracy: 0.8632\n",
      "Epoch 90/200\n",
      "17/17 [==============================] - 0s 287us/step - loss: 0.3877 - accuracy: 0.8916 - val_loss: 0.4598 - val_accuracy: 0.8632\n",
      "Epoch 91/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.3828 - accuracy: 0.8916 - val_loss: 0.4564 - val_accuracy: 0.8632\n",
      "Epoch 92/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.3781 - accuracy: 0.8916 - val_loss: 0.4529 - val_accuracy: 0.8632\n",
      "Epoch 93/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.3734 - accuracy: 0.8916 - val_loss: 0.4495 - val_accuracy: 0.8632\n",
      "Epoch 94/200\n",
      "17/17 [==============================] - 0s 115us/step - loss: 0.3687 - accuracy: 0.8916 - val_loss: 0.4462 - val_accuracy: 0.8632\n",
      "Epoch 95/200\n",
      "17/17 [==============================] - 0s 115us/step - loss: 0.3640 - accuracy: 0.8916 - val_loss: 0.4428 - val_accuracy: 0.8632\n",
      "Epoch 96/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.3595 - accuracy: 0.8916 - val_loss: 0.4396 - val_accuracy: 0.8632\n",
      "Epoch 97/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.3550 - accuracy: 0.8916 - val_loss: 0.4363 - val_accuracy: 0.8632\n",
      "Epoch 98/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.3505 - accuracy: 0.8947 - val_loss: 0.4331 - val_accuracy: 0.8632\n",
      "Epoch 99/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.3461 - accuracy: 0.8947 - val_loss: 0.4300 - val_accuracy: 0.8632\n",
      "Epoch 100/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.3418 - accuracy: 0.8947 - val_loss: 0.4269 - val_accuracy: 0.8526\n",
      "Epoch 101/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.3376 - accuracy: 0.8978 - val_loss: 0.4238 - val_accuracy: 0.8526\n",
      "Epoch 102/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.3335 - accuracy: 0.8978 - val_loss: 0.4208 - val_accuracy: 0.8526\n",
      "Epoch 103/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.3294 - accuracy: 0.8978 - val_loss: 0.4179 - val_accuracy: 0.8526\n",
      "Epoch 104/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.3254 - accuracy: 0.8978 - val_loss: 0.4151 - val_accuracy: 0.8842\n",
      "Epoch 105/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.3216 - accuracy: 0.9040 - val_loss: 0.4123 - val_accuracy: 0.8842\n",
      "Epoch 106/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.3178 - accuracy: 0.9133 - val_loss: 0.4097 - val_accuracy: 0.8947\n",
      "Epoch 107/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.3141 - accuracy: 0.9381 - val_loss: 0.4070 - val_accuracy: 0.8947\n",
      "Epoch 108/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.3105 - accuracy: 0.9381 - val_loss: 0.4045 - val_accuracy: 0.8947\n",
      "Epoch 109/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.3070 - accuracy: 0.9381 - val_loss: 0.4020 - val_accuracy: 0.8947\n",
      "Epoch 110/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.3036 - accuracy: 0.9381 - val_loss: 0.3997 - val_accuracy: 0.8947\n",
      "Epoch 111/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.3003 - accuracy: 0.9381 - val_loss: 0.3974 - val_accuracy: 0.8947\n",
      "Epoch 112/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2971 - accuracy: 0.9381 - val_loss: 0.3951 - val_accuracy: 0.8947\n",
      "Epoch 113/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2940 - accuracy: 0.9381 - val_loss: 0.3930 - val_accuracy: 0.8947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2910 - accuracy: 0.9381 - val_loss: 0.3909 - val_accuracy: 0.8947\n",
      "Epoch 115/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.2881 - accuracy: 0.9381 - val_loss: 0.3889 - val_accuracy: 0.8947\n",
      "Epoch 116/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2853 - accuracy: 0.9381 - val_loss: 0.3870 - val_accuracy: 0.8947\n",
      "Epoch 117/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2826 - accuracy: 0.9381 - val_loss: 0.3852 - val_accuracy: 0.8947\n",
      "Epoch 118/200\n",
      "17/17 [==============================] - 0s 115us/step - loss: 0.2800 - accuracy: 0.9381 - val_loss: 0.3834 - val_accuracy: 0.8947\n",
      "Epoch 119/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2774 - accuracy: 0.9381 - val_loss: 0.3818 - val_accuracy: 0.8947\n",
      "Epoch 120/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.2750 - accuracy: 0.9381 - val_loss: 0.3802 - val_accuracy: 0.8947\n",
      "Epoch 121/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2727 - accuracy: 0.9381 - val_loss: 0.3786 - val_accuracy: 0.8947\n",
      "Epoch 122/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.2705 - accuracy: 0.9381 - val_loss: 0.3771 - val_accuracy: 0.8947\n",
      "Epoch 123/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2683 - accuracy: 0.9381 - val_loss: 0.3758 - val_accuracy: 0.8947\n",
      "Epoch 124/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2663 - accuracy: 0.9381 - val_loss: 0.3744 - val_accuracy: 0.8947\n",
      "Epoch 125/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2643 - accuracy: 0.9381 - val_loss: 0.3732 - val_accuracy: 0.8947\n",
      "Epoch 126/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2625 - accuracy: 0.9381 - val_loss: 0.3720 - val_accuracy: 0.8947\n",
      "Epoch 127/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2607 - accuracy: 0.9381 - val_loss: 0.3708 - val_accuracy: 0.8947\n",
      "Epoch 128/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2589 - accuracy: 0.9381 - val_loss: 0.3698 - val_accuracy: 0.8947\n",
      "Epoch 129/200\n",
      "17/17 [==============================] - 0s 115us/step - loss: 0.2573 - accuracy: 0.9381 - val_loss: 0.3687 - val_accuracy: 0.8947\n",
      "Epoch 130/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2557 - accuracy: 0.9381 - val_loss: 0.3678 - val_accuracy: 0.8947\n",
      "Epoch 131/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2542 - accuracy: 0.9381 - val_loss: 0.3669 - val_accuracy: 0.8947\n",
      "Epoch 132/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.2528 - accuracy: 0.9381 - val_loss: 0.3660 - val_accuracy: 0.8947\n",
      "Epoch 133/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.2514 - accuracy: 0.9381 - val_loss: 0.3652 - val_accuracy: 0.8947\n",
      "Epoch 134/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2501 - accuracy: 0.9381 - val_loss: 0.3645 - val_accuracy: 0.8947\n",
      "Epoch 135/200\n",
      "17/17 [==============================] - 0s 115us/step - loss: 0.2489 - accuracy: 0.9381 - val_loss: 0.3638 - val_accuracy: 0.8947\n",
      "Epoch 136/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.2477 - accuracy: 0.9381 - val_loss: 0.3631 - val_accuracy: 0.8947\n",
      "Epoch 137/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.2466 - accuracy: 0.9381 - val_loss: 0.3625 - val_accuracy: 0.8947\n",
      "Epoch 138/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2455 - accuracy: 0.9381 - val_loss: 0.3619 - val_accuracy: 0.8947\n",
      "Epoch 139/200\n",
      "17/17 [==============================] - 0s 115us/step - loss: 0.2445 - accuracy: 0.9381 - val_loss: 0.3613 - val_accuracy: 0.8947\n",
      "Epoch 140/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.2435 - accuracy: 0.9381 - val_loss: 0.3608 - val_accuracy: 0.8947\n",
      "Epoch 141/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2426 - accuracy: 0.9381 - val_loss: 0.3604 - val_accuracy: 0.8947\n",
      "Epoch 142/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2417 - accuracy: 0.9381 - val_loss: 0.3599 - val_accuracy: 0.8947\n",
      "Epoch 143/200\n",
      "17/17 [==============================] - 0s 115us/step - loss: 0.2408 - accuracy: 0.9381 - val_loss: 0.3595 - val_accuracy: 0.8947\n",
      "Epoch 144/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.2400 - accuracy: 0.9381 - val_loss: 0.3591 - val_accuracy: 0.8947\n",
      "Epoch 145/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.2393 - accuracy: 0.9381 - val_loss: 0.3587 - val_accuracy: 0.8947\n",
      "Epoch 146/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2385 - accuracy: 0.9381 - val_loss: 0.3584 - val_accuracy: 0.8947\n",
      "Epoch 147/200\n",
      "17/17 [==============================] - 0s 115us/step - loss: 0.2378 - accuracy: 0.9381 - val_loss: 0.3581 - val_accuracy: 0.8947\n",
      "Epoch 148/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.2372 - accuracy: 0.9381 - val_loss: 0.3578 - val_accuracy: 0.8947\n",
      "Epoch 149/200\n",
      "17/17 [==============================] - 0s 115us/step - loss: 0.2365 - accuracy: 0.9381 - val_loss: 0.3575 - val_accuracy: 0.8947\n",
      "Epoch 150/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2359 - accuracy: 0.9381 - val_loss: 0.3573 - val_accuracy: 0.8947\n",
      "Epoch 151/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2353 - accuracy: 0.9381 - val_loss: 0.3571 - val_accuracy: 0.8947\n",
      "Epoch 152/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2348 - accuracy: 0.9381 - val_loss: 0.3569 - val_accuracy: 0.8947\n",
      "Epoch 153/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2343 - accuracy: 0.9381 - val_loss: 0.3567 - val_accuracy: 0.8947\n",
      "Epoch 154/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.2338 - accuracy: 0.9381 - val_loss: 0.3565 - val_accuracy: 0.8947\n",
      "Epoch 155/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2333 - accuracy: 0.9381 - val_loss: 0.3563 - val_accuracy: 0.8947\n",
      "Epoch 156/200\n",
      "17/17 [==============================] - 0s 287us/step - loss: 0.2328 - accuracy: 0.9381 - val_loss: 0.3562 - val_accuracy: 0.8947\n",
      "Epoch 157/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2324 - accuracy: 0.9381 - val_loss: 0.3561 - val_accuracy: 0.8947\n",
      "Epoch 158/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2320 - accuracy: 0.9381 - val_loss: 0.3560 - val_accuracy: 0.8947\n",
      "Epoch 159/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.2316 - accuracy: 0.9381 - val_loss: 0.3559 - val_accuracy: 0.8947\n",
      "Epoch 160/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.2312 - accuracy: 0.9381 - val_loss: 0.3558 - val_accuracy: 0.8947\n",
      "Epoch 161/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.2308 - accuracy: 0.9381 - val_loss: 0.3557 - val_accuracy: 0.8947\n",
      "Epoch 162/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2305 - accuracy: 0.9381 - val_loss: 0.3556 - val_accuracy: 0.8947\n",
      "Epoch 163/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2301 - accuracy: 0.9381 - val_loss: 0.3555 - val_accuracy: 0.8947\n",
      "Epoch 164/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2298 - accuracy: 0.9381 - val_loss: 0.3555 - val_accuracy: 0.8947\n",
      "Epoch 165/200\n",
      "17/17 [==============================] - 0s 115us/step - loss: 0.2295 - accuracy: 0.9381 - val_loss: 0.3554 - val_accuracy: 0.8947\n",
      "Epoch 166/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2292 - accuracy: 0.9381 - val_loss: 0.3554 - val_accuracy: 0.8947\n",
      "Epoch 167/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.2289 - accuracy: 0.9381 - val_loss: 0.3554 - val_accuracy: 0.8947\n",
      "Epoch 168/200\n",
      "17/17 [==============================] - 0s 173us/step - loss: 0.2286 - accuracy: 0.9381 - val_loss: 0.3554 - val_accuracy: 0.8947\n",
      "Epoch 169/200\n",
      "17/17 [==============================] - 0s 115us/step - loss: 0.2284 - accuracy: 0.9381 - val_loss: 0.3554 - val_accuracy: 0.8947\n",
      "Epoch 170/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2281 - accuracy: 0.9381 - val_loss: 0.3554 - val_accuracy: 0.8947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2279 - accuracy: 0.9381 - val_loss: 0.3554 - val_accuracy: 0.8947\n",
      "Epoch 172/200\n",
      "17/17 [==============================] - 0s 287us/step - loss: 0.2276 - accuracy: 0.9381 - val_loss: 0.3554 - val_accuracy: 0.8947\n",
      "Epoch 173/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2274 - accuracy: 0.9381 - val_loss: 0.3554 - val_accuracy: 0.8947\n",
      "Epoch 174/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2272 - accuracy: 0.9381 - val_loss: 0.3554 - val_accuracy: 0.8947\n",
      "Epoch 175/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2270 - accuracy: 0.9381 - val_loss: 0.3554 - val_accuracy: 0.8947\n",
      "Epoch 176/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.2268 - accuracy: 0.9381 - val_loss: 0.3554 - val_accuracy: 0.8947\n",
      "Epoch 177/200\n",
      "17/17 [==============================] - 0s 115us/step - loss: 0.2266 - accuracy: 0.9381 - val_loss: 0.3555 - val_accuracy: 0.8947\n",
      "Epoch 178/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2264 - accuracy: 0.9381 - val_loss: 0.3555 - val_accuracy: 0.8947\n",
      "Epoch 179/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.2262 - accuracy: 0.9381 - val_loss: 0.3555 - val_accuracy: 0.8947\n",
      "Epoch 180/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.2260 - accuracy: 0.9381 - val_loss: 0.3556 - val_accuracy: 0.8947\n",
      "Epoch 181/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2258 - accuracy: 0.9381 - val_loss: 0.3556 - val_accuracy: 0.8947\n",
      "Epoch 182/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2257 - accuracy: 0.9381 - val_loss: 0.3557 - val_accuracy: 0.8947\n",
      "Epoch 183/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.2255 - accuracy: 0.9381 - val_loss: 0.3557 - val_accuracy: 0.8947\n",
      "Epoch 184/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2253 - accuracy: 0.9381 - val_loss: 0.3558 - val_accuracy: 0.8947\n",
      "Epoch 185/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2252 - accuracy: 0.9381 - val_loss: 0.3558 - val_accuracy: 0.8947\n",
      "Epoch 186/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.2250 - accuracy: 0.9381 - val_loss: 0.3559 - val_accuracy: 0.8947\n",
      "Epoch 187/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2249 - accuracy: 0.9381 - val_loss: 0.3559 - val_accuracy: 0.8947\n",
      "Epoch 188/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.2248 - accuracy: 0.9381 - val_loss: 0.3560 - val_accuracy: 0.8947\n",
      "Epoch 189/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2246 - accuracy: 0.9381 - val_loss: 0.3561 - val_accuracy: 0.8947\n",
      "Epoch 190/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2245 - accuracy: 0.9381 - val_loss: 0.3561 - val_accuracy: 0.8947\n",
      "Epoch 191/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2244 - accuracy: 0.9381 - val_loss: 0.3562 - val_accuracy: 0.8947\n",
      "Epoch 192/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2243 - accuracy: 0.9381 - val_loss: 0.3563 - val_accuracy: 0.8947\n",
      "Epoch 193/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2241 - accuracy: 0.9381 - val_loss: 0.3563 - val_accuracy: 0.8947\n",
      "Epoch 194/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2240 - accuracy: 0.9381 - val_loss: 0.3564 - val_accuracy: 0.8947\n",
      "Epoch 195/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2239 - accuracy: 0.9381 - val_loss: 0.3565 - val_accuracy: 0.8947\n",
      "Epoch 196/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2238 - accuracy: 0.9381 - val_loss: 0.3565 - val_accuracy: 0.8947\n",
      "Epoch 197/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.2237 - accuracy: 0.9381 - val_loss: 0.3566 - val_accuracy: 0.8947\n",
      "Epoch 198/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2236 - accuracy: 0.9381 - val_loss: 0.3567 - val_accuracy: 0.8947\n",
      "Epoch 199/200\n",
      "17/17 [==============================] - 0s 115us/step - loss: 0.2235 - accuracy: 0.9381 - val_loss: 0.3567 - val_accuracy: 0.8947\n",
      "Epoch 200/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.2234 - accuracy: 0.9381 - val_loss: 0.3568 - val_accuracy: 0.8947\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clair\\Anaconda3\\envs\\CNNs\\lib\\site-packages\\ipykernel_launcher.py:128: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "C:\\Users\\clair\\Anaconda3\\envs\\CNNs\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:808: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17 samples, validate on 5 samples\n",
      "Epoch 1/200\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.6854 - accuracy: 0.5882 - val_loss: 0.6900 - val_accuracy: 0.5579\n",
      "Epoch 2/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.6845 - accuracy: 0.6037 - val_loss: 0.6891 - val_accuracy: 0.5579\n",
      "Epoch 3/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6836 - accuracy: 0.6316 - val_loss: 0.6882 - val_accuracy: 0.5684\n",
      "Epoch 4/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6828 - accuracy: 0.6440 - val_loss: 0.6873 - val_accuracy: 0.5684\n",
      "Epoch 5/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6819 - accuracy: 0.6471 - val_loss: 0.6865 - val_accuracy: 0.5789\n",
      "Epoch 6/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6810 - accuracy: 0.6533 - val_loss: 0.6856 - val_accuracy: 0.5895\n",
      "Epoch 7/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6802 - accuracy: 0.6563 - val_loss: 0.6847 - val_accuracy: 0.6000\n",
      "Epoch 8/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6794 - accuracy: 0.6625 - val_loss: 0.6839 - val_accuracy: 0.6211\n",
      "Epoch 9/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6785 - accuracy: 0.6656 - val_loss: 0.6830 - val_accuracy: 0.6316\n",
      "Epoch 10/200\n",
      "17/17 [==============================] - 0s 115us/step - loss: 0.6776 - accuracy: 0.6935 - val_loss: 0.6821 - val_accuracy: 0.6421\n",
      "Epoch 11/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6767 - accuracy: 0.6966 - val_loss: 0.6812 - val_accuracy: 0.6421\n",
      "Epoch 12/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6759 - accuracy: 0.6997 - val_loss: 0.6804 - val_accuracy: 0.6526\n",
      "Epoch 13/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6750 - accuracy: 0.7059 - val_loss: 0.6795 - val_accuracy: 0.6632\n",
      "Epoch 14/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.6741 - accuracy: 0.7059 - val_loss: 0.6786 - val_accuracy: 0.6632\n",
      "Epoch 15/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6732 - accuracy: 0.7090 - val_loss: 0.6777 - val_accuracy: 0.6737\n",
      "Epoch 16/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.6723 - accuracy: 0.7152 - val_loss: 0.6768 - val_accuracy: 0.7053\n",
      "Epoch 17/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6715 - accuracy: 0.7152 - val_loss: 0.6760 - val_accuracy: 0.7053\n",
      "Epoch 18/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6706 - accuracy: 0.7183 - val_loss: 0.6751 - val_accuracy: 0.7158\n",
      "Epoch 19/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6697 - accuracy: 0.7523 - val_loss: 0.6742 - val_accuracy: 0.7158\n",
      "Epoch 20/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.6688 - accuracy: 0.7523 - val_loss: 0.6733 - val_accuracy: 0.7263\n",
      "Epoch 21/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6679 - accuracy: 0.7616 - val_loss: 0.6725 - val_accuracy: 0.7263\n",
      "Epoch 22/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6671 - accuracy: 0.7709 - val_loss: 0.6716 - val_accuracy: 0.7474\n",
      "Epoch 23/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6662 - accuracy: 0.7709 - val_loss: 0.6707 - val_accuracy: 0.7474\n",
      "Epoch 24/200\n",
      "17/17 [==============================] - 0s 115us/step - loss: 0.6652 - accuracy: 0.7709 - val_loss: 0.6698 - val_accuracy: 0.7474\n",
      "Epoch 25/200\n",
      "17/17 [==============================] - 0s 231us/step - loss: 0.6643 - accuracy: 0.7740 - val_loss: 0.6688 - val_accuracy: 0.7579\n",
      "Epoch 26/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6634 - accuracy: 0.7740 - val_loss: 0.6679 - val_accuracy: 0.7579\n",
      "Epoch 27/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6624 - accuracy: 0.7709 - val_loss: 0.6670 - val_accuracy: 0.7579\n",
      "Epoch 28/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6615 - accuracy: 0.7709 - val_loss: 0.6661 - val_accuracy: 0.7579\n",
      "Epoch 29/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.6605 - accuracy: 0.7709 - val_loss: 0.6652 - val_accuracy: 0.7684\n",
      "Epoch 30/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6596 - accuracy: 0.7709 - val_loss: 0.6642 - val_accuracy: 0.7684\n",
      "Epoch 31/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6586 - accuracy: 0.7740 - val_loss: 0.6633 - val_accuracy: 0.7684\n",
      "Epoch 32/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6575 - accuracy: 0.7740 - val_loss: 0.6623 - val_accuracy: 0.7684\n",
      "Epoch 33/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6565 - accuracy: 0.7740 - val_loss: 0.6613 - val_accuracy: 0.7579\n",
      "Epoch 34/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.6554 - accuracy: 0.7740 - val_loss: 0.6604 - val_accuracy: 0.7579\n",
      "Epoch 35/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.6543 - accuracy: 0.7740 - val_loss: 0.6594 - val_accuracy: 0.7579\n",
      "Epoch 36/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6533 - accuracy: 0.7740 - val_loss: 0.6584 - val_accuracy: 0.7579\n",
      "Epoch 37/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.6522 - accuracy: 0.7833 - val_loss: 0.6574 - val_accuracy: 0.7684\n",
      "Epoch 38/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6511 - accuracy: 0.7833 - val_loss: 0.6565 - val_accuracy: 0.7684\n",
      "Epoch 39/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.6500 - accuracy: 0.7833 - val_loss: 0.6555 - val_accuracy: 0.7684\n",
      "Epoch 40/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6489 - accuracy: 0.7833 - val_loss: 0.6545 - val_accuracy: 0.7684\n",
      "Epoch 41/200\n",
      "17/17 [==============================] - 0s 115us/step - loss: 0.6479 - accuracy: 0.7833 - val_loss: 0.6535 - val_accuracy: 0.7684\n",
      "Epoch 42/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6468 - accuracy: 0.7833 - val_loss: 0.6525 - val_accuracy: 0.7684\n",
      "Epoch 43/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.6457 - accuracy: 0.7833 - val_loss: 0.6515 - val_accuracy: 0.7684\n",
      "Epoch 44/200\n",
      "17/17 [==============================] - 0s 165us/step - loss: 0.6445 - accuracy: 0.7833 - val_loss: 0.6505 - val_accuracy: 0.7789\n",
      "Epoch 45/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6433 - accuracy: 0.7833 - val_loss: 0.6494 - val_accuracy: 0.7789\n",
      "Epoch 46/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6421 - accuracy: 0.7833 - val_loss: 0.6484 - val_accuracy: 0.7789\n",
      "Epoch 47/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6409 - accuracy: 0.7895 - val_loss: 0.6474 - val_accuracy: 0.7789\n",
      "Epoch 48/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.6397 - accuracy: 0.7895 - val_loss: 0.6463 - val_accuracy: 0.7789\n",
      "Epoch 49/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.6385 - accuracy: 0.7895 - val_loss: 0.6453 - val_accuracy: 0.7789\n",
      "Epoch 50/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.6372 - accuracy: 0.7895 - val_loss: 0.6442 - val_accuracy: 0.7789\n",
      "Epoch 51/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6360 - accuracy: 0.7895 - val_loss: 0.6431 - val_accuracy: 0.7789\n",
      "Epoch 52/200\n",
      "17/17 [==============================] - 0s 287us/step - loss: 0.6347 - accuracy: 0.7895 - val_loss: 0.6420 - val_accuracy: 0.7789\n",
      "Epoch 53/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6334 - accuracy: 0.7895 - val_loss: 0.6410 - val_accuracy: 0.7789\n",
      "Epoch 54/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.6321 - accuracy: 0.7926 - val_loss: 0.6399 - val_accuracy: 0.7895\n",
      "Epoch 55/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.6308 - accuracy: 0.7926 - val_loss: 0.6387 - val_accuracy: 0.7895\n",
      "Epoch 56/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6295 - accuracy: 0.7926 - val_loss: 0.6376 - val_accuracy: 0.7895\n",
      "Epoch 57/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6282 - accuracy: 0.7926 - val_loss: 0.6365 - val_accuracy: 0.7895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/200\n",
      "17/17 [==============================] - 0s 115us/step - loss: 0.6268 - accuracy: 0.7926 - val_loss: 0.6353 - val_accuracy: 0.7895\n",
      "Epoch 59/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6255 - accuracy: 0.7926 - val_loss: 0.6341 - val_accuracy: 0.7895\n",
      "Epoch 60/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6241 - accuracy: 0.7988 - val_loss: 0.6329 - val_accuracy: 0.7895\n",
      "Epoch 61/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6227 - accuracy: 0.7988 - val_loss: 0.6317 - val_accuracy: 0.7895\n",
      "Epoch 62/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6212 - accuracy: 0.7988 - val_loss: 0.6305 - val_accuracy: 0.7895\n",
      "Epoch 63/200\n",
      "17/17 [==============================] - 0s 115us/step - loss: 0.6198 - accuracy: 0.7988 - val_loss: 0.6293 - val_accuracy: 0.7895\n",
      "Epoch 64/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6183 - accuracy: 0.7988 - val_loss: 0.6280 - val_accuracy: 0.7895\n",
      "Epoch 65/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6168 - accuracy: 0.7957 - val_loss: 0.6268 - val_accuracy: 0.8000\n",
      "Epoch 66/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6153 - accuracy: 0.7957 - val_loss: 0.6255 - val_accuracy: 0.8000\n",
      "Epoch 67/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6138 - accuracy: 0.7988 - val_loss: 0.6242 - val_accuracy: 0.8000\n",
      "Epoch 68/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6122 - accuracy: 0.7988 - val_loss: 0.6229 - val_accuracy: 0.8000\n",
      "Epoch 69/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6106 - accuracy: 0.7988 - val_loss: 0.6216 - val_accuracy: 0.8000\n",
      "Epoch 70/200\n",
      "17/17 [==============================] - 0s 115us/step - loss: 0.6090 - accuracy: 0.7988 - val_loss: 0.6203 - val_accuracy: 0.8105\n",
      "Epoch 71/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6074 - accuracy: 0.7988 - val_loss: 0.6189 - val_accuracy: 0.8211\n",
      "Epoch 72/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6058 - accuracy: 0.7988 - val_loss: 0.6176 - val_accuracy: 0.8211\n",
      "Epoch 73/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.6041 - accuracy: 0.7988 - val_loss: 0.6162 - val_accuracy: 0.8211\n",
      "Epoch 74/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6024 - accuracy: 0.7988 - val_loss: 0.6148 - val_accuracy: 0.8211\n",
      "Epoch 75/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6007 - accuracy: 0.8080 - val_loss: 0.6134 - val_accuracy: 0.8211\n",
      "Epoch 76/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.5990 - accuracy: 0.8080 - val_loss: 0.6119 - val_accuracy: 0.8211\n",
      "Epoch 77/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.5973 - accuracy: 0.8080 - val_loss: 0.6105 - val_accuracy: 0.8211\n",
      "Epoch 78/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.5955 - accuracy: 0.8080 - val_loss: 0.6090 - val_accuracy: 0.8211\n",
      "Epoch 79/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.5938 - accuracy: 0.8080 - val_loss: 0.6076 - val_accuracy: 0.8211\n",
      "Epoch 80/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.5920 - accuracy: 0.8080 - val_loss: 0.6061 - val_accuracy: 0.8211\n",
      "Epoch 81/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.5902 - accuracy: 0.8080 - val_loss: 0.6045 - val_accuracy: 0.8211\n",
      "Epoch 82/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.5883 - accuracy: 0.8111 - val_loss: 0.6030 - val_accuracy: 0.8211\n",
      "Epoch 83/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.5865 - accuracy: 0.8111 - val_loss: 0.6014 - val_accuracy: 0.8105\n",
      "Epoch 84/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.5846 - accuracy: 0.8142 - val_loss: 0.5999 - val_accuracy: 0.8105\n",
      "Epoch 85/200\n",
      "17/17 [==============================] - 0s 115us/step - loss: 0.5826 - accuracy: 0.8142 - val_loss: 0.5982 - val_accuracy: 0.8105\n",
      "Epoch 86/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.5807 - accuracy: 0.8142 - val_loss: 0.5966 - val_accuracy: 0.8105\n",
      "Epoch 87/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.5787 - accuracy: 0.8142 - val_loss: 0.5949 - val_accuracy: 0.8105\n",
      "Epoch 88/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.5767 - accuracy: 0.8142 - val_loss: 0.5932 - val_accuracy: 0.8211\n",
      "Epoch 89/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.5747 - accuracy: 0.8173 - val_loss: 0.5915 - val_accuracy: 0.8211\n",
      "Epoch 90/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.5727 - accuracy: 0.8173 - val_loss: 0.5898 - val_accuracy: 0.8211\n",
      "Epoch 91/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.5706 - accuracy: 0.8173 - val_loss: 0.5880 - val_accuracy: 0.8211\n",
      "Epoch 92/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.5685 - accuracy: 0.8173 - val_loss: 0.5862 - val_accuracy: 0.8211\n",
      "Epoch 93/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.5663 - accuracy: 0.8204 - val_loss: 0.5844 - val_accuracy: 0.8211\n",
      "Epoch 94/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.5642 - accuracy: 0.8204 - val_loss: 0.5826 - val_accuracy: 0.8211\n",
      "Epoch 95/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.5620 - accuracy: 0.8204 - val_loss: 0.5807 - val_accuracy: 0.8211\n",
      "Epoch 96/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.5598 - accuracy: 0.8204 - val_loss: 0.5788 - val_accuracy: 0.8211\n",
      "Epoch 97/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.5575 - accuracy: 0.8204 - val_loss: 0.5769 - val_accuracy: 0.8211\n",
      "Epoch 98/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.5552 - accuracy: 0.8204 - val_loss: 0.5750 - val_accuracy: 0.8211\n",
      "Epoch 99/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.5529 - accuracy: 0.8266 - val_loss: 0.5730 - val_accuracy: 0.8211\n",
      "Epoch 100/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.5506 - accuracy: 0.8266 - val_loss: 0.5711 - val_accuracy: 0.8211\n",
      "Epoch 101/200\n",
      "17/17 [==============================] - 0s 115us/step - loss: 0.5482 - accuracy: 0.8266 - val_loss: 0.5691 - val_accuracy: 0.8316\n",
      "Epoch 102/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.5458 - accuracy: 0.8297 - val_loss: 0.5670 - val_accuracy: 0.8316\n",
      "Epoch 103/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.5434 - accuracy: 0.8297 - val_loss: 0.5650 - val_accuracy: 0.8316\n",
      "Epoch 104/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.5409 - accuracy: 0.8390 - val_loss: 0.5629 - val_accuracy: 0.8316\n",
      "Epoch 105/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.5384 - accuracy: 0.8390 - val_loss: 0.5608 - val_accuracy: 0.8316\n",
      "Epoch 106/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.5359 - accuracy: 0.8390 - val_loss: 0.5587 - val_accuracy: 0.8316\n",
      "Epoch 107/200\n",
      "17/17 [==============================] - 0s 287us/step - loss: 0.5333 - accuracy: 0.8390 - val_loss: 0.5565 - val_accuracy: 0.8316\n",
      "Epoch 108/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.5308 - accuracy: 0.8390 - val_loss: 0.5544 - val_accuracy: 0.8316\n",
      "Epoch 109/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.5281 - accuracy: 0.8390 - val_loss: 0.5522 - val_accuracy: 0.8316\n",
      "Epoch 110/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.5255 - accuracy: 0.8390 - val_loss: 0.5499 - val_accuracy: 0.8316\n",
      "Epoch 111/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.5228 - accuracy: 0.8421 - val_loss: 0.5477 - val_accuracy: 0.8316\n",
      "Epoch 112/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.5201 - accuracy: 0.8421 - val_loss: 0.5454 - val_accuracy: 0.8316\n",
      "Epoch 113/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.5174 - accuracy: 0.8421 - val_loss: 0.5431 - val_accuracy: 0.8316\n",
      "Epoch 114/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.5146 - accuracy: 0.8421 - val_loss: 0.5408 - val_accuracy: 0.8316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.5118 - accuracy: 0.8421 - val_loss: 0.5384 - val_accuracy: 0.8316\n",
      "Epoch 116/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.5089 - accuracy: 0.8421 - val_loss: 0.5361 - val_accuracy: 0.8316\n",
      "Epoch 117/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.5060 - accuracy: 0.8421 - val_loss: 0.5337 - val_accuracy: 0.8316\n",
      "Epoch 118/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.5031 - accuracy: 0.8421 - val_loss: 0.5313 - val_accuracy: 0.8316\n",
      "Epoch 119/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.5002 - accuracy: 0.8421 - val_loss: 0.5288 - val_accuracy: 0.8316\n",
      "Epoch 120/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.4972 - accuracy: 0.8452 - val_loss: 0.5263 - val_accuracy: 0.8316\n",
      "Epoch 121/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.4942 - accuracy: 0.8452 - val_loss: 0.5239 - val_accuracy: 0.8316\n",
      "Epoch 122/200\n",
      "17/17 [==============================] - 0s 115us/step - loss: 0.4911 - accuracy: 0.8452 - val_loss: 0.5213 - val_accuracy: 0.8316\n",
      "Epoch 123/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.4881 - accuracy: 0.8452 - val_loss: 0.5188 - val_accuracy: 0.8316\n",
      "Epoch 124/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.4850 - accuracy: 0.8452 - val_loss: 0.5163 - val_accuracy: 0.8316\n",
      "Epoch 125/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.4818 - accuracy: 0.8452 - val_loss: 0.5137 - val_accuracy: 0.8316\n",
      "Epoch 126/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.4787 - accuracy: 0.8452 - val_loss: 0.5111 - val_accuracy: 0.8316\n",
      "Epoch 127/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.4755 - accuracy: 0.8452 - val_loss: 0.5085 - val_accuracy: 0.8316\n",
      "Epoch 128/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.4723 - accuracy: 0.8452 - val_loss: 0.5058 - val_accuracy: 0.8316\n",
      "Epoch 129/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.4690 - accuracy: 0.8452 - val_loss: 0.5032 - val_accuracy: 0.8316\n",
      "Epoch 130/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.4658 - accuracy: 0.8452 - val_loss: 0.5005 - val_accuracy: 0.8316\n",
      "Epoch 131/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.4625 - accuracy: 0.8452 - val_loss: 0.4978 - val_accuracy: 0.8316\n",
      "Epoch 132/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.4592 - accuracy: 0.8452 - val_loss: 0.4951 - val_accuracy: 0.8316\n",
      "Epoch 133/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.4559 - accuracy: 0.8452 - val_loss: 0.4924 - val_accuracy: 0.8316\n",
      "Epoch 134/200\n",
      "17/17 [==============================] - 0s 287us/step - loss: 0.4525 - accuracy: 0.8483 - val_loss: 0.4896 - val_accuracy: 0.8316\n",
      "Epoch 135/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.4492 - accuracy: 0.8483 - val_loss: 0.4869 - val_accuracy: 0.8316\n",
      "Epoch 136/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.4458 - accuracy: 0.8483 - val_loss: 0.4841 - val_accuracy: 0.8316\n",
      "Epoch 137/200\n",
      "17/17 [==============================] - 0s 115us/step - loss: 0.4424 - accuracy: 0.8483 - val_loss: 0.4813 - val_accuracy: 0.8316\n",
      "Epoch 138/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.4390 - accuracy: 0.8514 - val_loss: 0.4785 - val_accuracy: 0.8316\n",
      "Epoch 139/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.4356 - accuracy: 0.8514 - val_loss: 0.4757 - val_accuracy: 0.8316\n",
      "Epoch 140/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.4321 - accuracy: 0.8514 - val_loss: 0.4730 - val_accuracy: 0.8421\n",
      "Epoch 141/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.4287 - accuracy: 0.8514 - val_loss: 0.4702 - val_accuracy: 0.8421\n",
      "Epoch 142/200\n",
      "17/17 [==============================] - 0s 115us/step - loss: 0.4253 - accuracy: 0.8514 - val_loss: 0.4674 - val_accuracy: 0.8526\n",
      "Epoch 143/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.4218 - accuracy: 0.8514 - val_loss: 0.4646 - val_accuracy: 0.8526\n",
      "Epoch 144/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.4184 - accuracy: 0.8545 - val_loss: 0.4618 - val_accuracy: 0.8526\n",
      "Epoch 145/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.4149 - accuracy: 0.8545 - val_loss: 0.4590 - val_accuracy: 0.8526\n",
      "Epoch 146/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.4114 - accuracy: 0.8576 - val_loss: 0.4562 - val_accuracy: 0.8526\n",
      "Epoch 147/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.4080 - accuracy: 0.8576 - val_loss: 0.4534 - val_accuracy: 0.8526\n",
      "Epoch 148/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.4045 - accuracy: 0.8638 - val_loss: 0.4507 - val_accuracy: 0.8526\n",
      "Epoch 149/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.4011 - accuracy: 0.8669 - val_loss: 0.4479 - val_accuracy: 0.8632\n",
      "Epoch 150/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.3977 - accuracy: 0.8762 - val_loss: 0.4452 - val_accuracy: 0.8632\n",
      "Epoch 151/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.3942 - accuracy: 0.8824 - val_loss: 0.4425 - val_accuracy: 0.8737\n",
      "Epoch 152/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.3908 - accuracy: 0.8885 - val_loss: 0.4398 - val_accuracy: 0.8737\n",
      "Epoch 153/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.3874 - accuracy: 0.8885 - val_loss: 0.4371 - val_accuracy: 0.8737\n",
      "Epoch 154/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.3840 - accuracy: 0.8916 - val_loss: 0.4345 - val_accuracy: 0.8842\n",
      "Epoch 155/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.3807 - accuracy: 0.9009 - val_loss: 0.4318 - val_accuracy: 0.8842\n",
      "Epoch 156/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.3773 - accuracy: 0.9009 - val_loss: 0.4292 - val_accuracy: 0.8947\n",
      "Epoch 157/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.3740 - accuracy: 0.9071 - val_loss: 0.4266 - val_accuracy: 0.8947\n",
      "Epoch 158/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.3707 - accuracy: 0.9164 - val_loss: 0.4241 - val_accuracy: 0.8947\n",
      "Epoch 159/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.3674 - accuracy: 0.9226 - val_loss: 0.4215 - val_accuracy: 0.8947\n",
      "Epoch 160/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.3642 - accuracy: 0.9226 - val_loss: 0.4190 - val_accuracy: 0.8947\n",
      "Epoch 161/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.3610 - accuracy: 0.9319 - val_loss: 0.4166 - val_accuracy: 0.9053\n",
      "Epoch 162/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.3578 - accuracy: 0.9319 - val_loss: 0.4141 - val_accuracy: 0.9053\n",
      "Epoch 163/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.3547 - accuracy: 0.9350 - val_loss: 0.4117 - val_accuracy: 0.9053\n",
      "Epoch 164/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.3516 - accuracy: 0.9350 - val_loss: 0.4094 - val_accuracy: 0.9053\n",
      "Epoch 165/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.3485 - accuracy: 0.9350 - val_loss: 0.4070 - val_accuracy: 0.9053\n",
      "Epoch 166/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.3455 - accuracy: 0.9350 - val_loss: 0.4047 - val_accuracy: 0.9053\n",
      "Epoch 167/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.3425 - accuracy: 0.9350 - val_loss: 0.4025 - val_accuracy: 0.9053\n",
      "Epoch 168/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.3396 - accuracy: 0.9350 - val_loss: 0.4003 - val_accuracy: 0.9053\n",
      "Epoch 169/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.3367 - accuracy: 0.9350 - val_loss: 0.3982 - val_accuracy: 0.9053\n",
      "Epoch 170/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.3338 - accuracy: 0.9350 - val_loss: 0.3960 - val_accuracy: 0.9053\n",
      "Epoch 171/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.3310 - accuracy: 0.9350 - val_loss: 0.3940 - val_accuracy: 0.9053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.3283 - accuracy: 0.9350 - val_loss: 0.3920 - val_accuracy: 0.9053\n",
      "Epoch 173/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.3256 - accuracy: 0.9350 - val_loss: 0.3900 - val_accuracy: 0.9053\n",
      "Epoch 174/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.3230 - accuracy: 0.9350 - val_loss: 0.3881 - val_accuracy: 0.9053\n",
      "Epoch 175/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.3204 - accuracy: 0.9350 - val_loss: 0.3862 - val_accuracy: 0.9053\n",
      "Epoch 176/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.3178 - accuracy: 0.9350 - val_loss: 0.3844 - val_accuracy: 0.9053\n",
      "Epoch 177/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.3154 - accuracy: 0.9350 - val_loss: 0.3827 - val_accuracy: 0.9053\n",
      "Epoch 178/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.3130 - accuracy: 0.9350 - val_loss: 0.3810 - val_accuracy: 0.9053\n",
      "Epoch 179/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.3106 - accuracy: 0.9350 - val_loss: 0.3793 - val_accuracy: 0.9053\n",
      "Epoch 180/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.3083 - accuracy: 0.9350 - val_loss: 0.3777 - val_accuracy: 0.9053\n",
      "Epoch 181/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.3060 - accuracy: 0.9350 - val_loss: 0.3762 - val_accuracy: 0.9053\n",
      "Epoch 182/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.3039 - accuracy: 0.9350 - val_loss: 0.3747 - val_accuracy: 0.9053\n",
      "Epoch 183/200\n",
      "17/17 [==============================] - 0s 115us/step - loss: 0.3017 - accuracy: 0.9350 - val_loss: 0.3733 - val_accuracy: 0.9053\n",
      "Epoch 184/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2996 - accuracy: 0.9350 - val_loss: 0.3719 - val_accuracy: 0.9053\n",
      "Epoch 185/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2976 - accuracy: 0.9350 - val_loss: 0.3706 - val_accuracy: 0.9053\n",
      "Epoch 186/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2957 - accuracy: 0.9350 - val_loss: 0.3693 - val_accuracy: 0.9053\n",
      "Epoch 187/200\n",
      "17/17 [==============================] - 0s 115us/step - loss: 0.2937 - accuracy: 0.9350 - val_loss: 0.3680 - val_accuracy: 0.9053\n",
      "Epoch 188/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2919 - accuracy: 0.9350 - val_loss: 0.3668 - val_accuracy: 0.9053\n",
      "Epoch 189/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2901 - accuracy: 0.9350 - val_loss: 0.3657 - val_accuracy: 0.9053\n",
      "Epoch 190/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2883 - accuracy: 0.9350 - val_loss: 0.3646 - val_accuracy: 0.9053\n",
      "Epoch 191/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2867 - accuracy: 0.9350 - val_loss: 0.3636 - val_accuracy: 0.9053\n",
      "Epoch 192/200\n",
      "17/17 [==============================] - 0s 115us/step - loss: 0.2850 - accuracy: 0.9350 - val_loss: 0.3626 - val_accuracy: 0.9053\n",
      "Epoch 193/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2834 - accuracy: 0.9350 - val_loss: 0.3616 - val_accuracy: 0.9053\n",
      "Epoch 194/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2819 - accuracy: 0.9350 - val_loss: 0.3607 - val_accuracy: 0.9053\n",
      "Epoch 195/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2804 - accuracy: 0.9350 - val_loss: 0.3599 - val_accuracy: 0.9053\n",
      "Epoch 196/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2789 - accuracy: 0.9350 - val_loss: 0.3590 - val_accuracy: 0.9053\n",
      "Epoch 197/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2775 - accuracy: 0.9350 - val_loss: 0.3583 - val_accuracy: 0.9053\n",
      "Epoch 198/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2762 - accuracy: 0.9350 - val_loss: 0.3575 - val_accuracy: 0.9053\n",
      "Epoch 199/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2749 - accuracy: 0.9350 - val_loss: 0.3568 - val_accuracy: 0.9053\n",
      "Epoch 200/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2736 - accuracy: 0.9350 - val_loss: 0.3561 - val_accuracy: 0.9053\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clair\\Anaconda3\\envs\\CNNs\\lib\\site-packages\\ipykernel_launcher.py:128: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "C:\\Users\\clair\\Anaconda3\\envs\\CNNs\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:808: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17 samples, validate on 5 samples\n",
      "Epoch 1/200\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.6792 - accuracy: 0.5944 - val_loss: 0.6618 - val_accuracy: 0.6526\n",
      "Epoch 2/200\n",
      "17/17 [==============================] - 0s 229us/step - loss: 0.6767 - accuracy: 0.6006 - val_loss: 0.6598 - val_accuracy: 0.6632\n",
      "Epoch 3/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6742 - accuracy: 0.6006 - val_loss: 0.6577 - val_accuracy: 0.6842\n",
      "Epoch 4/200\n",
      "17/17 [==============================] - 0s 287us/step - loss: 0.6717 - accuracy: 0.6192 - val_loss: 0.6557 - val_accuracy: 0.6737\n",
      "Epoch 5/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.6691 - accuracy: 0.6254 - val_loss: 0.6536 - val_accuracy: 0.6737\n",
      "Epoch 6/200\n",
      "17/17 [==============================] - 0s 229us/step - loss: 0.6665 - accuracy: 0.6254 - val_loss: 0.6515 - val_accuracy: 0.6632\n",
      "Epoch 7/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6638 - accuracy: 0.6440 - val_loss: 0.6493 - val_accuracy: 0.6632\n",
      "Epoch 8/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.6611 - accuracy: 0.6533 - val_loss: 0.6471 - val_accuracy: 0.6737\n",
      "Epoch 9/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.6583 - accuracy: 0.6625 - val_loss: 0.6449 - val_accuracy: 0.6737\n",
      "Epoch 10/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6555 - accuracy: 0.6749 - val_loss: 0.6427 - val_accuracy: 0.6842\n",
      "Epoch 11/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6527 - accuracy: 0.6780 - val_loss: 0.6404 - val_accuracy: 0.6842\n",
      "Epoch 12/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.6499 - accuracy: 0.6842 - val_loss: 0.6381 - val_accuracy: 0.6947\n",
      "Epoch 13/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6471 - accuracy: 0.6842 - val_loss: 0.6358 - val_accuracy: 0.6947\n",
      "Epoch 14/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6442 - accuracy: 0.6873 - val_loss: 0.6335 - val_accuracy: 0.6947\n",
      "Epoch 15/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6413 - accuracy: 0.6873 - val_loss: 0.6311 - val_accuracy: 0.6947\n",
      "Epoch 16/200\n",
      "17/17 [==============================] - 0s 229us/step - loss: 0.6383 - accuracy: 0.6966 - val_loss: 0.6287 - val_accuracy: 0.6947\n",
      "Epoch 17/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.6352 - accuracy: 0.7121 - val_loss: 0.6262 - val_accuracy: 0.6947\n",
      "Epoch 18/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.6321 - accuracy: 0.7183 - val_loss: 0.6237 - val_accuracy: 0.7053\n",
      "Epoch 19/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6290 - accuracy: 0.7368 - val_loss: 0.6212 - val_accuracy: 0.7158\n",
      "Epoch 20/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.6259 - accuracy: 0.7368 - val_loss: 0.6187 - val_accuracy: 0.7263\n",
      "Epoch 21/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6228 - accuracy: 0.7492 - val_loss: 0.6161 - val_accuracy: 0.7368\n",
      "Epoch 22/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6196 - accuracy: 0.7554 - val_loss: 0.6136 - val_accuracy: 0.7579\n",
      "Epoch 23/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.6164 - accuracy: 0.7771 - val_loss: 0.6109 - val_accuracy: 0.7579\n",
      "Epoch 24/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.6132 - accuracy: 0.7771 - val_loss: 0.6083 - val_accuracy: 0.7684\n",
      "Epoch 25/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6099 - accuracy: 0.7802 - val_loss: 0.6056 - val_accuracy: 0.7789\n",
      "Epoch 26/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6066 - accuracy: 0.7895 - val_loss: 0.6029 - val_accuracy: 0.7895\n",
      "Epoch 27/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.6032 - accuracy: 0.8019 - val_loss: 0.6001 - val_accuracy: 0.7895\n",
      "Epoch 28/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.5998 - accuracy: 0.8080 - val_loss: 0.5973 - val_accuracy: 0.7895\n",
      "Epoch 29/200\n",
      "17/17 [==============================] - 0s 206us/step - loss: 0.5963 - accuracy: 0.8173 - val_loss: 0.5945 - val_accuracy: 0.7895\n",
      "Epoch 30/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.5928 - accuracy: 0.8173 - val_loss: 0.5916 - val_accuracy: 0.7895\n",
      "Epoch 31/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.5892 - accuracy: 0.8173 - val_loss: 0.5886 - val_accuracy: 0.8000\n",
      "Epoch 32/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.5855 - accuracy: 0.8204 - val_loss: 0.5856 - val_accuracy: 0.8000\n",
      "Epoch 33/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.5818 - accuracy: 0.8235 - val_loss: 0.5826 - val_accuracy: 0.8105\n",
      "Epoch 34/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.5780 - accuracy: 0.8266 - val_loss: 0.5795 - val_accuracy: 0.8211\n",
      "Epoch 35/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.5742 - accuracy: 0.8266 - val_loss: 0.5763 - val_accuracy: 0.8316\n",
      "Epoch 36/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.5703 - accuracy: 0.8266 - val_loss: 0.5732 - val_accuracy: 0.8316\n",
      "Epoch 37/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.5664 - accuracy: 0.8266 - val_loss: 0.5699 - val_accuracy: 0.8526\n",
      "Epoch 38/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.5624 - accuracy: 0.8297 - val_loss: 0.5667 - val_accuracy: 0.8526\n",
      "Epoch 39/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.5583 - accuracy: 0.8297 - val_loss: 0.5633 - val_accuracy: 0.8632\n",
      "Epoch 40/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.5542 - accuracy: 0.8421 - val_loss: 0.5600 - val_accuracy: 0.8632\n",
      "Epoch 41/200\n",
      "17/17 [==============================] - 0s 115us/step - loss: 0.5500 - accuracy: 0.8421 - val_loss: 0.5565 - val_accuracy: 0.8632\n",
      "Epoch 42/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.5457 - accuracy: 0.8638 - val_loss: 0.5530 - val_accuracy: 0.8737\n",
      "Epoch 43/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.5414 - accuracy: 0.8762 - val_loss: 0.5494 - val_accuracy: 0.8737\n",
      "Epoch 44/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.5370 - accuracy: 0.8885 - val_loss: 0.5458 - val_accuracy: 0.8737\n",
      "Epoch 45/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.5325 - accuracy: 0.8978 - val_loss: 0.5422 - val_accuracy: 0.8842\n",
      "Epoch 46/200\n",
      "17/17 [==============================] - 0s 225us/step - loss: 0.5279 - accuracy: 0.9040 - val_loss: 0.5385 - val_accuracy: 0.8842\n",
      "Epoch 47/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.5233 - accuracy: 0.9164 - val_loss: 0.5347 - val_accuracy: 0.8842\n",
      "Epoch 48/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.5186 - accuracy: 0.9195 - val_loss: 0.5309 - val_accuracy: 0.8842\n",
      "Epoch 49/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.5138 - accuracy: 0.9195 - val_loss: 0.5271 - val_accuracy: 0.8842\n",
      "Epoch 50/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.5089 - accuracy: 0.9288 - val_loss: 0.5232 - val_accuracy: 0.8842\n",
      "Epoch 51/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.5040 - accuracy: 0.9288 - val_loss: 0.5193 - val_accuracy: 0.8842\n",
      "Epoch 52/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.4990 - accuracy: 0.9288 - val_loss: 0.5153 - val_accuracy: 0.8842\n",
      "Epoch 53/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.4940 - accuracy: 0.9381 - val_loss: 0.5112 - val_accuracy: 0.8842\n",
      "Epoch 54/200\n",
      "17/17 [==============================] - 0s 229us/step - loss: 0.4889 - accuracy: 0.9381 - val_loss: 0.5072 - val_accuracy: 0.8947\n",
      "Epoch 55/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.4838 - accuracy: 0.9381 - val_loss: 0.5031 - val_accuracy: 0.9053\n",
      "Epoch 56/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.4786 - accuracy: 0.9381 - val_loss: 0.4990 - val_accuracy: 0.9053\n",
      "Epoch 57/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.4733 - accuracy: 0.9381 - val_loss: 0.4948 - val_accuracy: 0.9053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.4679 - accuracy: 0.9381 - val_loss: 0.4906 - val_accuracy: 0.9053\n",
      "Epoch 59/200\n",
      "17/17 [==============================] - 0s 115us/step - loss: 0.4625 - accuracy: 0.9381 - val_loss: 0.4864 - val_accuracy: 0.9053\n",
      "Epoch 60/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.4571 - accuracy: 0.9381 - val_loss: 0.4821 - val_accuracy: 0.9053\n",
      "Epoch 61/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.4515 - accuracy: 0.9381 - val_loss: 0.4779 - val_accuracy: 0.9053\n",
      "Epoch 62/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.4460 - accuracy: 0.9381 - val_loss: 0.4736 - val_accuracy: 0.9053\n",
      "Epoch 63/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.4404 - accuracy: 0.9381 - val_loss: 0.4693 - val_accuracy: 0.9053\n",
      "Epoch 64/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.4348 - accuracy: 0.9381 - val_loss: 0.4650 - val_accuracy: 0.9053\n",
      "Epoch 65/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.4292 - accuracy: 0.9381 - val_loss: 0.4607 - val_accuracy: 0.9053\n",
      "Epoch 66/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.4236 - accuracy: 0.9381 - val_loss: 0.4565 - val_accuracy: 0.9053\n",
      "Epoch 67/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.4180 - accuracy: 0.9381 - val_loss: 0.4522 - val_accuracy: 0.9053\n",
      "Epoch 68/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.4124 - accuracy: 0.9381 - val_loss: 0.4479 - val_accuracy: 0.9053\n",
      "Epoch 69/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.4068 - accuracy: 0.9381 - val_loss: 0.4437 - val_accuracy: 0.9053\n",
      "Epoch 70/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.4012 - accuracy: 0.9381 - val_loss: 0.4396 - val_accuracy: 0.9053\n",
      "Epoch 71/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.3956 - accuracy: 0.9381 - val_loss: 0.4354 - val_accuracy: 0.9053\n",
      "Epoch 72/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.3901 - accuracy: 0.9381 - val_loss: 0.4313 - val_accuracy: 0.9053\n",
      "Epoch 73/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.3846 - accuracy: 0.9381 - val_loss: 0.4273 - val_accuracy: 0.9053\n",
      "Epoch 74/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.3791 - accuracy: 0.9381 - val_loss: 0.4233 - val_accuracy: 0.9053\n",
      "Epoch 75/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.3737 - accuracy: 0.9381 - val_loss: 0.4194 - val_accuracy: 0.9053\n",
      "Epoch 76/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.3684 - accuracy: 0.9381 - val_loss: 0.4155 - val_accuracy: 0.9053\n",
      "Epoch 77/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.3631 - accuracy: 0.9381 - val_loss: 0.4117 - val_accuracy: 0.9053\n",
      "Epoch 78/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.3579 - accuracy: 0.9381 - val_loss: 0.4081 - val_accuracy: 0.9053\n",
      "Epoch 79/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.3528 - accuracy: 0.9381 - val_loss: 0.4044 - val_accuracy: 0.9053\n",
      "Epoch 80/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.3478 - accuracy: 0.9381 - val_loss: 0.4009 - val_accuracy: 0.9053\n",
      "Epoch 81/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.3428 - accuracy: 0.9381 - val_loss: 0.3975 - val_accuracy: 0.9053\n",
      "Epoch 82/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.3380 - accuracy: 0.9381 - val_loss: 0.3942 - val_accuracy: 0.9053\n",
      "Epoch 83/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.3333 - accuracy: 0.9381 - val_loss: 0.3909 - val_accuracy: 0.9053\n",
      "Epoch 84/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.3287 - accuracy: 0.9381 - val_loss: 0.3878 - val_accuracy: 0.9053\n",
      "Epoch 85/200\n",
      "17/17 [==============================] - 0s 115us/step - loss: 0.3242 - accuracy: 0.9381 - val_loss: 0.3848 - val_accuracy: 0.9053\n",
      "Epoch 86/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.3198 - accuracy: 0.9381 - val_loss: 0.3819 - val_accuracy: 0.9053\n",
      "Epoch 87/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.3156 - accuracy: 0.9381 - val_loss: 0.3791 - val_accuracy: 0.9053\n",
      "Epoch 88/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.3115 - accuracy: 0.9381 - val_loss: 0.3764 - val_accuracy: 0.9053\n",
      "Epoch 89/200\n",
      "17/17 [==============================] - 0s 115us/step - loss: 0.3075 - accuracy: 0.9381 - val_loss: 0.3739 - val_accuracy: 0.9053\n",
      "Epoch 90/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.3036 - accuracy: 0.9381 - val_loss: 0.3714 - val_accuracy: 0.9053\n",
      "Epoch 91/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2999 - accuracy: 0.9381 - val_loss: 0.3691 - val_accuracy: 0.9053\n",
      "Epoch 92/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2963 - accuracy: 0.9381 - val_loss: 0.3668 - val_accuracy: 0.9053\n",
      "Epoch 93/200\n",
      "17/17 [==============================] - 0s 115us/step - loss: 0.2929 - accuracy: 0.9381 - val_loss: 0.3647 - val_accuracy: 0.9053\n",
      "Epoch 94/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2895 - accuracy: 0.9381 - val_loss: 0.3627 - val_accuracy: 0.9053\n",
      "Epoch 95/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2863 - accuracy: 0.9381 - val_loss: 0.3608 - val_accuracy: 0.9053\n",
      "Epoch 96/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2833 - accuracy: 0.9381 - val_loss: 0.3590 - val_accuracy: 0.9053\n",
      "Epoch 97/200\n",
      "17/17 [==============================] - 0s 202us/step - loss: 0.2804 - accuracy: 0.9381 - val_loss: 0.3573 - val_accuracy: 0.9053\n",
      "Epoch 98/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2775 - accuracy: 0.9381 - val_loss: 0.3557 - val_accuracy: 0.9053\n",
      "Epoch 99/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.2749 - accuracy: 0.9381 - val_loss: 0.3542 - val_accuracy: 0.9053\n",
      "Epoch 100/200\n",
      "17/17 [==============================] - 0s 287us/step - loss: 0.2723 - accuracy: 0.9381 - val_loss: 0.3528 - val_accuracy: 0.9053\n",
      "Epoch 101/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2698 - accuracy: 0.9381 - val_loss: 0.3515 - val_accuracy: 0.9053\n",
      "Epoch 102/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2675 - accuracy: 0.9381 - val_loss: 0.3502 - val_accuracy: 0.9053\n",
      "Epoch 103/200\n",
      "17/17 [==============================] - 0s 287us/step - loss: 0.2653 - accuracy: 0.9381 - val_loss: 0.3491 - val_accuracy: 0.9053\n",
      "Epoch 104/200\n",
      "17/17 [==============================] - 0s 287us/step - loss: 0.2632 - accuracy: 0.9381 - val_loss: 0.3480 - val_accuracy: 0.9053\n",
      "Epoch 105/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2611 - accuracy: 0.9381 - val_loss: 0.3471 - val_accuracy: 0.9053\n",
      "Epoch 106/200\n",
      "17/17 [==============================] - 0s 229us/step - loss: 0.2592 - accuracy: 0.9381 - val_loss: 0.3461 - val_accuracy: 0.9053\n",
      "Epoch 107/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.2574 - accuracy: 0.9381 - val_loss: 0.3453 - val_accuracy: 0.9053\n",
      "Epoch 108/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2557 - accuracy: 0.9381 - val_loss: 0.3445 - val_accuracy: 0.9053\n",
      "Epoch 109/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2540 - accuracy: 0.9381 - val_loss: 0.3438 - val_accuracy: 0.9053\n",
      "Epoch 110/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.2525 - accuracy: 0.9381 - val_loss: 0.3432 - val_accuracy: 0.9053\n",
      "Epoch 111/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.2510 - accuracy: 0.9381 - val_loss: 0.3426 - val_accuracy: 0.9053\n",
      "Epoch 112/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2496 - accuracy: 0.9381 - val_loss: 0.3420 - val_accuracy: 0.9053\n",
      "Epoch 113/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2483 - accuracy: 0.9381 - val_loss: 0.3415 - val_accuracy: 0.9053\n",
      "Epoch 114/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2470 - accuracy: 0.9381 - val_loss: 0.3411 - val_accuracy: 0.9053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2458 - accuracy: 0.9381 - val_loss: 0.3407 - val_accuracy: 0.9053\n",
      "Epoch 116/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2447 - accuracy: 0.9381 - val_loss: 0.3403 - val_accuracy: 0.9053\n",
      "Epoch 117/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2436 - accuracy: 0.9381 - val_loss: 0.3400 - val_accuracy: 0.9053\n",
      "Epoch 118/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2426 - accuracy: 0.9381 - val_loss: 0.3398 - val_accuracy: 0.9053\n",
      "Epoch 119/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2416 - accuracy: 0.9381 - val_loss: 0.3395 - val_accuracy: 0.9053\n",
      "Epoch 120/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2407 - accuracy: 0.9381 - val_loss: 0.3393 - val_accuracy: 0.9053\n",
      "Epoch 121/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2398 - accuracy: 0.9381 - val_loss: 0.3391 - val_accuracy: 0.9053\n",
      "Epoch 122/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.2390 - accuracy: 0.9381 - val_loss: 0.3390 - val_accuracy: 0.9053\n",
      "Epoch 123/200\n",
      "17/17 [==============================] - 0s 115us/step - loss: 0.2382 - accuracy: 0.9381 - val_loss: 0.3389 - val_accuracy: 0.9053\n",
      "Epoch 124/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2375 - accuracy: 0.9381 - val_loss: 0.3388 - val_accuracy: 0.9053\n",
      "Epoch 125/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.2368 - accuracy: 0.9381 - val_loss: 0.3387 - val_accuracy: 0.9053\n",
      "Epoch 126/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2361 - accuracy: 0.9381 - val_loss: 0.3387 - val_accuracy: 0.9053\n",
      "Epoch 127/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2355 - accuracy: 0.9381 - val_loss: 0.3386 - val_accuracy: 0.9053\n",
      "Epoch 128/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2349 - accuracy: 0.9381 - val_loss: 0.3386 - val_accuracy: 0.9053\n",
      "Epoch 129/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2343 - accuracy: 0.9381 - val_loss: 0.3386 - val_accuracy: 0.9053\n",
      "Epoch 130/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2338 - accuracy: 0.9381 - val_loss: 0.3387 - val_accuracy: 0.9053\n",
      "Epoch 131/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2333 - accuracy: 0.9381 - val_loss: 0.3387 - val_accuracy: 0.9053\n",
      "Epoch 132/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2328 - accuracy: 0.9381 - val_loss: 0.3387 - val_accuracy: 0.9053\n",
      "Epoch 133/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2323 - accuracy: 0.9381 - val_loss: 0.3388 - val_accuracy: 0.9053\n",
      "Epoch 134/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2319 - accuracy: 0.9381 - val_loss: 0.3389 - val_accuracy: 0.9053\n",
      "Epoch 135/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2314 - accuracy: 0.9381 - val_loss: 0.3390 - val_accuracy: 0.9053\n",
      "Epoch 136/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2310 - accuracy: 0.9381 - val_loss: 0.3391 - val_accuracy: 0.9053\n",
      "Epoch 137/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2306 - accuracy: 0.9381 - val_loss: 0.3392 - val_accuracy: 0.9053\n",
      "Epoch 138/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.2303 - accuracy: 0.9381 - val_loss: 0.3393 - val_accuracy: 0.9053\n",
      "Epoch 139/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2299 - accuracy: 0.9381 - val_loss: 0.3394 - val_accuracy: 0.9053\n",
      "Epoch 140/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2296 - accuracy: 0.9381 - val_loss: 0.3395 - val_accuracy: 0.9053\n",
      "Epoch 141/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.2292 - accuracy: 0.9381 - val_loss: 0.3396 - val_accuracy: 0.9053\n",
      "Epoch 142/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.2289 - accuracy: 0.9381 - val_loss: 0.3398 - val_accuracy: 0.9053\n",
      "Epoch 143/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2286 - accuracy: 0.9381 - val_loss: 0.3399 - val_accuracy: 0.9053\n",
      "Epoch 144/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2284 - accuracy: 0.9381 - val_loss: 0.3401 - val_accuracy: 0.9053\n",
      "Epoch 145/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.2281 - accuracy: 0.9381 - val_loss: 0.3402 - val_accuracy: 0.9053\n",
      "Epoch 146/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2278 - accuracy: 0.9381 - val_loss: 0.3404 - val_accuracy: 0.9053\n",
      "Epoch 147/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.2276 - accuracy: 0.9381 - val_loss: 0.3405 - val_accuracy: 0.9053\n",
      "Epoch 148/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2273 - accuracy: 0.9381 - val_loss: 0.3407 - val_accuracy: 0.9053\n",
      "Epoch 149/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.2271 - accuracy: 0.9381 - val_loss: 0.3409 - val_accuracy: 0.9053\n",
      "Epoch 150/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.2269 - accuracy: 0.9381 - val_loss: 0.3410 - val_accuracy: 0.9053\n",
      "Epoch 151/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.2267 - accuracy: 0.9381 - val_loss: 0.3412 - val_accuracy: 0.9053\n",
      "Epoch 152/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2264 - accuracy: 0.9381 - val_loss: 0.3414 - val_accuracy: 0.9053\n",
      "Epoch 153/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2262 - accuracy: 0.9381 - val_loss: 0.3415 - val_accuracy: 0.9053\n",
      "Epoch 154/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2260 - accuracy: 0.9381 - val_loss: 0.3417 - val_accuracy: 0.9053\n",
      "Epoch 155/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2259 - accuracy: 0.9381 - val_loss: 0.3419 - val_accuracy: 0.9053\n",
      "Epoch 156/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2257 - accuracy: 0.9381 - val_loss: 0.3421 - val_accuracy: 0.9053\n",
      "Epoch 157/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2255 - accuracy: 0.9381 - val_loss: 0.3422 - val_accuracy: 0.9053\n",
      "Epoch 158/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2253 - accuracy: 0.9381 - val_loss: 0.3424 - val_accuracy: 0.9053\n",
      "Epoch 159/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.2252 - accuracy: 0.9381 - val_loss: 0.3426 - val_accuracy: 0.9053\n",
      "Epoch 160/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2250 - accuracy: 0.9381 - val_loss: 0.3427 - val_accuracy: 0.9053\n",
      "Epoch 161/200\n",
      "17/17 [==============================] - 0s 115us/step - loss: 0.2248 - accuracy: 0.9381 - val_loss: 0.3429 - val_accuracy: 0.9053\n",
      "Epoch 162/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2247 - accuracy: 0.9381 - val_loss: 0.3431 - val_accuracy: 0.9053\n",
      "Epoch 163/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2246 - accuracy: 0.9381 - val_loss: 0.3433 - val_accuracy: 0.9053\n",
      "Epoch 164/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2244 - accuracy: 0.9381 - val_loss: 0.3434 - val_accuracy: 0.9053\n",
      "Epoch 165/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2243 - accuracy: 0.9381 - val_loss: 0.3436 - val_accuracy: 0.9053\n",
      "Epoch 166/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2241 - accuracy: 0.9381 - val_loss: 0.3438 - val_accuracy: 0.9053\n",
      "Epoch 167/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.2240 - accuracy: 0.9381 - val_loss: 0.3439 - val_accuracy: 0.9053\n",
      "Epoch 168/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.2239 - accuracy: 0.9381 - val_loss: 0.3441 - val_accuracy: 0.9053\n",
      "Epoch 169/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2237 - accuracy: 0.9381 - val_loss: 0.3443 - val_accuracy: 0.9053\n",
      "Epoch 170/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.2236 - accuracy: 0.9381 - val_loss: 0.3444 - val_accuracy: 0.9053\n",
      "Epoch 171/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2235 - accuracy: 0.9381 - val_loss: 0.3446 - val_accuracy: 0.9053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2234 - accuracy: 0.9381 - val_loss: 0.3448 - val_accuracy: 0.9053\n",
      "Epoch 173/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.2233 - accuracy: 0.9381 - val_loss: 0.3449 - val_accuracy: 0.9053\n",
      "Epoch 174/200\n",
      "17/17 [==============================] - 0s 115us/step - loss: 0.2232 - accuracy: 0.9381 - val_loss: 0.3451 - val_accuracy: 0.9053\n",
      "Epoch 175/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2231 - accuracy: 0.9381 - val_loss: 0.3453 - val_accuracy: 0.9053\n",
      "Epoch 176/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.2230 - accuracy: 0.9381 - val_loss: 0.3454 - val_accuracy: 0.9053\n",
      "Epoch 177/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2229 - accuracy: 0.9381 - val_loss: 0.3456 - val_accuracy: 0.9053\n",
      "Epoch 178/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2228 - accuracy: 0.9381 - val_loss: 0.3458 - val_accuracy: 0.9053\n",
      "Epoch 179/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2227 - accuracy: 0.9381 - val_loss: 0.3459 - val_accuracy: 0.9053\n",
      "Epoch 180/200\n",
      "17/17 [==============================] - 0s 115us/step - loss: 0.2226 - accuracy: 0.9381 - val_loss: 0.3461 - val_accuracy: 0.9053\n",
      "Epoch 181/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2225 - accuracy: 0.9381 - val_loss: 0.3463 - val_accuracy: 0.9053\n",
      "Epoch 182/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2224 - accuracy: 0.9381 - val_loss: 0.3464 - val_accuracy: 0.9053\n",
      "Epoch 183/200\n",
      "17/17 [==============================] - 0s 115us/step - loss: 0.2223 - accuracy: 0.9381 - val_loss: 0.3465 - val_accuracy: 0.9053\n",
      "Epoch 184/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.2222 - accuracy: 0.9381 - val_loss: 0.3467 - val_accuracy: 0.9053\n",
      "Epoch 185/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2221 - accuracy: 0.9381 - val_loss: 0.3469 - val_accuracy: 0.9053\n",
      "Epoch 186/200\n",
      "17/17 [==============================] - 0s 115us/step - loss: 0.2220 - accuracy: 0.9381 - val_loss: 0.3470 - val_accuracy: 0.9053\n",
      "Epoch 187/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.2219 - accuracy: 0.9381 - val_loss: 0.3472 - val_accuracy: 0.9053\n",
      "Epoch 188/200\n",
      "17/17 [==============================] - 0s 230us/step - loss: 0.2219 - accuracy: 0.9381 - val_loss: 0.3473 - val_accuracy: 0.9053\n",
      "Epoch 189/200\n",
      "17/17 [==============================] - 0s 173us/step - loss: 0.2218 - accuracy: 0.9381 - val_loss: 0.3475 - val_accuracy: 0.9053\n",
      "Epoch 190/200\n",
      "17/17 [==============================] - 0s 115us/step - loss: 0.2217 - accuracy: 0.9381 - val_loss: 0.3476 - val_accuracy: 0.9053\n",
      "Epoch 191/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2216 - accuracy: 0.9381 - val_loss: 0.3478 - val_accuracy: 0.9053\n",
      "Epoch 192/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2215 - accuracy: 0.9381 - val_loss: 0.3479 - val_accuracy: 0.9053\n",
      "Epoch 193/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2215 - accuracy: 0.9381 - val_loss: 0.3480 - val_accuracy: 0.9053\n",
      "Epoch 194/200\n",
      "17/17 [==============================] - 0s 115us/step - loss: 0.2214 - accuracy: 0.9381 - val_loss: 0.3482 - val_accuracy: 0.9053\n",
      "Epoch 195/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2213 - accuracy: 0.9381 - val_loss: 0.3483 - val_accuracy: 0.9053\n",
      "Epoch 196/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2212 - accuracy: 0.9381 - val_loss: 0.3485 - val_accuracy: 0.9053\n",
      "Epoch 197/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2212 - accuracy: 0.9381 - val_loss: 0.3486 - val_accuracy: 0.9053\n",
      "Epoch 198/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2211 - accuracy: 0.9381 - val_loss: 0.3487 - val_accuracy: 0.9053\n",
      "Epoch 199/200\n",
      "17/17 [==============================] - 0s 172us/step - loss: 0.2210 - accuracy: 0.9381 - val_loss: 0.3489 - val_accuracy: 0.9053\n",
      "Epoch 200/200\n",
      "17/17 [==============================] - 0s 115us/step - loss: 0.2210 - accuracy: 0.9381 - val_loss: 0.3490 - val_accuracy: 0.9053\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clair\\Anaconda3\\envs\\CNNs\\lib\\site-packages\\ipykernel_launcher.py:128: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "C:\\Users\\clair\\Anaconda3\\envs\\CNNs\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:808: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18 samples, validate on 5 samples\n",
      "Epoch 1/200\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.6746 - accuracy: 0.5760 - val_loss: 0.6806 - val_accuracy: 0.5579\n",
      "Epoch 2/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6721 - accuracy: 0.5877 - val_loss: 0.6787 - val_accuracy: 0.5684\n",
      "Epoch 3/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6697 - accuracy: 0.5906 - val_loss: 0.6769 - val_accuracy: 0.5684\n",
      "Epoch 4/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.6673 - accuracy: 0.5965 - val_loss: 0.6750 - val_accuracy: 0.5684\n",
      "Epoch 5/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6649 - accuracy: 0.5994 - val_loss: 0.6732 - val_accuracy: 0.5895\n",
      "Epoch 6/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6625 - accuracy: 0.6023 - val_loss: 0.6713 - val_accuracy: 0.6000\n",
      "Epoch 7/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.6600 - accuracy: 0.6082 - val_loss: 0.6694 - val_accuracy: 0.6000\n",
      "Epoch 8/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6576 - accuracy: 0.6199 - val_loss: 0.6676 - val_accuracy: 0.6105\n",
      "Epoch 9/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6552 - accuracy: 0.6257 - val_loss: 0.6657 - val_accuracy: 0.6211\n",
      "Epoch 10/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6527 - accuracy: 0.6345 - val_loss: 0.6639 - val_accuracy: 0.6316\n",
      "Epoch 11/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6503 - accuracy: 0.6433 - val_loss: 0.6620 - val_accuracy: 0.6316\n",
      "Epoch 12/200\n",
      "18/18 [==============================] - 0s 108us/step - loss: 0.6478 - accuracy: 0.6491 - val_loss: 0.6601 - val_accuracy: 0.6421\n",
      "Epoch 13/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6453 - accuracy: 0.6579 - val_loss: 0.6582 - val_accuracy: 0.6526\n",
      "Epoch 14/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6429 - accuracy: 0.6608 - val_loss: 0.6563 - val_accuracy: 0.6526\n",
      "Epoch 15/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6404 - accuracy: 0.6696 - val_loss: 0.6543 - val_accuracy: 0.6737\n",
      "Epoch 16/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6379 - accuracy: 0.6930 - val_loss: 0.6524 - val_accuracy: 0.6737\n",
      "Epoch 17/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6353 - accuracy: 0.6959 - val_loss: 0.6505 - val_accuracy: 0.6737\n",
      "Epoch 18/200\n",
      "18/18 [==============================] - 0s 108us/step - loss: 0.6327 - accuracy: 0.7047 - val_loss: 0.6485 - val_accuracy: 0.6737\n",
      "Epoch 19/200\n",
      "18/18 [==============================] - 0s 185us/step - loss: 0.6301 - accuracy: 0.7135 - val_loss: 0.6466 - val_accuracy: 0.6737\n",
      "Epoch 20/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.6275 - accuracy: 0.7164 - val_loss: 0.6446 - val_accuracy: 0.6842\n",
      "Epoch 21/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6249 - accuracy: 0.7164 - val_loss: 0.6426 - val_accuracy: 0.6842\n",
      "Epoch 22/200\n",
      "18/18 [==============================] - 0s 109us/step - loss: 0.6222 - accuracy: 0.7193 - val_loss: 0.6405 - val_accuracy: 0.6842\n",
      "Epoch 23/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6194 - accuracy: 0.7193 - val_loss: 0.6385 - val_accuracy: 0.6947\n",
      "Epoch 24/200\n",
      "18/18 [==============================] - 0s 162us/step - loss: 0.6167 - accuracy: 0.7193 - val_loss: 0.6364 - val_accuracy: 0.6947\n",
      "Epoch 25/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6139 - accuracy: 0.7281 - val_loss: 0.6343 - val_accuracy: 0.6947\n",
      "Epoch 26/200\n",
      "18/18 [==============================] - 0s 108us/step - loss: 0.6111 - accuracy: 0.7368 - val_loss: 0.6322 - val_accuracy: 0.6947\n",
      "Epoch 27/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6082 - accuracy: 0.7368 - val_loss: 0.6300 - val_accuracy: 0.6947\n",
      "Epoch 28/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6054 - accuracy: 0.7368 - val_loss: 0.6278 - val_accuracy: 0.7053\n",
      "Epoch 29/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6024 - accuracy: 0.7368 - val_loss: 0.6256 - val_accuracy: 0.7158\n",
      "Epoch 30/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.5994 - accuracy: 0.7398 - val_loss: 0.6233 - val_accuracy: 0.7053\n",
      "Epoch 31/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.5964 - accuracy: 0.7398 - val_loss: 0.6210 - val_accuracy: 0.7053\n",
      "Epoch 32/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.5934 - accuracy: 0.7485 - val_loss: 0.6187 - val_accuracy: 0.7053\n",
      "Epoch 33/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.5902 - accuracy: 0.7456 - val_loss: 0.6163 - val_accuracy: 0.7053\n",
      "Epoch 34/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.5871 - accuracy: 0.7485 - val_loss: 0.6139 - val_accuracy: 0.7053\n",
      "Epoch 35/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.5839 - accuracy: 0.7602 - val_loss: 0.6115 - val_accuracy: 0.7158\n",
      "Epoch 36/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.5807 - accuracy: 0.7632 - val_loss: 0.6090 - val_accuracy: 0.7158\n",
      "Epoch 37/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.5774 - accuracy: 0.7661 - val_loss: 0.6065 - val_accuracy: 0.7368\n",
      "Epoch 38/200\n",
      "18/18 [==============================] - 0s 164us/step - loss: 0.5741 - accuracy: 0.7661 - val_loss: 0.6039 - val_accuracy: 0.7368\n",
      "Epoch 39/200\n",
      "18/18 [==============================] - 0s 108us/step - loss: 0.5707 - accuracy: 0.7690 - val_loss: 0.6014 - val_accuracy: 0.7368\n",
      "Epoch 40/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.5672 - accuracy: 0.7719 - val_loss: 0.5987 - val_accuracy: 0.7368\n",
      "Epoch 41/200\n",
      "18/18 [==============================] - 0s 108us/step - loss: 0.5638 - accuracy: 0.7719 - val_loss: 0.5960 - val_accuracy: 0.7474\n",
      "Epoch 42/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.5602 - accuracy: 0.7807 - val_loss: 0.5933 - val_accuracy: 0.7474\n",
      "Epoch 43/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.5566 - accuracy: 0.7865 - val_loss: 0.5905 - val_accuracy: 0.7474\n",
      "Epoch 44/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.5529 - accuracy: 0.7865 - val_loss: 0.5877 - val_accuracy: 0.7579\n",
      "Epoch 45/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.5491 - accuracy: 0.7982 - val_loss: 0.5848 - val_accuracy: 0.7684\n",
      "Epoch 46/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.5453 - accuracy: 0.7982 - val_loss: 0.5819 - val_accuracy: 0.7895\n",
      "Epoch 47/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.5414 - accuracy: 0.8041 - val_loss: 0.5790 - val_accuracy: 0.8000\n",
      "Epoch 48/200\n",
      "18/18 [==============================] - 0s 108us/step - loss: 0.5374 - accuracy: 0.8099 - val_loss: 0.5760 - val_accuracy: 0.8105\n",
      "Epoch 49/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.5333 - accuracy: 0.8275 - val_loss: 0.5729 - val_accuracy: 0.8105\n",
      "Epoch 50/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.5292 - accuracy: 0.8392 - val_loss: 0.5698 - val_accuracy: 0.8316\n",
      "Epoch 51/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.5251 - accuracy: 0.8392 - val_loss: 0.5666 - val_accuracy: 0.8316\n",
      "Epoch 52/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.5208 - accuracy: 0.8450 - val_loss: 0.5634 - val_accuracy: 0.8316\n",
      "Epoch 53/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.5165 - accuracy: 0.8538 - val_loss: 0.5601 - val_accuracy: 0.8316\n",
      "Epoch 54/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.5121 - accuracy: 0.8713 - val_loss: 0.5568 - val_accuracy: 0.8316\n",
      "Epoch 55/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.5076 - accuracy: 0.8713 - val_loss: 0.5534 - val_accuracy: 0.8316\n",
      "Epoch 56/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.5031 - accuracy: 0.8743 - val_loss: 0.5500 - val_accuracy: 0.8316\n",
      "Epoch 57/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.4985 - accuracy: 0.8743 - val_loss: 0.5465 - val_accuracy: 0.8421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.4939 - accuracy: 0.8801 - val_loss: 0.5429 - val_accuracy: 0.8526\n",
      "Epoch 59/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.4892 - accuracy: 0.8801 - val_loss: 0.5393 - val_accuracy: 0.8526\n",
      "Epoch 60/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.4844 - accuracy: 0.8830 - val_loss: 0.5357 - val_accuracy: 0.8632\n",
      "Epoch 61/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.4795 - accuracy: 0.8889 - val_loss: 0.5320 - val_accuracy: 0.8632\n",
      "Epoch 62/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.4746 - accuracy: 0.8918 - val_loss: 0.5283 - val_accuracy: 0.8526\n",
      "Epoch 63/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.4696 - accuracy: 0.8947 - val_loss: 0.5245 - val_accuracy: 0.8526\n",
      "Epoch 64/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.4646 - accuracy: 0.8947 - val_loss: 0.5207 - val_accuracy: 0.8632\n",
      "Epoch 65/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.4595 - accuracy: 0.8977 - val_loss: 0.5169 - val_accuracy: 0.8632\n",
      "Epoch 66/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.4544 - accuracy: 0.9006 - val_loss: 0.5130 - val_accuracy: 0.8737\n",
      "Epoch 67/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.4492 - accuracy: 0.8977 - val_loss: 0.5091 - val_accuracy: 0.8737\n",
      "Epoch 68/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.4439 - accuracy: 0.9035 - val_loss: 0.5051 - val_accuracy: 0.8842\n",
      "Epoch 69/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.4386 - accuracy: 0.9094 - val_loss: 0.5012 - val_accuracy: 0.8842\n",
      "Epoch 70/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.4333 - accuracy: 0.9152 - val_loss: 0.4972 - val_accuracy: 0.8842\n",
      "Epoch 71/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.4279 - accuracy: 0.9152 - val_loss: 0.4932 - val_accuracy: 0.8842\n",
      "Epoch 72/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.4225 - accuracy: 0.9240 - val_loss: 0.4891 - val_accuracy: 0.8842\n",
      "Epoch 73/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.4171 - accuracy: 0.9386 - val_loss: 0.4851 - val_accuracy: 0.8947\n",
      "Epoch 74/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.4117 - accuracy: 0.9386 - val_loss: 0.4810 - val_accuracy: 0.9053\n",
      "Epoch 75/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.4062 - accuracy: 0.9444 - val_loss: 0.4770 - val_accuracy: 0.9053\n",
      "Epoch 76/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.4008 - accuracy: 0.9444 - val_loss: 0.4729 - val_accuracy: 0.9053\n",
      "Epoch 77/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.3954 - accuracy: 0.9444 - val_loss: 0.4689 - val_accuracy: 0.9053\n",
      "Epoch 78/200\n",
      "18/18 [==============================] - 0s 108us/step - loss: 0.3900 - accuracy: 0.9444 - val_loss: 0.4649 - val_accuracy: 0.9053\n",
      "Epoch 79/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.3845 - accuracy: 0.9444 - val_loss: 0.4609 - val_accuracy: 0.9053\n",
      "Epoch 80/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.3792 - accuracy: 0.9444 - val_loss: 0.4569 - val_accuracy: 0.9053\n",
      "Epoch 81/200\n",
      "18/18 [==============================] - 0s 108us/step - loss: 0.3738 - accuracy: 0.9444 - val_loss: 0.4530 - val_accuracy: 0.9053\n",
      "Epoch 82/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.3685 - accuracy: 0.9444 - val_loss: 0.4491 - val_accuracy: 0.9053\n",
      "Epoch 83/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.3632 - accuracy: 0.9444 - val_loss: 0.4452 - val_accuracy: 0.9053\n",
      "Epoch 84/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.3580 - accuracy: 0.9444 - val_loss: 0.4414 - val_accuracy: 0.9053\n",
      "Epoch 85/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.3529 - accuracy: 0.9444 - val_loss: 0.4376 - val_accuracy: 0.9053\n",
      "Epoch 86/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.3478 - accuracy: 0.9444 - val_loss: 0.4339 - val_accuracy: 0.9053\n",
      "Epoch 87/200\n",
      "18/18 [==============================] - 0s 108us/step - loss: 0.3428 - accuracy: 0.9444 - val_loss: 0.4303 - val_accuracy: 0.9053\n",
      "Epoch 88/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.3378 - accuracy: 0.9444 - val_loss: 0.4267 - val_accuracy: 0.9053\n",
      "Epoch 89/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.3330 - accuracy: 0.9444 - val_loss: 0.4232 - val_accuracy: 0.9053\n",
      "Epoch 90/200\n",
      "18/18 [==============================] - 0s 108us/step - loss: 0.3282 - accuracy: 0.9444 - val_loss: 0.4198 - val_accuracy: 0.9053\n",
      "Epoch 91/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.3236 - accuracy: 0.9444 - val_loss: 0.4164 - val_accuracy: 0.9053\n",
      "Epoch 92/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.3190 - accuracy: 0.9444 - val_loss: 0.4131 - val_accuracy: 0.9053\n",
      "Epoch 93/200\n",
      "18/18 [==============================] - 0s 216us/step - loss: 0.3145 - accuracy: 0.9444 - val_loss: 0.4100 - val_accuracy: 0.9053\n",
      "Epoch 94/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.3102 - accuracy: 0.9444 - val_loss: 0.4069 - val_accuracy: 0.9053\n",
      "Epoch 95/200\n",
      "18/18 [==============================] - 0s 108us/step - loss: 0.3059 - accuracy: 0.9444 - val_loss: 0.4038 - val_accuracy: 0.9053\n",
      "Epoch 96/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.3017 - accuracy: 0.9444 - val_loss: 0.4009 - val_accuracy: 0.9053\n",
      "Epoch 97/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2977 - accuracy: 0.9444 - val_loss: 0.3981 - val_accuracy: 0.9053\n",
      "Epoch 98/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2938 - accuracy: 0.9444 - val_loss: 0.3953 - val_accuracy: 0.9053\n",
      "Epoch 99/200\n",
      "18/18 [==============================] - 0s 109us/step - loss: 0.2900 - accuracy: 0.9444 - val_loss: 0.3927 - val_accuracy: 0.9053\n",
      "Epoch 100/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2863 - accuracy: 0.9444 - val_loss: 0.3901 - val_accuracy: 0.9053\n",
      "Epoch 101/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2827 - accuracy: 0.9444 - val_loss: 0.3876 - val_accuracy: 0.9053\n",
      "Epoch 102/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2793 - accuracy: 0.9444 - val_loss: 0.3852 - val_accuracy: 0.9053\n",
      "Epoch 103/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2760 - accuracy: 0.9444 - val_loss: 0.3829 - val_accuracy: 0.9053\n",
      "Epoch 104/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2728 - accuracy: 0.9444 - val_loss: 0.3806 - val_accuracy: 0.9053\n",
      "Epoch 105/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2696 - accuracy: 0.9444 - val_loss: 0.3784 - val_accuracy: 0.9053\n",
      "Epoch 106/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2666 - accuracy: 0.9444 - val_loss: 0.3763 - val_accuracy: 0.9053\n",
      "Epoch 107/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2636 - accuracy: 0.9444 - val_loss: 0.3742 - val_accuracy: 0.9053\n",
      "Epoch 108/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.2609 - accuracy: 0.9444 - val_loss: 0.3723 - val_accuracy: 0.9053\n",
      "Epoch 109/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.2582 - accuracy: 0.9444 - val_loss: 0.3705 - val_accuracy: 0.9053\n",
      "Epoch 110/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2557 - accuracy: 0.9444 - val_loss: 0.3688 - val_accuracy: 0.9053\n",
      "Epoch 111/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.2532 - accuracy: 0.9444 - val_loss: 0.3672 - val_accuracy: 0.9053\n",
      "Epoch 112/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.2510 - accuracy: 0.9444 - val_loss: 0.3656 - val_accuracy: 0.9053\n",
      "Epoch 113/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.2488 - accuracy: 0.9444 - val_loss: 0.3642 - val_accuracy: 0.9053\n",
      "Epoch 114/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2467 - accuracy: 0.9444 - val_loss: 0.3628 - val_accuracy: 0.9053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2447 - accuracy: 0.9444 - val_loss: 0.3615 - val_accuracy: 0.9053\n",
      "Epoch 116/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2428 - accuracy: 0.9444 - val_loss: 0.3603 - val_accuracy: 0.9053\n",
      "Epoch 117/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2411 - accuracy: 0.9444 - val_loss: 0.3591 - val_accuracy: 0.9053\n",
      "Epoch 118/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.2394 - accuracy: 0.9444 - val_loss: 0.3581 - val_accuracy: 0.9053\n",
      "Epoch 119/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2378 - accuracy: 0.9444 - val_loss: 0.3571 - val_accuracy: 0.9053\n",
      "Epoch 120/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2363 - accuracy: 0.9444 - val_loss: 0.3561 - val_accuracy: 0.9053\n",
      "Epoch 121/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2349 - accuracy: 0.9444 - val_loss: 0.3553 - val_accuracy: 0.9053\n",
      "Epoch 122/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.2336 - accuracy: 0.9444 - val_loss: 0.3545 - val_accuracy: 0.9053\n",
      "Epoch 123/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.2323 - accuracy: 0.9444 - val_loss: 0.3537 - val_accuracy: 0.9053\n",
      "Epoch 124/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2311 - accuracy: 0.9444 - val_loss: 0.3530 - val_accuracy: 0.9053\n",
      "Epoch 125/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2300 - accuracy: 0.9444 - val_loss: 0.3523 - val_accuracy: 0.9053\n",
      "Epoch 126/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.2290 - accuracy: 0.9444 - val_loss: 0.3517 - val_accuracy: 0.9053\n",
      "Epoch 127/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2280 - accuracy: 0.9444 - val_loss: 0.3512 - val_accuracy: 0.9053\n",
      "Epoch 128/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2270 - accuracy: 0.9444 - val_loss: 0.3506 - val_accuracy: 0.9053\n",
      "Epoch 129/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2261 - accuracy: 0.9444 - val_loss: 0.3501 - val_accuracy: 0.9053\n",
      "Epoch 130/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.2253 - accuracy: 0.9444 - val_loss: 0.3497 - val_accuracy: 0.9053\n",
      "Epoch 131/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.2245 - accuracy: 0.9444 - val_loss: 0.3493 - val_accuracy: 0.9053\n",
      "Epoch 132/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2238 - accuracy: 0.9444 - val_loss: 0.3489 - val_accuracy: 0.9053\n",
      "Epoch 133/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.2231 - accuracy: 0.9444 - val_loss: 0.3485 - val_accuracy: 0.9053\n",
      "Epoch 134/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2224 - accuracy: 0.9444 - val_loss: 0.3482 - val_accuracy: 0.9053\n",
      "Epoch 135/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2218 - accuracy: 0.9444 - val_loss: 0.3478 - val_accuracy: 0.9053\n",
      "Epoch 136/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2212 - accuracy: 0.9444 - val_loss: 0.3476 - val_accuracy: 0.9053\n",
      "Epoch 137/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2206 - accuracy: 0.9444 - val_loss: 0.3473 - val_accuracy: 0.9053\n",
      "Epoch 138/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.2201 - accuracy: 0.9444 - val_loss: 0.3470 - val_accuracy: 0.9053\n",
      "Epoch 139/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2196 - accuracy: 0.9444 - val_loss: 0.3468 - val_accuracy: 0.9053\n",
      "Epoch 140/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2191 - accuracy: 0.9444 - val_loss: 0.3466 - val_accuracy: 0.9053\n",
      "Epoch 141/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2187 - accuracy: 0.9444 - val_loss: 0.3464 - val_accuracy: 0.9053\n",
      "Epoch 142/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2183 - accuracy: 0.9444 - val_loss: 0.3462 - val_accuracy: 0.9053\n",
      "Epoch 143/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2179 - accuracy: 0.9444 - val_loss: 0.3460 - val_accuracy: 0.9053\n",
      "Epoch 144/200\n",
      "18/18 [==============================] - 0s 162us/step - loss: 0.2175 - accuracy: 0.9444 - val_loss: 0.3459 - val_accuracy: 0.9053\n",
      "Epoch 145/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2171 - accuracy: 0.9444 - val_loss: 0.3458 - val_accuracy: 0.9053\n",
      "Epoch 146/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2168 - accuracy: 0.9444 - val_loss: 0.3456 - val_accuracy: 0.9053\n",
      "Epoch 147/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2165 - accuracy: 0.9444 - val_loss: 0.3455 - val_accuracy: 0.9053\n",
      "Epoch 148/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2162 - accuracy: 0.9444 - val_loss: 0.3454 - val_accuracy: 0.9053\n",
      "Epoch 149/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.2159 - accuracy: 0.9444 - val_loss: 0.3453 - val_accuracy: 0.9053\n",
      "Epoch 150/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2156 - accuracy: 0.9444 - val_loss: 0.3452 - val_accuracy: 0.9053\n",
      "Epoch 151/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2154 - accuracy: 0.9444 - val_loss: 0.3451 - val_accuracy: 0.9053\n",
      "Epoch 152/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2151 - accuracy: 0.9444 - val_loss: 0.3450 - val_accuracy: 0.9053\n",
      "Epoch 153/200\n",
      "18/18 [==============================] - 0s 271us/step - loss: 0.2149 - accuracy: 0.9444 - val_loss: 0.3450 - val_accuracy: 0.9053\n",
      "Epoch 154/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2146 - accuracy: 0.9444 - val_loss: 0.3449 - val_accuracy: 0.9053\n",
      "Epoch 155/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2144 - accuracy: 0.9444 - val_loss: 0.3449 - val_accuracy: 0.9053\n",
      "Epoch 156/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2142 - accuracy: 0.9444 - val_loss: 0.3448 - val_accuracy: 0.9053\n",
      "Epoch 157/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2140 - accuracy: 0.9444 - val_loss: 0.3448 - val_accuracy: 0.9053\n",
      "Epoch 158/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2138 - accuracy: 0.9444 - val_loss: 0.3447 - val_accuracy: 0.9053\n",
      "Epoch 159/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2137 - accuracy: 0.9444 - val_loss: 0.3447 - val_accuracy: 0.9053\n",
      "Epoch 160/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.2135 - accuracy: 0.9444 - val_loss: 0.3446 - val_accuracy: 0.9053\n",
      "Epoch 161/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.2133 - accuracy: 0.9444 - val_loss: 0.3446 - val_accuracy: 0.9053\n",
      "Epoch 162/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2132 - accuracy: 0.9444 - val_loss: 0.3446 - val_accuracy: 0.9053\n",
      "Epoch 163/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2130 - accuracy: 0.9444 - val_loss: 0.3445 - val_accuracy: 0.9053\n",
      "Epoch 164/200\n",
      "18/18 [==============================] - 0s 108us/step - loss: 0.2129 - accuracy: 0.9444 - val_loss: 0.3445 - val_accuracy: 0.9053\n",
      "Epoch 165/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2127 - accuracy: 0.9444 - val_loss: 0.3445 - val_accuracy: 0.9053\n",
      "Epoch 166/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2126 - accuracy: 0.9444 - val_loss: 0.3445 - val_accuracy: 0.9053\n",
      "Epoch 167/200\n",
      "18/18 [==============================] - 0s 108us/step - loss: 0.2125 - accuracy: 0.9444 - val_loss: 0.3445 - val_accuracy: 0.9053\n",
      "Epoch 168/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2123 - accuracy: 0.9444 - val_loss: 0.3444 - val_accuracy: 0.9053\n",
      "Epoch 169/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2122 - accuracy: 0.9444 - val_loss: 0.3444 - val_accuracy: 0.9053\n",
      "Epoch 170/200\n",
      "18/18 [==============================] - 0s 109us/step - loss: 0.2121 - accuracy: 0.9444 - val_loss: 0.3444 - val_accuracy: 0.9053\n",
      "Epoch 171/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2120 - accuracy: 0.9444 - val_loss: 0.3444 - val_accuracy: 0.9053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2119 - accuracy: 0.9444 - val_loss: 0.3444 - val_accuracy: 0.9053\n",
      "Epoch 173/200\n",
      "18/18 [==============================] - 0s 108us/step - loss: 0.2118 - accuracy: 0.9444 - val_loss: 0.3444 - val_accuracy: 0.9053\n",
      "Epoch 174/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2117 - accuracy: 0.9444 - val_loss: 0.3444 - val_accuracy: 0.9053\n",
      "Epoch 175/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2116 - accuracy: 0.9444 - val_loss: 0.3443 - val_accuracy: 0.9053\n",
      "Epoch 176/200\n",
      "18/18 [==============================] - 0s 108us/step - loss: 0.2115 - accuracy: 0.9444 - val_loss: 0.3443 - val_accuracy: 0.9053\n",
      "Epoch 177/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2114 - accuracy: 0.9444 - val_loss: 0.3443 - val_accuracy: 0.9053\n",
      "Epoch 178/200\n",
      "18/18 [==============================] - 0s 108us/step - loss: 0.2113 - accuracy: 0.9444 - val_loss: 0.3443 - val_accuracy: 0.9053\n",
      "Epoch 179/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2112 - accuracy: 0.9444 - val_loss: 0.3443 - val_accuracy: 0.9053\n",
      "Epoch 180/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.2112 - accuracy: 0.9444 - val_loss: 0.3443 - val_accuracy: 0.9053\n",
      "Epoch 181/200\n",
      "18/18 [==============================] - 0s 108us/step - loss: 0.2111 - accuracy: 0.9444 - val_loss: 0.3443 - val_accuracy: 0.9053\n",
      "Epoch 182/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2110 - accuracy: 0.9444 - val_loss: 0.3443 - val_accuracy: 0.9053\n",
      "Epoch 183/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2109 - accuracy: 0.9444 - val_loss: 0.3443 - val_accuracy: 0.9053\n",
      "Epoch 184/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2109 - accuracy: 0.9444 - val_loss: 0.3443 - val_accuracy: 0.9053\n",
      "Epoch 185/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2108 - accuracy: 0.9444 - val_loss: 0.3443 - val_accuracy: 0.9053\n",
      "Epoch 186/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.2107 - accuracy: 0.9444 - val_loss: 0.3443 - val_accuracy: 0.9053\n",
      "Epoch 187/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2106 - accuracy: 0.9444 - val_loss: 0.3443 - val_accuracy: 0.9053\n",
      "Epoch 188/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2106 - accuracy: 0.9444 - val_loss: 0.3443 - val_accuracy: 0.9053\n",
      "Epoch 189/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2105 - accuracy: 0.9444 - val_loss: 0.3443 - val_accuracy: 0.9053\n",
      "Epoch 190/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2105 - accuracy: 0.9444 - val_loss: 0.3443 - val_accuracy: 0.9053\n",
      "Epoch 191/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2104 - accuracy: 0.9444 - val_loss: 0.3443 - val_accuracy: 0.9053\n",
      "Epoch 192/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2103 - accuracy: 0.9444 - val_loss: 0.3443 - val_accuracy: 0.9053\n",
      "Epoch 193/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2103 - accuracy: 0.9444 - val_loss: 0.3443 - val_accuracy: 0.9053\n",
      "Epoch 194/200\n",
      "18/18 [==============================] - 0s 108us/step - loss: 0.2102 - accuracy: 0.9444 - val_loss: 0.3443 - val_accuracy: 0.9053\n",
      "Epoch 195/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.2102 - accuracy: 0.9444 - val_loss: 0.3443 - val_accuracy: 0.9053\n",
      "Epoch 196/200\n",
      "18/18 [==============================] - 0s 108us/step - loss: 0.2101 - accuracy: 0.9444 - val_loss: 0.3443 - val_accuracy: 0.9053\n",
      "Epoch 197/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2101 - accuracy: 0.9444 - val_loss: 0.3443 - val_accuracy: 0.9053\n",
      "Epoch 198/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.2100 - accuracy: 0.9444 - val_loss: 0.3443 - val_accuracy: 0.9053\n",
      "Epoch 199/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2099 - accuracy: 0.9444 - val_loss: 0.3443 - val_accuracy: 0.9053\n",
      "Epoch 200/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.2099 - accuracy: 0.9444 - val_loss: 0.3444 - val_accuracy: 0.9053\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clair\\Anaconda3\\envs\\CNNs\\lib\\site-packages\\ipykernel_launcher.py:128: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "C:\\Users\\clair\\Anaconda3\\envs\\CNNs\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:808: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18 samples, validate on 5 samples\n",
      "Epoch 1/200\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.7028 - accuracy: 0.4327 - val_loss: 0.7026 - val_accuracy: 0.3895\n",
      "Epoch 2/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.7013 - accuracy: 0.4561 - val_loss: 0.7014 - val_accuracy: 0.4105\n",
      "Epoch 3/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6998 - accuracy: 0.4561 - val_loss: 0.7002 - val_accuracy: 0.4211\n",
      "Epoch 4/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.6983 - accuracy: 0.4561 - val_loss: 0.6991 - val_accuracy: 0.4316\n",
      "Epoch 5/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6968 - accuracy: 0.4591 - val_loss: 0.6979 - val_accuracy: 0.4632\n",
      "Epoch 6/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.6954 - accuracy: 0.4591 - val_loss: 0.6968 - val_accuracy: 0.4632\n",
      "Epoch 7/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.6939 - accuracy: 0.4591 - val_loss: 0.6958 - val_accuracy: 0.4632\n",
      "Epoch 8/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.6926 - accuracy: 0.4649 - val_loss: 0.6947 - val_accuracy: 0.4947\n",
      "Epoch 9/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.6912 - accuracy: 0.4766 - val_loss: 0.6937 - val_accuracy: 0.5053\n",
      "Epoch 10/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.6899 - accuracy: 0.4854 - val_loss: 0.6927 - val_accuracy: 0.5158\n",
      "Epoch 11/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.6886 - accuracy: 0.4854 - val_loss: 0.6917 - val_accuracy: 0.5263\n",
      "Epoch 12/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6873 - accuracy: 0.4883 - val_loss: 0.6907 - val_accuracy: 0.5368\n",
      "Epoch 13/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.6861 - accuracy: 0.4942 - val_loss: 0.6897 - val_accuracy: 0.5368\n",
      "Epoch 14/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6849 - accuracy: 0.4912 - val_loss: 0.6888 - val_accuracy: 0.5474\n",
      "Epoch 15/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.6837 - accuracy: 0.5000 - val_loss: 0.6879 - val_accuracy: 0.5474\n",
      "Epoch 16/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6825 - accuracy: 0.5088 - val_loss: 0.6870 - val_accuracy: 0.5368\n",
      "Epoch 17/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6814 - accuracy: 0.5175 - val_loss: 0.6861 - val_accuracy: 0.5474\n",
      "Epoch 18/200\n",
      "18/18 [==============================] - 0s 108us/step - loss: 0.6803 - accuracy: 0.5205 - val_loss: 0.6852 - val_accuracy: 0.5579\n",
      "Epoch 19/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6791 - accuracy: 0.5263 - val_loss: 0.6843 - val_accuracy: 0.5789\n",
      "Epoch 20/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6780 - accuracy: 0.5322 - val_loss: 0.6834 - val_accuracy: 0.5789\n",
      "Epoch 21/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6769 - accuracy: 0.5439 - val_loss: 0.6825 - val_accuracy: 0.5789\n",
      "Epoch 22/200\n",
      "18/18 [==============================] - 0s 108us/step - loss: 0.6758 - accuracy: 0.5439 - val_loss: 0.6817 - val_accuracy: 0.5789\n",
      "Epoch 23/200\n",
      "18/18 [==============================] - 0s 108us/step - loss: 0.6747 - accuracy: 0.5468 - val_loss: 0.6808 - val_accuracy: 0.5789\n",
      "Epoch 24/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6737 - accuracy: 0.5556 - val_loss: 0.6800 - val_accuracy: 0.5789\n",
      "Epoch 25/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6726 - accuracy: 0.5614 - val_loss: 0.6791 - val_accuracy: 0.6211\n",
      "Epoch 26/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6715 - accuracy: 0.5848 - val_loss: 0.6783 - val_accuracy: 0.6211\n",
      "Epoch 27/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6704 - accuracy: 0.5936 - val_loss: 0.6774 - val_accuracy: 0.6421\n",
      "Epoch 28/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6693 - accuracy: 0.6023 - val_loss: 0.6766 - val_accuracy: 0.6421\n",
      "Epoch 29/200\n",
      "18/18 [==============================] - 0s 108us/step - loss: 0.6682 - accuracy: 0.6053 - val_loss: 0.6758 - val_accuracy: 0.6421\n",
      "Epoch 30/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6671 - accuracy: 0.6053 - val_loss: 0.6749 - val_accuracy: 0.6632\n",
      "Epoch 31/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6660 - accuracy: 0.6140 - val_loss: 0.6741 - val_accuracy: 0.6632\n",
      "Epoch 32/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6649 - accuracy: 0.6199 - val_loss: 0.6733 - val_accuracy: 0.6632\n",
      "Epoch 33/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6638 - accuracy: 0.6374 - val_loss: 0.6724 - val_accuracy: 0.6632\n",
      "Epoch 34/200\n",
      "18/18 [==============================] - 0s 108us/step - loss: 0.6627 - accuracy: 0.6491 - val_loss: 0.6716 - val_accuracy: 0.6737\n",
      "Epoch 35/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6616 - accuracy: 0.6520 - val_loss: 0.6708 - val_accuracy: 0.6842\n",
      "Epoch 36/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6605 - accuracy: 0.6550 - val_loss: 0.6700 - val_accuracy: 0.6842\n",
      "Epoch 37/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6594 - accuracy: 0.6637 - val_loss: 0.6691 - val_accuracy: 0.6842\n",
      "Epoch 38/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6583 - accuracy: 0.6754 - val_loss: 0.6683 - val_accuracy: 0.6842\n",
      "Epoch 39/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6572 - accuracy: 0.6813 - val_loss: 0.6675 - val_accuracy: 0.6842\n",
      "Epoch 40/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6561 - accuracy: 0.6930 - val_loss: 0.6666 - val_accuracy: 0.6947\n",
      "Epoch 41/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6550 - accuracy: 0.6930 - val_loss: 0.6658 - val_accuracy: 0.6842\n",
      "Epoch 42/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6538 - accuracy: 0.7076 - val_loss: 0.6649 - val_accuracy: 0.6842\n",
      "Epoch 43/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.6527 - accuracy: 0.7222 - val_loss: 0.6641 - val_accuracy: 0.6842\n",
      "Epoch 44/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6516 - accuracy: 0.7222 - val_loss: 0.6633 - val_accuracy: 0.6842\n",
      "Epoch 45/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6504 - accuracy: 0.7281 - val_loss: 0.6625 - val_accuracy: 0.6842\n",
      "Epoch 46/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.6493 - accuracy: 0.7310 - val_loss: 0.6616 - val_accuracy: 0.6947\n",
      "Epoch 47/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6482 - accuracy: 0.7310 - val_loss: 0.6608 - val_accuracy: 0.7368\n",
      "Epoch 48/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6470 - accuracy: 0.7398 - val_loss: 0.6600 - val_accuracy: 0.7368\n",
      "Epoch 49/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6459 - accuracy: 0.7398 - val_loss: 0.6591 - val_accuracy: 0.7368\n",
      "Epoch 50/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6447 - accuracy: 0.7427 - val_loss: 0.6583 - val_accuracy: 0.7368\n",
      "Epoch 51/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6435 - accuracy: 0.7485 - val_loss: 0.6575 - val_accuracy: 0.7579\n",
      "Epoch 52/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6423 - accuracy: 0.7515 - val_loss: 0.6566 - val_accuracy: 0.7579\n",
      "Epoch 53/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.6411 - accuracy: 0.7515 - val_loss: 0.6557 - val_accuracy: 0.7579\n",
      "Epoch 54/200\n",
      "18/18 [==============================] - 0s 108us/step - loss: 0.6399 - accuracy: 0.7515 - val_loss: 0.6549 - val_accuracy: 0.7579\n",
      "Epoch 55/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6387 - accuracy: 0.7485 - val_loss: 0.6540 - val_accuracy: 0.7579\n",
      "Epoch 56/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.6375 - accuracy: 0.7515 - val_loss: 0.6531 - val_accuracy: 0.7579\n",
      "Epoch 57/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6362 - accuracy: 0.7515 - val_loss: 0.6522 - val_accuracy: 0.7579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6350 - accuracy: 0.7515 - val_loss: 0.6513 - val_accuracy: 0.7684\n",
      "Epoch 59/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6337 - accuracy: 0.7690 - val_loss: 0.6504 - val_accuracy: 0.7684\n",
      "Epoch 60/200\n",
      "18/18 [==============================] - 0s 108us/step - loss: 0.6324 - accuracy: 0.7719 - val_loss: 0.6495 - val_accuracy: 0.7895\n",
      "Epoch 61/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6311 - accuracy: 0.7807 - val_loss: 0.6486 - val_accuracy: 0.8000\n",
      "Epoch 62/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6298 - accuracy: 0.7836 - val_loss: 0.6477 - val_accuracy: 0.8000\n",
      "Epoch 63/200\n",
      "18/18 [==============================] - 0s 108us/step - loss: 0.6285 - accuracy: 0.7924 - val_loss: 0.6467 - val_accuracy: 0.8105\n",
      "Epoch 64/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6272 - accuracy: 0.7924 - val_loss: 0.6457 - val_accuracy: 0.8105\n",
      "Epoch 65/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6258 - accuracy: 0.8012 - val_loss: 0.6448 - val_accuracy: 0.8105\n",
      "Epoch 66/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6245 - accuracy: 0.8012 - val_loss: 0.6438 - val_accuracy: 0.8105\n",
      "Epoch 67/200\n",
      "18/18 [==============================] - 0s 109us/step - loss: 0.6231 - accuracy: 0.8012 - val_loss: 0.6428 - val_accuracy: 0.8105\n",
      "Epoch 68/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6217 - accuracy: 0.8012 - val_loss: 0.6418 - val_accuracy: 0.8105\n",
      "Epoch 69/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6203 - accuracy: 0.8070 - val_loss: 0.6408 - val_accuracy: 0.8105\n",
      "Epoch 70/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6189 - accuracy: 0.8099 - val_loss: 0.6398 - val_accuracy: 0.8105\n",
      "Epoch 71/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6174 - accuracy: 0.8246 - val_loss: 0.6387 - val_accuracy: 0.8211\n",
      "Epoch 72/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6160 - accuracy: 0.8275 - val_loss: 0.6377 - val_accuracy: 0.8316\n",
      "Epoch 73/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6145 - accuracy: 0.8304 - val_loss: 0.6367 - val_accuracy: 0.8316\n",
      "Epoch 74/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.6130 - accuracy: 0.8333 - val_loss: 0.6356 - val_accuracy: 0.8316\n",
      "Epoch 75/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6115 - accuracy: 0.8333 - val_loss: 0.6345 - val_accuracy: 0.8316\n",
      "Epoch 76/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.6100 - accuracy: 0.8333 - val_loss: 0.6334 - val_accuracy: 0.8421\n",
      "Epoch 77/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6084 - accuracy: 0.8392 - val_loss: 0.6323 - val_accuracy: 0.8421\n",
      "Epoch 78/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6068 - accuracy: 0.8450 - val_loss: 0.6311 - val_accuracy: 0.8421\n",
      "Epoch 79/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.6052 - accuracy: 0.8509 - val_loss: 0.6300 - val_accuracy: 0.8421\n",
      "Epoch 80/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6036 - accuracy: 0.8480 - val_loss: 0.6288 - val_accuracy: 0.8421\n",
      "Epoch 81/200\n",
      "18/18 [==============================] - 0s 108us/step - loss: 0.6019 - accuracy: 0.8509 - val_loss: 0.6276 - val_accuracy: 0.8421\n",
      "Epoch 82/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.6002 - accuracy: 0.8538 - val_loss: 0.6264 - val_accuracy: 0.8526\n",
      "Epoch 83/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.5985 - accuracy: 0.8567 - val_loss: 0.6251 - val_accuracy: 0.8632\n",
      "Epoch 84/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.5968 - accuracy: 0.8596 - val_loss: 0.6239 - val_accuracy: 0.8632\n",
      "Epoch 85/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.5950 - accuracy: 0.8655 - val_loss: 0.6226 - val_accuracy: 0.8737\n",
      "Epoch 86/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.5932 - accuracy: 0.8684 - val_loss: 0.6213 - val_accuracy: 0.8737\n",
      "Epoch 87/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.5914 - accuracy: 0.8743 - val_loss: 0.6199 - val_accuracy: 0.8737\n",
      "Epoch 88/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.5895 - accuracy: 0.8830 - val_loss: 0.6186 - val_accuracy: 0.8737\n",
      "Epoch 89/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.5875 - accuracy: 0.8860 - val_loss: 0.6171 - val_accuracy: 0.8737\n",
      "Epoch 90/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.5854 - accuracy: 0.8889 - val_loss: 0.6157 - val_accuracy: 0.8737\n",
      "Epoch 91/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.5833 - accuracy: 0.8918 - val_loss: 0.6142 - val_accuracy: 0.8737\n",
      "Epoch 92/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.5812 - accuracy: 0.8947 - val_loss: 0.6126 - val_accuracy: 0.8737\n",
      "Epoch 93/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.5790 - accuracy: 0.8947 - val_loss: 0.6110 - val_accuracy: 0.8737\n",
      "Epoch 94/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.5767 - accuracy: 0.8947 - val_loss: 0.6094 - val_accuracy: 0.8737\n",
      "Epoch 95/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.5745 - accuracy: 0.8947 - val_loss: 0.6078 - val_accuracy: 0.8737\n",
      "Epoch 96/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.5722 - accuracy: 0.8977 - val_loss: 0.6061 - val_accuracy: 0.8737\n",
      "Epoch 97/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.5698 - accuracy: 0.9006 - val_loss: 0.6044 - val_accuracy: 0.8737\n",
      "Epoch 98/200\n",
      "18/18 [==============================] - 0s 108us/step - loss: 0.5674 - accuracy: 0.9006 - val_loss: 0.6027 - val_accuracy: 0.8737\n",
      "Epoch 99/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.5650 - accuracy: 0.9006 - val_loss: 0.6009 - val_accuracy: 0.8842\n",
      "Epoch 100/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.5625 - accuracy: 0.9094 - val_loss: 0.5991 - val_accuracy: 0.8842\n",
      "Epoch 101/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.5599 - accuracy: 0.9123 - val_loss: 0.5973 - val_accuracy: 0.8842\n",
      "Epoch 102/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.5573 - accuracy: 0.9123 - val_loss: 0.5955 - val_accuracy: 0.8842\n",
      "Epoch 103/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.5547 - accuracy: 0.9152 - val_loss: 0.5936 - val_accuracy: 0.8842\n",
      "Epoch 104/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.5520 - accuracy: 0.9269 - val_loss: 0.5917 - val_accuracy: 0.8947\n",
      "Epoch 105/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.5493 - accuracy: 0.9298 - val_loss: 0.5897 - val_accuracy: 0.8947\n",
      "Epoch 106/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.5465 - accuracy: 0.9327 - val_loss: 0.5878 - val_accuracy: 0.8947\n",
      "Epoch 107/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.5437 - accuracy: 0.9327 - val_loss: 0.5858 - val_accuracy: 0.8947\n",
      "Epoch 108/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.5408 - accuracy: 0.9327 - val_loss: 0.5838 - val_accuracy: 0.8947\n",
      "Epoch 109/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.5379 - accuracy: 0.9327 - val_loss: 0.5817 - val_accuracy: 0.8947\n",
      "Epoch 110/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.5349 - accuracy: 0.9327 - val_loss: 0.5796 - val_accuracy: 0.8947\n",
      "Epoch 111/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.5318 - accuracy: 0.9327 - val_loss: 0.5774 - val_accuracy: 0.8947\n",
      "Epoch 112/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.5287 - accuracy: 0.9327 - val_loss: 0.5752 - val_accuracy: 0.8947\n",
      "Epoch 113/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.5255 - accuracy: 0.9327 - val_loss: 0.5730 - val_accuracy: 0.9053\n",
      "Epoch 114/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.5222 - accuracy: 0.9327 - val_loss: 0.5707 - val_accuracy: 0.9053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.5189 - accuracy: 0.9327 - val_loss: 0.5684 - val_accuracy: 0.9053\n",
      "Epoch 116/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.5156 - accuracy: 0.9357 - val_loss: 0.5659 - val_accuracy: 0.9053\n",
      "Epoch 117/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.5122 - accuracy: 0.9357 - val_loss: 0.5635 - val_accuracy: 0.9053\n",
      "Epoch 118/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.5087 - accuracy: 0.9357 - val_loss: 0.5610 - val_accuracy: 0.9053\n",
      "Epoch 119/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.5052 - accuracy: 0.9357 - val_loss: 0.5584 - val_accuracy: 0.9053\n",
      "Epoch 120/200\n",
      "18/18 [==============================] - 0s 109us/step - loss: 0.5016 - accuracy: 0.9357 - val_loss: 0.5558 - val_accuracy: 0.9053\n",
      "Epoch 121/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.4980 - accuracy: 0.9357 - val_loss: 0.5532 - val_accuracy: 0.9053\n",
      "Epoch 122/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.4943 - accuracy: 0.9357 - val_loss: 0.5506 - val_accuracy: 0.9053\n",
      "Epoch 123/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.4906 - accuracy: 0.9357 - val_loss: 0.5478 - val_accuracy: 0.9053\n",
      "Epoch 124/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.4868 - accuracy: 0.9357 - val_loss: 0.5451 - val_accuracy: 0.9053\n",
      "Epoch 125/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.4829 - accuracy: 0.9357 - val_loss: 0.5423 - val_accuracy: 0.9053\n",
      "Epoch 126/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.4790 - accuracy: 0.9357 - val_loss: 0.5395 - val_accuracy: 0.9053\n",
      "Epoch 127/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.4751 - accuracy: 0.9357 - val_loss: 0.5366 - val_accuracy: 0.9053\n",
      "Epoch 128/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.4711 - accuracy: 0.9357 - val_loss: 0.5337 - val_accuracy: 0.9053\n",
      "Epoch 129/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.4670 - accuracy: 0.9357 - val_loss: 0.5307 - val_accuracy: 0.9053\n",
      "Epoch 130/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.4629 - accuracy: 0.9357 - val_loss: 0.5278 - val_accuracy: 0.9053\n",
      "Epoch 131/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.4588 - accuracy: 0.9357 - val_loss: 0.5247 - val_accuracy: 0.9053\n",
      "Epoch 132/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.4546 - accuracy: 0.9357 - val_loss: 0.5217 - val_accuracy: 0.9053\n",
      "Epoch 133/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.4504 - accuracy: 0.9357 - val_loss: 0.5186 - val_accuracy: 0.9053\n",
      "Epoch 134/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.4462 - accuracy: 0.9357 - val_loss: 0.5155 - val_accuracy: 0.9053\n",
      "Epoch 135/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.4419 - accuracy: 0.9357 - val_loss: 0.5124 - val_accuracy: 0.9053\n",
      "Epoch 136/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.4375 - accuracy: 0.9357 - val_loss: 0.5092 - val_accuracy: 0.9053\n",
      "Epoch 137/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.4331 - accuracy: 0.9357 - val_loss: 0.5060 - val_accuracy: 0.9053\n",
      "Epoch 138/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.4287 - accuracy: 0.9357 - val_loss: 0.5028 - val_accuracy: 0.9053\n",
      "Epoch 139/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.4243 - accuracy: 0.9357 - val_loss: 0.4996 - val_accuracy: 0.9053\n",
      "Epoch 140/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.4198 - accuracy: 0.9357 - val_loss: 0.4963 - val_accuracy: 0.9053\n",
      "Epoch 141/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.4154 - accuracy: 0.9357 - val_loss: 0.4930 - val_accuracy: 0.9053\n",
      "Epoch 142/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.4109 - accuracy: 0.9357 - val_loss: 0.4897 - val_accuracy: 0.9053\n",
      "Epoch 143/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.4063 - accuracy: 0.9357 - val_loss: 0.4864 - val_accuracy: 0.9053\n",
      "Epoch 144/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.4018 - accuracy: 0.9357 - val_loss: 0.4831 - val_accuracy: 0.9053\n",
      "Epoch 145/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.3973 - accuracy: 0.9357 - val_loss: 0.4798 - val_accuracy: 0.9053\n",
      "Epoch 146/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.3927 - accuracy: 0.9357 - val_loss: 0.4765 - val_accuracy: 0.9053\n",
      "Epoch 147/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.3882 - accuracy: 0.9357 - val_loss: 0.4732 - val_accuracy: 0.9053\n",
      "Epoch 148/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.3837 - accuracy: 0.9357 - val_loss: 0.4699 - val_accuracy: 0.9053\n",
      "Epoch 149/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.3792 - accuracy: 0.9357 - val_loss: 0.4666 - val_accuracy: 0.9053\n",
      "Epoch 150/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.3747 - accuracy: 0.9357 - val_loss: 0.4634 - val_accuracy: 0.9053\n",
      "Epoch 151/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.3702 - accuracy: 0.9357 - val_loss: 0.4601 - val_accuracy: 0.9053\n",
      "Epoch 152/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.3658 - accuracy: 0.9357 - val_loss: 0.4569 - val_accuracy: 0.9053\n",
      "Epoch 153/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.3615 - accuracy: 0.9357 - val_loss: 0.4538 - val_accuracy: 0.9053\n",
      "Epoch 154/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.3571 - accuracy: 0.9357 - val_loss: 0.4506 - val_accuracy: 0.9053\n",
      "Epoch 155/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.3528 - accuracy: 0.9357 - val_loss: 0.4475 - val_accuracy: 0.9053\n",
      "Epoch 156/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.3486 - accuracy: 0.9357 - val_loss: 0.4445 - val_accuracy: 0.9053\n",
      "Epoch 157/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.3444 - accuracy: 0.9357 - val_loss: 0.4415 - val_accuracy: 0.9053\n",
      "Epoch 158/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.3403 - accuracy: 0.9357 - val_loss: 0.4385 - val_accuracy: 0.9053\n",
      "Epoch 159/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.3363 - accuracy: 0.9357 - val_loss: 0.4356 - val_accuracy: 0.9053\n",
      "Epoch 160/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.3323 - accuracy: 0.9357 - val_loss: 0.4327 - val_accuracy: 0.9053\n",
      "Epoch 161/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.3284 - accuracy: 0.9357 - val_loss: 0.4299 - val_accuracy: 0.9053\n",
      "Epoch 162/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.3246 - accuracy: 0.9357 - val_loss: 0.4272 - val_accuracy: 0.9053\n",
      "Epoch 163/200\n",
      "18/18 [==============================] - 0s 108us/step - loss: 0.3208 - accuracy: 0.9357 - val_loss: 0.4245 - val_accuracy: 0.9053\n",
      "Epoch 164/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.3171 - accuracy: 0.9357 - val_loss: 0.4219 - val_accuracy: 0.9053\n",
      "Epoch 165/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.3136 - accuracy: 0.9357 - val_loss: 0.4194 - val_accuracy: 0.9053\n",
      "Epoch 166/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.3101 - accuracy: 0.9357 - val_loss: 0.4169 - val_accuracy: 0.9053\n",
      "Epoch 167/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.3067 - accuracy: 0.9357 - val_loss: 0.4145 - val_accuracy: 0.9053\n",
      "Epoch 168/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.3034 - accuracy: 0.9357 - val_loss: 0.4122 - val_accuracy: 0.9053\n",
      "Epoch 169/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.3002 - accuracy: 0.9357 - val_loss: 0.4099 - val_accuracy: 0.9053\n",
      "Epoch 170/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.2970 - accuracy: 0.9357 - val_loss: 0.4077 - val_accuracy: 0.9053\n",
      "Epoch 171/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2940 - accuracy: 0.9357 - val_loss: 0.4056 - val_accuracy: 0.9053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.2911 - accuracy: 0.9357 - val_loss: 0.4036 - val_accuracy: 0.9053\n",
      "Epoch 173/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2882 - accuracy: 0.9357 - val_loss: 0.4016 - val_accuracy: 0.9053\n",
      "Epoch 174/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2855 - accuracy: 0.9357 - val_loss: 0.3997 - val_accuracy: 0.9053\n",
      "Epoch 175/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2829 - accuracy: 0.9357 - val_loss: 0.3979 - val_accuracy: 0.9053\n",
      "Epoch 176/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2803 - accuracy: 0.9357 - val_loss: 0.3961 - val_accuracy: 0.9053\n",
      "Epoch 177/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2779 - accuracy: 0.9357 - val_loss: 0.3944 - val_accuracy: 0.9053\n",
      "Epoch 178/200\n",
      "18/18 [==============================] - 0s 109us/step - loss: 0.2755 - accuracy: 0.9357 - val_loss: 0.3928 - val_accuracy: 0.9053\n",
      "Epoch 179/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.2732 - accuracy: 0.9357 - val_loss: 0.3913 - val_accuracy: 0.9053\n",
      "Epoch 180/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2710 - accuracy: 0.9357 - val_loss: 0.3898 - val_accuracy: 0.9053\n",
      "Epoch 181/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2689 - accuracy: 0.9357 - val_loss: 0.3884 - val_accuracy: 0.9053\n",
      "Epoch 182/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2669 - accuracy: 0.9357 - val_loss: 0.3871 - val_accuracy: 0.9053\n",
      "Epoch 183/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.2650 - accuracy: 0.9357 - val_loss: 0.3858 - val_accuracy: 0.9053\n",
      "Epoch 184/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2632 - accuracy: 0.9357 - val_loss: 0.3846 - val_accuracy: 0.9053\n",
      "Epoch 185/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2614 - accuracy: 0.9357 - val_loss: 0.3835 - val_accuracy: 0.9053\n",
      "Epoch 186/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2597 - accuracy: 0.9357 - val_loss: 0.3824 - val_accuracy: 0.9053\n",
      "Epoch 187/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2581 - accuracy: 0.9357 - val_loss: 0.3814 - val_accuracy: 0.9053\n",
      "Epoch 188/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2566 - accuracy: 0.9357 - val_loss: 0.3804 - val_accuracy: 0.9053\n",
      "Epoch 189/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.2551 - accuracy: 0.9357 - val_loss: 0.3795 - val_accuracy: 0.9053\n",
      "Epoch 190/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2537 - accuracy: 0.9357 - val_loss: 0.3786 - val_accuracy: 0.9053\n",
      "Epoch 191/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2524 - accuracy: 0.9357 - val_loss: 0.3778 - val_accuracy: 0.9053\n",
      "Epoch 192/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.2511 - accuracy: 0.9357 - val_loss: 0.3771 - val_accuracy: 0.9053\n",
      "Epoch 193/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2499 - accuracy: 0.9357 - val_loss: 0.3763 - val_accuracy: 0.9053\n",
      "Epoch 194/200\n",
      "18/18 [==============================] - 0s 108us/step - loss: 0.2487 - accuracy: 0.9357 - val_loss: 0.3757 - val_accuracy: 0.9053\n",
      "Epoch 195/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2476 - accuracy: 0.9357 - val_loss: 0.3750 - val_accuracy: 0.9053\n",
      "Epoch 196/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2466 - accuracy: 0.9357 - val_loss: 0.3744 - val_accuracy: 0.9053\n",
      "Epoch 197/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2456 - accuracy: 0.9357 - val_loss: 0.3739 - val_accuracy: 0.9053\n",
      "Epoch 198/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.2446 - accuracy: 0.9357 - val_loss: 0.3734 - val_accuracy: 0.9053\n",
      "Epoch 199/200\n",
      "18/18 [==============================] - 0s 163us/step - loss: 0.2437 - accuracy: 0.9357 - val_loss: 0.3729 - val_accuracy: 0.9053\n",
      "Epoch 200/200\n",
      "18/18 [==============================] - 0s 217us/step - loss: 0.2429 - accuracy: 0.9357 - val_loss: 0.3725 - val_accuracy: 0.9053\n",
      "[4, 0]\n",
      "[1, 0]\n",
      "[4, 0]\n",
      "[1, 0]\n",
      "[0, 0]\n",
      "[5, 0]\n",
      "[0, 0]\n",
      "[5, 0]\n",
      "[0, 0]\n",
      "[5, 0]\n",
      "[0, 0]\n",
      "[5, 0]\n",
      "[0, 0]\n",
      "[5, 0]\n",
      "[0, 0]\n",
      "[5, 0]\n",
      "[0, 0]\n",
      "[4, 1]\n",
      "[0, 0]\n",
      "[4, 1]\n",
      "[3, 0]\n",
      "[2, 0]\n",
      "[3, 0]\n",
      "[1, 1]\n",
      "[4, 0]\n",
      "[1, 0]\n",
      "[4, 0]\n",
      "[1, 0]\n",
      "[2, 0]\n",
      "[2, 1]\n",
      "[2, 0]\n",
      "[2, 1]\n",
      "[4, 0]\n",
      "[1, 0]\n",
      "[3, 1]\n",
      "[1, 0]\n",
      "[0, 0]\n",
      "[5, 0]\n",
      "[0, 0]\n",
      "[5, 0]\n",
      "[1, 0]\n",
      "[4, 0]\n",
      "[0, 1]\n",
      "[4, 0]\n",
      "[0, 0]\n",
      "[5, 0]\n",
      "[0, 0]\n",
      "[4, 1]\n",
      "[0, 0]\n",
      "[5, 0]\n",
      "[0, 0]\n",
      "[5, 0]\n",
      "[0, 0]\n",
      "[5, 0]\n",
      "[0, 0]\n",
      "[5, 0]\n",
      "[4, 0]\n",
      "[1, 0]\n",
      "[4, 0]\n",
      "[1, 0]\n",
      "[3, 0]\n",
      "[2, 0]\n",
      "[3, 0]\n",
      "[1, 1]\n",
      "[3, 0]\n",
      "[2, 0]\n",
      "[3, 0]\n",
      "[2, 0]\n",
      "[4, 0]\n",
      "[1, 0]\n",
      "[3, 1]\n",
      "[1, 0]\n",
      "[0, 0]\n",
      "[5, 0]\n",
      "[0, 0]\n",
      "[5, 0]\n",
      "------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds of MWPM:\n",
      "> Accuracy: 0.9445614035087718 (+- 0.019173440129563922)\n",
      "> F1: 0.6328822055137845(+- 0.11809181267398161)\n",
      "> Time: 0.057779239999999996 (+- 0.012258319190916835)\n",
      "#####################################################################################\n",
      "Average scores for all folds of PLUT:\n",
      "> Accuracy: 0.9147368421052631 (+- 0.01161610200508548)\n",
      "> F1: 0.03636363636363636(+- 0.07272727272727272)\n",
      "> Time: 0.0 (+- 0.0)\n",
      "#####################################################################################\n",
      "Average scores for all folds of NN:\n",
      "> Accuracy: 0.8098245591866343 (+- 0.23680597463032543)\n",
      "> F1: 0.10283918979571154(+- 0.0611021301159481)\n",
      "> Time: 0.04157722 (+- 0.0007811601715397427)\n",
      "#####################################################################################\n",
      "> AUC for class : 0.0 (+- 0.0)\n",
      "X^2 for MWPM and NN: 4.0\n",
      "X^2 for PLUT and NN: 4.0\n",
      "> AUC for class X00: 0.15 (+- 0.15)\n",
      "X^2 for MWPM and NN: 7.2\n",
      "X^2 for PLUT and NN: 7.2\n",
      "> AUC for class X01: 0.6000000000000001 (+- 0.2)\n",
      "X^2 for MWPM and NN: 7.2\n",
      "X^2 for PLUT and NN: 7.2\n",
      "> AUC for class X02: 0.75 (+- 0.25)\n",
      "X^2 for MWPM and NN: 7.2\n",
      "X^2 for PLUT and NN: 7.2\n",
      "> AUC for class X10: 0.25 (+- 0.25)\n",
      "X^2 for MWPM and NN: 6.25\n",
      "X^2 for PLUT and NN: 6.25\n",
      "> AUC for class X11: 0.875 (+- 0.125)\n",
      "X^2 for MWPM and NN: 4.5\n",
      "X^2 for PLUT and NN: 4.0\n",
      "> AUC for class X12: 0.625 (+- 0.375)\n",
      "X^2 for MWPM and NN: 4.0\n",
      "X^2 for PLUT and NN: 4.0\n",
      "> AUC for class X20: 0.25 (+- 0.0)\n",
      "X^2 for MWPM and NN: 4.5\n",
      "X^2 for PLUT and NN: 4.5\n",
      "> AUC for class X21: 0.7 (+- 0.3)\n",
      "X^2 for MWPM and NN: 4.0\n",
      "X^2 for PLUT and NN: 0.5\n",
      "> AUC for class X22: 0.9 (+- 0.09999999999999998)\n",
      "X^2 for MWPM and NN: 7.2\n",
      "X^2 for PLUT and NN: 7.2\n",
      "> AUC for class Z00: 0.25 (+- 0.25)\n",
      "X^2 for MWPM and NN: 6.25\n",
      "X^2 for PLUT and NN: 3.2\n",
      "> AUC for class Z01: 0.55 (+- 0.25)\n",
      "X^2 for MWPM and NN: 7.2\n",
      "X^2 for PLUT and NN: 6.25\n",
      "> AUC for class Z02: 0.675 (+- 0.07500000000000001)\n",
      "X^2 for MWPM and NN: 7.2\n",
      "X^2 for PLUT and NN: 7.2\n",
      "> AUC for class Z10: 0.25 (+- 0.25)\n",
      "X^2 for MWPM and NN: 7.2\n",
      "X^2 for PLUT and NN: 7.2\n",
      "> AUC for class Z11: 0.8 (+- 0.2)\n",
      "X^2 for MWPM and NN: 4.0\n",
      "X^2 for PLUT and NN: 4.0\n",
      "> AUC for class Z12: 0.625 (+- 0.375)\n",
      "X^2 for MWPM and NN: 4.5\n",
      "X^2 for PLUT and NN: 4.0\n",
      "> AUC for class Z20: 0.5 (+- 0.5)\n",
      "X^2 for MWPM and NN: 4.5\n",
      "X^2 for PLUT and NN: 4.5\n",
      "> AUC for class Z21: 0.85 (+- 0.15000000000000002)\n",
      "X^2 for MWPM and NN: 4.0\n",
      "X^2 for PLUT and NN: 0.5\n",
      "> AUC for class Z22: 0.55 (+- 0.04999999999999999)\n",
      "X^2 for MWPM and NN: 7.2\n",
      "X^2 for PLUT and NN: 7.2\n",
      "###################################################################################\n",
      "TOTAL F1 NN: [0.0, 0.18181818181818182, 0.10256410256410256, 0.14285714285714288, 0.08695652173913042]\n",
      "TOTAL F1 PLUT: [0.18181818181818182, 0.0, 0.0, 0.0, 0.0]\n",
      "TOTAL F1 MWPM: [0.5333333333333333, 0.5714285714285714, 0.5263157894736842, 0.7000000000000001, 0.8333333333333334]\n",
      "TOTAL ACC NN: [0.9385964870452881, 0.9385964870452881, 0.9298245906829834, 0.9052631258964539, 0.3368421052631579]\n",
      "TOTAL ACC PLUT: [0.9210526315789473, 0.9298245614035087, 0.9122807017543858, 0.8947368421052632, 0.9157894736842105]\n",
      "TOTAL ACC MWPM: [0.9385964912280701, 0.9473684210526315, 0.9210526315789473, 0.9368421052631579, 0.9789473684210528]\n",
      "TOTAL TIME NN: [0.0419687, 0.0419678, 0.0419671, 0.0419676, 0.0400149]\n",
      "TOTAL TIME PLUT: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "TOTAL TIME MWPM: [0.05563269999999999, 0.0566077, 0.081007, 0.049777, 0.045871800000000004]\n",
      "------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clair\\Anaconda3\\envs\\CNNs\\lib\\site-packages\\ipykernel_launcher.py:128: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "C:\\Users\\clair\\Anaconda3\\envs\\CNNs\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:808: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "inputs = x_d3.copy()\n",
    "inputs = np.array(inputs)\n",
    "inputs = np.stack(inputs)\n",
    "targets = y_d3.copy()\n",
    "targets = np.array([np.array(xi) for xi in targets])\n",
    "\n",
    "n_classes = 19\n",
    "\n",
    "acc_per_fold = []\n",
    "f1_per_fold = []\n",
    "time_per_fold = []\n",
    "\n",
    "\n",
    "acc_per_fold_plut = []\n",
    "f1_per_fold_plut = []\n",
    "time_per_fold_plut = []\n",
    "\n",
    "acc_per_fold_mwpm = []\n",
    "f1_per_fold_mwpm = []\n",
    "time_per_fold_mwpm = []\n",
    "\n",
    "\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "aucs_classes = {}\n",
    "for i in mlb_d3.classes_:\n",
    "    aucs_classes[i] = []\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "#for i in range(5):\n",
    "    \n",
    "    # K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "for i, (train, test) in enumerate(kfold.split(inputs, targets)):\n",
    "    i_train = inputs[train].copy()\n",
    "    i_test = inputs[test].copy()\n",
    "    x_test_d3 = i_test[:,:2]\n",
    "    inputs_train = i_train[:,3:]\n",
    "    inputs_test = i_test[:,3:]\n",
    "    ####################################################################################################\n",
    "    #test MWPM decoder for this fold\n",
    "    #labels = targets[train], features = inputs[train]\n",
    "    #x_test_d3 = translate_to_graph(testData_d3_MWPM, targets[test], mlb_d3)\n",
    "    decoding_d3, time_mwpm = do_new_decoding(x_test_d3, 3, .03)\n",
    "    decoding_d3['combine'] = decoding_d3[[0, 1]].values.tolist()\n",
    "    decoding_d3['combine'].apply(lambda x: x[0].extend(x[1]))\n",
    "    decoding_d3 = np.array(decoding_d3[0])\n",
    "\n",
    "    time_per_fold_mwpm.append(time_mwpm)\n",
    "\n",
    "    pred_mwpm = mlb_d3.transform(decoding_d3)\n",
    "    if fold_no < 5:\n",
    "        acc = partial_accuracy(targets[test], pred_mwpm)\n",
    "    else:\n",
    "        acc, contingency_mwpm = partial_accuracy_and_contingency(targets[test], pred_mwpm, mlb_d3)\n",
    "        \n",
    "    acc_per_fold_mwpm.append(acc)\n",
    "    f1_per_fold_mwpm.append(f1_score(targets[test], pred_mwpm, average='micro'))\n",
    "\n",
    "\n",
    "    #####################################################################################################\n",
    "    #test the plut decoder for this fold\n",
    "\n",
    "    lookup_d3 = lookup_decoder(3)\n",
    "\n",
    "    lookup_d3 = train_plut(lookup_d3, inputs_train, targets[train])\n",
    "\n",
    "    start = time.time_ns()\n",
    "    pred_plut_d3 = test_plut(lookup_d3, inputs_test)\n",
    "    end = time.time_ns() \n",
    "    time_per_fold_plut.append((end - start)/ (10 ** 9))\n",
    "    \n",
    "    if fold_no < 5:\n",
    "        acc = partial_accuracy(targets[test], pred_plut_d3)\n",
    "    else:\n",
    "        acc, contingency_plut = partial_accuracy_and_contingency(targets[test], pred_plut_d3, mlb_d3)\n",
    "\n",
    "    acc_per_fold_plut.append(acc)\n",
    "    f1_per_fold_plut.append(f1_score(targets[test], pred_plut_d3, average='micro'))\n",
    "\n",
    "    #####################################################################################################\n",
    "    #Test the NN decoder for this fold\n",
    "\n",
    "    model = compile_FFNN_model_DepthThree(3)\n",
    "    # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    # Fit data to model\n",
    "\n",
    "    history = model.fit(\n",
    "        inputs_train, targets[train],\n",
    "        validation_split=.2,\n",
    "        epochs=200,\n",
    "        verbose=1)\n",
    "\n",
    "   # Generate generalization metrics\n",
    "    scores = model.evaluate(inputs_test, targets[test], verbose=0)\n",
    "\n",
    "    #get the time to predicting test\n",
    "    start = time.time_ns()\n",
    "    predictions_d3 = model.predict(inputs_test) #change here\n",
    "    end = time.time_ns()\n",
    "    time_per_fold.append((end - start)/ (10 ** 9))\n",
    "\n",
    "    #threshold based on previous tests with train_test_split in hyperparameter tuning\n",
    "    pred=predictions_d3.copy() #change here\n",
    "    pred[pred>=.1]=1 \n",
    "    pred[pred<.1]=0\n",
    "    \n",
    "    if fold_no <5:\n",
    "        acc = scores[1]\n",
    "    else:\n",
    "        acc, contingency_nn = partial_accuracy_and_contingency(targets[test], pred, mlb_d3)\n",
    "\n",
    "    acc_per_fold.append(acc)\n",
    "    f1_per_fold.append(f1_score(targets[test], pred, average='micro'))\n",
    "\n",
    "    #####################################################################################################\n",
    "    #comput ROC AUC for classes and the mircoaverage\n",
    "    fpr, tpr, thresholds = roc_curve(targets[test].ravel(), predictions_d3.ravel())\n",
    "    tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "    tprs[-1][0] = 0.0\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "    plt.plot(fpr, tpr, lw=1, alpha=0.3, label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
    "\n",
    "    #get the AUCs of each class, used to get average AUC of each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(targets[test][:, i], predictions_d3[:, i]) #change here\n",
    "        aucs_classes[mlb_d3.classes_[i]].append(auc(fpr[i], tpr[i]))\n",
    "        \n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n",
    "        \n",
    "#########################################################################################################\n",
    "#compute McNemar's statistic on results of last fold\n",
    "\n",
    "mcnemar_results_mwpm = {}\n",
    "mcnemar_results_plut = {}\n",
    "\n",
    "for class_ in mlb_d3.classes_:\n",
    "    \n",
    "    #compute the x^2 for NN and MWPM\n",
    "    mcnemar_results_mwpm[class_]=contingency_table_and_t(contingency_nn[class_], contingency_mwpm[class_])[1]    \n",
    "    #comput the x^2 for NN and PLUT\n",
    "    mcnemar_results_plut[class_] = contingency_table_and_t(contingency_nn[class_], contingency_plut[class_])[1]\n",
    "        \n",
    "    \n",
    "############print mean and stdev of AUC of each class#####################      \n",
    "    \n",
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds of MWPM:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold_mwpm)} (+- {np.std(acc_per_fold_mwpm)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold_mwpm)}(+- {np.std(f1_per_fold_mwpm)})')\n",
    "print(f'> Time: {np.mean(time_per_fold_mwpm)} (+- {np.std(time_per_fold_mwpm)})')\n",
    "print(\"#####################################################################################\")\n",
    "print('Average scores for all folds of PLUT:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold_plut)} (+- {np.std(acc_per_fold_plut)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold_plut)}(+- {np.std(f1_per_fold_plut)})')\n",
    "print(f'> Time: {np.mean(time_per_fold_plut)} (+- {np.std(time_per_fold_plut)})')\n",
    "print(\"#####################################################################################\")\n",
    "print('Average scores for all folds of NN:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold)}(+- {np.std(f1_per_fold)})')\n",
    "print(f'> Time: {np.mean(time_per_fold)} (+- {np.std(time_per_fold)})')\n",
    "print(\"#####################################################################################\")\n",
    "for key in aucs_classes:\n",
    "    cleanedList = [x for x in aucs_classes[key] if str(x) != 'nan']\n",
    "    print(f'> AUC for class {key}: {np.mean(cleanedList)} (+- {np.std(cleanedList)})') #this has to be a for loop\n",
    "    print(\"X^2 for MWPM and NN: \" + str(mcnemar_results_mwpm[key]))  \n",
    "    print(\"X^2 for PLUT and NN: \" + str(mcnemar_results_plut[key]))\n",
    "print(\"###################################################################################\")\n",
    "print(\"TOTAL F1 NN: \" + str(f1_per_fold))\n",
    "print(\"TOTAL F1 PLUT: \" + str(f1_per_fold_plut))\n",
    "print(\"TOTAL F1 MWPM: \" + str(f1_per_fold_mwpm))\n",
    "print(\"TOTAL ACC NN: \" + str(acc_per_fold))\n",
    "print(\"TOTAL ACC PLUT: \" + str(acc_per_fold_plut))\n",
    "print(\"TOTAL ACC MWPM: \" + str(acc_per_fold_mwpm))\n",
    "print(\"TOTAL TIME NN: \" + str(time_per_fold))\n",
    "print(\"TOTAL TIME PLUT: \" + str(time_per_fold_plut))\n",
    "print(\"TOTAL TIME MWPM: \" + str(time_per_fold_mwpm))\n",
    "print('------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "         label='Chance', alpha=.8)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "         label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "         lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                 label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "plt.xlim([-0.01, 1.01])\n",
    "plt.ylim([-0.01, 1.01])\n",
    "plt.xlabel('False Positive Rate',fontsize=18)\n",
    "plt.ylabel('True Positive Rate',fontsize=18)\n",
    "plt.title('Cross-Validation ROC of Depth 3 NN',fontsize=18)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "\n",
    "model = compile_FFNN_model_DepthThree(3)\n",
    "print(\"Fit model on training data\")\n",
    "history = model.fit(\n",
    "    x=x_train_d3.values,\n",
    "    y=Y_train_d3,\n",
    "    validation_split=.25,\n",
    "    epochs=200\n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Make a quick epoch vs. loss plot\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.grid()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Depth 3 Model Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (MSE)')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Make a quick epoch vs. accuracy plot\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.grid()\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Depth 3 Model Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "start = time.time()\n",
    "predictions_d3 = model.predict(x_test_d3.values)\n",
    "end = time.time()\n",
    "print(\"Time: \" + str(end - start))\n",
    "# predict\n",
    "\n",
    "thresholds=[0.1, .2, .3, .4, .5, .6, .7, .8, .9]\n",
    "for val in thresholds:\n",
    "    pred=predictions_d3.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "\n",
    "    precision = precision_score(Y_test_d3, pred, average='micro')\n",
    "    recall = recall_score(Y_test_d3, pred, average='micro')\n",
    "    f1 = f1_score(Y_test_d3, pred, average='micro')\n",
    "   \n",
    "    print(\"Micro-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "    print(\"Accuracy = \",accuracy_score(Y_test_d3, pred))\n",
    "    print(\"Partial Accuracy = \",partial_accuracy(Y_test_d3, pred))\n",
    "    print(\"\\n\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Train on 1665 samples, validate on 555 samples\n",
      "Epoch 1/800\n",
      "1665/1665 [==============================] - 0s 185us/step - loss: 0.6456 - accuracy: 0.7532 - val_loss: 0.5748 - val_accuracy: 0.8553\n",
      "Epoch 2/800\n",
      "1665/1665 [==============================] - 0s 96us/step - loss: 0.3547 - accuracy: 0.9357 - val_loss: 0.2544 - val_accuracy: 0.9327\n",
      "Epoch 3/800\n",
      "1665/1665 [==============================] - 0s 102us/step - loss: 0.1926 - accuracy: 0.9534 - val_loss: 0.2502 - val_accuracy: 0.9327\n",
      "Epoch 4/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.1890 - accuracy: 0.9534 - val_loss: 0.2503 - val_accuracy: 0.9327\n",
      "Epoch 5/800\n",
      "1665/1665 [==============================] - 0s 112us/step - loss: 0.1882 - accuracy: 0.9534 - val_loss: 0.2496 - val_accuracy: 0.9327\n",
      "Epoch 6/800\n",
      "1665/1665 [==============================] - 0s 112us/step - loss: 0.1879 - accuracy: 0.9534 - val_loss: 0.2495 - val_accuracy: 0.9327\n",
      "Epoch 7/800\n",
      "1665/1665 [==============================] - 0s 107us/step - loss: 0.1878 - accuracy: 0.9534 - val_loss: 0.2496 - val_accuracy: 0.9327\n",
      "Epoch 8/800\n",
      "1665/1665 [==============================] - 0s 106us/step - loss: 0.1876 - accuracy: 0.9534 - val_loss: 0.2495 - val_accuracy: 0.9327\n",
      "Epoch 9/800\n",
      "1665/1665 [==============================] - 0s 107us/step - loss: 0.1875 - accuracy: 0.9534 - val_loss: 0.2496 - val_accuracy: 0.9327\n",
      "Epoch 10/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.1874 - accuracy: 0.9534 - val_loss: 0.2501 - val_accuracy: 0.9327\n",
      "Epoch 11/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.1873 - accuracy: 0.9534 - val_loss: 0.2493 - val_accuracy: 0.9327\n",
      "Epoch 12/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.1872 - accuracy: 0.9534 - val_loss: 0.2489 - val_accuracy: 0.9327\n",
      "Epoch 13/800\n",
      "1665/1665 [==============================] - 0s 107us/step - loss: 0.1871 - accuracy: 0.9534 - val_loss: 0.2490 - val_accuracy: 0.9327\n",
      "Epoch 14/800\n",
      "1665/1665 [==============================] - 0s 106us/step - loss: 0.1870 - accuracy: 0.9534 - val_loss: 0.2492 - val_accuracy: 0.9327\n",
      "Epoch 15/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.1869 - accuracy: 0.9534 - val_loss: 0.2485 - val_accuracy: 0.9327\n",
      "Epoch 16/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.1868 - accuracy: 0.9534 - val_loss: 0.2490 - val_accuracy: 0.9327\n",
      "Epoch 17/800\n",
      "1665/1665 [==============================] - 0s 109us/step - loss: 0.1867 - accuracy: 0.9534 - val_loss: 0.2483 - val_accuracy: 0.9327\n",
      "Epoch 18/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.1866 - accuracy: 0.9534 - val_loss: 0.2481 - val_accuracy: 0.9327\n",
      "Epoch 19/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.1866 - accuracy: 0.9534 - val_loss: 0.2476 - val_accuracy: 0.9327\n",
      "Epoch 20/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.1865 - accuracy: 0.9534 - val_loss: 0.2476 - val_accuracy: 0.9327\n",
      "Epoch 21/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.1863 - accuracy: 0.9534 - val_loss: 0.2474 - val_accuracy: 0.9327\n",
      "Epoch 22/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.1862 - accuracy: 0.9534 - val_loss: 0.2477 - val_accuracy: 0.9327\n",
      "Epoch 23/800\n",
      "1665/1665 [==============================] - 0s 133us/step - loss: 0.1861 - accuracy: 0.9534 - val_loss: 0.2470 - val_accuracy: 0.9327\n",
      "Epoch 24/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.1860 - accuracy: 0.9534 - val_loss: 0.2472 - val_accuracy: 0.9327\n",
      "Epoch 25/800\n",
      "1665/1665 [==============================] - 0s 139us/step - loss: 0.1859 - accuracy: 0.9534 - val_loss: 0.2471 - val_accuracy: 0.9327\n",
      "Epoch 26/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.1858 - accuracy: 0.9534 - val_loss: 0.2471 - val_accuracy: 0.9327\n",
      "Epoch 27/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1857 - accuracy: 0.9534 - val_loss: 0.2463 - val_accuracy: 0.9327\n",
      "Epoch 28/800\n",
      "1665/1665 [==============================] - 0s 112us/step - loss: 0.1856 - accuracy: 0.9534 - val_loss: 0.2463 - val_accuracy: 0.9327\n",
      "Epoch 29/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.1855 - accuracy: 0.9534 - val_loss: 0.2463 - val_accuracy: 0.9327\n",
      "Epoch 30/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.1854 - accuracy: 0.9534 - val_loss: 0.2461 - val_accuracy: 0.9327\n",
      "Epoch 31/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.1854 - accuracy: 0.9534 - val_loss: 0.2465 - val_accuracy: 0.9327\n",
      "Epoch 32/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.1853 - accuracy: 0.9534 - val_loss: 0.2468 - val_accuracy: 0.9327\n",
      "Epoch 33/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.1851 - accuracy: 0.9534 - val_loss: 0.2470 - val_accuracy: 0.9327\n",
      "Epoch 34/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.1850 - accuracy: 0.9534 - val_loss: 0.2463 - val_accuracy: 0.9327\n",
      "Epoch 35/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.1849 - accuracy: 0.9534 - val_loss: 0.2459 - val_accuracy: 0.9327\n",
      "Epoch 36/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.1848 - accuracy: 0.9534 - val_loss: 0.2458 - val_accuracy: 0.9327\n",
      "Epoch 37/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.1847 - accuracy: 0.9534 - val_loss: 0.2450 - val_accuracy: 0.9327\n",
      "Epoch 38/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.1846 - accuracy: 0.9534 - val_loss: 0.2448 - val_accuracy: 0.9327\n",
      "Epoch 39/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.1845 - accuracy: 0.9534 - val_loss: 0.2447 - val_accuracy: 0.9327\n",
      "Epoch 40/800\n",
      "1665/1665 [==============================] - 0s 136us/step - loss: 0.1844 - accuracy: 0.9534 - val_loss: 0.2448 - val_accuracy: 0.9327\n",
      "Epoch 41/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.1842 - accuracy: 0.9534 - val_loss: 0.2447 - val_accuracy: 0.9327\n",
      "Epoch 42/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.1841 - accuracy: 0.9534 - val_loss: 0.2441 - val_accuracy: 0.9327\n",
      "Epoch 43/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.1840 - accuracy: 0.9534 - val_loss: 0.2443 - val_accuracy: 0.9327\n",
      "Epoch 44/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.1838 - accuracy: 0.9534 - val_loss: 0.2439 - val_accuracy: 0.9327\n",
      "Epoch 45/800\n",
      "1665/1665 [==============================] - 0s 139us/step - loss: 0.1837 - accuracy: 0.9534 - val_loss: 0.2437 - val_accuracy: 0.9327\n",
      "Epoch 46/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.1836 - accuracy: 0.9534 - val_loss: 0.2443 - val_accuracy: 0.9327\n",
      "Epoch 47/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.1835 - accuracy: 0.9534 - val_loss: 0.2434 - val_accuracy: 0.9327\n",
      "Epoch 48/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1834 - accuracy: 0.9534 - val_loss: 0.2429 - val_accuracy: 0.9327\n",
      "Epoch 49/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.1832 - accuracy: 0.9534 - val_loss: 0.2429 - val_accuracy: 0.9327\n",
      "Epoch 50/800\n",
      "1665/1665 [==============================] - 0s 107us/step - loss: 0.1830 - accuracy: 0.9534 - val_loss: 0.2423 - val_accuracy: 0.9327\n",
      "Epoch 51/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.1828 - accuracy: 0.9534 - val_loss: 0.2422 - val_accuracy: 0.9327\n",
      "Epoch 52/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.1827 - accuracy: 0.9534 - val_loss: 0.2422 - val_accuracy: 0.9327\n",
      "Epoch 53/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.1826 - accuracy: 0.9534 - val_loss: 0.2418 - val_accuracy: 0.9327\n",
      "Epoch 54/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.1824 - accuracy: 0.9534 - val_loss: 0.2413 - val_accuracy: 0.9327\n",
      "Epoch 55/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.1822 - accuracy: 0.9534 - val_loss: 0.2420 - val_accuracy: 0.9327\n",
      "Epoch 56/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.1820 - accuracy: 0.9534 - val_loss: 0.2412 - val_accuracy: 0.9327\n",
      "Epoch 57/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.1818 - accuracy: 0.9534 - val_loss: 0.2406 - val_accuracy: 0.9327\n",
      "Epoch 58/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.1816 - accuracy: 0.9534 - val_loss: 0.2408 - val_accuracy: 0.9327\n",
      "Epoch 59/800\n",
      "1665/1665 [==============================] - 0s 129us/step - loss: 0.1814 - accuracy: 0.9534 - val_loss: 0.2404 - val_accuracy: 0.9327\n",
      "Epoch 60/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.1812 - accuracy: 0.9534 - val_loss: 0.2402 - val_accuracy: 0.9327\n",
      "Epoch 61/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.1810 - accuracy: 0.9534 - val_loss: 0.2393 - val_accuracy: 0.9327\n",
      "Epoch 62/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.1807 - accuracy: 0.9534 - val_loss: 0.2395 - val_accuracy: 0.9327\n",
      "Epoch 63/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.1805 - accuracy: 0.9534 - val_loss: 0.2393 - val_accuracy: 0.9327\n",
      "Epoch 64/800\n",
      "1665/1665 [==============================] - 0s 106us/step - loss: 0.1802 - accuracy: 0.9534 - val_loss: 0.2391 - val_accuracy: 0.9327\n",
      "Epoch 65/800\n",
      "1665/1665 [==============================] - 0s 103us/step - loss: 0.1800 - accuracy: 0.9534 - val_loss: 0.2382 - val_accuracy: 0.9327\n",
      "Epoch 66/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.1798 - accuracy: 0.9534 - val_loss: 0.2383 - val_accuracy: 0.9327\n",
      "Epoch 67/800\n",
      "1665/1665 [==============================] - 0s 109us/step - loss: 0.1795 - accuracy: 0.9534 - val_loss: 0.2373 - val_accuracy: 0.9327\n",
      "Epoch 68/800\n",
      "1665/1665 [==============================] - 0s 106us/step - loss: 0.1792 - accuracy: 0.9534 - val_loss: 0.2370 - val_accuracy: 0.9327\n",
      "Epoch 69/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.1789 - accuracy: 0.9534 - val_loss: 0.2364 - val_accuracy: 0.9327\n",
      "Epoch 70/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.1786 - accuracy: 0.9534 - val_loss: 0.2359 - val_accuracy: 0.9327\n",
      "Epoch 71/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.1783 - accuracy: 0.9534 - val_loss: 0.2354 - val_accuracy: 0.9327\n",
      "Epoch 72/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.1780 - accuracy: 0.9534 - val_loss: 0.2353 - val_accuracy: 0.9327\n",
      "Epoch 73/800\n",
      "1665/1665 [==============================] - 0s 107us/step - loss: 0.1776 - accuracy: 0.9534 - val_loss: 0.2338 - val_accuracy: 0.9327\n",
      "Epoch 74/800\n",
      "1665/1665 [==============================] - 0s 105us/step - loss: 0.1773 - accuracy: 0.9534 - val_loss: 0.2337 - val_accuracy: 0.9327\n",
      "Epoch 75/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.1769 - accuracy: 0.9534 - val_loss: 0.2339 - val_accuracy: 0.9327\n",
      "Epoch 76/800\n",
      "1665/1665 [==============================] - 0s 105us/step - loss: 0.1765 - accuracy: 0.9534 - val_loss: 0.2324 - val_accuracy: 0.9327\n",
      "Epoch 77/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.1761 - accuracy: 0.9534 - val_loss: 0.2318 - val_accuracy: 0.9327\n",
      "Epoch 78/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.1757 - accuracy: 0.9534 - val_loss: 0.2315 - val_accuracy: 0.9327\n",
      "Epoch 79/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.1752 - accuracy: 0.9534 - val_loss: 0.2310 - val_accuracy: 0.9327\n",
      "Epoch 80/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.1748 - accuracy: 0.9534 - val_loss: 0.2297 - val_accuracy: 0.9327\n",
      "Epoch 81/800\n",
      "1665/1665 [==============================] - 0s 112us/step - loss: 0.1743 - accuracy: 0.9534 - val_loss: 0.2284 - val_accuracy: 0.9327\n",
      "Epoch 82/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1739 - accuracy: 0.9534 - val_loss: 0.2291 - val_accuracy: 0.9327\n",
      "Epoch 83/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.1734 - accuracy: 0.9534 - val_loss: 0.2275 - val_accuracy: 0.9327\n",
      "Epoch 84/800\n",
      "1665/1665 [==============================] - 0s 104us/step - loss: 0.1729 - accuracy: 0.9534 - val_loss: 0.2274 - val_accuracy: 0.9327\n",
      "Epoch 85/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.1723 - accuracy: 0.9534 - val_loss: 0.2260 - val_accuracy: 0.9327\n",
      "Epoch 86/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.1718 - accuracy: 0.9534 - val_loss: 0.2257 - val_accuracy: 0.9327\n",
      "Epoch 87/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.1713 - accuracy: 0.9534 - val_loss: 0.2255 - val_accuracy: 0.9327\n",
      "Epoch 88/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.1707 - accuracy: 0.9534 - val_loss: 0.2243 - val_accuracy: 0.9327\n",
      "Epoch 89/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.1701 - accuracy: 0.9534 - val_loss: 0.2233 - val_accuracy: 0.9327\n",
      "Epoch 90/800\n",
      "1665/1665 [==============================] - 0s 109us/step - loss: 0.1695 - accuracy: 0.9534 - val_loss: 0.2221 - val_accuracy: 0.9327\n",
      "Epoch 91/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.1690 - accuracy: 0.9534 - val_loss: 0.2208 - val_accuracy: 0.9327\n",
      "Epoch 92/800\n",
      "1665/1665 [==============================] - 0s 106us/step - loss: 0.1683 - accuracy: 0.9534 - val_loss: 0.2196 - val_accuracy: 0.9327\n",
      "Epoch 93/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.1677 - accuracy: 0.9534 - val_loss: 0.2198 - val_accuracy: 0.9327\n",
      "Epoch 94/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.1672 - accuracy: 0.9534 - val_loss: 0.2193 - val_accuracy: 0.9327\n",
      "Epoch 95/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.1664 - accuracy: 0.9534 - val_loss: 0.2180 - val_accuracy: 0.9327\n",
      "Epoch 96/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.1658 - accuracy: 0.9534 - val_loss: 0.2169 - val_accuracy: 0.9327\n",
      "Epoch 97/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.1652 - accuracy: 0.9534 - val_loss: 0.2162 - val_accuracy: 0.9328\n",
      "Epoch 98/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1646 - accuracy: 0.9534 - val_loss: 0.2154 - val_accuracy: 0.9327\n",
      "Epoch 99/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1639 - accuracy: 0.9534 - val_loss: 0.2149 - val_accuracy: 0.9328\n",
      "Epoch 100/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.1634 - accuracy: 0.9534 - val_loss: 0.2127 - val_accuracy: 0.9327\n",
      "Epoch 101/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.1627 - accuracy: 0.9534 - val_loss: 0.2121 - val_accuracy: 0.9327\n",
      "Epoch 102/800\n",
      "1665/1665 [==============================] - 0s 106us/step - loss: 0.1621 - accuracy: 0.9534 - val_loss: 0.2113 - val_accuracy: 0.9327\n",
      "Epoch 103/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.1615 - accuracy: 0.9534 - val_loss: 0.2106 - val_accuracy: 0.9328\n",
      "Epoch 104/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.1609 - accuracy: 0.9535 - val_loss: 0.2100 - val_accuracy: 0.9328\n",
      "Epoch 105/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1602 - accuracy: 0.9535 - val_loss: 0.2088 - val_accuracy: 0.9330\n",
      "Epoch 106/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.1597 - accuracy: 0.9535 - val_loss: 0.2082 - val_accuracy: 0.9329\n",
      "Epoch 107/800\n",
      "1665/1665 [==============================] - 0s 106us/step - loss: 0.1590 - accuracy: 0.9535 - val_loss: 0.2087 - val_accuracy: 0.9329\n",
      "Epoch 108/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.1585 - accuracy: 0.9535 - val_loss: 0.2066 - val_accuracy: 0.9331\n",
      "Epoch 109/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.1578 - accuracy: 0.9535 - val_loss: 0.2060 - val_accuracy: 0.9331\n",
      "Epoch 110/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.1572 - accuracy: 0.9536 - val_loss: 0.2048 - val_accuracy: 0.9332\n",
      "Epoch 111/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.1566 - accuracy: 0.9536 - val_loss: 0.2041 - val_accuracy: 0.9333\n",
      "Epoch 112/800\n",
      "1665/1665 [==============================] - 0s 107us/step - loss: 0.1560 - accuracy: 0.9537 - val_loss: 0.2028 - val_accuracy: 0.9334\n",
      "Epoch 113/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.1554 - accuracy: 0.9537 - val_loss: 0.2023 - val_accuracy: 0.9337\n",
      "Epoch 114/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.1547 - accuracy: 0.9538 - val_loss: 0.2015 - val_accuracy: 0.9333\n",
      "Epoch 115/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.1542 - accuracy: 0.9537 - val_loss: 0.1995 - val_accuracy: 0.9334\n",
      "Epoch 116/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.1535 - accuracy: 0.9538 - val_loss: 0.1985 - val_accuracy: 0.9334\n",
      "Epoch 117/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.1529 - accuracy: 0.9538 - val_loss: 0.1994 - val_accuracy: 0.9337\n",
      "Epoch 118/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.1522 - accuracy: 0.9539 - val_loss: 0.1977 - val_accuracy: 0.9335\n",
      "Epoch 119/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.1516 - accuracy: 0.9542 - val_loss: 0.1969 - val_accuracy: 0.9337\n",
      "Epoch 120/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.1510 - accuracy: 0.9540 - val_loss: 0.1966 - val_accuracy: 0.9340\n",
      "Epoch 121/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.1505 - accuracy: 0.9540 - val_loss: 0.1956 - val_accuracy: 0.9333\n",
      "Epoch 122/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1498 - accuracy: 0.9540 - val_loss: 0.1962 - val_accuracy: 0.9330\n",
      "Epoch 123/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.1493 - accuracy: 0.9542 - val_loss: 0.1926 - val_accuracy: 0.9338\n",
      "Epoch 124/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.1485 - accuracy: 0.9541 - val_loss: 0.1922 - val_accuracy: 0.9340\n",
      "Epoch 125/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.1480 - accuracy: 0.9542 - val_loss: 0.1917 - val_accuracy: 0.9341\n",
      "Epoch 126/800\n",
      "1665/1665 [==============================] - 0s 147us/step - loss: 0.1473 - accuracy: 0.9543 - val_loss: 0.1914 - val_accuracy: 0.9345\n",
      "Epoch 127/800\n",
      "1665/1665 [==============================] - 0s 109us/step - loss: 0.1467 - accuracy: 0.9544 - val_loss: 0.1897 - val_accuracy: 0.9345\n",
      "Epoch 128/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.1460 - accuracy: 0.9544 - val_loss: 0.1893 - val_accuracy: 0.9341\n",
      "Epoch 129/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.1454 - accuracy: 0.9545 - val_loss: 0.1908 - val_accuracy: 0.9333\n",
      "Epoch 130/800\n",
      "1665/1665 [==============================] - 0s 129us/step - loss: 0.1449 - accuracy: 0.9545 - val_loss: 0.1878 - val_accuracy: 0.9346\n",
      "Epoch 131/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.1442 - accuracy: 0.9548 - val_loss: 0.1906 - val_accuracy: 0.9339\n",
      "Epoch 132/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.1437 - accuracy: 0.9544 - val_loss: 0.1862 - val_accuracy: 0.9344\n",
      "Epoch 133/800\n",
      "1665/1665 [==============================] - 0s 126us/step - loss: 0.1431 - accuracy: 0.9548 - val_loss: 0.1850 - val_accuracy: 0.9345\n",
      "Epoch 134/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.1424 - accuracy: 0.9548 - val_loss: 0.1843 - val_accuracy: 0.9349\n",
      "Epoch 135/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.1417 - accuracy: 0.9549 - val_loss: 0.1836 - val_accuracy: 0.9352\n",
      "Epoch 136/800\n",
      "1665/1665 [==============================] - 0s 154us/step - loss: 0.1412 - accuracy: 0.9550 - val_loss: 0.1836 - val_accuracy: 0.9349\n",
      "Epoch 137/800\n",
      "1665/1665 [==============================] - 0s 147us/step - loss: 0.1406 - accuracy: 0.9550 - val_loss: 0.1826 - val_accuracy: 0.9353\n",
      "Epoch 138/800\n",
      "1665/1665 [==============================] - 0s 141us/step - loss: 0.1399 - accuracy: 0.9552 - val_loss: 0.1814 - val_accuracy: 0.9350\n",
      "Epoch 139/800\n",
      "1665/1665 [==============================] - 0s 146us/step - loss: 0.1395 - accuracy: 0.9553 - val_loss: 0.1834 - val_accuracy: 0.9362\n",
      "Epoch 140/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.1390 - accuracy: 0.9554 - val_loss: 0.1789 - val_accuracy: 0.9363\n",
      "Epoch 141/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.1383 - accuracy: 0.9556 - val_loss: 0.1796 - val_accuracy: 0.9354\n",
      "Epoch 142/800\n",
      "1665/1665 [==============================] - 0s 144us/step - loss: 0.1378 - accuracy: 0.9555 - val_loss: 0.1797 - val_accuracy: 0.9361\n",
      "Epoch 143/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.1373 - accuracy: 0.9556 - val_loss: 0.1779 - val_accuracy: 0.9371\n",
      "Epoch 144/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.1367 - accuracy: 0.9558 - val_loss: 0.1778 - val_accuracy: 0.9367\n",
      "Epoch 145/800\n",
      "1665/1665 [==============================] - 0s 126us/step - loss: 0.1361 - accuracy: 0.9561 - val_loss: 0.1767 - val_accuracy: 0.9370\n",
      "Epoch 146/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.1356 - accuracy: 0.9560 - val_loss: 0.1783 - val_accuracy: 0.9367\n",
      "Epoch 147/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.1352 - accuracy: 0.9562 - val_loss: 0.1772 - val_accuracy: 0.9361\n",
      "Epoch 148/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1346 - accuracy: 0.9562 - val_loss: 0.1761 - val_accuracy: 0.9362\n",
      "Epoch 149/800\n",
      "1665/1665 [==============================] - 0s 129us/step - loss: 0.1340 - accuracy: 0.9563 - val_loss: 0.1748 - val_accuracy: 0.9373\n",
      "Epoch 150/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.1335 - accuracy: 0.9565 - val_loss: 0.1733 - val_accuracy: 0.9380\n",
      "Epoch 151/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.1328 - accuracy: 0.9566 - val_loss: 0.1741 - val_accuracy: 0.9376\n",
      "Epoch 152/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.1323 - accuracy: 0.9567 - val_loss: 0.1717 - val_accuracy: 0.9381\n",
      "Epoch 153/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.1319 - accuracy: 0.9570 - val_loss: 0.1763 - val_accuracy: 0.9369\n",
      "Epoch 154/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.1316 - accuracy: 0.9569 - val_loss: 0.1707 - val_accuracy: 0.9388\n",
      "Epoch 155/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.1308 - accuracy: 0.9571 - val_loss: 0.1708 - val_accuracy: 0.9387\n",
      "Epoch 156/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.1303 - accuracy: 0.9573 - val_loss: 0.1696 - val_accuracy: 0.9390\n",
      "Epoch 157/800\n",
      "1665/1665 [==============================] - 0s 106us/step - loss: 0.1296 - accuracy: 0.9573 - val_loss: 0.1699 - val_accuracy: 0.9381\n",
      "Epoch 158/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.1292 - accuracy: 0.9575 - val_loss: 0.1706 - val_accuracy: 0.9387\n",
      "Epoch 159/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.1289 - accuracy: 0.9576 - val_loss: 0.1677 - val_accuracy: 0.9390\n",
      "Epoch 160/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.1283 - accuracy: 0.9577 - val_loss: 0.1684 - val_accuracy: 0.9389\n",
      "Epoch 161/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.1277 - accuracy: 0.9578 - val_loss: 0.1683 - val_accuracy: 0.9396\n",
      "Epoch 162/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.1271 - accuracy: 0.9579 - val_loss: 0.1672 - val_accuracy: 0.9393\n",
      "Epoch 163/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.1267 - accuracy: 0.9581 - val_loss: 0.1667 - val_accuracy: 0.9388\n",
      "Epoch 164/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.1262 - accuracy: 0.9580 - val_loss: 0.1656 - val_accuracy: 0.9389\n",
      "Epoch 165/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.1257 - accuracy: 0.9581 - val_loss: 0.1661 - val_accuracy: 0.9390\n",
      "Epoch 166/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.1251 - accuracy: 0.9582 - val_loss: 0.1638 - val_accuracy: 0.9400\n",
      "Epoch 167/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.1247 - accuracy: 0.9582 - val_loss: 0.1636 - val_accuracy: 0.9398\n",
      "Epoch 168/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1242 - accuracy: 0.9585 - val_loss: 0.1636 - val_accuracy: 0.9389\n",
      "Epoch 169/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.1237 - accuracy: 0.9583 - val_loss: 0.1616 - val_accuracy: 0.9407\n",
      "Epoch 170/800\n",
      "1665/1665 [==============================] - 0s 107us/step - loss: 0.1232 - accuracy: 0.9584 - val_loss: 0.1642 - val_accuracy: 0.9394\n",
      "Epoch 171/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1228 - accuracy: 0.9585 - val_loss: 0.1641 - val_accuracy: 0.9400\n",
      "Epoch 172/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1223 - accuracy: 0.9586 - val_loss: 0.1609 - val_accuracy: 0.9398\n",
      "Epoch 173/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.1217 - accuracy: 0.9589 - val_loss: 0.1623 - val_accuracy: 0.9404\n",
      "Epoch 174/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.1214 - accuracy: 0.9589 - val_loss: 0.1605 - val_accuracy: 0.9411\n",
      "Epoch 175/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.1208 - accuracy: 0.9590 - val_loss: 0.1599 - val_accuracy: 0.9414\n",
      "Epoch 176/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.1204 - accuracy: 0.9592 - val_loss: 0.1593 - val_accuracy: 0.9415\n",
      "Epoch 177/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.1198 - accuracy: 0.9595 - val_loss: 0.1597 - val_accuracy: 0.9404\n",
      "Epoch 178/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.1194 - accuracy: 0.9595 - val_loss: 0.1567 - val_accuracy: 0.9418\n",
      "Epoch 179/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.1188 - accuracy: 0.9599 - val_loss: 0.1571 - val_accuracy: 0.9415\n",
      "Epoch 180/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.1184 - accuracy: 0.9597 - val_loss: 0.1562 - val_accuracy: 0.9419\n",
      "Epoch 181/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.1179 - accuracy: 0.9598 - val_loss: 0.1575 - val_accuracy: 0.9413\n",
      "Epoch 182/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.1174 - accuracy: 0.9600 - val_loss: 0.1566 - val_accuracy: 0.9424\n",
      "Epoch 183/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.1171 - accuracy: 0.9600 - val_loss: 0.1573 - val_accuracy: 0.9422\n",
      "Epoch 184/800\n",
      "1665/1665 [==============================] - 0s 152us/step - loss: 0.1167 - accuracy: 0.9604 - val_loss: 0.1551 - val_accuracy: 0.9417\n",
      "Epoch 185/800\n",
      "1665/1665 [==============================] - 0s 129us/step - loss: 0.1162 - accuracy: 0.9605 - val_loss: 0.1530 - val_accuracy: 0.9433\n",
      "Epoch 186/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.1158 - accuracy: 0.9607 - val_loss: 0.1545 - val_accuracy: 0.9420\n",
      "Epoch 187/800\n",
      "1665/1665 [==============================] - 0s 133us/step - loss: 0.1154 - accuracy: 0.9608 - val_loss: 0.1535 - val_accuracy: 0.9432\n",
      "Epoch 188/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.1148 - accuracy: 0.9611 - val_loss: 0.1532 - val_accuracy: 0.9428\n",
      "Epoch 189/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.1145 - accuracy: 0.9611 - val_loss: 0.1526 - val_accuracy: 0.9426\n",
      "Epoch 190/800\n",
      "1665/1665 [==============================] - 0s 139us/step - loss: 0.1141 - accuracy: 0.9614 - val_loss: 0.1626 - val_accuracy: 0.9379\n",
      "Epoch 191/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.1141 - accuracy: 0.9614 - val_loss: 0.1505 - val_accuracy: 0.9433\n",
      "Epoch 192/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.1133 - accuracy: 0.9617 - val_loss: 0.1508 - val_accuracy: 0.9432\n",
      "Epoch 193/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.1129 - accuracy: 0.9617 - val_loss: 0.1514 - val_accuracy: 0.9436\n",
      "Epoch 194/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.1124 - accuracy: 0.9619 - val_loss: 0.1587 - val_accuracy: 0.9411\n",
      "Epoch 195/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.1125 - accuracy: 0.9619 - val_loss: 0.1504 - val_accuracy: 0.9442\n",
      "Epoch 196/800\n",
      "1665/1665 [==============================] - 0s 140us/step - loss: 0.1118 - accuracy: 0.9623 - val_loss: 0.1505 - val_accuracy: 0.9430\n",
      "Epoch 197/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.1114 - accuracy: 0.9624 - val_loss: 0.1478 - val_accuracy: 0.9436\n",
      "Epoch 198/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.1109 - accuracy: 0.9626 - val_loss: 0.1503 - val_accuracy: 0.9437\n",
      "Epoch 199/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.1106 - accuracy: 0.9629 - val_loss: 0.1545 - val_accuracy: 0.9427\n",
      "Epoch 200/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.1104 - accuracy: 0.9628 - val_loss: 0.1552 - val_accuracy: 0.9425\n",
      "Epoch 201/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.1100 - accuracy: 0.9633 - val_loss: 0.1500 - val_accuracy: 0.9442\n",
      "Epoch 202/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.1096 - accuracy: 0.9632 - val_loss: 0.1487 - val_accuracy: 0.9439\n",
      "Epoch 203/800\n",
      "1665/1665 [==============================] - 0s 159us/step - loss: 0.1091 - accuracy: 0.9631 - val_loss: 0.1487 - val_accuracy: 0.9434\n",
      "Epoch 204/800\n",
      "1665/1665 [==============================] - 0s 154us/step - loss: 0.1088 - accuracy: 0.9634 - val_loss: 0.1498 - val_accuracy: 0.9436\n",
      "Epoch 205/800\n",
      "1665/1665 [==============================] - 0s 152us/step - loss: 0.1084 - accuracy: 0.9634 - val_loss: 0.1524 - val_accuracy: 0.9442\n",
      "Epoch 206/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.1082 - accuracy: 0.9635 - val_loss: 0.1469 - val_accuracy: 0.9450\n",
      "Epoch 207/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.1076 - accuracy: 0.9637 - val_loss: 0.1459 - val_accuracy: 0.9447\n",
      "Epoch 208/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.1072 - accuracy: 0.9641 - val_loss: 0.1477 - val_accuracy: 0.9440\n",
      "Epoch 209/800\n",
      "1665/1665 [==============================] - 0s 155us/step - loss: 0.1068 - accuracy: 0.9636 - val_loss: 0.1461 - val_accuracy: 0.9450\n",
      "Epoch 210/800\n",
      "1665/1665 [==============================] - 0s 150us/step - loss: 0.1065 - accuracy: 0.9639 - val_loss: 0.1504 - val_accuracy: 0.9429\n",
      "Epoch 211/800\n",
      "1665/1665 [==============================] - 0s 147us/step - loss: 0.1065 - accuracy: 0.9639 - val_loss: 0.1436 - val_accuracy: 0.9457\n",
      "Epoch 212/800\n",
      "1665/1665 [==============================] - 0s 140us/step - loss: 0.1057 - accuracy: 0.9643 - val_loss: 0.1453 - val_accuracy: 0.9459\n",
      "Epoch 213/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.1055 - accuracy: 0.9645 - val_loss: 0.1452 - val_accuracy: 0.9450\n",
      "Epoch 214/800\n",
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.1052 - accuracy: 0.9645 - val_loss: 0.1462 - val_accuracy: 0.9447\n",
      "Epoch 215/800\n",
      "1665/1665 [==============================] - 0s 151us/step - loss: 0.1050 - accuracy: 0.9645 - val_loss: 0.1453 - val_accuracy: 0.9449\n",
      "Epoch 216/800\n",
      "1665/1665 [==============================] - 0s 142us/step - loss: 0.1045 - accuracy: 0.9648 - val_loss: 0.1443 - val_accuracy: 0.9459\n",
      "Epoch 217/800\n",
      "1665/1665 [==============================] - 0s 161us/step - loss: 0.1042 - accuracy: 0.9647 - val_loss: 0.1436 - val_accuracy: 0.9459\n",
      "Epoch 218/800\n",
      "1665/1665 [==============================] - 0s 148us/step - loss: 0.1038 - accuracy: 0.9650 - val_loss: 0.1482 - val_accuracy: 0.9460\n",
      "Epoch 219/800\n",
      "1665/1665 [==============================] - 0s 140us/step - loss: 0.1038 - accuracy: 0.9650 - val_loss: 0.1442 - val_accuracy: 0.9458\n",
      "Epoch 220/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.1032 - accuracy: 0.9652 - val_loss: 0.1431 - val_accuracy: 0.9461\n",
      "Epoch 221/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.1031 - accuracy: 0.9654 - val_loss: 0.1432 - val_accuracy: 0.9467\n",
      "Epoch 222/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.1025 - accuracy: 0.9659 - val_loss: 0.1423 - val_accuracy: 0.9458\n",
      "Epoch 223/800\n",
      "1665/1665 [==============================] - 0s 129us/step - loss: 0.1024 - accuracy: 0.9654 - val_loss: 0.1429 - val_accuracy: 0.9462\n",
      "Epoch 224/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.1021 - accuracy: 0.9655 - val_loss: 0.1432 - val_accuracy: 0.9471\n",
      "Epoch 225/800\n",
      "1665/1665 [==============================] - 0s 143us/step - loss: 0.1017 - accuracy: 0.9661 - val_loss: 0.1424 - val_accuracy: 0.9473\n",
      "Epoch 226/800\n",
      "1665/1665 [==============================] - 0s 140us/step - loss: 0.1015 - accuracy: 0.9660 - val_loss: 0.1426 - val_accuracy: 0.9467\n",
      "Epoch 227/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.1012 - accuracy: 0.9659 - val_loss: 0.1420 - val_accuracy: 0.9472\n",
      "Epoch 228/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.1008 - accuracy: 0.9662 - val_loss: 0.1419 - val_accuracy: 0.9469\n",
      "Epoch 229/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.1006 - accuracy: 0.9662 - val_loss: 0.1396 - val_accuracy: 0.9476\n",
      "Epoch 230/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.1002 - accuracy: 0.9663 - val_loss: 0.1424 - val_accuracy: 0.9470\n",
      "Epoch 231/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.1000 - accuracy: 0.9665 - val_loss: 0.1538 - val_accuracy: 0.9410\n",
      "Epoch 232/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.1003 - accuracy: 0.9661 - val_loss: 0.1415 - val_accuracy: 0.9472\n",
      "Epoch 233/800\n",
      "1665/1665 [==============================] - 0s 145us/step - loss: 0.0995 - accuracy: 0.9665 - val_loss: 0.1411 - val_accuracy: 0.9471\n",
      "Epoch 234/800\n",
      "1665/1665 [==============================] - 0s 140us/step - loss: 0.0994 - accuracy: 0.9668 - val_loss: 0.1413 - val_accuracy: 0.9469\n",
      "Epoch 235/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.0990 - accuracy: 0.9667 - val_loss: 0.1424 - val_accuracy: 0.9475\n",
      "Epoch 236/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0989 - accuracy: 0.9665 - val_loss: 0.1395 - val_accuracy: 0.9479\n",
      "Epoch 237/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0984 - accuracy: 0.9668 - val_loss: 0.1431 - val_accuracy: 0.9472\n",
      "Epoch 238/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.0983 - accuracy: 0.9668 - val_loss: 0.1413 - val_accuracy: 0.9474\n",
      "Epoch 239/800\n",
      "1665/1665 [==============================] - 0s 148us/step - loss: 0.0980 - accuracy: 0.9670 - val_loss: 0.1479 - val_accuracy: 0.9439\n",
      "Epoch 240/800\n",
      "1665/1665 [==============================] - 0s 139us/step - loss: 0.0983 - accuracy: 0.9670 - val_loss: 0.1431 - val_accuracy: 0.9456\n",
      "Epoch 241/800\n",
      "1665/1665 [==============================] - 0s 145us/step - loss: 0.0977 - accuracy: 0.9672 - val_loss: 0.1401 - val_accuracy: 0.9481\n",
      "Epoch 242/800\n",
      "1665/1665 [==============================] - 0s 147us/step - loss: 0.0973 - accuracy: 0.9671 - val_loss: 0.1391 - val_accuracy: 0.9485\n",
      "Epoch 243/800\n",
      "1665/1665 [==============================] - 0s 147us/step - loss: 0.0971 - accuracy: 0.9673 - val_loss: 0.1409 - val_accuracy: 0.9480\n",
      "Epoch 244/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.0969 - accuracy: 0.9675 - val_loss: 0.1455 - val_accuracy: 0.9464\n",
      "Epoch 245/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0970 - accuracy: 0.9675 - val_loss: 0.1408 - val_accuracy: 0.9474\n",
      "Epoch 246/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.0964 - accuracy: 0.9672 - val_loss: 0.1389 - val_accuracy: 0.9482\n",
      "Epoch 247/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0960 - accuracy: 0.9678 - val_loss: 0.1395 - val_accuracy: 0.9485\n",
      "Epoch 248/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.0959 - accuracy: 0.9676 - val_loss: 0.1372 - val_accuracy: 0.9480\n",
      "Epoch 249/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0956 - accuracy: 0.9676 - val_loss: 0.1380 - val_accuracy: 0.9485\n",
      "Epoch 250/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0955 - accuracy: 0.9677 - val_loss: 0.1378 - val_accuracy: 0.9488\n",
      "Epoch 251/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0951 - accuracy: 0.9678 - val_loss: 0.1507 - val_accuracy: 0.9454\n",
      "Epoch 252/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0955 - accuracy: 0.9679 - val_loss: 0.1365 - val_accuracy: 0.9490\n",
      "Epoch 253/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0949 - accuracy: 0.9681 - val_loss: 0.1499 - val_accuracy: 0.9444\n",
      "Epoch 254/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.0951 - accuracy: 0.9674 - val_loss: 0.1382 - val_accuracy: 0.9489\n",
      "Epoch 255/800\n",
      "1665/1665 [==============================] - 0s 107us/step - loss: 0.0943 - accuracy: 0.9682 - val_loss: 0.1390 - val_accuracy: 0.9483\n",
      "Epoch 256/800\n",
      "1665/1665 [==============================] - 0s 103us/step - loss: 0.0942 - accuracy: 0.9680 - val_loss: 0.1543 - val_accuracy: 0.9414\n",
      "Epoch 257/800\n",
      "1665/1665 [==============================] - 0s 109us/step - loss: 0.0945 - accuracy: 0.9679 - val_loss: 0.1405 - val_accuracy: 0.9471\n",
      "Epoch 258/800\n",
      "1665/1665 [==============================] - 0s 106us/step - loss: 0.0939 - accuracy: 0.9681 - val_loss: 0.1377 - val_accuracy: 0.9487\n",
      "Epoch 259/800\n",
      "1665/1665 [==============================] - 0s 106us/step - loss: 0.0936 - accuracy: 0.9683 - val_loss: 0.1367 - val_accuracy: 0.9497\n",
      "Epoch 260/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.0934 - accuracy: 0.9684 - val_loss: 0.1372 - val_accuracy: 0.9486\n",
      "Epoch 261/800\n",
      "1665/1665 [==============================] - 0s 105us/step - loss: 0.0930 - accuracy: 0.9685 - val_loss: 0.1378 - val_accuracy: 0.9497\n",
      "Epoch 262/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0929 - accuracy: 0.9685 - val_loss: 0.1366 - val_accuracy: 0.9496\n",
      "Epoch 263/800\n",
      "1665/1665 [==============================] - 0s 107us/step - loss: 0.0927 - accuracy: 0.9685 - val_loss: 0.1361 - val_accuracy: 0.9495\n",
      "Epoch 264/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0925 - accuracy: 0.9685 - val_loss: 0.1361 - val_accuracy: 0.9492\n",
      "Epoch 265/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0923 - accuracy: 0.9687 - val_loss: 0.1358 - val_accuracy: 0.9490\n",
      "Epoch 266/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.0922 - accuracy: 0.9687 - val_loss: 0.1391 - val_accuracy: 0.9483\n",
      "Epoch 267/800\n",
      "1665/1665 [==============================] - 0s 102us/step - loss: 0.0920 - accuracy: 0.9687 - val_loss: 0.1376 - val_accuracy: 0.9492\n",
      "Epoch 268/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.0918 - accuracy: 0.9687 - val_loss: 0.1358 - val_accuracy: 0.9492\n",
      "Epoch 269/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.0916 - accuracy: 0.9686 - val_loss: 0.1579 - val_accuracy: 0.9405\n",
      "Epoch 270/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.0924 - accuracy: 0.9687 - val_loss: 0.1386 - val_accuracy: 0.9488\n",
      "Epoch 271/800\n",
      "1665/1665 [==============================] - 0s 109us/step - loss: 0.0915 - accuracy: 0.9685 - val_loss: 0.1344 - val_accuracy: 0.9504\n",
      "Epoch 272/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.0908 - accuracy: 0.9689 - val_loss: 0.1373 - val_accuracy: 0.9497\n",
      "Epoch 273/800\n",
      "1665/1665 [==============================] - 0s 112us/step - loss: 0.0908 - accuracy: 0.9690 - val_loss: 0.1372 - val_accuracy: 0.9487\n",
      "Epoch 274/800\n",
      "1665/1665 [==============================] - 0s 103us/step - loss: 0.0908 - accuracy: 0.9692 - val_loss: 0.1372 - val_accuracy: 0.9487\n",
      "Epoch 275/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0907 - accuracy: 0.9692 - val_loss: 0.1349 - val_accuracy: 0.9502\n",
      "Epoch 276/800\n",
      "1665/1665 [==============================] - 0s 126us/step - loss: 0.0903 - accuracy: 0.9690 - val_loss: 0.1535 - val_accuracy: 0.9425\n",
      "Epoch 277/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.0908 - accuracy: 0.9688 - val_loss: 0.1345 - val_accuracy: 0.9505\n",
      "Epoch 278/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.0899 - accuracy: 0.9692 - val_loss: 0.1441 - val_accuracy: 0.9458\n",
      "Epoch 279/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.0903 - accuracy: 0.9692 - val_loss: 0.1368 - val_accuracy: 0.9501\n",
      "Epoch 280/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.0896 - accuracy: 0.9692 - val_loss: 0.1351 - val_accuracy: 0.9503\n",
      "Epoch 281/800\n",
      "1665/1665 [==============================] - 0s 133us/step - loss: 0.0894 - accuracy: 0.9697 - val_loss: 0.1358 - val_accuracy: 0.9498\n",
      "Epoch 282/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0894 - accuracy: 0.9695 - val_loss: 0.1353 - val_accuracy: 0.9495\n",
      "Epoch 283/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0892 - accuracy: 0.9695 - val_loss: 0.1355 - val_accuracy: 0.9493\n",
      "Epoch 284/800\n",
      "1665/1665 [==============================] - 0s 158us/step - loss: 0.0890 - accuracy: 0.9695 - val_loss: 0.1369 - val_accuracy: 0.9498\n",
      "Epoch 285/800\n",
      "1665/1665 [==============================] - 0s 142us/step - loss: 0.0889 - accuracy: 0.9697 - val_loss: 0.1334 - val_accuracy: 0.9507\n",
      "Epoch 286/800\n",
      "1665/1665 [==============================] - 0s 133us/step - loss: 0.0885 - accuracy: 0.9700 - val_loss: 0.1336 - val_accuracy: 0.9513\n",
      "Epoch 287/800\n",
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.0884 - accuracy: 0.9699 - val_loss: 0.1351 - val_accuracy: 0.9506\n",
      "Epoch 288/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0884 - accuracy: 0.9698 - val_loss: 0.1377 - val_accuracy: 0.9498\n",
      "Epoch 289/800\n",
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.0883 - accuracy: 0.9698 - val_loss: 0.1336 - val_accuracy: 0.9506\n",
      "Epoch 290/800\n",
      "1665/1665 [==============================] - 0s 142us/step - loss: 0.0879 - accuracy: 0.9700 - val_loss: 0.1347 - val_accuracy: 0.9500\n",
      "Epoch 291/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.0878 - accuracy: 0.9700 - val_loss: 0.1337 - val_accuracy: 0.9509\n",
      "Epoch 292/800\n",
      "1665/1665 [==============================] - 0s 129us/step - loss: 0.0876 - accuracy: 0.9699 - val_loss: 0.1334 - val_accuracy: 0.9508\n",
      "Epoch 293/800\n",
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.0876 - accuracy: 0.9701 - val_loss: 0.1327 - val_accuracy: 0.9509\n",
      "Epoch 294/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0874 - accuracy: 0.9702 - val_loss: 0.1344 - val_accuracy: 0.9506\n",
      "Epoch 295/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.0872 - accuracy: 0.9700 - val_loss: 0.1337 - val_accuracy: 0.9511\n",
      "Epoch 296/800\n",
      "1665/1665 [==============================] - 0s 109us/step - loss: 0.0869 - accuracy: 0.9701 - val_loss: 0.1349 - val_accuracy: 0.9498\n",
      "Epoch 297/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.0870 - accuracy: 0.9699 - val_loss: 0.1629 - val_accuracy: 0.9443\n",
      "Epoch 298/800\n",
      "1665/1665 [==============================] - 0s 154us/step - loss: 0.0880 - accuracy: 0.9696 - val_loss: 0.1399 - val_accuracy: 0.9475\n",
      "Epoch 299/800\n",
      "1665/1665 [==============================] - 0s 142us/step - loss: 0.0870 - accuracy: 0.9701 - val_loss: 0.1323 - val_accuracy: 0.9518\n",
      "Epoch 300/800\n",
      "1665/1665 [==============================] - 0s 156us/step - loss: 0.0864 - accuracy: 0.9705 - val_loss: 0.1316 - val_accuracy: 0.9511\n",
      "Epoch 301/800\n",
      "1665/1665 [==============================] - 0s 160us/step - loss: 0.0863 - accuracy: 0.9705 - val_loss: 0.1343 - val_accuracy: 0.9506\n",
      "Epoch 302/800\n",
      "1665/1665 [==============================] - 0s 140us/step - loss: 0.0864 - accuracy: 0.9704 - val_loss: 0.1341 - val_accuracy: 0.9514\n",
      "Epoch 303/800\n",
      "1665/1665 [==============================] - 0s 150us/step - loss: 0.0860 - accuracy: 0.9704 - val_loss: 0.1326 - val_accuracy: 0.9505\n",
      "Epoch 304/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0860 - accuracy: 0.9703 - val_loss: 0.1323 - val_accuracy: 0.9513\n",
      "Epoch 305/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0858 - accuracy: 0.9706 - val_loss: 0.1322 - val_accuracy: 0.9516\n",
      "Epoch 306/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.0856 - accuracy: 0.9706 - val_loss: 0.1328 - val_accuracy: 0.9515\n",
      "Epoch 307/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.0855 - accuracy: 0.9707 - val_loss: 0.1330 - val_accuracy: 0.9513\n",
      "Epoch 308/800\n",
      "1665/1665 [==============================] - 0s 112us/step - loss: 0.0854 - accuracy: 0.9709 - val_loss: 0.1330 - val_accuracy: 0.9512\n",
      "Epoch 309/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.0852 - accuracy: 0.9708 - val_loss: 0.1324 - val_accuracy: 0.9516\n",
      "Epoch 310/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0851 - accuracy: 0.9708 - val_loss: 0.1412 - val_accuracy: 0.9475\n",
      "Epoch 311/800\n",
      "1665/1665 [==============================] - 0s 153us/step - loss: 0.0853 - accuracy: 0.9706 - val_loss: 0.1314 - val_accuracy: 0.9512\n",
      "Epoch 312/800\n",
      "1665/1665 [==============================] - 0s 145us/step - loss: 0.0850 - accuracy: 0.9709 - val_loss: 0.1309 - val_accuracy: 0.9521\n",
      "Epoch 313/800\n",
      "1665/1665 [==============================] - 0s 146us/step - loss: 0.0848 - accuracy: 0.9709 - val_loss: 0.1346 - val_accuracy: 0.9500\n",
      "Epoch 314/800\n",
      "1665/1665 [==============================] - 0s 148us/step - loss: 0.0848 - accuracy: 0.9709 - val_loss: 0.1316 - val_accuracy: 0.9510\n",
      "Epoch 315/800\n",
      "1665/1665 [==============================] - 0s 162us/step - loss: 0.0847 - accuracy: 0.9709 - val_loss: 0.1325 - val_accuracy: 0.9517\n",
      "Epoch 316/800\n",
      "1665/1665 [==============================] - 0s 170us/step - loss: 0.0844 - accuracy: 0.9711 - val_loss: 0.1310 - val_accuracy: 0.9511\n",
      "Epoch 317/800\n",
      "1665/1665 [==============================] - 0s 150us/step - loss: 0.0844 - accuracy: 0.9710 - val_loss: 0.1346 - val_accuracy: 0.9521\n",
      "Epoch 318/800\n",
      "1665/1665 [==============================] - 0s 147us/step - loss: 0.0844 - accuracy: 0.9709 - val_loss: 0.1307 - val_accuracy: 0.9518\n",
      "Epoch 319/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0841 - accuracy: 0.9711 - val_loss: 0.1317 - val_accuracy: 0.9510\n",
      "Epoch 320/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0839 - accuracy: 0.9709 - val_loss: 0.1313 - val_accuracy: 0.9515\n",
      "Epoch 321/800\n",
      "1665/1665 [==============================] - 0s 148us/step - loss: 0.0839 - accuracy: 0.9708 - val_loss: 0.1311 - val_accuracy: 0.9521\n",
      "Epoch 322/800\n",
      "1665/1665 [==============================] - 0s 146us/step - loss: 0.0837 - accuracy: 0.9713 - val_loss: 0.1344 - val_accuracy: 0.9519\n",
      "Epoch 323/800\n",
      "1665/1665 [==============================] - 0s 141us/step - loss: 0.0836 - accuracy: 0.9711 - val_loss: 0.1318 - val_accuracy: 0.9519\n",
      "Epoch 324/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0835 - accuracy: 0.9714 - val_loss: 0.1432 - val_accuracy: 0.9453\n",
      "Epoch 325/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0840 - accuracy: 0.9710 - val_loss: 0.1309 - val_accuracy: 0.9516\n",
      "Epoch 326/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0833 - accuracy: 0.9714 - val_loss: 0.1313 - val_accuracy: 0.9521\n",
      "Epoch 327/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0831 - accuracy: 0.9712 - val_loss: 0.1330 - val_accuracy: 0.9521\n",
      "Epoch 328/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0830 - accuracy: 0.9714 - val_loss: 0.1317 - val_accuracy: 0.9513\n",
      "Epoch 329/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.0829 - accuracy: 0.9714 - val_loss: 0.1320 - val_accuracy: 0.9520\n",
      "Epoch 330/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1665/1665 [==============================] - 0s 144us/step - loss: 0.0829 - accuracy: 0.9714 - val_loss: 0.1296 - val_accuracy: 0.9523\n",
      "Epoch 331/800\n",
      "1665/1665 [==============================] - 0s 152us/step - loss: 0.0825 - accuracy: 0.9716 - val_loss: 0.1311 - val_accuracy: 0.9522\n",
      "Epoch 332/800\n",
      "1665/1665 [==============================] - 0s 145us/step - loss: 0.0826 - accuracy: 0.9714 - val_loss: 0.1313 - val_accuracy: 0.9525\n",
      "Epoch 333/800\n",
      "1665/1665 [==============================] - 0s 148us/step - loss: 0.0826 - accuracy: 0.9713 - val_loss: 0.1305 - val_accuracy: 0.9525\n",
      "Epoch 334/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.0824 - accuracy: 0.9716 - val_loss: 0.1319 - val_accuracy: 0.9518\n",
      "Epoch 335/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0823 - accuracy: 0.9715 - val_loss: 0.1296 - val_accuracy: 0.9530\n",
      "Epoch 336/800\n",
      "1665/1665 [==============================] - 0s 154us/step - loss: 0.0820 - accuracy: 0.9719 - val_loss: 0.1294 - val_accuracy: 0.9528\n",
      "Epoch 337/800\n",
      "1665/1665 [==============================] - 0s 148us/step - loss: 0.0819 - accuracy: 0.9716 - val_loss: 0.1301 - val_accuracy: 0.9524\n",
      "Epoch 338/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0819 - accuracy: 0.9718 - val_loss: 0.1303 - val_accuracy: 0.9526\n",
      "Epoch 339/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0817 - accuracy: 0.9719 - val_loss: 0.1459 - val_accuracy: 0.9459\n",
      "Epoch 340/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0823 - accuracy: 0.9715 - val_loss: 0.1308 - val_accuracy: 0.9516\n",
      "Epoch 341/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0818 - accuracy: 0.9719 - val_loss: 0.1282 - val_accuracy: 0.9532\n",
      "Epoch 342/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0814 - accuracy: 0.9720 - val_loss: 0.1303 - val_accuracy: 0.9524\n",
      "Epoch 343/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0813 - accuracy: 0.9720 - val_loss: 0.1316 - val_accuracy: 0.9516\n",
      "Epoch 344/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0813 - accuracy: 0.9717 - val_loss: 0.1306 - val_accuracy: 0.9520\n",
      "Epoch 345/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0813 - accuracy: 0.9717 - val_loss: 0.1500 - val_accuracy: 0.9445\n",
      "Epoch 346/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0818 - accuracy: 0.9715 - val_loss: 0.1375 - val_accuracy: 0.9498\n",
      "Epoch 347/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0813 - accuracy: 0.9718 - val_loss: 0.1307 - val_accuracy: 0.9524\n",
      "Epoch 348/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0810 - accuracy: 0.9720 - val_loss: 0.1313 - val_accuracy: 0.9533\n",
      "Epoch 349/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0807 - accuracy: 0.9720 - val_loss: 0.1283 - val_accuracy: 0.9529\n",
      "Epoch 350/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0806 - accuracy: 0.9722 - val_loss: 0.1306 - val_accuracy: 0.9530\n",
      "Epoch 351/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0805 - accuracy: 0.9723 - val_loss: 0.1286 - val_accuracy: 0.9528\n",
      "Epoch 352/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0803 - accuracy: 0.9723 - val_loss: 0.1292 - val_accuracy: 0.9528\n",
      "Epoch 353/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0804 - accuracy: 0.9718 - val_loss: 0.1291 - val_accuracy: 0.9534\n",
      "Epoch 354/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0802 - accuracy: 0.9721 - val_loss: 0.1344 - val_accuracy: 0.9519\n",
      "Epoch 355/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.0804 - accuracy: 0.9724 - val_loss: 0.1384 - val_accuracy: 0.9505\n",
      "Epoch 356/800\n",
      "1665/1665 [==============================] - 0s 133us/step - loss: 0.0806 - accuracy: 0.9720 - val_loss: 0.1405 - val_accuracy: 0.9476\n",
      "Epoch 357/800\n",
      "1665/1665 [==============================] - 0s 133us/step - loss: 0.0804 - accuracy: 0.9720 - val_loss: 0.1286 - val_accuracy: 0.9535\n",
      "Epoch 358/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.0799 - accuracy: 0.9724 - val_loss: 0.1279 - val_accuracy: 0.9541\n",
      "Epoch 359/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.0796 - accuracy: 0.9726 - val_loss: 0.1378 - val_accuracy: 0.9484\n",
      "Epoch 360/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0802 - accuracy: 0.9720 - val_loss: 0.1281 - val_accuracy: 0.9527\n",
      "Epoch 361/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0795 - accuracy: 0.9724 - val_loss: 0.1277 - val_accuracy: 0.9534\n",
      "Epoch 362/800\n",
      "1665/1665 [==============================] - 0s 133us/step - loss: 0.0794 - accuracy: 0.9725 - val_loss: 0.1287 - val_accuracy: 0.9537\n",
      "Epoch 363/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.0793 - accuracy: 0.9725 - val_loss: 0.1285 - val_accuracy: 0.9531\n",
      "Epoch 364/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.0794 - accuracy: 0.9725 - val_loss: 0.1306 - val_accuracy: 0.9533\n",
      "Epoch 365/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0793 - accuracy: 0.9724 - val_loss: 0.1409 - val_accuracy: 0.9489\n",
      "Epoch 366/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0796 - accuracy: 0.9725 - val_loss: 0.1284 - val_accuracy: 0.9536\n",
      "Epoch 367/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0791 - accuracy: 0.9727 - val_loss: 0.1277 - val_accuracy: 0.9535\n",
      "Epoch 368/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0790 - accuracy: 0.9725 - val_loss: 0.1283 - val_accuracy: 0.9532\n",
      "Epoch 369/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.0788 - accuracy: 0.9727 - val_loss: 0.1262 - val_accuracy: 0.9537\n",
      "Epoch 370/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0786 - accuracy: 0.9726 - val_loss: 0.1287 - val_accuracy: 0.9539\n",
      "Epoch 371/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0786 - accuracy: 0.9727 - val_loss: 0.1281 - val_accuracy: 0.9540\n",
      "Epoch 372/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0785 - accuracy: 0.9728 - val_loss: 0.1281 - val_accuracy: 0.9529\n",
      "Epoch 373/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0783 - accuracy: 0.9728 - val_loss: 0.1268 - val_accuracy: 0.9535\n",
      "Epoch 374/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0782 - accuracy: 0.9727 - val_loss: 0.1287 - val_accuracy: 0.9529\n",
      "Epoch 375/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0783 - accuracy: 0.9727 - val_loss: 0.1277 - val_accuracy: 0.9528\n",
      "Epoch 376/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0780 - accuracy: 0.9728 - val_loss: 0.1370 - val_accuracy: 0.9502\n",
      "Epoch 377/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0783 - accuracy: 0.9726 - val_loss: 0.1274 - val_accuracy: 0.9533\n",
      "Epoch 378/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0779 - accuracy: 0.9728 - val_loss: 0.1265 - val_accuracy: 0.9538\n",
      "Epoch 379/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0778 - accuracy: 0.9729 - val_loss: 0.1274 - val_accuracy: 0.9542\n",
      "Epoch 380/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0777 - accuracy: 0.9731 - val_loss: 0.1267 - val_accuracy: 0.9530\n",
      "Epoch 381/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0777 - accuracy: 0.9730 - val_loss: 0.1277 - val_accuracy: 0.9543\n",
      "Epoch 382/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0776 - accuracy: 0.9730 - val_loss: 0.1353 - val_accuracy: 0.9505\n",
      "Epoch 383/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0778 - accuracy: 0.9729 - val_loss: 0.1265 - val_accuracy: 0.9541\n",
      "Epoch 384/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.0773 - accuracy: 0.9733 - val_loss: 0.1275 - val_accuracy: 0.9527\n",
      "Epoch 385/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0773 - accuracy: 0.9731 - val_loss: 0.1258 - val_accuracy: 0.9536\n",
      "Epoch 386/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0771 - accuracy: 0.9734 - val_loss: 0.1262 - val_accuracy: 0.9534\n",
      "Epoch 387/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0769 - accuracy: 0.9732 - val_loss: 0.1261 - val_accuracy: 0.9541\n",
      "Epoch 388/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.0769 - accuracy: 0.9733 - val_loss: 0.1250 - val_accuracy: 0.9540\n",
      "Epoch 389/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.0768 - accuracy: 0.9733 - val_loss: 0.1275 - val_accuracy: 0.9531\n",
      "Epoch 390/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0769 - accuracy: 0.9732 - val_loss: 0.1271 - val_accuracy: 0.9544\n",
      "Epoch 391/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0767 - accuracy: 0.9733 - val_loss: 0.1255 - val_accuracy: 0.9540\n",
      "Epoch 392/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0764 - accuracy: 0.9736 - val_loss: 0.1272 - val_accuracy: 0.9536\n",
      "Epoch 393/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.0766 - accuracy: 0.9734 - val_loss: 0.1258 - val_accuracy: 0.9539\n",
      "Epoch 394/800\n",
      "1665/1665 [==============================] - 0s 133us/step - loss: 0.0764 - accuracy: 0.9736 - val_loss: 0.1262 - val_accuracy: 0.9538\n",
      "Epoch 395/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0763 - accuracy: 0.9734 - val_loss: 0.1269 - val_accuracy: 0.9523\n",
      "Epoch 396/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0763 - accuracy: 0.9734 - val_loss: 0.1252 - val_accuracy: 0.9548\n",
      "Epoch 397/800\n",
      "1665/1665 [==============================] - 0s 129us/step - loss: 0.0761 - accuracy: 0.9735 - val_loss: 0.1259 - val_accuracy: 0.9537\n",
      "Epoch 398/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0761 - accuracy: 0.9737 - val_loss: 0.1314 - val_accuracy: 0.9527\n",
      "Epoch 399/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0765 - accuracy: 0.9735 - val_loss: 0.1251 - val_accuracy: 0.9551\n",
      "Epoch 400/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0761 - accuracy: 0.9738 - val_loss: 0.1280 - val_accuracy: 0.9558\n",
      "Epoch 401/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0758 - accuracy: 0.9735 - val_loss: 0.1254 - val_accuracy: 0.9544\n",
      "Epoch 402/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0759 - accuracy: 0.9737 - val_loss: 0.1243 - val_accuracy: 0.9553\n",
      "Epoch 403/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0756 - accuracy: 0.9739 - val_loss: 0.1250 - val_accuracy: 0.9532\n",
      "Epoch 404/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0756 - accuracy: 0.9738 - val_loss: 0.1253 - val_accuracy: 0.9542\n",
      "Epoch 405/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0753 - accuracy: 0.9739 - val_loss: 0.1250 - val_accuracy: 0.9550\n",
      "Epoch 406/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0753 - accuracy: 0.9738 - val_loss: 0.1247 - val_accuracy: 0.9541\n",
      "Epoch 407/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.0753 - accuracy: 0.9739 - val_loss: 0.1234 - val_accuracy: 0.9552\n",
      "Epoch 408/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0750 - accuracy: 0.9740 - val_loss: 0.1266 - val_accuracy: 0.9546\n",
      "Epoch 409/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0750 - accuracy: 0.9740 - val_loss: 0.1307 - val_accuracy: 0.9505\n",
      "Epoch 410/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0754 - accuracy: 0.9738 - val_loss: 0.1238 - val_accuracy: 0.9550\n",
      "Epoch 411/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0747 - accuracy: 0.9738 - val_loss: 0.1243 - val_accuracy: 0.9543\n",
      "Epoch 412/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0746 - accuracy: 0.9741 - val_loss: 0.1392 - val_accuracy: 0.9490\n",
      "Epoch 413/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0755 - accuracy: 0.9737 - val_loss: 0.1245 - val_accuracy: 0.9556\n",
      "Epoch 414/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0745 - accuracy: 0.9742 - val_loss: 0.1237 - val_accuracy: 0.9553\n",
      "Epoch 415/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0743 - accuracy: 0.9742 - val_loss: 0.1349 - val_accuracy: 0.9493\n",
      "Epoch 416/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0753 - accuracy: 0.9736 - val_loss: 0.1250 - val_accuracy: 0.9538\n",
      "Epoch 417/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0745 - accuracy: 0.9741 - val_loss: 0.1248 - val_accuracy: 0.9549\n",
      "Epoch 418/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0742 - accuracy: 0.9743 - val_loss: 0.1231 - val_accuracy: 0.9549\n",
      "Epoch 419/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0741 - accuracy: 0.9743 - val_loss: 0.1231 - val_accuracy: 0.9548\n",
      "Epoch 420/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0740 - accuracy: 0.9741 - val_loss: 0.1228 - val_accuracy: 0.9554\n",
      "Epoch 421/800\n",
      "1665/1665 [==============================] - 0s 129us/step - loss: 0.0738 - accuracy: 0.9743 - val_loss: 0.1225 - val_accuracy: 0.9561\n",
      "Epoch 422/800\n",
      "1665/1665 [==============================] - 0s 126us/step - loss: 0.0738 - accuracy: 0.9742 - val_loss: 0.1236 - val_accuracy: 0.9553\n",
      "Epoch 423/800\n",
      "1665/1665 [==============================] - 0s 133us/step - loss: 0.0738 - accuracy: 0.9743 - val_loss: 0.1229 - val_accuracy: 0.9557\n",
      "Epoch 424/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0736 - accuracy: 0.9742 - val_loss: 0.1228 - val_accuracy: 0.9547\n",
      "Epoch 425/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0736 - accuracy: 0.9745 - val_loss: 0.1239 - val_accuracy: 0.9548\n",
      "Epoch 426/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.0735 - accuracy: 0.9744 - val_loss: 0.1218 - val_accuracy: 0.9557\n",
      "Epoch 427/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0733 - accuracy: 0.9745 - val_loss: 0.1220 - val_accuracy: 0.9554\n",
      "Epoch 428/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0732 - accuracy: 0.9746 - val_loss: 0.1551 - val_accuracy: 0.9442\n",
      "Epoch 429/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0746 - accuracy: 0.9742 - val_loss: 0.1241 - val_accuracy: 0.9549\n",
      "Epoch 430/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0733 - accuracy: 0.9744 - val_loss: 0.1236 - val_accuracy: 0.9543\n",
      "Epoch 431/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0731 - accuracy: 0.9746 - val_loss: 0.1433 - val_accuracy: 0.9505\n",
      "Epoch 432/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0744 - accuracy: 0.9739 - val_loss: 0.1245 - val_accuracy: 0.9543\n",
      "Epoch 433/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0732 - accuracy: 0.9745 - val_loss: 0.1214 - val_accuracy: 0.9563\n",
      "Epoch 434/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.0728 - accuracy: 0.9746 - val_loss: 0.1232 - val_accuracy: 0.9548\n",
      "Epoch 435/800\n",
      "1665/1665 [==============================] - 0s 140us/step - loss: 0.0728 - accuracy: 0.9745 - val_loss: 0.1219 - val_accuracy: 0.9548\n",
      "Epoch 436/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0727 - accuracy: 0.9747 - val_loss: 0.1211 - val_accuracy: 0.9554\n",
      "Epoch 437/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.0726 - accuracy: 0.9749 - val_loss: 0.1222 - val_accuracy: 0.9559\n",
      "Epoch 438/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.0723 - accuracy: 0.9747 - val_loss: 0.1211 - val_accuracy: 0.9558\n",
      "Epoch 439/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0725 - accuracy: 0.9745 - val_loss: 0.1210 - val_accuracy: 0.9561\n",
      "Epoch 440/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.0721 - accuracy: 0.9747 - val_loss: 0.1217 - val_accuracy: 0.9556\n",
      "Epoch 441/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0722 - accuracy: 0.9748 - val_loss: 0.1208 - val_accuracy: 0.9563\n",
      "Epoch 442/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.0724 - accuracy: 0.9748 - val_loss: 0.1219 - val_accuracy: 0.9559\n",
      "Epoch 443/800\n",
      "1665/1665 [==============================] - 0s 133us/step - loss: 0.0721 - accuracy: 0.9749 - val_loss: 0.1216 - val_accuracy: 0.9563\n",
      "Epoch 444/800\n",
      "1665/1665 [==============================] - 0s 126us/step - loss: 0.0720 - accuracy: 0.9751 - val_loss: 0.1216 - val_accuracy: 0.9556\n",
      "Epoch 445/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0719 - accuracy: 0.9750 - val_loss: 0.1226 - val_accuracy: 0.9555\n",
      "Epoch 446/800\n",
      "1665/1665 [==============================] - 0s 112us/step - loss: 0.0717 - accuracy: 0.9750 - val_loss: 0.1201 - val_accuracy: 0.9558\n",
      "Epoch 447/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0716 - accuracy: 0.9751 - val_loss: 0.1208 - val_accuracy: 0.9553\n",
      "Epoch 448/800\n",
      "1665/1665 [==============================] - 0s 139us/step - loss: 0.0719 - accuracy: 0.9747 - val_loss: 0.1218 - val_accuracy: 0.9558\n",
      "Epoch 449/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.0715 - accuracy: 0.9748 - val_loss: 0.1218 - val_accuracy: 0.9556\n",
      "Epoch 450/800\n",
      "1665/1665 [==============================] - 0s 151us/step - loss: 0.0714 - accuracy: 0.9751 - val_loss: 0.1192 - val_accuracy: 0.9573\n",
      "Epoch 451/800\n",
      "1665/1665 [==============================] - 0s 139us/step - loss: 0.0715 - accuracy: 0.9751 - val_loss: 0.1220 - val_accuracy: 0.9554\n",
      "Epoch 452/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0715 - accuracy: 0.9751 - val_loss: 0.1192 - val_accuracy: 0.9566\n",
      "Epoch 453/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.0712 - accuracy: 0.9751 - val_loss: 0.1200 - val_accuracy: 0.9558\n",
      "Epoch 454/800\n",
      "1665/1665 [==============================] - 0s 141us/step - loss: 0.0711 - accuracy: 0.9754 - val_loss: 0.1197 - val_accuracy: 0.9561\n",
      "Epoch 455/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0708 - accuracy: 0.9755 - val_loss: 0.1199 - val_accuracy: 0.9564\n",
      "Epoch 456/800\n",
      "1665/1665 [==============================] - 0s 142us/step - loss: 0.0710 - accuracy: 0.9751 - val_loss: 0.1189 - val_accuracy: 0.9554\n",
      "Epoch 457/800\n",
      "1665/1665 [==============================] - 0s 129us/step - loss: 0.0711 - accuracy: 0.9750 - val_loss: 0.1248 - val_accuracy: 0.9551\n",
      "Epoch 458/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0711 - accuracy: 0.9755 - val_loss: 0.1192 - val_accuracy: 0.9557\n",
      "Epoch 459/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0710 - accuracy: 0.9753 - val_loss: 0.1189 - val_accuracy: 0.9571\n",
      "Epoch 460/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0706 - accuracy: 0.9754 - val_loss: 0.1330 - val_accuracy: 0.9518\n",
      "Epoch 461/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0712 - accuracy: 0.9747 - val_loss: 0.1195 - val_accuracy: 0.9557\n",
      "Epoch 462/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0704 - accuracy: 0.9754 - val_loss: 0.1209 - val_accuracy: 0.9561\n",
      "Epoch 463/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0704 - accuracy: 0.9752 - val_loss: 0.1263 - val_accuracy: 0.9541\n",
      "Epoch 464/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.0708 - accuracy: 0.9750 - val_loss: 0.1456 - val_accuracy: 0.9483\n",
      "Epoch 465/800\n",
      "1665/1665 [==============================] - 0s 147us/step - loss: 0.0714 - accuracy: 0.9747 - val_loss: 0.1199 - val_accuracy: 0.9567\n",
      "Epoch 466/800\n",
      "1665/1665 [==============================] - 0s 142us/step - loss: 0.0703 - accuracy: 0.9754 - val_loss: 0.1203 - val_accuracy: 0.9560\n",
      "Epoch 467/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0702 - accuracy: 0.9754 - val_loss: 0.1191 - val_accuracy: 0.9560\n",
      "Epoch 468/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.0701 - accuracy: 0.9754 - val_loss: 0.1184 - val_accuracy: 0.9564\n",
      "Epoch 469/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0698 - accuracy: 0.9756 - val_loss: 0.1211 - val_accuracy: 0.9559\n",
      "Epoch 470/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0699 - accuracy: 0.9756 - val_loss: 0.1290 - val_accuracy: 0.9529\n",
      "Epoch 471/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0701 - accuracy: 0.9755 - val_loss: 0.1359 - val_accuracy: 0.9500\n",
      "Epoch 472/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0707 - accuracy: 0.9752 - val_loss: 0.1198 - val_accuracy: 0.9576\n",
      "Epoch 473/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0698 - accuracy: 0.9757 - val_loss: 0.1195 - val_accuracy: 0.9565\n",
      "Epoch 474/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0695 - accuracy: 0.9757 - val_loss: 0.1178 - val_accuracy: 0.9568\n",
      "Epoch 475/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.0693 - accuracy: 0.9758 - val_loss: 0.1608 - val_accuracy: 0.9429\n",
      "Epoch 476/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.0711 - accuracy: 0.9751 - val_loss: 0.1393 - val_accuracy: 0.9474\n",
      "Epoch 477/800\n",
      "1665/1665 [==============================] - 0s 112us/step - loss: 0.0705 - accuracy: 0.9751 - val_loss: 0.1203 - val_accuracy: 0.9554\n",
      "Epoch 478/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0693 - accuracy: 0.9758 - val_loss: 0.1164 - val_accuracy: 0.9564\n",
      "Epoch 479/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0695 - accuracy: 0.9756 - val_loss: 0.1173 - val_accuracy: 0.9564\n",
      "Epoch 480/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0692 - accuracy: 0.9755 - val_loss: 0.1177 - val_accuracy: 0.9572\n",
      "Epoch 481/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0690 - accuracy: 0.9758 - val_loss: 0.1183 - val_accuracy: 0.9568\n",
      "Epoch 482/800\n",
      "1665/1665 [==============================] - 0s 107us/step - loss: 0.0689 - accuracy: 0.9758 - val_loss: 0.1364 - val_accuracy: 0.9498\n",
      "Epoch 483/800\n",
      "1665/1665 [==============================] - 0s 112us/step - loss: 0.0699 - accuracy: 0.9753 - val_loss: 0.1173 - val_accuracy: 0.9564\n",
      "Epoch 484/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.0686 - accuracy: 0.9756 - val_loss: 0.1187 - val_accuracy: 0.9569\n",
      "Epoch 485/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0689 - accuracy: 0.9756 - val_loss: 0.1184 - val_accuracy: 0.9573\n",
      "Epoch 486/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0686 - accuracy: 0.9762 - val_loss: 0.1306 - val_accuracy: 0.9518\n",
      "Epoch 487/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.0692 - accuracy: 0.9755 - val_loss: 0.1167 - val_accuracy: 0.9572\n",
      "Epoch 488/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0683 - accuracy: 0.9761 - val_loss: 0.1171 - val_accuracy: 0.9567\n",
      "Epoch 489/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.0683 - accuracy: 0.9760 - val_loss: 0.1191 - val_accuracy: 0.9553\n",
      "Epoch 490/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.0682 - accuracy: 0.9762 - val_loss: 0.1165 - val_accuracy: 0.9571\n",
      "Epoch 491/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0680 - accuracy: 0.9760 - val_loss: 0.1164 - val_accuracy: 0.9563\n",
      "Epoch 492/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0684 - accuracy: 0.9758 - val_loss: 0.1176 - val_accuracy: 0.9570\n",
      "Epoch 493/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0679 - accuracy: 0.9763 - val_loss: 0.1193 - val_accuracy: 0.9555\n",
      "Epoch 494/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.0681 - accuracy: 0.9762 - val_loss: 0.1184 - val_accuracy: 0.9570\n",
      "Epoch 495/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0678 - accuracy: 0.9760 - val_loss: 0.1266 - val_accuracy: 0.9536\n",
      "Epoch 496/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0683 - accuracy: 0.9759 - val_loss: 0.1166 - val_accuracy: 0.9579\n",
      "Epoch 497/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0679 - accuracy: 0.9759 - val_loss: 0.1261 - val_accuracy: 0.9538\n",
      "Epoch 498/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.0683 - accuracy: 0.9761 - val_loss: 0.1168 - val_accuracy: 0.9575\n",
      "Epoch 499/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0677 - accuracy: 0.9761 - val_loss: 0.1178 - val_accuracy: 0.9568\n",
      "Epoch 500/800\n",
      "1665/1665 [==============================] - 0s 126us/step - loss: 0.0676 - accuracy: 0.9764 - val_loss: 0.1589 - val_accuracy: 0.9470\n",
      "Epoch 501/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.0691 - accuracy: 0.9756 - val_loss: 0.1402 - val_accuracy: 0.9499\n",
      "Epoch 502/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0687 - accuracy: 0.9755 - val_loss: 0.1167 - val_accuracy: 0.9570\n",
      "Epoch 503/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.0675 - accuracy: 0.9760 - val_loss: 0.1232 - val_accuracy: 0.9568\n",
      "Epoch 504/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0677 - accuracy: 0.9761 - val_loss: 0.1171 - val_accuracy: 0.9567\n",
      "Epoch 505/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0675 - accuracy: 0.9761 - val_loss: 0.1147 - val_accuracy: 0.9577\n",
      "Epoch 506/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0670 - accuracy: 0.9762 - val_loss: 0.1159 - val_accuracy: 0.9572\n",
      "Epoch 507/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0670 - accuracy: 0.9762 - val_loss: 0.1150 - val_accuracy: 0.9568\n",
      "Epoch 508/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0668 - accuracy: 0.9766 - val_loss: 0.1205 - val_accuracy: 0.9560\n",
      "Epoch 509/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0671 - accuracy: 0.9762 - val_loss: 0.1327 - val_accuracy: 0.9522\n",
      "Epoch 510/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0677 - accuracy: 0.9761 - val_loss: 0.1249 - val_accuracy: 0.9543\n",
      "Epoch 511/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0673 - accuracy: 0.9762 - val_loss: 0.1158 - val_accuracy: 0.9575\n",
      "Epoch 512/800\n",
      "1665/1665 [==============================] - 0s 140us/step - loss: 0.0666 - accuracy: 0.9764 - val_loss: 0.1167 - val_accuracy: 0.9571\n",
      "Epoch 513/800\n",
      "1665/1665 [==============================] - 0s 139us/step - loss: 0.0666 - accuracy: 0.9767 - val_loss: 0.1177 - val_accuracy: 0.9571\n",
      "Epoch 514/800\n",
      "1665/1665 [==============================] - 0s 145us/step - loss: 0.0668 - accuracy: 0.9766 - val_loss: 0.1166 - val_accuracy: 0.9573\n",
      "Epoch 515/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0665 - accuracy: 0.9764 - val_loss: 0.1177 - val_accuracy: 0.9575\n",
      "Epoch 516/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0665 - accuracy: 0.9766 - val_loss: 0.1257 - val_accuracy: 0.9547\n",
      "Epoch 517/800\n",
      "1665/1665 [==============================] - 0s 156us/step - loss: 0.0668 - accuracy: 0.9766 - val_loss: 0.1151 - val_accuracy: 0.9573\n",
      "Epoch 518/800\n",
      "1665/1665 [==============================] - 0s 162us/step - loss: 0.0661 - accuracy: 0.9764 - val_loss: 0.1138 - val_accuracy: 0.9584\n",
      "Epoch 519/800\n",
      "1665/1665 [==============================] - 0s 155us/step - loss: 0.0661 - accuracy: 0.9766 - val_loss: 0.1364 - val_accuracy: 0.9522\n",
      "Epoch 520/800\n",
      "1665/1665 [==============================] - 0s 143us/step - loss: 0.0668 - accuracy: 0.9764 - val_loss: 0.1167 - val_accuracy: 0.9574\n",
      "Epoch 521/800\n",
      "1665/1665 [==============================] - 0s 140us/step - loss: 0.0661 - accuracy: 0.9769 - val_loss: 0.1148 - val_accuracy: 0.9583\n",
      "Epoch 522/800\n",
      "1665/1665 [==============================] - 0s 141us/step - loss: 0.0658 - accuracy: 0.9767 - val_loss: 0.1262 - val_accuracy: 0.9548\n",
      "Epoch 523/800\n",
      "1665/1665 [==============================] - 0s 153us/step - loss: 0.0664 - accuracy: 0.9767 - val_loss: 0.1289 - val_accuracy: 0.9544\n",
      "Epoch 524/800\n",
      "1665/1665 [==============================] - 0s 142us/step - loss: 0.0665 - accuracy: 0.9762 - val_loss: 0.1165 - val_accuracy: 0.9567\n",
      "Epoch 525/800\n",
      "1665/1665 [==============================] - 0s 151us/step - loss: 0.0659 - accuracy: 0.9768 - val_loss: 0.1127 - val_accuracy: 0.9589\n",
      "Epoch 526/800\n",
      "1665/1665 [==============================] - 0s 158us/step - loss: 0.0656 - accuracy: 0.9769 - val_loss: 0.1156 - val_accuracy: 0.9570\n",
      "Epoch 527/800\n",
      "1665/1665 [==============================] - 0s 153us/step - loss: 0.0657 - accuracy: 0.9769 - val_loss: 0.1134 - val_accuracy: 0.9582\n",
      "Epoch 528/800\n",
      "1665/1665 [==============================] - 0s 152us/step - loss: 0.0655 - accuracy: 0.9770 - val_loss: 0.1156 - val_accuracy: 0.9574\n",
      "Epoch 529/800\n",
      "1665/1665 [==============================] - 0s 145us/step - loss: 0.0655 - accuracy: 0.9768 - val_loss: 0.1191 - val_accuracy: 0.9564\n",
      "Epoch 530/800\n",
      "1665/1665 [==============================] - 0s 139us/step - loss: 0.0657 - accuracy: 0.9768 - val_loss: 0.1127 - val_accuracy: 0.9584\n",
      "Epoch 531/800\n",
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.0653 - accuracy: 0.9771 - val_loss: 0.1535 - val_accuracy: 0.9424\n",
      "Epoch 532/800\n",
      "1665/1665 [==============================] - 0s 141us/step - loss: 0.0669 - accuracy: 0.9766 - val_loss: 0.1173 - val_accuracy: 0.9574\n",
      "Epoch 533/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0658 - accuracy: 0.9768 - val_loss: 0.1148 - val_accuracy: 0.9575\n",
      "Epoch 534/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0651 - accuracy: 0.9769 - val_loss: 0.1145 - val_accuracy: 0.9577\n",
      "Epoch 535/800\n",
      "1665/1665 [==============================] - 0s 112us/step - loss: 0.0651 - accuracy: 0.9769 - val_loss: 0.1143 - val_accuracy: 0.9579\n",
      "Epoch 536/800\n",
      "1665/1665 [==============================] - 0s 112us/step - loss: 0.0651 - accuracy: 0.9771 - val_loss: 0.1131 - val_accuracy: 0.9581\n",
      "Epoch 537/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0650 - accuracy: 0.9770 - val_loss: 0.1139 - val_accuracy: 0.9575\n",
      "Epoch 538/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0650 - accuracy: 0.9771 - val_loss: 0.1145 - val_accuracy: 0.9582\n",
      "Epoch 539/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.0649 - accuracy: 0.9772 - val_loss: 0.1134 - val_accuracy: 0.9580\n",
      "Epoch 540/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0647 - accuracy: 0.9770 - val_loss: 0.1135 - val_accuracy: 0.9586\n",
      "Epoch 541/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0646 - accuracy: 0.9770 - val_loss: 0.1133 - val_accuracy: 0.9587\n",
      "Epoch 542/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.0647 - accuracy: 0.9771 - val_loss: 0.1131 - val_accuracy: 0.9581\n",
      "Epoch 543/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.0645 - accuracy: 0.9770 - val_loss: 0.1148 - val_accuracy: 0.9583\n",
      "Epoch 544/800\n",
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.0644 - accuracy: 0.9770 - val_loss: 0.1140 - val_accuracy: 0.9575\n",
      "Epoch 545/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0644 - accuracy: 0.9772 - val_loss: 0.1127 - val_accuracy: 0.9584\n",
      "Epoch 546/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0644 - accuracy: 0.9771 - val_loss: 0.1175 - val_accuracy: 0.9573\n",
      "Epoch 547/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0646 - accuracy: 0.9770 - val_loss: 0.1393 - val_accuracy: 0.9514\n",
      "Epoch 548/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0655 - accuracy: 0.9769 - val_loss: 0.1140 - val_accuracy: 0.9582\n",
      "Epoch 549/800\n",
      "1665/1665 [==============================] - 0s 140us/step - loss: 0.0643 - accuracy: 0.9773 - val_loss: 0.1118 - val_accuracy: 0.9588\n",
      "Epoch 550/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1665/1665 [==============================] - 0s 141us/step - loss: 0.0641 - accuracy: 0.9772 - val_loss: 0.1125 - val_accuracy: 0.9585\n",
      "Epoch 551/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.0639 - accuracy: 0.9772 - val_loss: 0.1120 - val_accuracy: 0.9590\n",
      "Epoch 552/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.0639 - accuracy: 0.9773 - val_loss: 0.1477 - val_accuracy: 0.9496\n",
      "Epoch 553/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0658 - accuracy: 0.9769 - val_loss: 0.1117 - val_accuracy: 0.9595\n",
      "Epoch 554/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.0639 - accuracy: 0.9773 - val_loss: 0.1116 - val_accuracy: 0.9589\n",
      "Epoch 555/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.0637 - accuracy: 0.9772 - val_loss: 0.1154 - val_accuracy: 0.9592\n",
      "Epoch 556/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0638 - accuracy: 0.9774 - val_loss: 0.1117 - val_accuracy: 0.9586\n",
      "Epoch 557/800\n",
      "1665/1665 [==============================] - 0s 136us/step - loss: 0.0639 - accuracy: 0.9773 - val_loss: 0.1155 - val_accuracy: 0.9580\n",
      "Epoch 558/800\n",
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.0640 - accuracy: 0.9772 - val_loss: 0.1136 - val_accuracy: 0.9575\n",
      "Epoch 559/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.0637 - accuracy: 0.9773 - val_loss: 0.1136 - val_accuracy: 0.9593\n",
      "Epoch 560/800\n",
      "1665/1665 [==============================] - 0s 145us/step - loss: 0.0636 - accuracy: 0.9774 - val_loss: 0.1120 - val_accuracy: 0.9590\n",
      "Epoch 561/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0635 - accuracy: 0.9774 - val_loss: 0.1129 - val_accuracy: 0.9586\n",
      "Epoch 562/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0635 - accuracy: 0.9774 - val_loss: 0.1123 - val_accuracy: 0.9586\n",
      "Epoch 563/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0633 - accuracy: 0.9774 - val_loss: 0.1128 - val_accuracy: 0.9583\n",
      "Epoch 564/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0634 - accuracy: 0.9776 - val_loss: 0.1104 - val_accuracy: 0.9594\n",
      "Epoch 565/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0634 - accuracy: 0.9776 - val_loss: 0.1143 - val_accuracy: 0.9583\n",
      "Epoch 566/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0634 - accuracy: 0.9775 - val_loss: 0.1125 - val_accuracy: 0.9588\n",
      "Epoch 567/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0630 - accuracy: 0.9776 - val_loss: 0.1128 - val_accuracy: 0.9585\n",
      "Epoch 568/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.0630 - accuracy: 0.9774 - val_loss: 0.1111 - val_accuracy: 0.9594\n",
      "Epoch 569/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0629 - accuracy: 0.9776 - val_loss: 0.1111 - val_accuracy: 0.9588\n",
      "Epoch 570/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0629 - accuracy: 0.9775 - val_loss: 0.1118 - val_accuracy: 0.9586\n",
      "Epoch 571/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0631 - accuracy: 0.9774 - val_loss: 0.1115 - val_accuracy: 0.9596\n",
      "Epoch 572/800\n",
      "1665/1665 [==============================] - 0s 112us/step - loss: 0.0627 - accuracy: 0.9776 - val_loss: 0.1105 - val_accuracy: 0.9589\n",
      "Epoch 573/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0626 - accuracy: 0.9776 - val_loss: 0.1145 - val_accuracy: 0.9585\n",
      "Epoch 574/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0628 - accuracy: 0.9775 - val_loss: 0.1131 - val_accuracy: 0.9583\n",
      "Epoch 575/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0626 - accuracy: 0.9777 - val_loss: 0.1124 - val_accuracy: 0.9582\n",
      "Epoch 576/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.0625 - accuracy: 0.9776 - val_loss: 0.1117 - val_accuracy: 0.9591\n",
      "Epoch 577/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0626 - accuracy: 0.9774 - val_loss: 0.1112 - val_accuracy: 0.9594\n",
      "Epoch 578/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0627 - accuracy: 0.9776 - val_loss: 0.1098 - val_accuracy: 0.9600\n",
      "Epoch 579/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.0625 - accuracy: 0.9778 - val_loss: 0.1239 - val_accuracy: 0.9547\n",
      "Epoch 580/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.0630 - accuracy: 0.9773 - val_loss: 0.1130 - val_accuracy: 0.9583\n",
      "Epoch 581/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0623 - accuracy: 0.9778 - val_loss: 0.1258 - val_accuracy: 0.9529\n",
      "Epoch 582/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0633 - accuracy: 0.9772 - val_loss: 0.1121 - val_accuracy: 0.9596\n",
      "Epoch 583/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0624 - accuracy: 0.9778 - val_loss: 0.1114 - val_accuracy: 0.9588\n",
      "Epoch 584/800\n",
      "1665/1665 [==============================] - 0s 149us/step - loss: 0.0623 - accuracy: 0.9777 - val_loss: 0.1112 - val_accuracy: 0.9593\n",
      "Epoch 585/800\n",
      "1665/1665 [==============================] - 0s 165us/step - loss: 0.0624 - accuracy: 0.9775 - val_loss: 0.1117 - val_accuracy: 0.9595\n",
      "Epoch 586/800\n",
      "1665/1665 [==============================] - 0s 148us/step - loss: 0.0621 - accuracy: 0.9778 - val_loss: 0.1127 - val_accuracy: 0.9591\n",
      "Epoch 587/800\n",
      "1665/1665 [==============================] - 0s 151us/step - loss: 0.0622 - accuracy: 0.9776 - val_loss: 0.1117 - val_accuracy: 0.9588\n",
      "Epoch 588/800\n",
      "1665/1665 [==============================] - 0s 155us/step - loss: 0.0621 - accuracy: 0.9778 - val_loss: 0.1111 - val_accuracy: 0.9590\n",
      "Epoch 589/800\n",
      "1665/1665 [==============================] - 0s 158us/step - loss: 0.0619 - accuracy: 0.9778 - val_loss: 0.1112 - val_accuracy: 0.9590\n",
      "Epoch 590/800\n",
      "1665/1665 [==============================] - 0s 152us/step - loss: 0.0616 - accuracy: 0.9777 - val_loss: 0.1139 - val_accuracy: 0.9582\n",
      "Epoch 591/800\n",
      "1665/1665 [==============================] - 0s 149us/step - loss: 0.0620 - accuracy: 0.9777 - val_loss: 0.1123 - val_accuracy: 0.9593\n",
      "Epoch 592/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0618 - accuracy: 0.9779 - val_loss: 0.1277 - val_accuracy: 0.9544\n",
      "Epoch 593/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.0627 - accuracy: 0.9776 - val_loss: 0.1109 - val_accuracy: 0.9593\n",
      "Epoch 594/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0616 - accuracy: 0.9778 - val_loss: 0.1092 - val_accuracy: 0.9595\n",
      "Epoch 595/800\n",
      "1665/1665 [==============================] - 0s 139us/step - loss: 0.0614 - accuracy: 0.9781 - val_loss: 0.1122 - val_accuracy: 0.9593\n",
      "Epoch 596/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.0616 - accuracy: 0.9781 - val_loss: 0.1245 - val_accuracy: 0.9547\n",
      "Epoch 597/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.0624 - accuracy: 0.9778 - val_loss: 0.1127 - val_accuracy: 0.9585\n",
      "Epoch 598/800\n",
      "1665/1665 [==============================] - 0s 154us/step - loss: 0.0614 - accuracy: 0.9781 - val_loss: 0.1110 - val_accuracy: 0.9588\n",
      "Epoch 599/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.0614 - accuracy: 0.9780 - val_loss: 0.1104 - val_accuracy: 0.9588\n",
      "Epoch 600/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0612 - accuracy: 0.9781 - val_loss: 0.1113 - val_accuracy: 0.9595\n",
      "Epoch 601/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0611 - accuracy: 0.9781 - val_loss: 0.1080 - val_accuracy: 0.9613\n",
      "Epoch 602/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0611 - accuracy: 0.9780 - val_loss: 0.1151 - val_accuracy: 0.9586\n",
      "Epoch 603/800\n",
      "1665/1665 [==============================] - 0s 141us/step - loss: 0.0615 - accuracy: 0.9779 - val_loss: 0.1093 - val_accuracy: 0.9597\n",
      "Epoch 604/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.0609 - accuracy: 0.9780 - val_loss: 0.1100 - val_accuracy: 0.9596\n",
      "Epoch 605/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1665/1665 [==============================] - 0s 147us/step - loss: 0.0609 - accuracy: 0.9780 - val_loss: 0.1095 - val_accuracy: 0.9597\n",
      "Epoch 606/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0609 - accuracy: 0.9782 - val_loss: 0.1101 - val_accuracy: 0.9589\n",
      "Epoch 607/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.0608 - accuracy: 0.9782 - val_loss: 0.1109 - val_accuracy: 0.9582\n",
      "Epoch 608/800\n",
      "1665/1665 [==============================] - 0s 136us/step - loss: 0.0609 - accuracy: 0.9781 - val_loss: 0.1089 - val_accuracy: 0.9594\n",
      "Epoch 609/800\n",
      "1665/1665 [==============================] - 0s 142us/step - loss: 0.0606 - accuracy: 0.9783 - val_loss: 0.1108 - val_accuracy: 0.9588\n",
      "Epoch 610/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0608 - accuracy: 0.9782 - val_loss: 0.1099 - val_accuracy: 0.9588\n",
      "Epoch 611/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0606 - accuracy: 0.9781 - val_loss: 0.1119 - val_accuracy: 0.9599\n",
      "Epoch 612/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0608 - accuracy: 0.9782 - val_loss: 0.1093 - val_accuracy: 0.9597\n",
      "Epoch 613/800\n",
      "1665/1665 [==============================] - 0s 153us/step - loss: 0.0605 - accuracy: 0.9782 - val_loss: 0.1103 - val_accuracy: 0.9593\n",
      "Epoch 614/800\n",
      "1665/1665 [==============================] - 0s 141us/step - loss: 0.0605 - accuracy: 0.9784 - val_loss: 0.1118 - val_accuracy: 0.9577\n",
      "Epoch 615/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0606 - accuracy: 0.9782 - val_loss: 0.1081 - val_accuracy: 0.9604\n",
      "Epoch 616/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.0603 - accuracy: 0.9783 - val_loss: 0.1105 - val_accuracy: 0.9604\n",
      "Epoch 617/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0604 - accuracy: 0.9783 - val_loss: 0.1107 - val_accuracy: 0.9583\n",
      "Epoch 618/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0604 - accuracy: 0.9782 - val_loss: 0.1100 - val_accuracy: 0.9597\n",
      "Epoch 619/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0603 - accuracy: 0.9782 - val_loss: 0.1111 - val_accuracy: 0.9600\n",
      "Epoch 620/800\n",
      "1665/1665 [==============================] - 0s 146us/step - loss: 0.0604 - accuracy: 0.9781 - val_loss: 0.1095 - val_accuracy: 0.9592\n",
      "Epoch 621/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.0602 - accuracy: 0.9785 - val_loss: 0.1265 - val_accuracy: 0.9532\n",
      "Epoch 622/800\n",
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.0612 - accuracy: 0.9781 - val_loss: 0.1101 - val_accuracy: 0.9599\n",
      "Epoch 623/800\n",
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.0602 - accuracy: 0.9783 - val_loss: 0.1103 - val_accuracy: 0.9590\n",
      "Epoch 624/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0599 - accuracy: 0.9784 - val_loss: 0.1124 - val_accuracy: 0.9591\n",
      "Epoch 625/800\n",
      "1665/1665 [==============================] - 0s 126us/step - loss: 0.0602 - accuracy: 0.9780 - val_loss: 0.1098 - val_accuracy: 0.9595\n",
      "Epoch 626/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.0600 - accuracy: 0.9781 - val_loss: 0.1099 - val_accuracy: 0.9595\n",
      "Epoch 627/800\n",
      "1665/1665 [==============================] - 0s 136us/step - loss: 0.0601 - accuracy: 0.9785 - val_loss: 0.1090 - val_accuracy: 0.9609\n",
      "Epoch 628/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0598 - accuracy: 0.9784 - val_loss: 0.1108 - val_accuracy: 0.9597\n",
      "Epoch 629/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0600 - accuracy: 0.9782 - val_loss: 0.1232 - val_accuracy: 0.9547\n",
      "Epoch 630/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.0605 - accuracy: 0.9781 - val_loss: 0.1092 - val_accuracy: 0.9597\n",
      "Epoch 631/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0598 - accuracy: 0.9784 - val_loss: 0.1106 - val_accuracy: 0.9588\n",
      "Epoch 632/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0597 - accuracy: 0.9784 - val_loss: 0.1100 - val_accuracy: 0.9593\n",
      "Epoch 633/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0596 - accuracy: 0.9782 - val_loss: 0.1112 - val_accuracy: 0.9591\n",
      "Epoch 634/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0597 - accuracy: 0.9783 - val_loss: 0.1092 - val_accuracy: 0.9593\n",
      "Epoch 635/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0595 - accuracy: 0.9785 - val_loss: 0.1255 - val_accuracy: 0.9538\n",
      "Epoch 636/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0606 - accuracy: 0.9781 - val_loss: 0.1129 - val_accuracy: 0.9591\n",
      "Epoch 637/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0597 - accuracy: 0.9784 - val_loss: 0.1063 - val_accuracy: 0.9606\n",
      "Epoch 638/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0595 - accuracy: 0.9785 - val_loss: 0.1094 - val_accuracy: 0.9592\n",
      "Epoch 639/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0593 - accuracy: 0.9786 - val_loss: 0.1125 - val_accuracy: 0.9593\n",
      "Epoch 640/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.0593 - accuracy: 0.9786 - val_loss: 0.1101 - val_accuracy: 0.9605\n",
      "Epoch 641/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0594 - accuracy: 0.9784 - val_loss: 0.1084 - val_accuracy: 0.9599\n",
      "Epoch 642/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0591 - accuracy: 0.9786 - val_loss: 0.1074 - val_accuracy: 0.9606\n",
      "Epoch 643/800\n",
      "1665/1665 [==============================] - 0s 126us/step - loss: 0.0588 - accuracy: 0.9788 - val_loss: 0.1074 - val_accuracy: 0.9607\n",
      "Epoch 644/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0589 - accuracy: 0.9788 - val_loss: 0.1143 - val_accuracy: 0.9587\n",
      "Epoch 645/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.0594 - accuracy: 0.9783 - val_loss: 0.1132 - val_accuracy: 0.9604\n",
      "Epoch 646/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.0591 - accuracy: 0.9787 - val_loss: 0.1082 - val_accuracy: 0.9604\n",
      "Epoch 647/800\n",
      "1665/1665 [==============================] - 0s 140us/step - loss: 0.0588 - accuracy: 0.9786 - val_loss: 0.1073 - val_accuracy: 0.9604\n",
      "Epoch 648/800\n",
      "1665/1665 [==============================] - 0s 152us/step - loss: 0.0586 - accuracy: 0.9787 - val_loss: 0.1075 - val_accuracy: 0.9606\n",
      "Epoch 649/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.0587 - accuracy: 0.9786 - val_loss: 0.1057 - val_accuracy: 0.9606\n",
      "Epoch 650/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0589 - accuracy: 0.9786 - val_loss: 0.1083 - val_accuracy: 0.9612\n",
      "Epoch 651/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.0586 - accuracy: 0.9784 - val_loss: 0.1092 - val_accuracy: 0.9591\n",
      "Epoch 652/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0588 - accuracy: 0.9785 - val_loss: 0.1087 - val_accuracy: 0.9599\n",
      "Epoch 653/800\n",
      "1665/1665 [==============================] - 0s 126us/step - loss: 0.0587 - accuracy: 0.9788 - val_loss: 0.1074 - val_accuracy: 0.9607\n",
      "Epoch 654/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0583 - accuracy: 0.9789 - val_loss: 0.1120 - val_accuracy: 0.9577\n",
      "Epoch 655/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.0593 - accuracy: 0.9784 - val_loss: 0.1079 - val_accuracy: 0.9606\n",
      "Epoch 656/800\n",
      "1665/1665 [==============================] - 0s 145us/step - loss: 0.0585 - accuracy: 0.9788 - val_loss: 0.1094 - val_accuracy: 0.9604\n",
      "Epoch 657/800\n",
      "1665/1665 [==============================] - 0s 140us/step - loss: 0.0585 - accuracy: 0.9788 - val_loss: 0.1087 - val_accuracy: 0.9606\n",
      "Epoch 658/800\n",
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.0585 - accuracy: 0.9788 - val_loss: 0.1076 - val_accuracy: 0.9601\n",
      "Epoch 659/800\n",
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.0583 - accuracy: 0.9790 - val_loss: 0.1186 - val_accuracy: 0.9583\n",
      "Epoch 660/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1665/1665 [==============================] - 0s 140us/step - loss: 0.0593 - accuracy: 0.9783 - val_loss: 0.1259 - val_accuracy: 0.9545\n",
      "Epoch 661/800\n",
      "1665/1665 [==============================] - 0s 136us/step - loss: 0.0595 - accuracy: 0.9787 - val_loss: 0.1053 - val_accuracy: 0.9611\n",
      "Epoch 662/800\n",
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.0582 - accuracy: 0.9789 - val_loss: 0.1092 - val_accuracy: 0.9605\n",
      "Epoch 663/800\n",
      "1665/1665 [==============================] - 0s 139us/step - loss: 0.0582 - accuracy: 0.9788 - val_loss: 0.1094 - val_accuracy: 0.9598\n",
      "Epoch 664/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.0581 - accuracy: 0.9791 - val_loss: 0.1056 - val_accuracy: 0.9607\n",
      "Epoch 665/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.0580 - accuracy: 0.9788 - val_loss: 0.1058 - val_accuracy: 0.9613\n",
      "Epoch 666/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0579 - accuracy: 0.9791 - val_loss: 0.1056 - val_accuracy: 0.9609\n",
      "Epoch 667/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.0581 - accuracy: 0.9788 - val_loss: 0.1300 - val_accuracy: 0.9522\n",
      "Epoch 668/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0589 - accuracy: 0.9787 - val_loss: 0.1088 - val_accuracy: 0.9611\n",
      "Epoch 669/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0578 - accuracy: 0.9789 - val_loss: 0.1356 - val_accuracy: 0.9499\n",
      "Epoch 670/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0595 - accuracy: 0.9785 - val_loss: 0.1073 - val_accuracy: 0.9615\n",
      "Epoch 671/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0581 - accuracy: 0.9787 - val_loss: 0.1087 - val_accuracy: 0.9605\n",
      "Epoch 672/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0579 - accuracy: 0.9789 - val_loss: 0.1088 - val_accuracy: 0.9611\n",
      "Epoch 673/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0574 - accuracy: 0.9793 - val_loss: 0.1052 - val_accuracy: 0.9617\n",
      "Epoch 674/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0577 - accuracy: 0.9792 - val_loss: 0.1077 - val_accuracy: 0.9601\n",
      "Epoch 675/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0577 - accuracy: 0.9790 - val_loss: 0.1064 - val_accuracy: 0.9612\n",
      "Epoch 676/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0573 - accuracy: 0.9792 - val_loss: 0.1081 - val_accuracy: 0.9603\n",
      "Epoch 677/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.0577 - accuracy: 0.9790 - val_loss: 0.1088 - val_accuracy: 0.9603\n",
      "Epoch 678/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0574 - accuracy: 0.9791 - val_loss: 0.1115 - val_accuracy: 0.9604\n",
      "Epoch 679/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0575 - accuracy: 0.9790 - val_loss: 0.1041 - val_accuracy: 0.9605\n",
      "Epoch 680/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0576 - accuracy: 0.9790 - val_loss: 0.1467 - val_accuracy: 0.9494\n",
      "Epoch 681/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.0594 - accuracy: 0.9785 - val_loss: 0.1155 - val_accuracy: 0.9585\n",
      "Epoch 682/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0576 - accuracy: 0.9790 - val_loss: 0.1075 - val_accuracy: 0.9607\n",
      "Epoch 683/800\n",
      "1665/1665 [==============================] - 0s 126us/step - loss: 0.0574 - accuracy: 0.9790 - val_loss: 0.1061 - val_accuracy: 0.9616\n",
      "Epoch 684/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.0570 - accuracy: 0.9793 - val_loss: 0.1126 - val_accuracy: 0.9588\n",
      "Epoch 685/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0575 - accuracy: 0.9791 - val_loss: 0.1062 - val_accuracy: 0.9615\n",
      "Epoch 686/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0572 - accuracy: 0.9792 - val_loss: 0.1068 - val_accuracy: 0.9606\n",
      "Epoch 687/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0570 - accuracy: 0.9791 - val_loss: 0.1073 - val_accuracy: 0.9616\n",
      "Epoch 688/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0569 - accuracy: 0.9794 - val_loss: 0.1053 - val_accuracy: 0.9610\n",
      "Epoch 689/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0569 - accuracy: 0.9794 - val_loss: 0.1099 - val_accuracy: 0.9599\n",
      "Epoch 690/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0572 - accuracy: 0.9792 - val_loss: 0.1272 - val_accuracy: 0.9544\n",
      "Epoch 691/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0579 - accuracy: 0.9789 - val_loss: 0.1088 - val_accuracy: 0.9603\n",
      "Epoch 692/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0570 - accuracy: 0.9793 - val_loss: 0.1153 - val_accuracy: 0.9580\n",
      "Epoch 693/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0571 - accuracy: 0.9789 - val_loss: 0.1061 - val_accuracy: 0.9605\n",
      "Epoch 694/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0565 - accuracy: 0.9793 - val_loss: 0.1044 - val_accuracy: 0.9611\n",
      "Epoch 695/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0565 - accuracy: 0.9794 - val_loss: 0.1041 - val_accuracy: 0.9623\n",
      "Epoch 696/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0564 - accuracy: 0.9794 - val_loss: 0.1087 - val_accuracy: 0.9606\n",
      "Epoch 697/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0566 - accuracy: 0.9792 - val_loss: 0.1066 - val_accuracy: 0.9612\n",
      "Epoch 698/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0565 - accuracy: 0.9791 - val_loss: 0.1387 - val_accuracy: 0.9520\n",
      "Epoch 699/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.0581 - accuracy: 0.9789 - val_loss: 0.1058 - val_accuracy: 0.9612\n",
      "Epoch 700/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0567 - accuracy: 0.9794 - val_loss: 0.1057 - val_accuracy: 0.9614\n",
      "Epoch 701/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0562 - accuracy: 0.9795 - val_loss: 0.1047 - val_accuracy: 0.9619\n",
      "Epoch 702/800\n",
      "1665/1665 [==============================] - 0s 141us/step - loss: 0.0561 - accuracy: 0.9795 - val_loss: 0.1061 - val_accuracy: 0.9620\n",
      "Epoch 703/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.0561 - accuracy: 0.9795 - val_loss: 0.1059 - val_accuracy: 0.9620\n",
      "Epoch 704/800\n",
      "1665/1665 [==============================] - 0s 148us/step - loss: 0.0560 - accuracy: 0.9794 - val_loss: 0.1049 - val_accuracy: 0.9613\n",
      "Epoch 705/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0563 - accuracy: 0.9793 - val_loss: 0.1089 - val_accuracy: 0.9609\n",
      "Epoch 706/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0564 - accuracy: 0.9793 - val_loss: 0.1305 - val_accuracy: 0.9536\n",
      "Epoch 707/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0573 - accuracy: 0.9788 - val_loss: 0.1048 - val_accuracy: 0.9620\n",
      "Epoch 708/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0559 - accuracy: 0.9794 - val_loss: 0.1038 - val_accuracy: 0.9613\n",
      "Epoch 709/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0558 - accuracy: 0.9796 - val_loss: 0.1062 - val_accuracy: 0.9606\n",
      "Epoch 710/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0560 - accuracy: 0.9795 - val_loss: 0.1080 - val_accuracy: 0.9607\n",
      "Epoch 711/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0563 - accuracy: 0.9794 - val_loss: 0.1048 - val_accuracy: 0.9627\n",
      "Epoch 712/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0560 - accuracy: 0.9797 - val_loss: 0.1127 - val_accuracy: 0.9596\n",
      "Epoch 713/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0563 - accuracy: 0.9794 - val_loss: 0.1082 - val_accuracy: 0.9602\n",
      "Epoch 714/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0558 - accuracy: 0.9794 - val_loss: 0.1086 - val_accuracy: 0.9612\n",
      "Epoch 715/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0560 - accuracy: 0.9795 - val_loss: 0.1071 - val_accuracy: 0.9614\n",
      "Epoch 716/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0559 - accuracy: 0.9795 - val_loss: 0.1038 - val_accuracy: 0.9617\n",
      "Epoch 717/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0554 - accuracy: 0.9795 - val_loss: 0.1064 - val_accuracy: 0.9612\n",
      "Epoch 718/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0556 - accuracy: 0.9797 - val_loss: 0.1059 - val_accuracy: 0.9616\n",
      "Epoch 719/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0558 - accuracy: 0.9794 - val_loss: 0.1048 - val_accuracy: 0.9620\n",
      "Epoch 720/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0555 - accuracy: 0.9798 - val_loss: 0.1057 - val_accuracy: 0.9617\n",
      "Epoch 721/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0554 - accuracy: 0.9797 - val_loss: 0.1046 - val_accuracy: 0.9616\n",
      "Epoch 722/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0556 - accuracy: 0.9795 - val_loss: 0.1042 - val_accuracy: 0.9623\n",
      "Epoch 723/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0553 - accuracy: 0.9799 - val_loss: 0.1045 - val_accuracy: 0.9621\n",
      "Epoch 724/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0554 - accuracy: 0.9798 - val_loss: 0.1054 - val_accuracy: 0.9621\n",
      "Epoch 725/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0552 - accuracy: 0.9799 - val_loss: 0.1040 - val_accuracy: 0.9627\n",
      "Epoch 726/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0550 - accuracy: 0.9798 - val_loss: 0.1043 - val_accuracy: 0.9613\n",
      "Epoch 727/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0551 - accuracy: 0.9798 - val_loss: 0.1043 - val_accuracy: 0.9629\n",
      "Epoch 728/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0553 - accuracy: 0.9797 - val_loss: 0.1059 - val_accuracy: 0.9617\n",
      "Epoch 729/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0551 - accuracy: 0.9796 - val_loss: 0.1052 - val_accuracy: 0.9611\n",
      "Epoch 730/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0552 - accuracy: 0.9798 - val_loss: 0.1070 - val_accuracy: 0.9613\n",
      "Epoch 731/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0551 - accuracy: 0.9799 - val_loss: 0.1398 - val_accuracy: 0.9505\n",
      "Epoch 732/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.0571 - accuracy: 0.9792 - val_loss: 0.1071 - val_accuracy: 0.9607\n",
      "Epoch 733/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.0552 - accuracy: 0.9801 - val_loss: 0.1036 - val_accuracy: 0.9623\n",
      "Epoch 734/800\n",
      "1665/1665 [==============================] - 0s 129us/step - loss: 0.0548 - accuracy: 0.9799 - val_loss: 0.1157 - val_accuracy: 0.9575\n",
      "Epoch 735/800\n",
      "1665/1665 [==============================] - 0s 133us/step - loss: 0.0555 - accuracy: 0.9795 - val_loss: 0.1067 - val_accuracy: 0.9607\n",
      "Epoch 736/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0548 - accuracy: 0.9799 - val_loss: 0.1027 - val_accuracy: 0.9624\n",
      "Epoch 737/800\n",
      "1665/1665 [==============================] - 0s 133us/step - loss: 0.0548 - accuracy: 0.9799 - val_loss: 0.1048 - val_accuracy: 0.9626\n",
      "Epoch 738/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.0546 - accuracy: 0.9798 - val_loss: 0.1049 - val_accuracy: 0.9617\n",
      "Epoch 739/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0547 - accuracy: 0.9798 - val_loss: 0.1032 - val_accuracy: 0.9626\n",
      "Epoch 740/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.0544 - accuracy: 0.9803 - val_loss: 0.1146 - val_accuracy: 0.9587\n",
      "Epoch 741/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0552 - accuracy: 0.9794 - val_loss: 0.1041 - val_accuracy: 0.9613\n",
      "Epoch 742/800\n",
      "1665/1665 [==============================] - 0s 126us/step - loss: 0.0547 - accuracy: 0.9799 - val_loss: 0.1021 - val_accuracy: 0.9627\n",
      "Epoch 743/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0544 - accuracy: 0.9799 - val_loss: 0.1151 - val_accuracy: 0.9592\n",
      "Epoch 744/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0551 - accuracy: 0.9796 - val_loss: 0.1037 - val_accuracy: 0.9619\n",
      "Epoch 745/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0544 - accuracy: 0.9801 - val_loss: 0.1438 - val_accuracy: 0.9507\n",
      "Epoch 746/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0569 - accuracy: 0.9788 - val_loss: 0.1058 - val_accuracy: 0.9615\n",
      "Epoch 747/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0544 - accuracy: 0.9799 - val_loss: 0.1063 - val_accuracy: 0.9616\n",
      "Epoch 748/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0542 - accuracy: 0.9802 - val_loss: 0.1215 - val_accuracy: 0.9562\n",
      "Epoch 749/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0558 - accuracy: 0.9793 - val_loss: 0.1042 - val_accuracy: 0.9633\n",
      "Epoch 750/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0544 - accuracy: 0.9799 - val_loss: 0.1031 - val_accuracy: 0.9623\n",
      "Epoch 751/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0542 - accuracy: 0.9800 - val_loss: 0.1196 - val_accuracy: 0.9563\n",
      "Epoch 752/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.0553 - accuracy: 0.9796 - val_loss: 0.1061 - val_accuracy: 0.9616\n",
      "Epoch 753/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0543 - accuracy: 0.9798 - val_loss: 0.1469 - val_accuracy: 0.9513\n",
      "Epoch 754/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0561 - accuracy: 0.9793 - val_loss: 0.1067 - val_accuracy: 0.9618\n",
      "Epoch 755/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0541 - accuracy: 0.9797 - val_loss: 0.1060 - val_accuracy: 0.9632\n",
      "Epoch 756/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0543 - accuracy: 0.9800 - val_loss: 0.1039 - val_accuracy: 0.9627\n",
      "Epoch 757/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0540 - accuracy: 0.9800 - val_loss: 0.1042 - val_accuracy: 0.9620\n",
      "Epoch 758/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0537 - accuracy: 0.9802 - val_loss: 0.1054 - val_accuracy: 0.9613\n",
      "Epoch 759/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0540 - accuracy: 0.9799 - val_loss: 0.1041 - val_accuracy: 0.9623\n",
      "Epoch 760/800\n",
      "1665/1665 [==============================] - 0s 126us/step - loss: 0.0537 - accuracy: 0.9802 - val_loss: 0.1053 - val_accuracy: 0.9626\n",
      "Epoch 761/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0537 - accuracy: 0.9802 - val_loss: 0.1069 - val_accuracy: 0.9608\n",
      "Epoch 762/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0539 - accuracy: 0.9797 - val_loss: 0.1063 - val_accuracy: 0.9606\n",
      "Epoch 763/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.0540 - accuracy: 0.9797 - val_loss: 0.1059 - val_accuracy: 0.9620\n",
      "Epoch 764/800\n",
      "1665/1665 [==============================] - 0s 159us/step - loss: 0.0537 - accuracy: 0.9798 - val_loss: 0.1036 - val_accuracy: 0.9621\n",
      "Epoch 765/800\n",
      "1665/1665 [==============================] - 0s 136us/step - loss: 0.0534 - accuracy: 0.9803 - val_loss: 0.1030 - val_accuracy: 0.9621\n",
      "Epoch 766/800\n",
      "1665/1665 [==============================] - 0s 172us/step - loss: 0.0535 - accuracy: 0.9802 - val_loss: 0.1245 - val_accuracy: 0.9550\n",
      "Epoch 767/800\n",
      "1665/1665 [==============================] - 0s 175us/step - loss: 0.0545 - accuracy: 0.9800 - val_loss: 0.1058 - val_accuracy: 0.9628\n",
      "Epoch 768/800\n",
      "1665/1665 [==============================] - 0s 144us/step - loss: 0.0538 - accuracy: 0.9799 - val_loss: 0.1051 - val_accuracy: 0.9624\n",
      "Epoch 769/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.0536 - accuracy: 0.9803 - val_loss: 0.1030 - val_accuracy: 0.9623\n",
      "Epoch 770/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0532 - accuracy: 0.9800 - val_loss: 0.1032 - val_accuracy: 0.9627\n",
      "Epoch 771/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.0532 - accuracy: 0.9803 - val_loss: 0.1050 - val_accuracy: 0.9628\n",
      "Epoch 772/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0536 - accuracy: 0.9799 - val_loss: 0.1012 - val_accuracy: 0.9631\n",
      "Epoch 773/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0532 - accuracy: 0.9802 - val_loss: 0.1039 - val_accuracy: 0.9626\n",
      "Epoch 774/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0530 - accuracy: 0.9805 - val_loss: 0.1027 - val_accuracy: 0.9629\n",
      "Epoch 775/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0531 - accuracy: 0.9804 - val_loss: 0.1023 - val_accuracy: 0.9630\n",
      "Epoch 776/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0530 - accuracy: 0.9802 - val_loss: 0.1041 - val_accuracy: 0.9622\n",
      "Epoch 777/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0531 - accuracy: 0.9801 - val_loss: 0.1044 - val_accuracy: 0.9623\n",
      "Epoch 778/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0529 - accuracy: 0.9805 - val_loss: 0.1021 - val_accuracy: 0.9626\n",
      "Epoch 779/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0531 - accuracy: 0.9804 - val_loss: 0.1041 - val_accuracy: 0.9617\n",
      "Epoch 780/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0528 - accuracy: 0.9805 - val_loss: 0.1397 - val_accuracy: 0.9526\n",
      "Epoch 781/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0547 - accuracy: 0.9798 - val_loss: 0.1003 - val_accuracy: 0.9632\n",
      "Epoch 782/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0529 - accuracy: 0.9801 - val_loss: 0.1016 - val_accuracy: 0.9630\n",
      "Epoch 783/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0526 - accuracy: 0.9804 - val_loss: 0.1007 - val_accuracy: 0.9635\n",
      "Epoch 784/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0527 - accuracy: 0.9803 - val_loss: 0.1047 - val_accuracy: 0.9631\n",
      "Epoch 785/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0527 - accuracy: 0.9803 - val_loss: 0.1053 - val_accuracy: 0.9612\n",
      "Epoch 786/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0530 - accuracy: 0.9803 - val_loss: 0.1077 - val_accuracy: 0.9613\n",
      "Epoch 787/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0530 - accuracy: 0.9802 - val_loss: 0.1045 - val_accuracy: 0.9620\n",
      "Epoch 788/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0527 - accuracy: 0.9801 - val_loss: 0.1048 - val_accuracy: 0.9620\n",
      "Epoch 789/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0529 - accuracy: 0.9802 - val_loss: 0.1028 - val_accuracy: 0.9630\n",
      "Epoch 790/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0524 - accuracy: 0.9804 - val_loss: 0.1029 - val_accuracy: 0.9628\n",
      "Epoch 791/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0524 - accuracy: 0.9805 - val_loss: 0.1024 - val_accuracy: 0.9630\n",
      "Epoch 792/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0523 - accuracy: 0.9805 - val_loss: 0.1038 - val_accuracy: 0.9621\n",
      "Epoch 793/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0524 - accuracy: 0.9805 - val_loss: 0.1061 - val_accuracy: 0.9616\n",
      "Epoch 794/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0527 - accuracy: 0.9803 - val_loss: 0.1127 - val_accuracy: 0.9595\n",
      "Epoch 795/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0528 - accuracy: 0.9803 - val_loss: 0.1073 - val_accuracy: 0.9626\n",
      "Epoch 796/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0527 - accuracy: 0.9804 - val_loss: 0.1039 - val_accuracy: 0.9620\n",
      "Epoch 797/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0523 - accuracy: 0.9804 - val_loss: 0.1382 - val_accuracy: 0.9535\n",
      "Epoch 798/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0542 - accuracy: 0.9797 - val_loss: 0.1016 - val_accuracy: 0.9628\n",
      "Epoch 799/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0521 - accuracy: 0.9803 - val_loss: 0.1048 - val_accuracy: 0.9625\n",
      "Epoch 800/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0525 - accuracy: 0.9803 - val_loss: 0.1070 - val_accuracy: 0.9605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clair\\Anaconda3\\envs\\CNNs\\lib\\site-packages\\ipykernel_launcher.py:125: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "C:\\Users\\clair\\Anaconda3\\envs\\CNNs\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:808: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Train on 1665 samples, validate on 556 samples\n",
      "Epoch 1/800\n",
      "1665/1665 [==============================] - 0s 192us/step - loss: 0.6526 - accuracy: 0.7601 - val_loss: 0.5980 - val_accuracy: 0.8752\n",
      "Epoch 2/800\n",
      "1665/1665 [==============================] - 0s 106us/step - loss: 0.4028 - accuracy: 0.9251 - val_loss: 0.2653 - val_accuracy: 0.9325\n",
      "Epoch 3/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.1934 - accuracy: 0.9537 - val_loss: 0.2521 - val_accuracy: 0.9325\n",
      "Epoch 4/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.1874 - accuracy: 0.9537 - val_loss: 0.2509 - val_accuracy: 0.9325\n",
      "Epoch 5/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.1865 - accuracy: 0.9537 - val_loss: 0.2507 - val_accuracy: 0.9325\n",
      "Epoch 6/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.1861 - accuracy: 0.9537 - val_loss: 0.2502 - val_accuracy: 0.9325\n",
      "Epoch 7/800\n",
      "1665/1665 [==============================] - 0s 133us/step - loss: 0.1860 - accuracy: 0.9537 - val_loss: 0.2499 - val_accuracy: 0.9325\n",
      "Epoch 8/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.1859 - accuracy: 0.9537 - val_loss: 0.2497 - val_accuracy: 0.9325\n",
      "Epoch 9/800\n",
      "1665/1665 [==============================] - 0s 129us/step - loss: 0.1858 - accuracy: 0.9537 - val_loss: 0.2498 - val_accuracy: 0.9325\n",
      "Epoch 10/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.1858 - accuracy: 0.9537 - val_loss: 0.2497 - val_accuracy: 0.9325\n",
      "Epoch 11/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.1857 - accuracy: 0.9537 - val_loss: 0.2486 - val_accuracy: 0.9325\n",
      "Epoch 12/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.1856 - accuracy: 0.9537 - val_loss: 0.2485 - val_accuracy: 0.9325\n",
      "Epoch 13/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.1856 - accuracy: 0.9537 - val_loss: 0.2495 - val_accuracy: 0.9325\n",
      "Epoch 14/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.1855 - accuracy: 0.9537 - val_loss: 0.2487 - val_accuracy: 0.9325\n",
      "Epoch 15/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.1854 - accuracy: 0.9537 - val_loss: 0.2488 - val_accuracy: 0.9325\n",
      "Epoch 16/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1854 - accuracy: 0.9537 - val_loss: 0.2493 - val_accuracy: 0.9325\n",
      "Epoch 17/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.1853 - accuracy: 0.9537 - val_loss: 0.2487 - val_accuracy: 0.9325\n",
      "Epoch 18/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.1852 - accuracy: 0.9537 - val_loss: 0.2481 - val_accuracy: 0.9325\n",
      "Epoch 19/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.1852 - accuracy: 0.9537 - val_loss: 0.2491 - val_accuracy: 0.9325\n",
      "Epoch 20/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.1851 - accuracy: 0.9537 - val_loss: 0.2484 - val_accuracy: 0.9325\n",
      "Epoch 21/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.1850 - accuracy: 0.9537 - val_loss: 0.2486 - val_accuracy: 0.9325\n",
      "Epoch 22/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.1849 - accuracy: 0.9537 - val_loss: 0.2488 - val_accuracy: 0.9325\n",
      "Epoch 23/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.1849 - accuracy: 0.9537 - val_loss: 0.2490 - val_accuracy: 0.9325\n",
      "Epoch 24/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.1849 - accuracy: 0.9537 - val_loss: 0.2486 - val_accuracy: 0.9325\n",
      "Epoch 25/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.1848 - accuracy: 0.9537 - val_loss: 0.2490 - val_accuracy: 0.9325\n",
      "Epoch 26/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1847 - accuracy: 0.9537 - val_loss: 0.2482 - val_accuracy: 0.9325\n",
      "Epoch 27/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.1847 - accuracy: 0.9537 - val_loss: 0.2481 - val_accuracy: 0.9325\n",
      "Epoch 28/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1846 - accuracy: 0.9537 - val_loss: 0.2481 - val_accuracy: 0.9325\n",
      "Epoch 29/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1846 - accuracy: 0.9537 - val_loss: 0.2478 - val_accuracy: 0.9325\n",
      "Epoch 30/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1845 - accuracy: 0.9537 - val_loss: 0.2474 - val_accuracy: 0.9325\n",
      "Epoch 31/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.1844 - accuracy: 0.9537 - val_loss: 0.2481 - val_accuracy: 0.9325\n",
      "Epoch 32/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.1844 - accuracy: 0.9537 - val_loss: 0.2475 - val_accuracy: 0.9325\n",
      "Epoch 33/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1843 - accuracy: 0.9537 - val_loss: 0.2479 - val_accuracy: 0.9325\n",
      "Epoch 34/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.1842 - accuracy: 0.9537 - val_loss: 0.2472 - val_accuracy: 0.9325\n",
      "Epoch 35/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1841 - accuracy: 0.9537 - val_loss: 0.2475 - val_accuracy: 0.9325\n",
      "Epoch 36/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1841 - accuracy: 0.9537 - val_loss: 0.2477 - val_accuracy: 0.9325\n",
      "Epoch 37/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1840 - accuracy: 0.9537 - val_loss: 0.2472 - val_accuracy: 0.9325\n",
      "Epoch 38/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1839 - accuracy: 0.9537 - val_loss: 0.2467 - val_accuracy: 0.9325\n",
      "Epoch 39/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1838 - accuracy: 0.9537 - val_loss: 0.2466 - val_accuracy: 0.9325\n",
      "Epoch 40/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1838 - accuracy: 0.9537 - val_loss: 0.2470 - val_accuracy: 0.9325\n",
      "Epoch 41/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.1837 - accuracy: 0.9537 - val_loss: 0.2470 - val_accuracy: 0.9325\n",
      "Epoch 42/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.1836 - accuracy: 0.9537 - val_loss: 0.2459 - val_accuracy: 0.9325\n",
      "Epoch 43/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1835 - accuracy: 0.9537 - val_loss: 0.2463 - val_accuracy: 0.9325\n",
      "Epoch 44/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1835 - accuracy: 0.9537 - val_loss: 0.2462 - val_accuracy: 0.9325\n",
      "Epoch 45/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1834 - accuracy: 0.9537 - val_loss: 0.2466 - val_accuracy: 0.9325\n",
      "Epoch 46/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.1832 - accuracy: 0.9537 - val_loss: 0.2462 - val_accuracy: 0.9325\n",
      "Epoch 47/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.1832 - accuracy: 0.9537 - val_loss: 0.2461 - val_accuracy: 0.9325\n",
      "Epoch 48/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1832 - accuracy: 0.9537 - val_loss: 0.2458 - val_accuracy: 0.9325\n",
      "Epoch 49/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.1830 - accuracy: 0.9537 - val_loss: 0.2455 - val_accuracy: 0.9325\n",
      "Epoch 50/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1829 - accuracy: 0.9537 - val_loss: 0.2454 - val_accuracy: 0.9325\n",
      "Epoch 51/800\n",
      "1665/1665 [==============================] - 0s 112us/step - loss: 0.1828 - accuracy: 0.9537 - val_loss: 0.2453 - val_accuracy: 0.9325\n",
      "Epoch 52/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.1827 - accuracy: 0.9537 - val_loss: 0.2448 - val_accuracy: 0.9325\n",
      "Epoch 53/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.1826 - accuracy: 0.9537 - val_loss: 0.2442 - val_accuracy: 0.9325\n",
      "Epoch 54/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.1825 - accuracy: 0.9537 - val_loss: 0.2447 - val_accuracy: 0.9325\n",
      "Epoch 55/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1824 - accuracy: 0.9537 - val_loss: 0.2449 - val_accuracy: 0.9325\n",
      "Epoch 56/800\n",
      "1665/1665 [==============================] - 0s 112us/step - loss: 0.1822 - accuracy: 0.9537 - val_loss: 0.2447 - val_accuracy: 0.9325\n",
      "Epoch 57/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.1822 - accuracy: 0.9537 - val_loss: 0.2441 - val_accuracy: 0.9325\n",
      "Epoch 58/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.1820 - accuracy: 0.9537 - val_loss: 0.2434 - val_accuracy: 0.9325\n",
      "Epoch 59/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.1819 - accuracy: 0.9537 - val_loss: 0.2436 - val_accuracy: 0.9325\n",
      "Epoch 60/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1818 - accuracy: 0.9537 - val_loss: 0.2430 - val_accuracy: 0.9325\n",
      "Epoch 61/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.1816 - accuracy: 0.9537 - val_loss: 0.2434 - val_accuracy: 0.9325\n",
      "Epoch 62/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.1815 - accuracy: 0.9537 - val_loss: 0.2436 - val_accuracy: 0.9325\n",
      "Epoch 63/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.1813 - accuracy: 0.9537 - val_loss: 0.2423 - val_accuracy: 0.9325\n",
      "Epoch 64/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.1812 - accuracy: 0.9537 - val_loss: 0.2434 - val_accuracy: 0.9325\n",
      "Epoch 65/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.1810 - accuracy: 0.9537 - val_loss: 0.2423 - val_accuracy: 0.9325\n",
      "Epoch 66/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.1810 - accuracy: 0.9537 - val_loss: 0.2430 - val_accuracy: 0.9325\n",
      "Epoch 67/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1808 - accuracy: 0.9537 - val_loss: 0.2426 - val_accuracy: 0.9325\n",
      "Epoch 68/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.1806 - accuracy: 0.9537 - val_loss: 0.2426 - val_accuracy: 0.9325\n",
      "Epoch 69/800\n",
      "1665/1665 [==============================] - 0s 112us/step - loss: 0.1804 - accuracy: 0.9537 - val_loss: 0.2420 - val_accuracy: 0.9325\n",
      "Epoch 70/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1802 - accuracy: 0.9537 - val_loss: 0.2417 - val_accuracy: 0.9325\n",
      "Epoch 71/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.1800 - accuracy: 0.9537 - val_loss: 0.2408 - val_accuracy: 0.9325\n",
      "Epoch 72/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.1799 - accuracy: 0.9537 - val_loss: 0.2408 - val_accuracy: 0.9325\n",
      "Epoch 73/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.1797 - accuracy: 0.9537 - val_loss: 0.2409 - val_accuracy: 0.9325\n",
      "Epoch 74/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.1795 - accuracy: 0.9537 - val_loss: 0.2409 - val_accuracy: 0.9325\n",
      "Epoch 75/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.1793 - accuracy: 0.9537 - val_loss: 0.2407 - val_accuracy: 0.9325\n",
      "Epoch 76/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.1790 - accuracy: 0.9537 - val_loss: 0.2401 - val_accuracy: 0.9325\n",
      "Epoch 77/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.1788 - accuracy: 0.9537 - val_loss: 0.2400 - val_accuracy: 0.9325\n",
      "Epoch 78/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.1786 - accuracy: 0.9537 - val_loss: 0.2393 - val_accuracy: 0.9325\n",
      "Epoch 79/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.1783 - accuracy: 0.9537 - val_loss: 0.2393 - val_accuracy: 0.9325\n",
      "Epoch 80/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.1781 - accuracy: 0.9537 - val_loss: 0.2390 - val_accuracy: 0.9325\n",
      "Epoch 81/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.1778 - accuracy: 0.9537 - val_loss: 0.2379 - val_accuracy: 0.9325\n",
      "Epoch 82/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.1775 - accuracy: 0.9537 - val_loss: 0.2377 - val_accuracy: 0.9325\n",
      "Epoch 83/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.1772 - accuracy: 0.9537 - val_loss: 0.2372 - val_accuracy: 0.9325\n",
      "Epoch 84/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.1769 - accuracy: 0.9537 - val_loss: 0.2364 - val_accuracy: 0.9325\n",
      "Epoch 85/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.1766 - accuracy: 0.9537 - val_loss: 0.2366 - val_accuracy: 0.9325\n",
      "Epoch 86/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.1763 - accuracy: 0.9537 - val_loss: 0.2357 - val_accuracy: 0.9325\n",
      "Epoch 87/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.1759 - accuracy: 0.9537 - val_loss: 0.2351 - val_accuracy: 0.9325\n",
      "Epoch 88/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.1756 - accuracy: 0.9537 - val_loss: 0.2350 - val_accuracy: 0.9325\n",
      "Epoch 89/800\n",
      "1665/1665 [==============================] - 0s 136us/step - loss: 0.1753 - accuracy: 0.9537 - val_loss: 0.2340 - val_accuracy: 0.9325\n",
      "Epoch 90/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.1749 - accuracy: 0.9537 - val_loss: 0.2344 - val_accuracy: 0.9325\n",
      "Epoch 91/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.1745 - accuracy: 0.9537 - val_loss: 0.2332 - val_accuracy: 0.9325\n",
      "Epoch 92/800\n",
      "1665/1665 [==============================] - 0s 129us/step - loss: 0.1741 - accuracy: 0.9537 - val_loss: 0.2329 - val_accuracy: 0.9325\n",
      "Epoch 93/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.1737 - accuracy: 0.9537 - val_loss: 0.2327 - val_accuracy: 0.9325\n",
      "Epoch 94/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.1733 - accuracy: 0.9537 - val_loss: 0.2318 - val_accuracy: 0.9325\n",
      "Epoch 95/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.1728 - accuracy: 0.9537 - val_loss: 0.2312 - val_accuracy: 0.9325\n",
      "Epoch 96/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.1724 - accuracy: 0.9537 - val_loss: 0.2308 - val_accuracy: 0.9325\n",
      "Epoch 97/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.1719 - accuracy: 0.9537 - val_loss: 0.2299 - val_accuracy: 0.9325\n",
      "Epoch 98/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1714 - accuracy: 0.9537 - val_loss: 0.2283 - val_accuracy: 0.9325\n",
      "Epoch 99/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.1709 - accuracy: 0.9537 - val_loss: 0.2277 - val_accuracy: 0.9325\n",
      "Epoch 100/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1705 - accuracy: 0.9537 - val_loss: 0.2268 - val_accuracy: 0.9325\n",
      "Epoch 101/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.1699 - accuracy: 0.9537 - val_loss: 0.2271 - val_accuracy: 0.9325\n",
      "Epoch 102/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1695 - accuracy: 0.9537 - val_loss: 0.2263 - val_accuracy: 0.9325\n",
      "Epoch 103/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1689 - accuracy: 0.9537 - val_loss: 0.2258 - val_accuracy: 0.9325\n",
      "Epoch 104/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.1683 - accuracy: 0.9537 - val_loss: 0.2248 - val_accuracy: 0.9325\n",
      "Epoch 105/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1678 - accuracy: 0.9537 - val_loss: 0.2237 - val_accuracy: 0.9325\n",
      "Epoch 106/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1672 - accuracy: 0.9538 - val_loss: 0.2231 - val_accuracy: 0.9325\n",
      "Epoch 107/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.1667 - accuracy: 0.9538 - val_loss: 0.2224 - val_accuracy: 0.9325\n",
      "Epoch 108/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.1661 - accuracy: 0.9538 - val_loss: 0.2215 - val_accuracy: 0.9325\n",
      "Epoch 109/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.1654 - accuracy: 0.9538 - val_loss: 0.2208 - val_accuracy: 0.9325\n",
      "Epoch 110/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1649 - accuracy: 0.9538 - val_loss: 0.2197 - val_accuracy: 0.9325\n",
      "Epoch 111/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.1642 - accuracy: 0.9538 - val_loss: 0.2188 - val_accuracy: 0.9326\n",
      "Epoch 112/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.1636 - accuracy: 0.9538 - val_loss: 0.2180 - val_accuracy: 0.9326\n",
      "Epoch 113/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.1630 - accuracy: 0.9538 - val_loss: 0.2180 - val_accuracy: 0.9327\n",
      "Epoch 114/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.1625 - accuracy: 0.9538 - val_loss: 0.2163 - val_accuracy: 0.9328\n",
      "Epoch 115/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.1619 - accuracy: 0.9538 - val_loss: 0.2155 - val_accuracy: 0.9328\n",
      "Epoch 116/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.1613 - accuracy: 0.9538 - val_loss: 0.2148 - val_accuracy: 0.9329\n",
      "Epoch 117/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.1606 - accuracy: 0.9538 - val_loss: 0.2141 - val_accuracy: 0.9329\n",
      "Epoch 118/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.1601 - accuracy: 0.9538 - val_loss: 0.2134 - val_accuracy: 0.9329\n",
      "Epoch 119/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.1595 - accuracy: 0.9538 - val_loss: 0.2124 - val_accuracy: 0.9330\n",
      "Epoch 120/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.1589 - accuracy: 0.9539 - val_loss: 0.2120 - val_accuracy: 0.9330\n",
      "Epoch 121/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1583 - accuracy: 0.9539 - val_loss: 0.2108 - val_accuracy: 0.9330\n",
      "Epoch 122/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.1577 - accuracy: 0.9540 - val_loss: 0.2091 - val_accuracy: 0.9331\n",
      "Epoch 123/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.1572 - accuracy: 0.9540 - val_loss: 0.2101 - val_accuracy: 0.9331\n",
      "Epoch 124/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.1566 - accuracy: 0.9540 - val_loss: 0.2092 - val_accuracy: 0.9333\n",
      "Epoch 125/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1561 - accuracy: 0.9540 - val_loss: 0.2091 - val_accuracy: 0.9333\n",
      "Epoch 126/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1555 - accuracy: 0.9541 - val_loss: 0.2087 - val_accuracy: 0.9336\n",
      "Epoch 127/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.1550 - accuracy: 0.9543 - val_loss: 0.2062 - val_accuracy: 0.9334\n",
      "Epoch 128/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.1545 - accuracy: 0.9542 - val_loss: 0.2079 - val_accuracy: 0.9332\n",
      "Epoch 129/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1541 - accuracy: 0.9543 - val_loss: 0.2066 - val_accuracy: 0.9332\n",
      "Epoch 130/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1534 - accuracy: 0.9543 - val_loss: 0.2051 - val_accuracy: 0.9333\n",
      "Epoch 131/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.1528 - accuracy: 0.9543 - val_loss: 0.2041 - val_accuracy: 0.9333\n",
      "Epoch 132/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.1523 - accuracy: 0.9544 - val_loss: 0.2040 - val_accuracy: 0.9335\n",
      "Epoch 133/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1518 - accuracy: 0.9545 - val_loss: 0.2037 - val_accuracy: 0.9336\n",
      "Epoch 134/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.1513 - accuracy: 0.9545 - val_loss: 0.2018 - val_accuracy: 0.9333\n",
      "Epoch 135/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1508 - accuracy: 0.9545 - val_loss: 0.2023 - val_accuracy: 0.9331\n",
      "Epoch 136/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1502 - accuracy: 0.9545 - val_loss: 0.2019 - val_accuracy: 0.9336\n",
      "Epoch 137/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1498 - accuracy: 0.9544 - val_loss: 0.2005 - val_accuracy: 0.9334\n",
      "Epoch 138/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.1493 - accuracy: 0.9545 - val_loss: 0.1995 - val_accuracy: 0.9335\n",
      "Epoch 139/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1487 - accuracy: 0.9546 - val_loss: 0.1992 - val_accuracy: 0.9338\n",
      "Epoch 140/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1483 - accuracy: 0.9545 - val_loss: 0.1991 - val_accuracy: 0.9336\n",
      "Epoch 141/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.1477 - accuracy: 0.9546 - val_loss: 0.1985 - val_accuracy: 0.9341\n",
      "Epoch 142/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.1472 - accuracy: 0.9546 - val_loss: 0.1969 - val_accuracy: 0.9339\n",
      "Epoch 143/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1467 - accuracy: 0.9547 - val_loss: 0.1968 - val_accuracy: 0.9336\n",
      "Epoch 144/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.1461 - accuracy: 0.9547 - val_loss: 0.1965 - val_accuracy: 0.9338\n",
      "Epoch 145/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.1457 - accuracy: 0.9547 - val_loss: 0.1952 - val_accuracy: 0.9338\n",
      "Epoch 146/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.1453 - accuracy: 0.9547 - val_loss: 0.1942 - val_accuracy: 0.9339\n",
      "Epoch 147/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.1446 - accuracy: 0.9548 - val_loss: 0.1948 - val_accuracy: 0.9343\n",
      "Epoch 148/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.1443 - accuracy: 0.9549 - val_loss: 0.1941 - val_accuracy: 0.9345\n",
      "Epoch 149/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.1437 - accuracy: 0.9549 - val_loss: 0.1934 - val_accuracy: 0.9339\n",
      "Epoch 150/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.1432 - accuracy: 0.9550 - val_loss: 0.1921 - val_accuracy: 0.9347\n",
      "Epoch 151/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.1427 - accuracy: 0.9551 - val_loss: 0.1925 - val_accuracy: 0.9349\n",
      "Epoch 152/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.1423 - accuracy: 0.9552 - val_loss: 0.1913 - val_accuracy: 0.9338\n",
      "Epoch 153/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.1417 - accuracy: 0.9552 - val_loss: 0.1911 - val_accuracy: 0.9351\n",
      "Epoch 154/800\n",
      "1665/1665 [==============================] - 0s 126us/step - loss: 0.1413 - accuracy: 0.9553 - val_loss: 0.1910 - val_accuracy: 0.9344\n",
      "Epoch 155/800\n",
      "1665/1665 [==============================] - 0s 160us/step - loss: 0.1408 - accuracy: 0.9553 - val_loss: 0.1907 - val_accuracy: 0.9348\n",
      "Epoch 156/800\n",
      "1665/1665 [==============================] - 0s 141us/step - loss: 0.1403 - accuracy: 0.9554 - val_loss: 0.1892 - val_accuracy: 0.9345\n",
      "Epoch 157/800\n",
      "1665/1665 [==============================] - 0s 139us/step - loss: 0.1398 - accuracy: 0.9556 - val_loss: 0.1963 - val_accuracy: 0.9316\n",
      "Epoch 158/800\n",
      "1665/1665 [==============================] - 0s 140us/step - loss: 0.1396 - accuracy: 0.9555 - val_loss: 0.1879 - val_accuracy: 0.9349\n",
      "Epoch 159/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.1389 - accuracy: 0.9556 - val_loss: 0.1890 - val_accuracy: 0.9342\n",
      "Epoch 160/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.1385 - accuracy: 0.9558 - val_loss: 0.1879 - val_accuracy: 0.9355\n",
      "Epoch 161/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.1381 - accuracy: 0.9559 - val_loss: 0.1874 - val_accuracy: 0.9354\n",
      "Epoch 162/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.1375 - accuracy: 0.9559 - val_loss: 0.1867 - val_accuracy: 0.9355\n",
      "Epoch 163/800\n",
      "1665/1665 [==============================] - 0s 145us/step - loss: 0.1373 - accuracy: 0.9559 - val_loss: 0.1854 - val_accuracy: 0.9353\n",
      "Epoch 164/800\n",
      "1665/1665 [==============================] - 0s 156us/step - loss: 0.1367 - accuracy: 0.9560 - val_loss: 0.1855 - val_accuracy: 0.9354\n",
      "Epoch 165/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1665/1665 [==============================] - 0s 149us/step - loss: 0.1363 - accuracy: 0.9561 - val_loss: 0.1854 - val_accuracy: 0.9356\n",
      "Epoch 166/800\n",
      "1665/1665 [==============================] - 0s 149us/step - loss: 0.1359 - accuracy: 0.9563 - val_loss: 0.1855 - val_accuracy: 0.9355\n",
      "Epoch 167/800\n",
      "1665/1665 [==============================] - 0s 164us/step - loss: 0.1355 - accuracy: 0.9563 - val_loss: 0.1848 - val_accuracy: 0.9359\n",
      "Epoch 168/800\n",
      "1665/1665 [==============================] - 0s 168us/step - loss: 0.1350 - accuracy: 0.9564 - val_loss: 0.1837 - val_accuracy: 0.9359\n",
      "Epoch 169/800\n",
      "1665/1665 [==============================] - 0s 155us/step - loss: 0.1346 - accuracy: 0.9565 - val_loss: 0.1846 - val_accuracy: 0.9353\n",
      "Epoch 170/800\n",
      "1665/1665 [==============================] - 0s 144us/step - loss: 0.1342 - accuracy: 0.9563 - val_loss: 0.1832 - val_accuracy: 0.9360\n",
      "Epoch 171/800\n",
      "1665/1665 [==============================] - 0s 144us/step - loss: 0.1338 - accuracy: 0.9565 - val_loss: 0.1843 - val_accuracy: 0.9358\n",
      "Epoch 172/800\n",
      "1665/1665 [==============================] - 0s 150us/step - loss: 0.1334 - accuracy: 0.9566 - val_loss: 0.1824 - val_accuracy: 0.9360\n",
      "Epoch 173/800\n",
      "1665/1665 [==============================] - 0s 152us/step - loss: 0.1330 - accuracy: 0.9568 - val_loss: 0.1829 - val_accuracy: 0.9360\n",
      "Epoch 174/800\n",
      "1665/1665 [==============================] - 0s 141us/step - loss: 0.1326 - accuracy: 0.9567 - val_loss: 0.1817 - val_accuracy: 0.9362\n",
      "Epoch 175/800\n",
      "1665/1665 [==============================] - 0s 129us/step - loss: 0.1322 - accuracy: 0.9569 - val_loss: 0.1816 - val_accuracy: 0.9357\n",
      "Epoch 176/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.1319 - accuracy: 0.9571 - val_loss: 0.1800 - val_accuracy: 0.9365\n",
      "Epoch 177/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1313 - accuracy: 0.9571 - val_loss: 0.1796 - val_accuracy: 0.9367\n",
      "Epoch 178/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.1310 - accuracy: 0.9573 - val_loss: 0.1793 - val_accuracy: 0.9369\n",
      "Epoch 179/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.1307 - accuracy: 0.9572 - val_loss: 0.1805 - val_accuracy: 0.9365\n",
      "Epoch 180/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1303 - accuracy: 0.9573 - val_loss: 0.1784 - val_accuracy: 0.9369\n",
      "Epoch 181/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1298 - accuracy: 0.9574 - val_loss: 0.1786 - val_accuracy: 0.9368\n",
      "Epoch 182/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1295 - accuracy: 0.9577 - val_loss: 0.1792 - val_accuracy: 0.9366\n",
      "Epoch 183/800\n",
      "1665/1665 [==============================] - ETA: 0s - loss: 0.1282 - accuracy: 0.95 - 0s 110us/step - loss: 0.1292 - accuracy: 0.9574 - val_loss: 0.1779 - val_accuracy: 0.9364\n",
      "Epoch 184/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.1287 - accuracy: 0.9573 - val_loss: 0.1785 - val_accuracy: 0.9369\n",
      "Epoch 185/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1285 - accuracy: 0.9575 - val_loss: 0.1771 - val_accuracy: 0.9366\n",
      "Epoch 186/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.1280 - accuracy: 0.9577 - val_loss: 0.1765 - val_accuracy: 0.9374\n",
      "Epoch 187/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.1276 - accuracy: 0.9577 - val_loss: 0.1762 - val_accuracy: 0.9372\n",
      "Epoch 188/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1272 - accuracy: 0.9580 - val_loss: 0.1755 - val_accuracy: 0.9372\n",
      "Epoch 189/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.1269 - accuracy: 0.9580 - val_loss: 0.1753 - val_accuracy: 0.9373\n",
      "Epoch 190/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.1266 - accuracy: 0.9577 - val_loss: 0.1748 - val_accuracy: 0.9369\n",
      "Epoch 191/800\n",
      "1665/1665 [==============================] - 0s 126us/step - loss: 0.1262 - accuracy: 0.9580 - val_loss: 0.1751 - val_accuracy: 0.9371\n",
      "Epoch 192/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.1258 - accuracy: 0.9581 - val_loss: 0.1739 - val_accuracy: 0.9371\n",
      "Epoch 193/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.1256 - accuracy: 0.9581 - val_loss: 0.1836 - val_accuracy: 0.9345\n",
      "Epoch 194/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.1255 - accuracy: 0.9584 - val_loss: 0.1726 - val_accuracy: 0.9375\n",
      "Epoch 195/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.1246 - accuracy: 0.9583 - val_loss: 0.1777 - val_accuracy: 0.9357\n",
      "Epoch 196/800\n",
      "1665/1665 [==============================] - 0s 126us/step - loss: 0.1246 - accuracy: 0.9586 - val_loss: 0.1736 - val_accuracy: 0.9367\n",
      "Epoch 197/800\n",
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.1241 - accuracy: 0.9584 - val_loss: 0.1731 - val_accuracy: 0.9373\n",
      "Epoch 198/800\n",
      "1665/1665 [==============================] - 0s 141us/step - loss: 0.1237 - accuracy: 0.9586 - val_loss: 0.1715 - val_accuracy: 0.9370\n",
      "Epoch 199/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.1233 - accuracy: 0.9585 - val_loss: 0.1723 - val_accuracy: 0.9376\n",
      "Epoch 200/800\n",
      "1665/1665 [==============================] - 0s 133us/step - loss: 0.1230 - accuracy: 0.9587 - val_loss: 0.1705 - val_accuracy: 0.9381\n",
      "Epoch 201/800\n",
      "1665/1665 [==============================] - 0s 141us/step - loss: 0.1226 - accuracy: 0.9589 - val_loss: 0.1703 - val_accuracy: 0.9382\n",
      "Epoch 202/800\n",
      "1665/1665 [==============================] - 0s 136us/step - loss: 0.1222 - accuracy: 0.9587 - val_loss: 0.1688 - val_accuracy: 0.9382\n",
      "Epoch 203/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1218 - accuracy: 0.9593 - val_loss: 0.1692 - val_accuracy: 0.9384\n",
      "Epoch 204/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.1215 - accuracy: 0.9591 - val_loss: 0.1695 - val_accuracy: 0.9377\n",
      "Epoch 205/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.1212 - accuracy: 0.9591 - val_loss: 0.1694 - val_accuracy: 0.9388\n",
      "Epoch 206/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.1209 - accuracy: 0.9594 - val_loss: 0.1690 - val_accuracy: 0.9384\n",
      "Epoch 207/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.1204 - accuracy: 0.9594 - val_loss: 0.1680 - val_accuracy: 0.9391\n",
      "Epoch 208/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.1201 - accuracy: 0.9595 - val_loss: 0.1673 - val_accuracy: 0.9388\n",
      "Epoch 209/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.1197 - accuracy: 0.9595 - val_loss: 0.1676 - val_accuracy: 0.9393\n",
      "Epoch 210/800\n",
      "1665/1665 [==============================] - 0s 129us/step - loss: 0.1193 - accuracy: 0.9596 - val_loss: 0.1673 - val_accuracy: 0.9391\n",
      "Epoch 211/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.1189 - accuracy: 0.9599 - val_loss: 0.1675 - val_accuracy: 0.9375\n",
      "Epoch 212/800\n",
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.1187 - accuracy: 0.9600 - val_loss: 0.1667 - val_accuracy: 0.9385\n",
      "Epoch 213/800\n",
      "1665/1665 [==============================] - 0s 143us/step - loss: 0.1183 - accuracy: 0.9600 - val_loss: 0.1652 - val_accuracy: 0.9394\n",
      "Epoch 214/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.1179 - accuracy: 0.9601 - val_loss: 0.1687 - val_accuracy: 0.9385\n",
      "Epoch 215/800\n",
      "1665/1665 [==============================] - 0s 144us/step - loss: 0.1178 - accuracy: 0.9602 - val_loss: 0.1645 - val_accuracy: 0.9388\n",
      "Epoch 216/800\n",
      "1665/1665 [==============================] - 0s 129us/step - loss: 0.1171 - accuracy: 0.9603 - val_loss: 0.1646 - val_accuracy: 0.9402\n",
      "Epoch 217/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.1169 - accuracy: 0.9602 - val_loss: 0.1651 - val_accuracy: 0.9390\n",
      "Epoch 218/800\n",
      "1665/1665 [==============================] - 0s 139us/step - loss: 0.1165 - accuracy: 0.9602 - val_loss: 0.1628 - val_accuracy: 0.9395\n",
      "Epoch 219/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.1162 - accuracy: 0.9605 - val_loss: 0.1633 - val_accuracy: 0.9397\n",
      "Epoch 220/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1157 - accuracy: 0.9605 - val_loss: 0.1615 - val_accuracy: 0.9398\n",
      "Epoch 221/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1154 - accuracy: 0.9604 - val_loss: 0.1699 - val_accuracy: 0.9394\n",
      "Epoch 222/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.1153 - accuracy: 0.9606 - val_loss: 0.1757 - val_accuracy: 0.9350\n",
      "Epoch 223/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.1154 - accuracy: 0.9606 - val_loss: 0.1593 - val_accuracy: 0.9409\n",
      "Epoch 224/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1143 - accuracy: 0.9609 - val_loss: 0.1599 - val_accuracy: 0.9406\n",
      "Epoch 225/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.1139 - accuracy: 0.9611 - val_loss: 0.1607 - val_accuracy: 0.9412\n",
      "Epoch 226/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.1136 - accuracy: 0.9615 - val_loss: 0.1591 - val_accuracy: 0.9401\n",
      "Epoch 227/800\n",
      "1665/1665 [==============================] - 0s 141us/step - loss: 0.1134 - accuracy: 0.9609 - val_loss: 0.1604 - val_accuracy: 0.9409\n",
      "Epoch 228/800\n",
      "1665/1665 [==============================] - 0s 139us/step - loss: 0.1130 - accuracy: 0.9612 - val_loss: 0.1617 - val_accuracy: 0.9402\n",
      "Epoch 229/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.1127 - accuracy: 0.9614 - val_loss: 0.1580 - val_accuracy: 0.9409\n",
      "Epoch 230/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.1122 - accuracy: 0.9614 - val_loss: 0.1652 - val_accuracy: 0.9388\n",
      "Epoch 231/800\n",
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.1122 - accuracy: 0.9612 - val_loss: 0.1585 - val_accuracy: 0.9414\n",
      "Epoch 232/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1117 - accuracy: 0.9618 - val_loss: 0.1588 - val_accuracy: 0.9408\n",
      "Epoch 233/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.1113 - accuracy: 0.9619 - val_loss: 0.1582 - val_accuracy: 0.9420\n",
      "Epoch 234/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.1109 - accuracy: 0.9618 - val_loss: 0.1566 - val_accuracy: 0.9415\n",
      "Epoch 235/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.1106 - accuracy: 0.9621 - val_loss: 0.1582 - val_accuracy: 0.9414\n",
      "Epoch 236/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1104 - accuracy: 0.9624 - val_loss: 0.1561 - val_accuracy: 0.9422\n",
      "Epoch 237/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1099 - accuracy: 0.9623 - val_loss: 0.1566 - val_accuracy: 0.9420\n",
      "Epoch 238/800\n",
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.1096 - accuracy: 0.9624 - val_loss: 0.1653 - val_accuracy: 0.9389\n",
      "Epoch 239/800\n",
      "1665/1665 [==============================] - 0s 144us/step - loss: 0.1098 - accuracy: 0.9622 - val_loss: 0.1559 - val_accuracy: 0.9426\n",
      "Epoch 240/800\n",
      "1665/1665 [==============================] - 0s 145us/step - loss: 0.1090 - accuracy: 0.9624 - val_loss: 0.1554 - val_accuracy: 0.9426\n",
      "Epoch 241/800\n",
      "1665/1665 [==============================] - 0s 142us/step - loss: 0.1087 - accuracy: 0.9623 - val_loss: 0.1658 - val_accuracy: 0.9386\n",
      "Epoch 242/800\n",
      "1665/1665 [==============================] - 0s 143us/step - loss: 0.1089 - accuracy: 0.9623 - val_loss: 0.1545 - val_accuracy: 0.9425\n",
      "Epoch 243/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.1081 - accuracy: 0.9627 - val_loss: 0.1545 - val_accuracy: 0.9425\n",
      "Epoch 244/800\n",
      "1665/1665 [==============================] - 0s 140us/step - loss: 0.1078 - accuracy: 0.9629 - val_loss: 0.1555 - val_accuracy: 0.9416\n",
      "Epoch 245/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.1075 - accuracy: 0.9626 - val_loss: 0.1549 - val_accuracy: 0.9428\n",
      "Epoch 246/800\n",
      "1665/1665 [==============================] - 0s 133us/step - loss: 0.1071 - accuracy: 0.9631 - val_loss: 0.1528 - val_accuracy: 0.9427\n",
      "Epoch 247/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1070 - accuracy: 0.9630 - val_loss: 0.1534 - val_accuracy: 0.9426\n",
      "Epoch 248/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1065 - accuracy: 0.9635 - val_loss: 0.1537 - val_accuracy: 0.9430\n",
      "Epoch 249/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.1063 - accuracy: 0.9631 - val_loss: 0.1523 - val_accuracy: 0.9424\n",
      "Epoch 250/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.1061 - accuracy: 0.9633 - val_loss: 0.1509 - val_accuracy: 0.9425\n",
      "Epoch 251/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1058 - accuracy: 0.9632 - val_loss: 0.1526 - val_accuracy: 0.9432\n",
      "Epoch 252/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.1054 - accuracy: 0.9637 - val_loss: 0.1538 - val_accuracy: 0.9427\n",
      "Epoch 253/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.1052 - accuracy: 0.9636 - val_loss: 0.1518 - val_accuracy: 0.9432\n",
      "Epoch 254/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.1049 - accuracy: 0.9635 - val_loss: 0.1560 - val_accuracy: 0.9406\n",
      "Epoch 255/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1049 - accuracy: 0.9636 - val_loss: 0.1602 - val_accuracy: 0.9396\n",
      "Epoch 256/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.1047 - accuracy: 0.9635 - val_loss: 0.1517 - val_accuracy: 0.9432\n",
      "Epoch 257/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.1042 - accuracy: 0.9638 - val_loss: 0.1518 - val_accuracy: 0.9437\n",
      "Epoch 258/800\n",
      "1665/1665 [==============================] - 0s 126us/step - loss: 0.1040 - accuracy: 0.9637 - val_loss: 0.1502 - val_accuracy: 0.9439\n",
      "Epoch 259/800\n",
      "1665/1665 [==============================] - 0s 126us/step - loss: 0.1036 - accuracy: 0.9640 - val_loss: 0.1510 - val_accuracy: 0.9431\n",
      "Epoch 260/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.1032 - accuracy: 0.9639 - val_loss: 0.1516 - val_accuracy: 0.9428\n",
      "Epoch 261/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.1031 - accuracy: 0.9640 - val_loss: 0.1505 - val_accuracy: 0.9441\n",
      "Epoch 262/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.1029 - accuracy: 0.9645 - val_loss: 0.1534 - val_accuracy: 0.9421\n",
      "Epoch 263/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.1028 - accuracy: 0.9640 - val_loss: 0.1582 - val_accuracy: 0.9414\n",
      "Epoch 264/800\n",
      "1665/1665 [==============================] - 0s 167us/step - loss: 0.1027 - accuracy: 0.9639 - val_loss: 0.1538 - val_accuracy: 0.9417\n",
      "Epoch 265/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.1023 - accuracy: 0.9646 - val_loss: 0.1499 - val_accuracy: 0.9442\n",
      "Epoch 266/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.1018 - accuracy: 0.9647 - val_loss: 0.1488 - val_accuracy: 0.9438\n",
      "Epoch 267/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.1017 - accuracy: 0.9649 - val_loss: 0.1495 - val_accuracy: 0.9427\n",
      "Epoch 268/800\n",
      "1665/1665 [==============================] - 0s 143us/step - loss: 0.1015 - accuracy: 0.9649 - val_loss: 0.1494 - val_accuracy: 0.9436\n",
      "Epoch 269/800\n",
      "1665/1665 [==============================] - 0s 126us/step - loss: 0.1012 - accuracy: 0.9648 - val_loss: 0.1495 - val_accuracy: 0.9440\n",
      "Epoch 270/800\n",
      "1665/1665 [==============================] - 0s 141us/step - loss: 0.1010 - accuracy: 0.9650 - val_loss: 0.1498 - val_accuracy: 0.9438\n",
      "Epoch 271/800\n",
      "1665/1665 [==============================] - 0s 141us/step - loss: 0.1008 - accuracy: 0.9650 - val_loss: 0.1604 - val_accuracy: 0.9384\n",
      "Epoch 272/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.1012 - accuracy: 0.9652 - val_loss: 0.1480 - val_accuracy: 0.9434\n",
      "Epoch 273/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.1003 - accuracy: 0.9651 - val_loss: 0.1483 - val_accuracy: 0.9443\n",
      "Epoch 274/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.1000 - accuracy: 0.9652 - val_loss: 0.1484 - val_accuracy: 0.9440\n",
      "Epoch 275/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.1001 - accuracy: 0.9650 - val_loss: 0.1472 - val_accuracy: 0.9453\n",
      "Epoch 276/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.0997 - accuracy: 0.9653 - val_loss: 0.1479 - val_accuracy: 0.9445\n",
      "Epoch 277/800\n",
      "1665/1665 [==============================] - 0s 145us/step - loss: 0.0996 - accuracy: 0.9653 - val_loss: 0.1488 - val_accuracy: 0.9446\n",
      "Epoch 278/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0994 - accuracy: 0.9654 - val_loss: 0.1487 - val_accuracy: 0.9449\n",
      "Epoch 279/800\n",
      "1665/1665 [==============================] - 0s 126us/step - loss: 0.0990 - accuracy: 0.9654 - val_loss: 0.1480 - val_accuracy: 0.9433\n",
      "Epoch 280/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.0990 - accuracy: 0.9653 - val_loss: 0.1472 - val_accuracy: 0.9452\n",
      "Epoch 281/800\n",
      "1665/1665 [==============================] - 0s 145us/step - loss: 0.0987 - accuracy: 0.9658 - val_loss: 0.1468 - val_accuracy: 0.9451\n",
      "Epoch 282/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0984 - accuracy: 0.9658 - val_loss: 0.1475 - val_accuracy: 0.9454\n",
      "Epoch 283/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0982 - accuracy: 0.9658 - val_loss: 0.1465 - val_accuracy: 0.9449\n",
      "Epoch 284/800\n",
      "1665/1665 [==============================] - 0s 142us/step - loss: 0.0982 - accuracy: 0.9661 - val_loss: 0.1482 - val_accuracy: 0.9453\n",
      "Epoch 285/800\n",
      "1665/1665 [==============================] - 0s 141us/step - loss: 0.0980 - accuracy: 0.9662 - val_loss: 0.1609 - val_accuracy: 0.9403\n",
      "Epoch 286/800\n",
      "1665/1665 [==============================] - 0s 136us/step - loss: 0.0984 - accuracy: 0.9658 - val_loss: 0.1562 - val_accuracy: 0.9418\n",
      "Epoch 287/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0981 - accuracy: 0.9658 - val_loss: 0.1472 - val_accuracy: 0.9448\n",
      "Epoch 288/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0975 - accuracy: 0.9661 - val_loss: 0.1460 - val_accuracy: 0.9458\n",
      "Epoch 289/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.0972 - accuracy: 0.9661 - val_loss: 0.1574 - val_accuracy: 0.9402\n",
      "Epoch 290/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.0977 - accuracy: 0.9659 - val_loss: 0.1478 - val_accuracy: 0.9448\n",
      "Epoch 291/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0969 - accuracy: 0.9660 - val_loss: 0.1461 - val_accuracy: 0.9455\n",
      "Epoch 292/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0966 - accuracy: 0.9664 - val_loss: 0.1474 - val_accuracy: 0.9449\n",
      "Epoch 293/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0967 - accuracy: 0.9663 - val_loss: 0.1568 - val_accuracy: 0.9408\n",
      "Epoch 294/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.0970 - accuracy: 0.9662 - val_loss: 0.1460 - val_accuracy: 0.9456\n",
      "Epoch 295/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.0964 - accuracy: 0.9663 - val_loss: 0.1472 - val_accuracy: 0.9453\n",
      "Epoch 296/800\n",
      "1665/1665 [==============================] - 0s 133us/step - loss: 0.0961 - accuracy: 0.9667 - val_loss: 0.1451 - val_accuracy: 0.9454\n",
      "Epoch 297/800\n",
      "1665/1665 [==============================] - 0s 147us/step - loss: 0.0959 - accuracy: 0.9667 - val_loss: 0.1458 - val_accuracy: 0.9451\n",
      "Epoch 298/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0958 - accuracy: 0.9666 - val_loss: 0.1472 - val_accuracy: 0.9452\n",
      "Epoch 299/800\n",
      "1665/1665 [==============================] - 0s 140us/step - loss: 0.0957 - accuracy: 0.9666 - val_loss: 0.1451 - val_accuracy: 0.9461\n",
      "Epoch 300/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.0955 - accuracy: 0.9669 - val_loss: 0.1464 - val_accuracy: 0.9457\n",
      "Epoch 301/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0954 - accuracy: 0.9670 - val_loss: 0.1463 - val_accuracy: 0.9454\n",
      "Epoch 302/800\n",
      "1665/1665 [==============================] - 0s 141us/step - loss: 0.0953 - accuracy: 0.9670 - val_loss: 0.1437 - val_accuracy: 0.9473\n",
      "Epoch 303/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0949 - accuracy: 0.9670 - val_loss: 0.1442 - val_accuracy: 0.9465\n",
      "Epoch 304/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.0948 - accuracy: 0.9670 - val_loss: 0.1449 - val_accuracy: 0.9458\n",
      "Epoch 305/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0946 - accuracy: 0.9673 - val_loss: 0.1449 - val_accuracy: 0.9461\n",
      "Epoch 306/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0946 - accuracy: 0.9669 - val_loss: 0.1443 - val_accuracy: 0.9472\n",
      "Epoch 307/800\n",
      "1665/1665 [==============================] - 0s 143us/step - loss: 0.0943 - accuracy: 0.9677 - val_loss: 0.1439 - val_accuracy: 0.9467\n",
      "Epoch 308/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.0942 - accuracy: 0.9672 - val_loss: 0.1448 - val_accuracy: 0.9459\n",
      "Epoch 309/800\n",
      "1665/1665 [==============================] - 0s 129us/step - loss: 0.0941 - accuracy: 0.9673 - val_loss: 0.1461 - val_accuracy: 0.9457\n",
      "Epoch 310/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.0939 - accuracy: 0.9674 - val_loss: 0.1431 - val_accuracy: 0.9470\n",
      "Epoch 311/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0936 - accuracy: 0.9676 - val_loss: 0.1431 - val_accuracy: 0.9469\n",
      "Epoch 312/800\n",
      "1665/1665 [==============================] - 0s 141us/step - loss: 0.0937 - accuracy: 0.9674 - val_loss: 0.1433 - val_accuracy: 0.9463\n",
      "Epoch 313/800\n",
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.0935 - accuracy: 0.9674 - val_loss: 0.1427 - val_accuracy: 0.9475\n",
      "Epoch 314/800\n",
      "1665/1665 [==============================] - 0s 165us/step - loss: 0.0934 - accuracy: 0.9678 - val_loss: 0.1435 - val_accuracy: 0.9472\n",
      "Epoch 315/800\n",
      "1665/1665 [==============================] - 0s 197us/step - loss: 0.0930 - accuracy: 0.9675 - val_loss: 0.1450 - val_accuracy: 0.9471\n",
      "Epoch 316/800\n",
      "1665/1665 [==============================] - 0s 153us/step - loss: 0.0931 - accuracy: 0.9680 - val_loss: 0.1440 - val_accuracy: 0.9465\n",
      "Epoch 317/800\n",
      "1665/1665 [==============================] - 0s 196us/step - loss: 0.0929 - accuracy: 0.9678 - val_loss: 0.1442 - val_accuracy: 0.9469\n",
      "Epoch 318/800\n",
      "1665/1665 [==============================] - 0s 144us/step - loss: 0.0927 - accuracy: 0.9678 - val_loss: 0.1439 - val_accuracy: 0.9473\n",
      "Epoch 319/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0926 - accuracy: 0.9680 - val_loss: 0.1437 - val_accuracy: 0.9470\n",
      "Epoch 320/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0926 - accuracy: 0.9682 - val_loss: 0.1439 - val_accuracy: 0.9473\n",
      "Epoch 321/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0923 - accuracy: 0.9680 - val_loss: 0.1435 - val_accuracy: 0.9471\n",
      "Epoch 322/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0922 - accuracy: 0.9682 - val_loss: 0.1427 - val_accuracy: 0.9476\n",
      "Epoch 323/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0920 - accuracy: 0.9679 - val_loss: 0.1434 - val_accuracy: 0.9469\n",
      "Epoch 324/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.0919 - accuracy: 0.9681 - val_loss: 0.1457 - val_accuracy: 0.9477\n",
      "Epoch 325/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0918 - accuracy: 0.9682 - val_loss: 0.1442 - val_accuracy: 0.9475\n",
      "Epoch 326/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0916 - accuracy: 0.9682 - val_loss: 0.1428 - val_accuracy: 0.9471\n",
      "Epoch 327/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0915 - accuracy: 0.9685 - val_loss: 0.1431 - val_accuracy: 0.9478\n",
      "Epoch 328/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0915 - accuracy: 0.9681 - val_loss: 0.1424 - val_accuracy: 0.9481\n",
      "Epoch 329/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0911 - accuracy: 0.9685 - val_loss: 0.1420 - val_accuracy: 0.9464\n",
      "Epoch 330/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0911 - accuracy: 0.9686 - val_loss: 0.1431 - val_accuracy: 0.9476\n",
      "Epoch 331/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0909 - accuracy: 0.9686 - val_loss: 0.1426 - val_accuracy: 0.9475\n",
      "Epoch 332/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0908 - accuracy: 0.9687 - val_loss: 0.1518 - val_accuracy: 0.9440\n",
      "Epoch 333/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0911 - accuracy: 0.9685 - val_loss: 0.1423 - val_accuracy: 0.9477\n",
      "Epoch 334/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0905 - accuracy: 0.9688 - val_loss: 0.1471 - val_accuracy: 0.9454\n",
      "Epoch 335/800\n",
      "1665/1665 [==============================] - 0s 177us/step - loss: 0.0907 - accuracy: 0.9689 - val_loss: 0.1414 - val_accuracy: 0.9480\n",
      "Epoch 336/800\n",
      "1665/1665 [==============================] - 0s 141us/step - loss: 0.0903 - accuracy: 0.9689 - val_loss: 0.1420 - val_accuracy: 0.9486\n",
      "Epoch 337/800\n",
      "1665/1665 [==============================] - 0s 146us/step - loss: 0.0901 - accuracy: 0.9690 - val_loss: 0.1413 - val_accuracy: 0.9469\n",
      "Epoch 338/800\n",
      "1665/1665 [==============================] - 0s 163us/step - loss: 0.0900 - accuracy: 0.9690 - val_loss: 0.1407 - val_accuracy: 0.9481\n",
      "Epoch 339/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0898 - accuracy: 0.9690 - val_loss: 0.1569 - val_accuracy: 0.9419\n",
      "Epoch 340/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0906 - accuracy: 0.9687 - val_loss: 0.1404 - val_accuracy: 0.9477\n",
      "Epoch 341/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0895 - accuracy: 0.9690 - val_loss: 0.1421 - val_accuracy: 0.9487\n",
      "Epoch 342/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0894 - accuracy: 0.9690 - val_loss: 0.1409 - val_accuracy: 0.9482\n",
      "Epoch 343/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0894 - accuracy: 0.9691 - val_loss: 0.1404 - val_accuracy: 0.9487\n",
      "Epoch 344/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0891 - accuracy: 0.9693 - val_loss: 0.1593 - val_accuracy: 0.9427\n",
      "Epoch 345/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.0901 - accuracy: 0.9690 - val_loss: 0.1400 - val_accuracy: 0.9487\n",
      "Epoch 346/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0889 - accuracy: 0.9694 - val_loss: 0.1626 - val_accuracy: 0.9412\n",
      "Epoch 347/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.0898 - accuracy: 0.9689 - val_loss: 0.1395 - val_accuracy: 0.9490\n",
      "Epoch 348/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0885 - accuracy: 0.9693 - val_loss: 0.1397 - val_accuracy: 0.9496\n",
      "Epoch 349/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0883 - accuracy: 0.9694 - val_loss: 0.1405 - val_accuracy: 0.9489\n",
      "Epoch 350/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0883 - accuracy: 0.9692 - val_loss: 0.1401 - val_accuracy: 0.9490\n",
      "Epoch 351/800\n",
      "1665/1665 [==============================] - 0s 107us/step - loss: 0.0883 - accuracy: 0.9697 - val_loss: 0.1394 - val_accuracy: 0.9494\n",
      "Epoch 352/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0880 - accuracy: 0.9698 - val_loss: 0.1517 - val_accuracy: 0.9440\n",
      "Epoch 353/800\n",
      "1665/1665 [==============================] - 0s 112us/step - loss: 0.0886 - accuracy: 0.9694 - val_loss: 0.1401 - val_accuracy: 0.9489\n",
      "Epoch 354/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0878 - accuracy: 0.9698 - val_loss: 0.1525 - val_accuracy: 0.9448\n",
      "Epoch 355/800\n",
      "1665/1665 [==============================] - 0s 112us/step - loss: 0.0882 - accuracy: 0.9694 - val_loss: 0.1427 - val_accuracy: 0.9475\n",
      "Epoch 356/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0877 - accuracy: 0.9699 - val_loss: 0.1400 - val_accuracy: 0.9487\n",
      "Epoch 357/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0875 - accuracy: 0.9698 - val_loss: 0.1390 - val_accuracy: 0.9496\n",
      "Epoch 358/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.0872 - accuracy: 0.9701 - val_loss: 0.1385 - val_accuracy: 0.9490\n",
      "Epoch 359/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.0871 - accuracy: 0.9699 - val_loss: 0.1379 - val_accuracy: 0.9493\n",
      "Epoch 360/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0869 - accuracy: 0.9699 - val_loss: 0.1385 - val_accuracy: 0.9497\n",
      "Epoch 361/800\n",
      "1665/1665 [==============================] - 0s 112us/step - loss: 0.0868 - accuracy: 0.9701 - val_loss: 0.1390 - val_accuracy: 0.9489\n",
      "Epoch 362/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.0867 - accuracy: 0.9701 - val_loss: 0.1522 - val_accuracy: 0.9444\n",
      "Epoch 363/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0872 - accuracy: 0.9699 - val_loss: 0.1515 - val_accuracy: 0.9437\n",
      "Epoch 364/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0870 - accuracy: 0.9699 - val_loss: 0.1373 - val_accuracy: 0.9491\n",
      "Epoch 365/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0862 - accuracy: 0.9701 - val_loss: 0.1398 - val_accuracy: 0.9489\n",
      "Epoch 366/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.0862 - accuracy: 0.9702 - val_loss: 0.1387 - val_accuracy: 0.9490\n",
      "Epoch 367/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.0859 - accuracy: 0.9704 - val_loss: 0.1386 - val_accuracy: 0.9496\n",
      "Epoch 368/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0857 - accuracy: 0.9703 - val_loss: 0.1376 - val_accuracy: 0.9501\n",
      "Epoch 369/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.0855 - accuracy: 0.9704 - val_loss: 0.1383 - val_accuracy: 0.9497\n",
      "Epoch 370/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0855 - accuracy: 0.9703 - val_loss: 0.1384 - val_accuracy: 0.9494\n",
      "Epoch 371/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0854 - accuracy: 0.9706 - val_loss: 0.1386 - val_accuracy: 0.9501\n",
      "Epoch 372/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0852 - accuracy: 0.9706 - val_loss: 0.1379 - val_accuracy: 0.9499\n",
      "Epoch 373/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0851 - accuracy: 0.9708 - val_loss: 0.1372 - val_accuracy: 0.9502\n",
      "Epoch 374/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0849 - accuracy: 0.9706 - val_loss: 0.1395 - val_accuracy: 0.9497\n",
      "Epoch 375/800\n",
      "1665/1665 [==============================] - 0s 126us/step - loss: 0.0849 - accuracy: 0.9705 - val_loss: 0.1367 - val_accuracy: 0.9501\n",
      "Epoch 376/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.0845 - accuracy: 0.9707 - val_loss: 0.1375 - val_accuracy: 0.9498\n",
      "Epoch 377/800\n",
      "1665/1665 [==============================] - 0s 129us/step - loss: 0.0845 - accuracy: 0.9707 - val_loss: 0.1396 - val_accuracy: 0.9493\n",
      "Epoch 378/800\n",
      "1665/1665 [==============================] - 0s 126us/step - loss: 0.0845 - accuracy: 0.9709 - val_loss: 0.1386 - val_accuracy: 0.9494\n",
      "Epoch 379/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0842 - accuracy: 0.9708 - val_loss: 0.1371 - val_accuracy: 0.9503\n",
      "Epoch 380/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0842 - accuracy: 0.9708 - val_loss: 0.1377 - val_accuracy: 0.9502\n",
      "Epoch 381/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0839 - accuracy: 0.9710 - val_loss: 0.1400 - val_accuracy: 0.9490\n",
      "Epoch 382/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.0842 - accuracy: 0.9708 - val_loss: 0.1397 - val_accuracy: 0.9486\n",
      "Epoch 383/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.0838 - accuracy: 0.9710 - val_loss: 0.1363 - val_accuracy: 0.9506\n",
      "Epoch 384/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1665/1665 [==============================] - 0s 152us/step - loss: 0.0837 - accuracy: 0.9713 - val_loss: 0.1510 - val_accuracy: 0.9449\n",
      "Epoch 385/800\n",
      "1665/1665 [==============================] - 0s 129us/step - loss: 0.0842 - accuracy: 0.9711 - val_loss: 0.1367 - val_accuracy: 0.9510\n",
      "Epoch 386/800\n",
      "1665/1665 [==============================] - 0s 155us/step - loss: 0.0834 - accuracy: 0.9711 - val_loss: 0.1402 - val_accuracy: 0.9498\n",
      "Epoch 387/800\n",
      "1665/1665 [==============================] - 0s 142us/step - loss: 0.0836 - accuracy: 0.9710 - val_loss: 0.1407 - val_accuracy: 0.9497\n",
      "Epoch 388/800\n",
      "1665/1665 [==============================] - 0s 140us/step - loss: 0.0833 - accuracy: 0.9713 - val_loss: 0.1366 - val_accuracy: 0.9509\n",
      "Epoch 389/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.0831 - accuracy: 0.9714 - val_loss: 0.1378 - val_accuracy: 0.9503\n",
      "Epoch 390/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0830 - accuracy: 0.9712 - val_loss: 0.1358 - val_accuracy: 0.9513\n",
      "Epoch 391/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.0828 - accuracy: 0.9711 - val_loss: 0.1370 - val_accuracy: 0.9508\n",
      "Epoch 392/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0828 - accuracy: 0.9712 - val_loss: 0.1350 - val_accuracy: 0.9515\n",
      "Epoch 393/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.0825 - accuracy: 0.9716 - val_loss: 0.1351 - val_accuracy: 0.9511\n",
      "Epoch 394/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.0824 - accuracy: 0.9712 - val_loss: 0.1362 - val_accuracy: 0.9505\n",
      "Epoch 395/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0823 - accuracy: 0.9716 - val_loss: 0.1359 - val_accuracy: 0.9506\n",
      "Epoch 396/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0821 - accuracy: 0.9716 - val_loss: 0.1360 - val_accuracy: 0.9511\n",
      "Epoch 397/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0820 - accuracy: 0.9717 - val_loss: 0.1545 - val_accuracy: 0.9437\n",
      "Epoch 398/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0827 - accuracy: 0.9713 - val_loss: 0.1366 - val_accuracy: 0.9510\n",
      "Epoch 399/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0818 - accuracy: 0.9717 - val_loss: 0.1347 - val_accuracy: 0.9516\n",
      "Epoch 400/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0817 - accuracy: 0.9718 - val_loss: 0.1344 - val_accuracy: 0.9513\n",
      "Epoch 401/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0814 - accuracy: 0.9719 - val_loss: 0.1343 - val_accuracy: 0.9518\n",
      "Epoch 402/800\n",
      "1665/1665 [==============================] - 0s 136us/step - loss: 0.0814 - accuracy: 0.9719 - val_loss: 0.1344 - val_accuracy: 0.9515\n",
      "Epoch 403/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0812 - accuracy: 0.9719 - val_loss: 0.1353 - val_accuracy: 0.9513\n",
      "Epoch 404/800\n",
      "1665/1665 [==============================] - 0s 140us/step - loss: 0.0811 - accuracy: 0.9718 - val_loss: 0.1355 - val_accuracy: 0.9512\n",
      "Epoch 405/800\n",
      "1665/1665 [==============================] - 0s 162us/step - loss: 0.0813 - accuracy: 0.9718 - val_loss: 0.1358 - val_accuracy: 0.9505\n",
      "Epoch 406/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0811 - accuracy: 0.9719 - val_loss: 0.1359 - val_accuracy: 0.9510\n",
      "Epoch 407/800\n",
      "1665/1665 [==============================] - 0s 157us/step - loss: 0.0810 - accuracy: 0.9720 - val_loss: 0.1349 - val_accuracy: 0.9511\n",
      "Epoch 408/800\n",
      "1665/1665 [==============================] - 0s 142us/step - loss: 0.0808 - accuracy: 0.9718 - val_loss: 0.1363 - val_accuracy: 0.9514\n",
      "Epoch 409/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0806 - accuracy: 0.9721 - val_loss: 0.1346 - val_accuracy: 0.9509\n",
      "Epoch 410/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0808 - accuracy: 0.9718 - val_loss: 0.1411 - val_accuracy: 0.9490\n",
      "Epoch 411/800\n",
      "1665/1665 [==============================] - 0s 155us/step - loss: 0.0808 - accuracy: 0.9720 - val_loss: 0.1352 - val_accuracy: 0.9515\n",
      "Epoch 412/800\n",
      "1665/1665 [==============================] - 0s 156us/step - loss: 0.0804 - accuracy: 0.9721 - val_loss: 0.1354 - val_accuracy: 0.9521\n",
      "Epoch 413/800\n",
      "1665/1665 [==============================] - 0s 126us/step - loss: 0.0802 - accuracy: 0.9721 - val_loss: 0.1346 - val_accuracy: 0.9511\n",
      "Epoch 414/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.0803 - accuracy: 0.9721 - val_loss: 0.1335 - val_accuracy: 0.9517\n",
      "Epoch 415/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0799 - accuracy: 0.9722 - val_loss: 0.1336 - val_accuracy: 0.9515\n",
      "Epoch 416/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0800 - accuracy: 0.9724 - val_loss: 0.1350 - val_accuracy: 0.9516\n",
      "Epoch 417/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0800 - accuracy: 0.9726 - val_loss: 0.1334 - val_accuracy: 0.9514\n",
      "Epoch 418/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0796 - accuracy: 0.9722 - val_loss: 0.1450 - val_accuracy: 0.9485\n",
      "Epoch 419/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0803 - accuracy: 0.9722 - val_loss: 0.1341 - val_accuracy: 0.9513\n",
      "Epoch 420/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0796 - accuracy: 0.9723 - val_loss: 0.1329 - val_accuracy: 0.9524\n",
      "Epoch 421/800\n",
      "1665/1665 [==============================] - 0s 126us/step - loss: 0.0794 - accuracy: 0.9725 - val_loss: 0.1340 - val_accuracy: 0.9527\n",
      "Epoch 422/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0795 - accuracy: 0.9724 - val_loss: 0.1436 - val_accuracy: 0.9488\n",
      "Epoch 423/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0799 - accuracy: 0.9723 - val_loss: 0.1335 - val_accuracy: 0.9527\n",
      "Epoch 424/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0791 - accuracy: 0.9725 - val_loss: 0.1340 - val_accuracy: 0.9519\n",
      "Epoch 425/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0792 - accuracy: 0.9724 - val_loss: 0.1326 - val_accuracy: 0.9526\n",
      "Epoch 426/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0790 - accuracy: 0.9726 - val_loss: 0.1335 - val_accuracy: 0.9527\n",
      "Epoch 427/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0789 - accuracy: 0.9727 - val_loss: 0.1337 - val_accuracy: 0.9516\n",
      "Epoch 428/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0789 - accuracy: 0.9727 - val_loss: 0.1335 - val_accuracy: 0.9511\n",
      "Epoch 429/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0789 - accuracy: 0.9725 - val_loss: 0.1334 - val_accuracy: 0.9525\n",
      "Epoch 430/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0786 - accuracy: 0.9730 - val_loss: 0.1317 - val_accuracy: 0.9526\n",
      "Epoch 431/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0785 - accuracy: 0.9727 - val_loss: 0.1343 - val_accuracy: 0.9524\n",
      "Epoch 432/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0784 - accuracy: 0.9730 - val_loss: 0.1346 - val_accuracy: 0.9520\n",
      "Epoch 433/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.0784 - accuracy: 0.9727 - val_loss: 0.1343 - val_accuracy: 0.9515\n",
      "Epoch 434/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.0785 - accuracy: 0.9726 - val_loss: 0.1349 - val_accuracy: 0.9521\n",
      "Epoch 435/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0782 - accuracy: 0.9728 - val_loss: 0.1339 - val_accuracy: 0.9521\n",
      "Epoch 436/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0782 - accuracy: 0.9729 - val_loss: 0.1321 - val_accuracy: 0.9528\n",
      "Epoch 437/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0780 - accuracy: 0.9731 - val_loss: 0.1317 - val_accuracy: 0.9528\n",
      "Epoch 438/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0780 - accuracy: 0.9727 - val_loss: 0.1321 - val_accuracy: 0.9526\n",
      "Epoch 439/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0778 - accuracy: 0.9729 - val_loss: 0.1313 - val_accuracy: 0.9530\n",
      "Epoch 440/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0777 - accuracy: 0.9730 - val_loss: 0.1316 - val_accuracy: 0.9526\n",
      "Epoch 441/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0779 - accuracy: 0.9730 - val_loss: 0.1311 - val_accuracy: 0.9523\n",
      "Epoch 442/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0775 - accuracy: 0.9731 - val_loss: 0.1314 - val_accuracy: 0.9533\n",
      "Epoch 443/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0775 - accuracy: 0.9729 - val_loss: 0.1477 - val_accuracy: 0.9461\n",
      "Epoch 444/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0782 - accuracy: 0.9725 - val_loss: 0.1316 - val_accuracy: 0.9530\n",
      "Epoch 445/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0771 - accuracy: 0.9733 - val_loss: 0.1318 - val_accuracy: 0.9521\n",
      "Epoch 446/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0771 - accuracy: 0.9732 - val_loss: 0.1318 - val_accuracy: 0.9528\n",
      "Epoch 447/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.0772 - accuracy: 0.9731 - val_loss: 0.1306 - val_accuracy: 0.9530\n",
      "Epoch 448/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0768 - accuracy: 0.9734 - val_loss: 0.1304 - val_accuracy: 0.9536\n",
      "Epoch 449/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0770 - accuracy: 0.9733 - val_loss: 0.1423 - val_accuracy: 0.9492\n",
      "Epoch 450/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0773 - accuracy: 0.9729 - val_loss: 0.1528 - val_accuracy: 0.9447\n",
      "Epoch 451/800\n",
      "1665/1665 [==============================] - 0s 133us/step - loss: 0.0778 - accuracy: 0.9730 - val_loss: 0.1309 - val_accuracy: 0.9533\n",
      "Epoch 452/800\n",
      "1665/1665 [==============================] - 0s 155us/step - loss: 0.0765 - accuracy: 0.9734 - val_loss: 0.1326 - val_accuracy: 0.9521\n",
      "Epoch 453/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.0768 - accuracy: 0.9732 - val_loss: 0.1321 - val_accuracy: 0.9536\n",
      "Epoch 454/800\n",
      "1665/1665 [==============================] - 0s 164us/step - loss: 0.0766 - accuracy: 0.9733 - val_loss: 0.1309 - val_accuracy: 0.9536\n",
      "Epoch 455/800\n",
      "1665/1665 [==============================] - 0s 155us/step - loss: 0.0764 - accuracy: 0.9731 - val_loss: 0.1302 - val_accuracy: 0.9534\n",
      "Epoch 456/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.0762 - accuracy: 0.9733 - val_loss: 0.1310 - val_accuracy: 0.9529\n",
      "Epoch 457/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.0762 - accuracy: 0.9732 - val_loss: 0.1301 - val_accuracy: 0.9533\n",
      "Epoch 458/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0761 - accuracy: 0.9735 - val_loss: 0.1308 - val_accuracy: 0.9530\n",
      "Epoch 459/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0761 - accuracy: 0.9734 - val_loss: 0.1328 - val_accuracy: 0.9528\n",
      "Epoch 460/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0760 - accuracy: 0.9736 - val_loss: 0.1318 - val_accuracy: 0.9524\n",
      "Epoch 461/800\n",
      "1665/1665 [==============================] - 0s 159us/step - loss: 0.0760 - accuracy: 0.9734 - val_loss: 0.1298 - val_accuracy: 0.9531\n",
      "Epoch 462/800\n",
      "1665/1665 [==============================] - 0s 136us/step - loss: 0.0758 - accuracy: 0.9734 - val_loss: 0.1310 - val_accuracy: 0.9536\n",
      "Epoch 463/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.0759 - accuracy: 0.9736 - val_loss: 0.1308 - val_accuracy: 0.9530\n",
      "Epoch 464/800\n",
      "1665/1665 [==============================] - 0s 142us/step - loss: 0.0756 - accuracy: 0.9735 - val_loss: 0.1300 - val_accuracy: 0.9541\n",
      "Epoch 465/800\n",
      "1665/1665 [==============================] - 0s 139us/step - loss: 0.0755 - accuracy: 0.9736 - val_loss: 0.1385 - val_accuracy: 0.9509\n",
      "Epoch 466/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0759 - accuracy: 0.9735 - val_loss: 0.1284 - val_accuracy: 0.9542\n",
      "Epoch 467/800\n",
      "1665/1665 [==============================] - 0s 154us/step - loss: 0.0754 - accuracy: 0.9735 - val_loss: 0.1303 - val_accuracy: 0.9536\n",
      "Epoch 468/800\n",
      "1665/1665 [==============================] - 0s 150us/step - loss: 0.0753 - accuracy: 0.9736 - val_loss: 0.1299 - val_accuracy: 0.9524\n",
      "Epoch 469/800\n",
      "1665/1665 [==============================] - 0s 157us/step - loss: 0.0754 - accuracy: 0.9735 - val_loss: 0.1299 - val_accuracy: 0.9533\n",
      "Epoch 470/800\n",
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.0750 - accuracy: 0.9739 - val_loss: 0.1313 - val_accuracy: 0.9532\n",
      "Epoch 471/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.0753 - accuracy: 0.9735 - val_loss: 0.1295 - val_accuracy: 0.9537\n",
      "Epoch 472/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0748 - accuracy: 0.9736 - val_loss: 0.1297 - val_accuracy: 0.9540\n",
      "Epoch 473/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0748 - accuracy: 0.9737 - val_loss: 0.1298 - val_accuracy: 0.9543\n",
      "Epoch 474/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.0750 - accuracy: 0.9736 - val_loss: 0.1290 - val_accuracy: 0.9543\n",
      "Epoch 475/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0748 - accuracy: 0.9738 - val_loss: 0.1302 - val_accuracy: 0.9535\n",
      "Epoch 476/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0748 - accuracy: 0.9735 - val_loss: 0.1302 - val_accuracy: 0.9538\n",
      "Epoch 477/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0746 - accuracy: 0.9738 - val_loss: 0.1295 - val_accuracy: 0.9539\n",
      "Epoch 478/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0745 - accuracy: 0.9737 - val_loss: 0.1385 - val_accuracy: 0.9508\n",
      "Epoch 479/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0749 - accuracy: 0.9737 - val_loss: 0.1304 - val_accuracy: 0.9545\n",
      "Epoch 480/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0744 - accuracy: 0.9740 - val_loss: 0.1277 - val_accuracy: 0.9545\n",
      "Epoch 481/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0741 - accuracy: 0.9739 - val_loss: 0.1285 - val_accuracy: 0.9545\n",
      "Epoch 482/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0740 - accuracy: 0.9740 - val_loss: 0.1293 - val_accuracy: 0.9542\n",
      "Epoch 483/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.0741 - accuracy: 0.9739 - val_loss: 0.1287 - val_accuracy: 0.9534\n",
      "Epoch 484/800\n",
      "1665/1665 [==============================] - 0s 126us/step - loss: 0.0742 - accuracy: 0.9738 - val_loss: 0.1338 - val_accuracy: 0.9523\n",
      "Epoch 485/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0743 - accuracy: 0.9739 - val_loss: 0.1280 - val_accuracy: 0.9546\n",
      "Epoch 486/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0738 - accuracy: 0.9742 - val_loss: 0.1286 - val_accuracy: 0.9545\n",
      "Epoch 487/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0737 - accuracy: 0.9739 - val_loss: 0.1273 - val_accuracy: 0.9545\n",
      "Epoch 488/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0737 - accuracy: 0.9740 - val_loss: 0.1295 - val_accuracy: 0.9539\n",
      "Epoch 489/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0738 - accuracy: 0.9740 - val_loss: 0.1370 - val_accuracy: 0.9513\n",
      "Epoch 490/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0740 - accuracy: 0.9739 - val_loss: 0.1305 - val_accuracy: 0.9541\n",
      "Epoch 491/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0736 - accuracy: 0.9739 - val_loss: 0.1292 - val_accuracy: 0.9545\n",
      "Epoch 492/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0735 - accuracy: 0.9744 - val_loss: 0.1285 - val_accuracy: 0.9549\n",
      "Epoch 493/800\n",
      "1665/1665 [==============================] - 0s 129us/step - loss: 0.0734 - accuracy: 0.9742 - val_loss: 0.1299 - val_accuracy: 0.9545\n",
      "Epoch 494/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0733 - accuracy: 0.9740 - val_loss: 0.1300 - val_accuracy: 0.9537\n",
      "Epoch 495/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0732 - accuracy: 0.9740 - val_loss: 0.1270 - val_accuracy: 0.9543\n",
      "Epoch 496/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0732 - accuracy: 0.9740 - val_loss: 0.1329 - val_accuracy: 0.9526\n",
      "Epoch 497/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0735 - accuracy: 0.9739 - val_loss: 0.1362 - val_accuracy: 0.9505\n",
      "Epoch 498/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0734 - accuracy: 0.9741 - val_loss: 0.1478 - val_accuracy: 0.9459\n",
      "Epoch 499/800\n",
      "1665/1665 [==============================] - 0s 152us/step - loss: 0.0737 - accuracy: 0.9738 - val_loss: 0.1273 - val_accuracy: 0.9541\n",
      "Epoch 500/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.0729 - accuracy: 0.9739 - val_loss: 0.1296 - val_accuracy: 0.9537\n",
      "Epoch 501/800\n",
      "1665/1665 [==============================] - 0s 149us/step - loss: 0.0728 - accuracy: 0.9743 - val_loss: 0.1299 - val_accuracy: 0.9537\n",
      "Epoch 502/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0727 - accuracy: 0.9743 - val_loss: 0.1271 - val_accuracy: 0.9549\n",
      "Epoch 503/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.0725 - accuracy: 0.9745 - val_loss: 0.1290 - val_accuracy: 0.9556\n",
      "Epoch 504/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.0725 - accuracy: 0.9742 - val_loss: 0.1269 - val_accuracy: 0.9551\n",
      "Epoch 505/800\n",
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.0724 - accuracy: 0.9744 - val_loss: 0.1262 - val_accuracy: 0.9560\n",
      "Epoch 506/800\n",
      "1665/1665 [==============================] - 0s 129us/step - loss: 0.0723 - accuracy: 0.9744 - val_loss: 0.1299 - val_accuracy: 0.9549\n",
      "Epoch 507/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0724 - accuracy: 0.9743 - val_loss: 0.1268 - val_accuracy: 0.9550\n",
      "Epoch 508/800\n",
      "1665/1665 [==============================] - 0s 129us/step - loss: 0.0722 - accuracy: 0.9747 - val_loss: 0.1356 - val_accuracy: 0.9535\n",
      "Epoch 509/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0726 - accuracy: 0.9743 - val_loss: 0.1320 - val_accuracy: 0.9531\n",
      "Epoch 510/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0723 - accuracy: 0.9746 - val_loss: 0.1293 - val_accuracy: 0.9547\n",
      "Epoch 511/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.0721 - accuracy: 0.9745 - val_loss: 0.1279 - val_accuracy: 0.9549\n",
      "Epoch 512/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.0719 - accuracy: 0.9746 - val_loss: 0.1299 - val_accuracy: 0.9537\n",
      "Epoch 513/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0720 - accuracy: 0.9746 - val_loss: 0.1297 - val_accuracy: 0.9547\n",
      "Epoch 514/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0719 - accuracy: 0.9745 - val_loss: 0.1271 - val_accuracy: 0.9560\n",
      "Epoch 515/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0716 - accuracy: 0.9745 - val_loss: 0.1267 - val_accuracy: 0.9548\n",
      "Epoch 516/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0716 - accuracy: 0.9747 - val_loss: 0.1275 - val_accuracy: 0.9550\n",
      "Epoch 517/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0715 - accuracy: 0.9748 - val_loss: 0.1295 - val_accuracy: 0.9544\n",
      "Epoch 518/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0716 - accuracy: 0.9746 - val_loss: 0.1260 - val_accuracy: 0.9561\n",
      "Epoch 519/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0713 - accuracy: 0.9749 - val_loss: 0.1269 - val_accuracy: 0.9555\n",
      "Epoch 520/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0713 - accuracy: 0.9746 - val_loss: 0.1258 - val_accuracy: 0.9556\n",
      "Epoch 521/800\n",
      "1665/1665 [==============================] - 0s 142us/step - loss: 0.0713 - accuracy: 0.9746 - val_loss: 0.1266 - val_accuracy: 0.9554\n",
      "Epoch 522/800\n",
      "1665/1665 [==============================] - 0s 144us/step - loss: 0.0714 - accuracy: 0.9746 - val_loss: 0.1279 - val_accuracy: 0.9552\n",
      "Epoch 523/800\n",
      "1665/1665 [==============================] - 0s 133us/step - loss: 0.0710 - accuracy: 0.9747 - val_loss: 0.1267 - val_accuracy: 0.9554\n",
      "Epoch 524/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0712 - accuracy: 0.9747 - val_loss: 0.1262 - val_accuracy: 0.9555\n",
      "Epoch 525/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.0708 - accuracy: 0.9749 - val_loss: 0.1319 - val_accuracy: 0.9538\n",
      "Epoch 526/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.0712 - accuracy: 0.9747 - val_loss: 0.1257 - val_accuracy: 0.9556\n",
      "Epoch 527/800\n",
      "1665/1665 [==============================] - 0s 129us/step - loss: 0.0706 - accuracy: 0.9750 - val_loss: 0.1267 - val_accuracy: 0.9551\n",
      "Epoch 528/800\n",
      "1665/1665 [==============================] - 0s 140us/step - loss: 0.0707 - accuracy: 0.9749 - val_loss: 0.1305 - val_accuracy: 0.9533\n",
      "Epoch 529/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0708 - accuracy: 0.9748 - val_loss: 0.1273 - val_accuracy: 0.9548\n",
      "Epoch 530/800\n",
      "1665/1665 [==============================] - 0s 153us/step - loss: 0.0707 - accuracy: 0.9751 - val_loss: 0.1260 - val_accuracy: 0.9560\n",
      "Epoch 531/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.0704 - accuracy: 0.9752 - val_loss: 0.1276 - val_accuracy: 0.9558\n",
      "Epoch 532/800\n",
      "1665/1665 [==============================] - 0s 136us/step - loss: 0.0704 - accuracy: 0.9749 - val_loss: 0.1284 - val_accuracy: 0.9543\n",
      "Epoch 533/800\n",
      "1665/1665 [==============================] - 0s 136us/step - loss: 0.0707 - accuracy: 0.9748 - val_loss: 0.1274 - val_accuracy: 0.9558\n",
      "Epoch 534/800\n",
      "1665/1665 [==============================] - 0s 139us/step - loss: 0.0703 - accuracy: 0.9749 - val_loss: 0.1275 - val_accuracy: 0.9556\n",
      "Epoch 535/800\n",
      "1665/1665 [==============================] - 0s 163us/step - loss: 0.0701 - accuracy: 0.9750 - val_loss: 0.1254 - val_accuracy: 0.9552\n",
      "Epoch 536/800\n",
      "1665/1665 [==============================] - 0s 144us/step - loss: 0.0699 - accuracy: 0.9750 - val_loss: 0.1248 - val_accuracy: 0.9552\n",
      "Epoch 537/800\n",
      "1665/1665 [==============================] - 0s 147us/step - loss: 0.0699 - accuracy: 0.9751 - val_loss: 0.1395 - val_accuracy: 0.9502\n",
      "Epoch 538/800\n",
      "1665/1665 [==============================] - 0s 147us/step - loss: 0.0707 - accuracy: 0.9750 - val_loss: 0.1267 - val_accuracy: 0.9562\n",
      "Epoch 539/800\n",
      "1665/1665 [==============================] - 0s 156us/step - loss: 0.0698 - accuracy: 0.9747 - val_loss: 0.1251 - val_accuracy: 0.9550\n",
      "Epoch 540/800\n",
      "1665/1665 [==============================] - 0s 149us/step - loss: 0.0698 - accuracy: 0.9752 - val_loss: 0.1242 - val_accuracy: 0.9562\n",
      "Epoch 541/800\n",
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.0696 - accuracy: 0.9754 - val_loss: 0.1271 - val_accuracy: 0.9545\n",
      "Epoch 542/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0699 - accuracy: 0.9752 - val_loss: 0.1241 - val_accuracy: 0.9562\n",
      "Epoch 543/800\n",
      "1665/1665 [==============================] - 0s 126us/step - loss: 0.0697 - accuracy: 0.9752 - val_loss: 0.1270 - val_accuracy: 0.9553\n",
      "Epoch 544/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0694 - accuracy: 0.9752 - val_loss: 0.1258 - val_accuracy: 0.9556\n",
      "Epoch 545/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0694 - accuracy: 0.9754 - val_loss: 0.1248 - val_accuracy: 0.9560\n",
      "Epoch 546/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0691 - accuracy: 0.9755 - val_loss: 0.1265 - val_accuracy: 0.9551\n",
      "Epoch 547/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0693 - accuracy: 0.9751 - val_loss: 0.1302 - val_accuracy: 0.9533\n",
      "Epoch 548/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0694 - accuracy: 0.9752 - val_loss: 0.1236 - val_accuracy: 0.9565\n",
      "Epoch 549/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0689 - accuracy: 0.9753 - val_loss: 0.1235 - val_accuracy: 0.9569\n",
      "Epoch 550/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.0690 - accuracy: 0.9754 - val_loss: 0.1266 - val_accuracy: 0.9556\n",
      "Epoch 551/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0688 - accuracy: 0.9754 - val_loss: 0.1412 - val_accuracy: 0.9502\n",
      "Epoch 552/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0694 - accuracy: 0.9753 - val_loss: 0.1243 - val_accuracy: 0.9566\n",
      "Epoch 553/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0688 - accuracy: 0.9754 - val_loss: 0.1250 - val_accuracy: 0.9559\n",
      "Epoch 554/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0687 - accuracy: 0.9754 - val_loss: 0.1242 - val_accuracy: 0.9560\n",
      "Epoch 555/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0685 - accuracy: 0.9755 - val_loss: 0.1237 - val_accuracy: 0.9569\n",
      "Epoch 556/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0684 - accuracy: 0.9755 - val_loss: 0.1358 - val_accuracy: 0.9533\n",
      "Epoch 557/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0689 - accuracy: 0.9753 - val_loss: 0.1250 - val_accuracy: 0.9561\n",
      "Epoch 558/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0684 - accuracy: 0.9753 - val_loss: 0.1245 - val_accuracy: 0.9561\n",
      "Epoch 559/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0682 - accuracy: 0.9755 - val_loss: 0.1234 - val_accuracy: 0.9560\n",
      "Epoch 560/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0682 - accuracy: 0.9757 - val_loss: 0.1234 - val_accuracy: 0.9567\n",
      "Epoch 561/800\n",
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.0680 - accuracy: 0.9756 - val_loss: 0.1257 - val_accuracy: 0.9570\n",
      "Epoch 562/800\n",
      "1665/1665 [==============================] - 0s 153us/step - loss: 0.0682 - accuracy: 0.9757 - val_loss: 0.1240 - val_accuracy: 0.9570\n",
      "Epoch 563/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0679 - accuracy: 0.9756 - val_loss: 0.1237 - val_accuracy: 0.9564\n",
      "Epoch 564/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.0680 - accuracy: 0.9755 - val_loss: 0.1441 - val_accuracy: 0.9489\n",
      "Epoch 565/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0690 - accuracy: 0.9750 - val_loss: 0.1246 - val_accuracy: 0.9564\n",
      "Epoch 566/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.0678 - accuracy: 0.9758 - val_loss: 0.1241 - val_accuracy: 0.9563\n",
      "Epoch 567/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0677 - accuracy: 0.9756 - val_loss: 0.1224 - val_accuracy: 0.9567\n",
      "Epoch 568/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0675 - accuracy: 0.9757 - val_loss: 0.1239 - val_accuracy: 0.9565\n",
      "Epoch 569/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0676 - accuracy: 0.9758 - val_loss: 0.1272 - val_accuracy: 0.9561\n",
      "Epoch 570/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0676 - accuracy: 0.9756 - val_loss: 0.1234 - val_accuracy: 0.9572\n",
      "Epoch 571/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0673 - accuracy: 0.9760 - val_loss: 0.1261 - val_accuracy: 0.9574\n",
      "Epoch 572/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0673 - accuracy: 0.9759 - val_loss: 0.1213 - val_accuracy: 0.9563\n",
      "Epoch 573/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0673 - accuracy: 0.9756 - val_loss: 0.1303 - val_accuracy: 0.9561\n",
      "Epoch 574/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0675 - accuracy: 0.9758 - val_loss: 0.1230 - val_accuracy: 0.9561\n",
      "Epoch 575/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0672 - accuracy: 0.9758 - val_loss: 0.1241 - val_accuracy: 0.9564\n",
      "Epoch 576/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0671 - accuracy: 0.9759 - val_loss: 0.1225 - val_accuracy: 0.9572\n",
      "Epoch 577/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0667 - accuracy: 0.9759 - val_loss: 0.1218 - val_accuracy: 0.9569\n",
      "Epoch 578/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0668 - accuracy: 0.9761 - val_loss: 0.1273 - val_accuracy: 0.9543\n",
      "Epoch 579/800\n",
      "1665/1665 [==============================] - 0s 140us/step - loss: 0.0669 - accuracy: 0.9761 - val_loss: 0.1222 - val_accuracy: 0.9570\n",
      "Epoch 580/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0665 - accuracy: 0.9761 - val_loss: 0.1272 - val_accuracy: 0.9554\n",
      "Epoch 581/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0668 - accuracy: 0.9758 - val_loss: 0.1211 - val_accuracy: 0.9571\n",
      "Epoch 582/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0665 - accuracy: 0.9760 - val_loss: 0.1228 - val_accuracy: 0.9579\n",
      "Epoch 583/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0664 - accuracy: 0.9761 - val_loss: 0.1233 - val_accuracy: 0.9562\n",
      "Epoch 584/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0664 - accuracy: 0.9761 - val_loss: 0.1253 - val_accuracy: 0.9561\n",
      "Epoch 585/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0665 - accuracy: 0.9758 - val_loss: 0.1198 - val_accuracy: 0.9582\n",
      "Epoch 586/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0663 - accuracy: 0.9760 - val_loss: 0.1194 - val_accuracy: 0.9584\n",
      "Epoch 587/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0660 - accuracy: 0.9762 - val_loss: 0.1216 - val_accuracy: 0.9570\n",
      "Epoch 588/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0659 - accuracy: 0.9762 - val_loss: 0.1210 - val_accuracy: 0.9580\n",
      "Epoch 589/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0659 - accuracy: 0.9764 - val_loss: 0.1205 - val_accuracy: 0.9575\n",
      "Epoch 590/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0660 - accuracy: 0.9761 - val_loss: 0.1194 - val_accuracy: 0.9586\n",
      "Epoch 591/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.0658 - accuracy: 0.9762 - val_loss: 0.1207 - val_accuracy: 0.9576\n",
      "Epoch 592/800\n",
      "1665/1665 [==============================] - 0s 142us/step - loss: 0.0658 - accuracy: 0.9763 - val_loss: 0.1226 - val_accuracy: 0.9578\n",
      "Epoch 593/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.0657 - accuracy: 0.9760 - val_loss: 0.1229 - val_accuracy: 0.9568\n",
      "Epoch 594/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0656 - accuracy: 0.9763 - val_loss: 0.1229 - val_accuracy: 0.9570\n",
      "Epoch 595/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0655 - accuracy: 0.9760 - val_loss: 0.1215 - val_accuracy: 0.9570\n",
      "Epoch 596/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0655 - accuracy: 0.9763 - val_loss: 0.1221 - val_accuracy: 0.9571\n",
      "Epoch 597/800\n",
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.0656 - accuracy: 0.9762 - val_loss: 0.1222 - val_accuracy: 0.9580\n",
      "Epoch 598/800\n",
      "1665/1665 [==============================] - 0s 147us/step - loss: 0.0653 - accuracy: 0.9763 - val_loss: 0.1203 - val_accuracy: 0.9587\n",
      "Epoch 599/800\n",
      "1665/1665 [==============================] - 0s 150us/step - loss: 0.0651 - accuracy: 0.9764 - val_loss: 0.1223 - val_accuracy: 0.9576\n",
      "Epoch 600/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.0651 - accuracy: 0.9763 - val_loss: 0.1255 - val_accuracy: 0.9562\n",
      "Epoch 601/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.0656 - accuracy: 0.9759 - val_loss: 0.1294 - val_accuracy: 0.9543\n",
      "Epoch 602/800\n",
      "1665/1665 [==============================] - 0s 133us/step - loss: 0.0654 - accuracy: 0.9764 - val_loss: 0.1227 - val_accuracy: 0.9574\n",
      "Epoch 603/800\n",
      "1665/1665 [==============================] - 0s 133us/step - loss: 0.0651 - accuracy: 0.9763 - val_loss: 0.1229 - val_accuracy: 0.9574\n",
      "Epoch 604/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1665/1665 [==============================] - 0s 142us/step - loss: 0.0651 - accuracy: 0.9764 - val_loss: 0.1209 - val_accuracy: 0.9576\n",
      "Epoch 605/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.0648 - accuracy: 0.9762 - val_loss: 0.1205 - val_accuracy: 0.9580\n",
      "Epoch 606/800\n",
      "1665/1665 [==============================] - 0s 141us/step - loss: 0.0646 - accuracy: 0.9766 - val_loss: 0.1231 - val_accuracy: 0.9572\n",
      "Epoch 607/800\n",
      "1665/1665 [==============================] - 0s 136us/step - loss: 0.0649 - accuracy: 0.9762 - val_loss: 0.1211 - val_accuracy: 0.9577\n",
      "Epoch 608/800\n",
      "1665/1665 [==============================] - 0s 140us/step - loss: 0.0645 - accuracy: 0.9765 - val_loss: 0.1207 - val_accuracy: 0.9582\n",
      "Epoch 609/800\n",
      "1665/1665 [==============================] - 0s 152us/step - loss: 0.0645 - accuracy: 0.9764 - val_loss: 0.1200 - val_accuracy: 0.9578\n",
      "Epoch 610/800\n",
      "1665/1665 [==============================] - 0s 136us/step - loss: 0.0644 - accuracy: 0.9767 - val_loss: 0.1223 - val_accuracy: 0.9584\n",
      "Epoch 611/800\n",
      "1665/1665 [==============================] - 0s 160us/step - loss: 0.0644 - accuracy: 0.9766 - val_loss: 0.1252 - val_accuracy: 0.9560\n",
      "Epoch 612/800\n",
      "1665/1665 [==============================] - 0s 157us/step - loss: 0.0645 - accuracy: 0.9764 - val_loss: 0.1265 - val_accuracy: 0.9548\n",
      "Epoch 613/800\n",
      "1665/1665 [==============================] - 0s 159us/step - loss: 0.0646 - accuracy: 0.9763 - val_loss: 0.1203 - val_accuracy: 0.9579\n",
      "Epoch 614/800\n",
      "1665/1665 [==============================] - 0s 172us/step - loss: 0.0641 - accuracy: 0.9767 - val_loss: 0.1208 - val_accuracy: 0.9584\n",
      "Epoch 615/800\n",
      "1665/1665 [==============================] - 0s 159us/step - loss: 0.0640 - accuracy: 0.9767 - val_loss: 0.1202 - val_accuracy: 0.9584\n",
      "Epoch 616/800\n",
      "1665/1665 [==============================] - 0s 179us/step - loss: 0.0639 - accuracy: 0.9767 - val_loss: 0.1208 - val_accuracy: 0.9576\n",
      "Epoch 617/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.0639 - accuracy: 0.9767 - val_loss: 0.1200 - val_accuracy: 0.9580\n",
      "Epoch 618/800\n",
      "1665/1665 [==============================] - 0s 133us/step - loss: 0.0639 - accuracy: 0.9768 - val_loss: 0.1262 - val_accuracy: 0.9556\n",
      "Epoch 619/800\n",
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.0642 - accuracy: 0.9765 - val_loss: 0.1278 - val_accuracy: 0.9549\n",
      "Epoch 620/800\n",
      "1665/1665 [==============================] - 0s 136us/step - loss: 0.0642 - accuracy: 0.9765 - val_loss: 0.1244 - val_accuracy: 0.9573\n",
      "Epoch 621/800\n",
      "1665/1665 [==============================] - 0s 141us/step - loss: 0.0639 - accuracy: 0.9767 - val_loss: 0.1322 - val_accuracy: 0.9526\n",
      "Epoch 622/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.0641 - accuracy: 0.9766 - val_loss: 0.1193 - val_accuracy: 0.9575\n",
      "Epoch 623/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0638 - accuracy: 0.9766 - val_loss: 0.1188 - val_accuracy: 0.9587\n",
      "Epoch 624/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0634 - accuracy: 0.9768 - val_loss: 0.1188 - val_accuracy: 0.9591\n",
      "Epoch 625/800\n",
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.0633 - accuracy: 0.9769 - val_loss: 0.1183 - val_accuracy: 0.9588\n",
      "Epoch 626/800\n",
      "1665/1665 [==============================] - 0s 139us/step - loss: 0.0632 - accuracy: 0.9771 - val_loss: 0.1211 - val_accuracy: 0.9584\n",
      "Epoch 627/800\n",
      "1665/1665 [==============================] - 0s 140us/step - loss: 0.0633 - accuracy: 0.9769 - val_loss: 0.1184 - val_accuracy: 0.9586\n",
      "Epoch 628/800\n",
      "1665/1665 [==============================] - 0s 145us/step - loss: 0.0633 - accuracy: 0.9767 - val_loss: 0.1182 - val_accuracy: 0.9587\n",
      "Epoch 629/800\n",
      "1665/1665 [==============================] - 0s 142us/step - loss: 0.0631 - accuracy: 0.9772 - val_loss: 0.1209 - val_accuracy: 0.9586\n",
      "Epoch 630/800\n",
      "1665/1665 [==============================] - 0s 141us/step - loss: 0.0632 - accuracy: 0.9770 - val_loss: 0.1278 - val_accuracy: 0.9554\n",
      "Epoch 631/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.0635 - accuracy: 0.9767 - val_loss: 0.1188 - val_accuracy: 0.9585\n",
      "Epoch 632/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0629 - accuracy: 0.9773 - val_loss: 0.1172 - val_accuracy: 0.9590\n",
      "Epoch 633/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0633 - accuracy: 0.9769 - val_loss: 0.1185 - val_accuracy: 0.9581\n",
      "Epoch 634/800\n",
      "1665/1665 [==============================] - 0s 144us/step - loss: 0.0628 - accuracy: 0.9770 - val_loss: 0.1203 - val_accuracy: 0.9589\n",
      "Epoch 635/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0627 - accuracy: 0.9769 - val_loss: 0.1187 - val_accuracy: 0.9589\n",
      "Epoch 636/800\n",
      "1665/1665 [==============================] - 0s 136us/step - loss: 0.0625 - accuracy: 0.9771 - val_loss: 0.1314 - val_accuracy: 0.9534\n",
      "Epoch 637/800\n",
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.0632 - accuracy: 0.9768 - val_loss: 0.1193 - val_accuracy: 0.9590\n",
      "Epoch 638/800\n",
      "1665/1665 [==============================] - 0s 140us/step - loss: 0.0624 - accuracy: 0.9772 - val_loss: 0.1207 - val_accuracy: 0.9585\n",
      "Epoch 639/800\n",
      "1665/1665 [==============================] - 0s 126us/step - loss: 0.0625 - accuracy: 0.9772 - val_loss: 0.1174 - val_accuracy: 0.9587\n",
      "Epoch 640/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0625 - accuracy: 0.9770 - val_loss: 0.1187 - val_accuracy: 0.9596\n",
      "Epoch 641/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.0623 - accuracy: 0.9772 - val_loss: 0.1203 - val_accuracy: 0.9579\n",
      "Epoch 642/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0625 - accuracy: 0.9772 - val_loss: 0.1177 - val_accuracy: 0.9591\n",
      "Epoch 643/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0621 - accuracy: 0.9773 - val_loss: 0.1185 - val_accuracy: 0.9592\n",
      "Epoch 644/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0621 - accuracy: 0.9773 - val_loss: 0.1545 - val_accuracy: 0.9471\n",
      "Epoch 645/800\n",
      "1665/1665 [==============================] - 0s 136us/step - loss: 0.0633 - accuracy: 0.9764 - val_loss: 0.1182 - val_accuracy: 0.9594\n",
      "Epoch 646/800\n",
      "1665/1665 [==============================] - 0s 147us/step - loss: 0.0621 - accuracy: 0.9771 - val_loss: 0.1176 - val_accuracy: 0.9595\n",
      "Epoch 647/800\n",
      "1665/1665 [==============================] - 0s 133us/step - loss: 0.0619 - accuracy: 0.9775 - val_loss: 0.1182 - val_accuracy: 0.9591\n",
      "Epoch 648/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.0618 - accuracy: 0.9770 - val_loss: 0.1175 - val_accuracy: 0.9591\n",
      "Epoch 649/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0619 - accuracy: 0.9772 - val_loss: 0.1212 - val_accuracy: 0.9580\n",
      "Epoch 650/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0619 - accuracy: 0.9773 - val_loss: 0.1231 - val_accuracy: 0.9574\n",
      "Epoch 651/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0620 - accuracy: 0.9773 - val_loss: 0.1174 - val_accuracy: 0.9588\n",
      "Epoch 652/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0617 - accuracy: 0.9771 - val_loss: 0.1190 - val_accuracy: 0.9592\n",
      "Epoch 653/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0616 - accuracy: 0.9775 - val_loss: 0.1169 - val_accuracy: 0.9600\n",
      "Epoch 654/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0614 - accuracy: 0.9774 - val_loss: 0.1186 - val_accuracy: 0.9592\n",
      "Epoch 655/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0613 - accuracy: 0.9775 - val_loss: 0.1176 - val_accuracy: 0.9594\n",
      "Epoch 656/800\n",
      "1665/1665 [==============================] - 0s 129us/step - loss: 0.0613 - accuracy: 0.9775 - val_loss: 0.1203 - val_accuracy: 0.9586\n",
      "Epoch 657/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0612 - accuracy: 0.9776 - val_loss: 0.1177 - val_accuracy: 0.9591\n",
      "Epoch 658/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0612 - accuracy: 0.9774 - val_loss: 0.1339 - val_accuracy: 0.9536\n",
      "Epoch 659/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1665/1665 [==============================] - 0s 168us/step - loss: 0.0619 - accuracy: 0.9771 - val_loss: 0.1182 - val_accuracy: 0.9588\n",
      "Epoch 660/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0612 - accuracy: 0.9775 - val_loss: 0.1204 - val_accuracy: 0.9593\n",
      "Epoch 661/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0611 - accuracy: 0.9775 - val_loss: 0.1182 - val_accuracy: 0.9592\n",
      "Epoch 662/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0611 - accuracy: 0.9775 - val_loss: 0.1301 - val_accuracy: 0.9549\n",
      "Epoch 663/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0615 - accuracy: 0.9772 - val_loss: 0.1178 - val_accuracy: 0.9607\n",
      "Epoch 664/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0607 - accuracy: 0.9773 - val_loss: 0.1606 - val_accuracy: 0.9439\n",
      "Epoch 665/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0626 - accuracy: 0.9768 - val_loss: 0.1194 - val_accuracy: 0.9592\n",
      "Epoch 666/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.0609 - accuracy: 0.9775 - val_loss: 0.1191 - val_accuracy: 0.9590\n",
      "Epoch 667/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0609 - accuracy: 0.9776 - val_loss: 0.1195 - val_accuracy: 0.9594\n",
      "Epoch 668/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0607 - accuracy: 0.9776 - val_loss: 0.1152 - val_accuracy: 0.9606\n",
      "Epoch 669/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0606 - accuracy: 0.9776 - val_loss: 0.1181 - val_accuracy: 0.9592\n",
      "Epoch 670/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0604 - accuracy: 0.9776 - val_loss: 0.1183 - val_accuracy: 0.9593\n",
      "Epoch 671/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0604 - accuracy: 0.9778 - val_loss: 0.1147 - val_accuracy: 0.9602\n",
      "Epoch 672/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0605 - accuracy: 0.9778 - val_loss: 0.1191 - val_accuracy: 0.9590\n",
      "Epoch 673/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0605 - accuracy: 0.9778 - val_loss: 0.1195 - val_accuracy: 0.9576\n",
      "Epoch 674/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0606 - accuracy: 0.9775 - val_loss: 0.1176 - val_accuracy: 0.9586\n",
      "Epoch 675/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0603 - accuracy: 0.9777 - val_loss: 0.1229 - val_accuracy: 0.9581\n",
      "Epoch 676/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0606 - accuracy: 0.9775 - val_loss: 0.1168 - val_accuracy: 0.9592\n",
      "Epoch 677/800\n",
      "1665/1665 [==============================] - 0s 126us/step - loss: 0.0599 - accuracy: 0.9778 - val_loss: 0.1308 - val_accuracy: 0.9565\n",
      "Epoch 678/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0606 - accuracy: 0.9776 - val_loss: 0.1164 - val_accuracy: 0.9601\n",
      "Epoch 679/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0599 - accuracy: 0.9779 - val_loss: 0.1193 - val_accuracy: 0.9585\n",
      "Epoch 680/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0601 - accuracy: 0.9777 - val_loss: 0.1193 - val_accuracy: 0.9592\n",
      "Epoch 681/800\n",
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.0603 - accuracy: 0.9777 - val_loss: 0.1849 - val_accuracy: 0.9410\n",
      "Epoch 682/800\n",
      "1665/1665 [==============================] - 0s 140us/step - loss: 0.0627 - accuracy: 0.9771 - val_loss: 0.1248 - val_accuracy: 0.9566\n",
      "Epoch 683/800\n",
      "1665/1665 [==============================] - 0s 167us/step - loss: 0.0602 - accuracy: 0.9777 - val_loss: 0.1162 - val_accuracy: 0.9598\n",
      "Epoch 684/800\n",
      "1665/1665 [==============================] - 0s 165us/step - loss: 0.0596 - accuracy: 0.9780 - val_loss: 0.1173 - val_accuracy: 0.9586\n",
      "Epoch 685/800\n",
      "1665/1665 [==============================] - 0s 154us/step - loss: 0.0597 - accuracy: 0.9779 - val_loss: 0.1173 - val_accuracy: 0.9604\n",
      "Epoch 686/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.0599 - accuracy: 0.9778 - val_loss: 0.1196 - val_accuracy: 0.9586\n",
      "Epoch 687/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.0599 - accuracy: 0.9779 - val_loss: 0.1160 - val_accuracy: 0.9605\n",
      "Epoch 688/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0593 - accuracy: 0.9781 - val_loss: 0.1171 - val_accuracy: 0.9596\n",
      "Epoch 689/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.0594 - accuracy: 0.9780 - val_loss: 0.1163 - val_accuracy: 0.9594\n",
      "Epoch 690/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0593 - accuracy: 0.9778 - val_loss: 0.1173 - val_accuracy: 0.9598\n",
      "Epoch 691/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0594 - accuracy: 0.9778 - val_loss: 0.1176 - val_accuracy: 0.9597\n",
      "Epoch 692/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0594 - accuracy: 0.9778 - val_loss: 0.1169 - val_accuracy: 0.9596\n",
      "Epoch 693/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0590 - accuracy: 0.9784 - val_loss: 0.1160 - val_accuracy: 0.9605\n",
      "Epoch 694/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0590 - accuracy: 0.9782 - val_loss: 0.1340 - val_accuracy: 0.9548\n",
      "Epoch 695/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0601 - accuracy: 0.9780 - val_loss: 0.1153 - val_accuracy: 0.9600\n",
      "Epoch 696/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0589 - accuracy: 0.9783 - val_loss: 0.1177 - val_accuracy: 0.9598\n",
      "Epoch 697/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0589 - accuracy: 0.9780 - val_loss: 0.1162 - val_accuracy: 0.9600\n",
      "Epoch 698/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0588 - accuracy: 0.9782 - val_loss: 0.1313 - val_accuracy: 0.9561\n",
      "Epoch 699/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0596 - accuracy: 0.9781 - val_loss: 0.1191 - val_accuracy: 0.9598\n",
      "Epoch 700/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.0589 - accuracy: 0.9781 - val_loss: 0.1195 - val_accuracy: 0.9595\n",
      "Epoch 701/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0589 - accuracy: 0.9782 - val_loss: 0.1350 - val_accuracy: 0.9545\n",
      "Epoch 702/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0596 - accuracy: 0.9779 - val_loss: 0.1169 - val_accuracy: 0.9601\n",
      "Epoch 703/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0588 - accuracy: 0.9782 - val_loss: 0.1194 - val_accuracy: 0.9590\n",
      "Epoch 704/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0586 - accuracy: 0.9782 - val_loss: 0.1151 - val_accuracy: 0.9605\n",
      "Epoch 705/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0584 - accuracy: 0.9783 - val_loss: 0.1151 - val_accuracy: 0.9605\n",
      "Epoch 706/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0583 - accuracy: 0.9783 - val_loss: 0.1178 - val_accuracy: 0.9581\n",
      "Epoch 707/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0586 - accuracy: 0.9783 - val_loss: 0.1147 - val_accuracy: 0.9605\n",
      "Epoch 708/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0582 - accuracy: 0.9783 - val_loss: 0.1167 - val_accuracy: 0.9606\n",
      "Epoch 709/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0583 - accuracy: 0.9782 - val_loss: 0.1187 - val_accuracy: 0.9603\n",
      "Epoch 710/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0582 - accuracy: 0.9785 - val_loss: 0.1181 - val_accuracy: 0.9593\n",
      "Epoch 711/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0580 - accuracy: 0.9785 - val_loss: 0.1166 - val_accuracy: 0.9606\n",
      "Epoch 712/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0581 - accuracy: 0.9782 - val_loss: 0.1161 - val_accuracy: 0.9604\n",
      "Epoch 713/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0580 - accuracy: 0.9783 - val_loss: 0.1165 - val_accuracy: 0.9607\n",
      "Epoch 714/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.0578 - accuracy: 0.9785 - val_loss: 0.1159 - val_accuracy: 0.9600\n",
      "Epoch 715/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0578 - accuracy: 0.9786 - val_loss: 0.1153 - val_accuracy: 0.9602\n",
      "Epoch 716/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0577 - accuracy: 0.9785 - val_loss: 0.1166 - val_accuracy: 0.9601\n",
      "Epoch 717/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0578 - accuracy: 0.9783 - val_loss: 0.1175 - val_accuracy: 0.9607\n",
      "Epoch 718/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0577 - accuracy: 0.9783 - val_loss: 0.1282 - val_accuracy: 0.9546\n",
      "Epoch 719/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0582 - accuracy: 0.9781 - val_loss: 0.1167 - val_accuracy: 0.9601\n",
      "Epoch 720/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0576 - accuracy: 0.9788 - val_loss: 0.1152 - val_accuracy: 0.9603\n",
      "Epoch 721/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0574 - accuracy: 0.9788 - val_loss: 0.1146 - val_accuracy: 0.9610\n",
      "Epoch 722/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0574 - accuracy: 0.9788 - val_loss: 0.1186 - val_accuracy: 0.9602\n",
      "Epoch 723/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0577 - accuracy: 0.9785 - val_loss: 0.1155 - val_accuracy: 0.9608\n",
      "Epoch 724/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0577 - accuracy: 0.9785 - val_loss: 0.1175 - val_accuracy: 0.9588\n",
      "Epoch 725/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0577 - accuracy: 0.9784 - val_loss: 0.1155 - val_accuracy: 0.9610\n",
      "Epoch 726/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0576 - accuracy: 0.9786 - val_loss: 0.1391 - val_accuracy: 0.9517\n",
      "Epoch 727/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0584 - accuracy: 0.9781 - val_loss: 0.1154 - val_accuracy: 0.9597\n",
      "Epoch 728/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0574 - accuracy: 0.9784 - val_loss: 0.1245 - val_accuracy: 0.9581\n",
      "Epoch 729/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.0576 - accuracy: 0.9783 - val_loss: 0.1151 - val_accuracy: 0.9610\n",
      "Epoch 730/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0572 - accuracy: 0.9789 - val_loss: 0.1149 - val_accuracy: 0.9598\n",
      "Epoch 731/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0571 - accuracy: 0.9785 - val_loss: 0.1167 - val_accuracy: 0.9604\n",
      "Epoch 732/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0571 - accuracy: 0.9785 - val_loss: 0.1134 - val_accuracy: 0.9611\n",
      "Epoch 733/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.0572 - accuracy: 0.9788 - val_loss: 0.1185 - val_accuracy: 0.9598\n",
      "Epoch 734/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0570 - accuracy: 0.9787 - val_loss: 0.1160 - val_accuracy: 0.9601\n",
      "Epoch 735/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0568 - accuracy: 0.9789 - val_loss: 0.1397 - val_accuracy: 0.9514\n",
      "Epoch 736/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0582 - accuracy: 0.9784 - val_loss: 0.1150 - val_accuracy: 0.9604\n",
      "Epoch 737/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.0566 - accuracy: 0.9790 - val_loss: 0.1164 - val_accuracy: 0.9595\n",
      "Epoch 738/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0568 - accuracy: 0.9787 - val_loss: 0.1132 - val_accuracy: 0.9613\n",
      "Epoch 739/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0565 - accuracy: 0.9788 - val_loss: 0.1166 - val_accuracy: 0.9600\n",
      "Epoch 740/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0568 - accuracy: 0.9789 - val_loss: 0.1143 - val_accuracy: 0.9611\n",
      "Epoch 741/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0564 - accuracy: 0.9789 - val_loss: 0.1156 - val_accuracy: 0.9603\n",
      "Epoch 742/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0567 - accuracy: 0.9789 - val_loss: 0.1162 - val_accuracy: 0.9603\n",
      "Epoch 743/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0566 - accuracy: 0.9789 - val_loss: 0.1159 - val_accuracy: 0.9602\n",
      "Epoch 744/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0565 - accuracy: 0.9789 - val_loss: 0.1165 - val_accuracy: 0.9586\n",
      "Epoch 745/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0565 - accuracy: 0.9788 - val_loss: 0.1148 - val_accuracy: 0.9607\n",
      "Epoch 746/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0563 - accuracy: 0.9790 - val_loss: 0.1158 - val_accuracy: 0.9610\n",
      "Epoch 747/800\n",
      "1665/1665 [==============================] - 0s 109us/step - loss: 0.0563 - accuracy: 0.9789 - val_loss: 0.1163 - val_accuracy: 0.9600\n",
      "Epoch 748/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0563 - accuracy: 0.9792 - val_loss: 0.1156 - val_accuracy: 0.9601\n",
      "Epoch 749/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0562 - accuracy: 0.9790 - val_loss: 0.1157 - val_accuracy: 0.9607\n",
      "Epoch 750/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0560 - accuracy: 0.9791 - val_loss: 0.1138 - val_accuracy: 0.9611\n",
      "Epoch 751/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0560 - accuracy: 0.9790 - val_loss: 0.1157 - val_accuracy: 0.9608\n",
      "Epoch 752/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0560 - accuracy: 0.9789 - val_loss: 0.1189 - val_accuracy: 0.9589\n",
      "Epoch 753/800\n",
      "1665/1665 [==============================] - 0s 112us/step - loss: 0.0561 - accuracy: 0.9789 - val_loss: 0.1169 - val_accuracy: 0.9610\n",
      "Epoch 754/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0559 - accuracy: 0.9791 - val_loss: 0.1152 - val_accuracy: 0.9606\n",
      "Epoch 755/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.0558 - accuracy: 0.9789 - val_loss: 0.1143 - val_accuracy: 0.9619\n",
      "Epoch 756/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0557 - accuracy: 0.9792 - val_loss: 0.1166 - val_accuracy: 0.9605\n",
      "Epoch 757/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0561 - accuracy: 0.9791 - val_loss: 0.1162 - val_accuracy: 0.9610\n",
      "Epoch 758/800\n",
      "1665/1665 [==============================] - 0s 112us/step - loss: 0.0556 - accuracy: 0.9794 - val_loss: 0.1175 - val_accuracy: 0.9606\n",
      "Epoch 759/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.0557 - accuracy: 0.9791 - val_loss: 0.1149 - val_accuracy: 0.9614\n",
      "Epoch 760/800\n",
      "1665/1665 [==============================] - 0s 142us/step - loss: 0.0556 - accuracy: 0.9790 - val_loss: 0.1202 - val_accuracy: 0.9584\n",
      "Epoch 761/800\n",
      "1665/1665 [==============================] - 0s 136us/step - loss: 0.0559 - accuracy: 0.9793 - val_loss: 0.1138 - val_accuracy: 0.9612\n",
      "Epoch 762/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.0553 - accuracy: 0.9791 - val_loss: 0.1148 - val_accuracy: 0.9609\n",
      "Epoch 763/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.0555 - accuracy: 0.9791 - val_loss: 0.1155 - val_accuracy: 0.9595\n",
      "Epoch 764/800\n",
      "1665/1665 [==============================] - 0s 150us/step - loss: 0.0555 - accuracy: 0.9788 - val_loss: 0.1207 - val_accuracy: 0.9587\n",
      "Epoch 765/800\n",
      "1665/1665 [==============================] - 0s 157us/step - loss: 0.0556 - accuracy: 0.9789 - val_loss: 0.1163 - val_accuracy: 0.9594\n",
      "Epoch 766/800\n",
      "1665/1665 [==============================] - 0s 156us/step - loss: 0.0558 - accuracy: 0.9788 - val_loss: 0.1163 - val_accuracy: 0.9608\n",
      "Epoch 767/800\n",
      "1665/1665 [==============================] - 0s 154us/step - loss: 0.0559 - accuracy: 0.9788 - val_loss: 0.1234 - val_accuracy: 0.9566\n",
      "Epoch 768/800\n",
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.0558 - accuracy: 0.9789 - val_loss: 0.1150 - val_accuracy: 0.9609\n",
      "Epoch 769/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0552 - accuracy: 0.9793 - val_loss: 0.1135 - val_accuracy: 0.9619\n",
      "Epoch 770/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0551 - accuracy: 0.9794 - val_loss: 0.1157 - val_accuracy: 0.9611\n",
      "Epoch 771/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0549 - accuracy: 0.9794 - val_loss: 0.1176 - val_accuracy: 0.9602\n",
      "Epoch 772/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0551 - accuracy: 0.9792 - val_loss: 0.1149 - val_accuracy: 0.9613\n",
      "Epoch 773/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.0549 - accuracy: 0.9795 - val_loss: 0.1143 - val_accuracy: 0.9604\n",
      "Epoch 774/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0549 - accuracy: 0.9794 - val_loss: 0.1143 - val_accuracy: 0.9614\n",
      "Epoch 775/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.0548 - accuracy: 0.9795 - val_loss: 0.1155 - val_accuracy: 0.9613\n",
      "Epoch 776/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0547 - accuracy: 0.9794 - val_loss: 0.1159 - val_accuracy: 0.9610\n",
      "Epoch 777/800\n",
      "1665/1665 [==============================] - 0s 126us/step - loss: 0.0547 - accuracy: 0.9795 - val_loss: 0.1159 - val_accuracy: 0.9610\n",
      "Epoch 778/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0547 - accuracy: 0.9794 - val_loss: 0.1142 - val_accuracy: 0.9614\n",
      "Epoch 779/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.0546 - accuracy: 0.9794 - val_loss: 0.1136 - val_accuracy: 0.9614\n",
      "Epoch 780/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0545 - accuracy: 0.9794 - val_loss: 0.1190 - val_accuracy: 0.9604\n",
      "Epoch 781/800\n",
      "1665/1665 [==============================] - 0s 139us/step - loss: 0.0547 - accuracy: 0.9792 - val_loss: 0.1147 - val_accuracy: 0.9610\n",
      "Epoch 782/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.0546 - accuracy: 0.9790 - val_loss: 0.1157 - val_accuracy: 0.9616\n",
      "Epoch 783/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0544 - accuracy: 0.9794 - val_loss: 0.1139 - val_accuracy: 0.9614\n",
      "Epoch 784/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0544 - accuracy: 0.9796 - val_loss: 0.1149 - val_accuracy: 0.9607\n",
      "Epoch 785/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0543 - accuracy: 0.9795 - val_loss: 0.1162 - val_accuracy: 0.9607\n",
      "Epoch 786/800\n",
      "1665/1665 [==============================] - 0s 164us/step - loss: 0.0544 - accuracy: 0.9793 - val_loss: 0.1131 - val_accuracy: 0.9617\n",
      "Epoch 787/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.0540 - accuracy: 0.9794 - val_loss: 0.1161 - val_accuracy: 0.9614\n",
      "Epoch 788/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0542 - accuracy: 0.9796 - val_loss: 0.1141 - val_accuracy: 0.9624\n",
      "Epoch 789/800\n",
      "1665/1665 [==============================] - 0s 151us/step - loss: 0.0540 - accuracy: 0.9796 - val_loss: 0.1157 - val_accuracy: 0.9603\n",
      "Epoch 790/800\n",
      "1665/1665 [==============================] - 0s 136us/step - loss: 0.0544 - accuracy: 0.9795 - val_loss: 0.1131 - val_accuracy: 0.9618\n",
      "Epoch 791/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0539 - accuracy: 0.9798 - val_loss: 0.1168 - val_accuracy: 0.9609\n",
      "Epoch 792/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0544 - accuracy: 0.9796 - val_loss: 0.1167 - val_accuracy: 0.9610\n",
      "Epoch 793/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0542 - accuracy: 0.9793 - val_loss: 0.1126 - val_accuracy: 0.9624\n",
      "Epoch 794/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.0540 - accuracy: 0.9794 - val_loss: 0.1147 - val_accuracy: 0.9617\n",
      "Epoch 795/800\n",
      "1665/1665 [==============================] - 0s 151us/step - loss: 0.0539 - accuracy: 0.9794 - val_loss: 0.1154 - val_accuracy: 0.9607\n",
      "Epoch 796/800\n",
      "1665/1665 [==============================] - 0s 129us/step - loss: 0.0541 - accuracy: 0.9794 - val_loss: 0.1248 - val_accuracy: 0.9567\n",
      "Epoch 797/800\n",
      "1665/1665 [==============================] - 0s 157us/step - loss: 0.0544 - accuracy: 0.9795 - val_loss: 0.1145 - val_accuracy: 0.9611\n",
      "Epoch 798/800\n",
      "1665/1665 [==============================] - 0s 166us/step - loss: 0.0536 - accuracy: 0.9797 - val_loss: 0.1141 - val_accuracy: 0.9622\n",
      "Epoch 799/800\n",
      "1665/1665 [==============================] - 0s 166us/step - loss: 0.0537 - accuracy: 0.9795 - val_loss: 0.1125 - val_accuracy: 0.9625\n",
      "Epoch 800/800\n",
      "1665/1665 [==============================] - 0s 158us/step - loss: 0.0536 - accuracy: 0.9796 - val_loss: 0.1165 - val_accuracy: 0.9604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clair\\Anaconda3\\envs\\CNNs\\lib\\site-packages\\ipykernel_launcher.py:125: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "C:\\Users\\clair\\Anaconda3\\envs\\CNNs\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:808: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Train on 1665 samples, validate on 556 samples\n",
      "Epoch 1/800\n",
      "1665/1665 [==============================] - 0s 240us/step - loss: 0.6665 - accuracy: 0.7533 - val_loss: 0.6346 - val_accuracy: 0.8244\n",
      "Epoch 2/800\n",
      "1665/1665 [==============================] - 0s 129us/step - loss: 0.5210 - accuracy: 0.8809 - val_loss: 0.3470 - val_accuracy: 0.9329\n",
      "Epoch 3/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.2172 - accuracy: 0.9538 - val_loss: 0.2504 - val_accuracy: 0.9329\n",
      "Epoch 4/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1888 - accuracy: 0.9538 - val_loss: 0.2494 - val_accuracy: 0.9329\n",
      "Epoch 5/800\n",
      "1665/1665 [==============================] - 0s 136us/step - loss: 0.1870 - accuracy: 0.9538 - val_loss: 0.2489 - val_accuracy: 0.9329\n",
      "Epoch 6/800\n",
      "1665/1665 [==============================] - 0s 136us/step - loss: 0.1865 - accuracy: 0.9538 - val_loss: 0.2488 - val_accuracy: 0.9329\n",
      "Epoch 7/800\n",
      "1665/1665 [==============================] - 0s 142us/step - loss: 0.1862 - accuracy: 0.9538 - val_loss: 0.2486 - val_accuracy: 0.9329\n",
      "Epoch 8/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.1861 - accuracy: 0.9538 - val_loss: 0.2491 - val_accuracy: 0.9329\n",
      "Epoch 9/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.1860 - accuracy: 0.9538 - val_loss: 0.2491 - val_accuracy: 0.9329\n",
      "Epoch 10/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.1860 - accuracy: 0.9538 - val_loss: 0.2486 - val_accuracy: 0.9329\n",
      "Epoch 11/800\n",
      "1665/1665 [==============================] - 0s 156us/step - loss: 0.1859 - accuracy: 0.9538 - val_loss: 0.2483 - val_accuracy: 0.9329\n",
      "Epoch 12/800\n",
      "1665/1665 [==============================] - 0s 159us/step - loss: 0.1859 - accuracy: 0.9538 - val_loss: 0.2486 - val_accuracy: 0.9329\n",
      "Epoch 13/800\n",
      "1665/1665 [==============================] - 0s 157us/step - loss: 0.1858 - accuracy: 0.9538 - val_loss: 0.2483 - val_accuracy: 0.9329\n",
      "Epoch 14/800\n",
      "1665/1665 [==============================] - 0s 164us/step - loss: 0.1858 - accuracy: 0.9538 - val_loss: 0.2486 - val_accuracy: 0.9329\n",
      "Epoch 15/800\n",
      "1665/1665 [==============================] - 0s 160us/step - loss: 0.1857 - accuracy: 0.9538 - val_loss: 0.2483 - val_accuracy: 0.9329\n",
      "Epoch 16/800\n",
      "1665/1665 [==============================] - 0s 165us/step - loss: 0.1857 - accuracy: 0.9538 - val_loss: 0.2487 - val_accuracy: 0.9329\n",
      "Epoch 17/800\n",
      "1665/1665 [==============================] - 0s 155us/step - loss: 0.1856 - accuracy: 0.9538 - val_loss: 0.2481 - val_accuracy: 0.9329\n",
      "Epoch 18/800\n",
      "1665/1665 [==============================] - 0s 164us/step - loss: 0.1856 - accuracy: 0.9538 - val_loss: 0.2473 - val_accuracy: 0.9329\n",
      "Epoch 19/800\n",
      "1665/1665 [==============================] - 0s 161us/step - loss: 0.1855 - accuracy: 0.9538 - val_loss: 0.2476 - val_accuracy: 0.9329\n",
      "Epoch 20/800\n",
      "1665/1665 [==============================] - 0s 126us/step - loss: 0.1855 - accuracy: 0.9538 - val_loss: 0.2482 - val_accuracy: 0.9329\n",
      "Epoch 21/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.1854 - accuracy: 0.9538 - val_loss: 0.2475 - val_accuracy: 0.9329\n",
      "Epoch 22/800\n",
      "1665/1665 [==============================] - 0s 151us/step - loss: 0.1854 - accuracy: 0.9538 - val_loss: 0.2479 - val_accuracy: 0.9329\n",
      "Epoch 23/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.1853 - accuracy: 0.9538 - val_loss: 0.2476 - val_accuracy: 0.9329\n",
      "Epoch 24/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.1853 - accuracy: 0.9538 - val_loss: 0.2474 - val_accuracy: 0.9329\n",
      "Epoch 25/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.1852 - accuracy: 0.9538 - val_loss: 0.2477 - val_accuracy: 0.9329\n",
      "Epoch 26/800\n",
      "1665/1665 [==============================] - 0s 142us/step - loss: 0.1852 - accuracy: 0.9538 - val_loss: 0.2479 - val_accuracy: 0.9329\n",
      "Epoch 27/800\n",
      "1665/1665 [==============================] - 0s 145us/step - loss: 0.1851 - accuracy: 0.9538 - val_loss: 0.2476 - val_accuracy: 0.9329\n",
      "Epoch 28/800\n",
      "1665/1665 [==============================] - 0s 150us/step - loss: 0.1851 - accuracy: 0.9538 - val_loss: 0.2476 - val_accuracy: 0.9329\n",
      "Epoch 29/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.1850 - accuracy: 0.9538 - val_loss: 0.2474 - val_accuracy: 0.9329\n",
      "Epoch 30/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1850 - accuracy: 0.9538 - val_loss: 0.2475 - val_accuracy: 0.9329\n",
      "Epoch 31/800\n",
      "1665/1665 [==============================] - 0s 142us/step - loss: 0.1849 - accuracy: 0.9538 - val_loss: 0.2475 - val_accuracy: 0.9329\n",
      "Epoch 32/800\n",
      "1665/1665 [==============================] - 0s 142us/step - loss: 0.1849 - accuracy: 0.9538 - val_loss: 0.2471 - val_accuracy: 0.9329\n",
      "Epoch 33/800\n",
      "1665/1665 [==============================] - 0s 161us/step - loss: 0.1848 - accuracy: 0.9538 - val_loss: 0.2472 - val_accuracy: 0.9329\n",
      "Epoch 34/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.1847 - accuracy: 0.9538 - val_loss: 0.2467 - val_accuracy: 0.9329\n",
      "Epoch 35/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.1847 - accuracy: 0.9538 - val_loss: 0.2466 - val_accuracy: 0.9329\n",
      "Epoch 36/800\n",
      "1665/1665 [==============================] - 0s 140us/step - loss: 0.1846 - accuracy: 0.9538 - val_loss: 0.2464 - val_accuracy: 0.9329\n",
      "Epoch 37/800\n",
      "1665/1665 [==============================] - 0s 147us/step - loss: 0.1846 - accuracy: 0.9538 - val_loss: 0.2468 - val_accuracy: 0.9329\n",
      "Epoch 38/800\n",
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.1845 - accuracy: 0.9538 - val_loss: 0.2465 - val_accuracy: 0.9329\n",
      "Epoch 39/800\n",
      "1665/1665 [==============================] - 0s 140us/step - loss: 0.1844 - accuracy: 0.9538 - val_loss: 0.2469 - val_accuracy: 0.9329\n",
      "Epoch 40/800\n",
      "1665/1665 [==============================] - 0s 145us/step - loss: 0.1843 - accuracy: 0.9538 - val_loss: 0.2461 - val_accuracy: 0.9329\n",
      "Epoch 41/800\n",
      "1665/1665 [==============================] - 0s 136us/step - loss: 0.1843 - accuracy: 0.9538 - val_loss: 0.2461 - val_accuracy: 0.9329\n",
      "Epoch 42/800\n",
      "1665/1665 [==============================] - 0s 154us/step - loss: 0.1842 - accuracy: 0.9538 - val_loss: 0.2459 - val_accuracy: 0.9329\n",
      "Epoch 43/800\n",
      "1665/1665 [==============================] - 0s 141us/step - loss: 0.1842 - accuracy: 0.9538 - val_loss: 0.2460 - val_accuracy: 0.9329\n",
      "Epoch 44/800\n",
      "1665/1665 [==============================] - 0s 139us/step - loss: 0.1840 - accuracy: 0.9538 - val_loss: 0.2455 - val_accuracy: 0.9329\n",
      "Epoch 45/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.1840 - accuracy: 0.9538 - val_loss: 0.2459 - val_accuracy: 0.9329\n",
      "Epoch 46/800\n",
      "1665/1665 [==============================] - 0s 144us/step - loss: 0.1839 - accuracy: 0.9538 - val_loss: 0.2458 - val_accuracy: 0.9329\n",
      "Epoch 47/800\n",
      "1665/1665 [==============================] - 0s 146us/step - loss: 0.1838 - accuracy: 0.9538 - val_loss: 0.2457 - val_accuracy: 0.9329\n",
      "Epoch 48/800\n",
      "1665/1665 [==============================] - 0s 146us/step - loss: 0.1838 - accuracy: 0.9538 - val_loss: 0.2456 - val_accuracy: 0.9329\n",
      "Epoch 49/800\n",
      "1665/1665 [==============================] - 0s 141us/step - loss: 0.1837 - accuracy: 0.9538 - val_loss: 0.2454 - val_accuracy: 0.9329\n",
      "Epoch 50/800\n",
      "1665/1665 [==============================] - 0s 143us/step - loss: 0.1836 - accuracy: 0.9538 - val_loss: 0.2449 - val_accuracy: 0.9329\n",
      "Epoch 51/800\n",
      "1665/1665 [==============================] - 0s 141us/step - loss: 0.1836 - accuracy: 0.9538 - val_loss: 0.2450 - val_accuracy: 0.9329\n",
      "Epoch 52/800\n",
      "1665/1665 [==============================] - 0s 144us/step - loss: 0.1835 - accuracy: 0.9538 - val_loss: 0.2452 - val_accuracy: 0.9329\n",
      "Epoch 53/800\n",
      "1665/1665 [==============================] - 0s 147us/step - loss: 0.1834 - accuracy: 0.9538 - val_loss: 0.2448 - val_accuracy: 0.9329\n",
      "Epoch 54/800\n",
      "1665/1665 [==============================] - 0s 145us/step - loss: 0.1833 - accuracy: 0.9538 - val_loss: 0.2443 - val_accuracy: 0.9329\n",
      "Epoch 55/800\n",
      "1665/1665 [==============================] - 0s 148us/step - loss: 0.1832 - accuracy: 0.9538 - val_loss: 0.2447 - val_accuracy: 0.9329\n",
      "Epoch 56/800\n",
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.1830 - accuracy: 0.9538 - val_loss: 0.2447 - val_accuracy: 0.9329\n",
      "Epoch 57/800\n",
      "1665/1665 [==============================] - 0s 126us/step - loss: 0.1829 - accuracy: 0.9538 - val_loss: 0.2449 - val_accuracy: 0.9329\n",
      "Epoch 58/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.1828 - accuracy: 0.9538 - val_loss: 0.2442 - val_accuracy: 0.9329\n",
      "Epoch 59/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.1827 - accuracy: 0.9538 - val_loss: 0.2438 - val_accuracy: 0.9329\n",
      "Epoch 60/800\n",
      "1665/1665 [==============================] - 0s 145us/step - loss: 0.1827 - accuracy: 0.9538 - val_loss: 0.2430 - val_accuracy: 0.9329\n",
      "Epoch 61/800\n",
      "1665/1665 [==============================] - 0s 146us/step - loss: 0.1825 - accuracy: 0.9538 - val_loss: 0.2435 - val_accuracy: 0.9329\n",
      "Epoch 62/800\n",
      "1665/1665 [==============================] - 0s 149us/step - loss: 0.1824 - accuracy: 0.9538 - val_loss: 0.2430 - val_accuracy: 0.9329\n",
      "Epoch 63/800\n",
      "1665/1665 [==============================] - 0s 142us/step - loss: 0.1823 - accuracy: 0.9538 - val_loss: 0.2428 - val_accuracy: 0.9329\n",
      "Epoch 64/800\n",
      "1665/1665 [==============================] - 0s 149us/step - loss: 0.1821 - accuracy: 0.9538 - val_loss: 0.2426 - val_accuracy: 0.9329\n",
      "Epoch 65/800\n",
      "1665/1665 [==============================] - 0s 142us/step - loss: 0.1820 - accuracy: 0.9538 - val_loss: 0.2424 - val_accuracy: 0.9329\n",
      "Epoch 66/800\n",
      "1665/1665 [==============================] - 0s 139us/step - loss: 0.1818 - accuracy: 0.9538 - val_loss: 0.2421 - val_accuracy: 0.9329\n",
      "Epoch 67/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.1817 - accuracy: 0.9538 - val_loss: 0.2424 - val_accuracy: 0.9329\n",
      "Epoch 68/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.1815 - accuracy: 0.9538 - val_loss: 0.2424 - val_accuracy: 0.9329\n",
      "Epoch 69/800\n",
      "1665/1665 [==============================] - 0s 142us/step - loss: 0.1813 - accuracy: 0.9538 - val_loss: 0.2421 - val_accuracy: 0.9329\n",
      "Epoch 70/800\n",
      "1665/1665 [==============================] - 0s 148us/step - loss: 0.1812 - accuracy: 0.9538 - val_loss: 0.2413 - val_accuracy: 0.9329\n",
      "Epoch 71/800\n",
      "1665/1665 [==============================] - 0s 147us/step - loss: 0.1810 - accuracy: 0.9538 - val_loss: 0.2410 - val_accuracy: 0.9329\n",
      "Epoch 72/800\n",
      "1665/1665 [==============================] - 0s 139us/step - loss: 0.1808 - accuracy: 0.9538 - val_loss: 0.2406 - val_accuracy: 0.9329\n",
      "Epoch 73/800\n",
      "1665/1665 [==============================] - 0s 144us/step - loss: 0.1806 - accuracy: 0.9538 - val_loss: 0.2407 - val_accuracy: 0.9329\n",
      "Epoch 74/800\n",
      "1665/1665 [==============================] - 0s 140us/step - loss: 0.1805 - accuracy: 0.9538 - val_loss: 0.2400 - val_accuracy: 0.9329\n",
      "Epoch 75/800\n",
      "1665/1665 [==============================] - 0s 136us/step - loss: 0.1803 - accuracy: 0.9538 - val_loss: 0.2400 - val_accuracy: 0.9329\n",
      "Epoch 76/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.1800 - accuracy: 0.9538 - val_loss: 0.2401 - val_accuracy: 0.9329\n",
      "Epoch 77/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1798 - accuracy: 0.9538 - val_loss: 0.2392 - val_accuracy: 0.9329\n",
      "Epoch 78/800\n",
      "1665/1665 [==============================] - 0s 164us/step - loss: 0.1796 - accuracy: 0.9538 - val_loss: 0.2387 - val_accuracy: 0.9329\n",
      "Epoch 79/800\n",
      "1665/1665 [==============================] - 0s 171us/step - loss: 0.1794 - accuracy: 0.9538 - val_loss: 0.2386 - val_accuracy: 0.9329\n",
      "Epoch 80/800\n",
      "1665/1665 [==============================] - 0s 156us/step - loss: 0.1791 - accuracy: 0.9538 - val_loss: 0.2381 - val_accuracy: 0.9329\n",
      "Epoch 81/800\n",
      "1665/1665 [==============================] - 0s 141us/step - loss: 0.1788 - accuracy: 0.9538 - val_loss: 0.2380 - val_accuracy: 0.9329\n",
      "Epoch 82/800\n",
      "1665/1665 [==============================] - 0s 139us/step - loss: 0.1786 - accuracy: 0.9538 - val_loss: 0.2375 - val_accuracy: 0.9329\n",
      "Epoch 83/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.1783 - accuracy: 0.9538 - val_loss: 0.2364 - val_accuracy: 0.9329\n",
      "Epoch 84/800\n",
      "1665/1665 [==============================] - 0s 133us/step - loss: 0.1780 - accuracy: 0.9538 - val_loss: 0.2365 - val_accuracy: 0.9329\n",
      "Epoch 85/800\n",
      "1665/1665 [==============================] - 0s 155us/step - loss: 0.1777 - accuracy: 0.9538 - val_loss: 0.2360 - val_accuracy: 0.9329\n",
      "Epoch 86/800\n",
      "1665/1665 [==============================] - 0s 159us/step - loss: 0.1774 - accuracy: 0.9538 - val_loss: 0.2359 - val_accuracy: 0.9329\n",
      "Epoch 87/800\n",
      "1665/1665 [==============================] - 0s 159us/step - loss: 0.1771 - accuracy: 0.9538 - val_loss: 0.2356 - val_accuracy: 0.9329\n",
      "Epoch 88/800\n",
      "1665/1665 [==============================] - 0s 144us/step - loss: 0.1767 - accuracy: 0.9538 - val_loss: 0.2349 - val_accuracy: 0.9329\n",
      "Epoch 89/800\n",
      "1665/1665 [==============================] - 0s 129us/step - loss: 0.1764 - accuracy: 0.9538 - val_loss: 0.2344 - val_accuracy: 0.9329\n",
      "Epoch 90/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.1760 - accuracy: 0.9538 - val_loss: 0.2337 - val_accuracy: 0.9329\n",
      "Epoch 91/800\n",
      "1665/1665 [==============================] - 0s 142us/step - loss: 0.1757 - accuracy: 0.9538 - val_loss: 0.2331 - val_accuracy: 0.9329\n",
      "Epoch 92/800\n",
      "1665/1665 [==============================] - 0s 146us/step - loss: 0.1753 - accuracy: 0.9538 - val_loss: 0.2321 - val_accuracy: 0.9329\n",
      "Epoch 93/800\n",
      "1665/1665 [==============================] - 0s 154us/step - loss: 0.1750 - accuracy: 0.9538 - val_loss: 0.2314 - val_accuracy: 0.9329\n",
      "Epoch 94/800\n",
      "1665/1665 [==============================] - 0s 158us/step - loss: 0.1746 - accuracy: 0.9538 - val_loss: 0.2312 - val_accuracy: 0.9329\n",
      "Epoch 95/800\n",
      "1665/1665 [==============================] - 0s 140us/step - loss: 0.1742 - accuracy: 0.9538 - val_loss: 0.2311 - val_accuracy: 0.9329\n",
      "Epoch 96/800\n",
      "1665/1665 [==============================] - 0s 142us/step - loss: 0.1737 - accuracy: 0.9538 - val_loss: 0.2305 - val_accuracy: 0.9329\n",
      "Epoch 97/800\n",
      "1665/1665 [==============================] - 0s 142us/step - loss: 0.1734 - accuracy: 0.9538 - val_loss: 0.2296 - val_accuracy: 0.9329\n",
      "Epoch 98/800\n",
      "1665/1665 [==============================] - 0s 140us/step - loss: 0.1731 - accuracy: 0.9538 - val_loss: 0.2290 - val_accuracy: 0.9329\n",
      "Epoch 99/800\n",
      "1665/1665 [==============================] - 0s 140us/step - loss: 0.1726 - accuracy: 0.9538 - val_loss: 0.2286 - val_accuracy: 0.9329\n",
      "Epoch 100/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.1722 - accuracy: 0.9538 - val_loss: 0.2280 - val_accuracy: 0.9329\n",
      "Epoch 101/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.1718 - accuracy: 0.9538 - val_loss: 0.2267 - val_accuracy: 0.9329\n",
      "Epoch 102/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.1715 - accuracy: 0.9538 - val_loss: 0.2266 - val_accuracy: 0.9329\n",
      "Epoch 103/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.1711 - accuracy: 0.9538 - val_loss: 0.2268 - val_accuracy: 0.9329\n",
      "Epoch 104/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1707 - accuracy: 0.9538 - val_loss: 0.2262 - val_accuracy: 0.9329\n",
      "Epoch 105/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.1703 - accuracy: 0.9538 - val_loss: 0.2253 - val_accuracy: 0.9329\n",
      "Epoch 106/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.1700 - accuracy: 0.9538 - val_loss: 0.2255 - val_accuracy: 0.9329\n",
      "Epoch 107/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.1696 - accuracy: 0.9538 - val_loss: 0.2242 - val_accuracy: 0.9329\n",
      "Epoch 108/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.1692 - accuracy: 0.9538 - val_loss: 0.2241 - val_accuracy: 0.9329\n",
      "Epoch 109/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1688 - accuracy: 0.9538 - val_loss: 0.2237 - val_accuracy: 0.9329\n",
      "Epoch 110/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.1683 - accuracy: 0.9538 - val_loss: 0.2232 - val_accuracy: 0.9330\n",
      "Epoch 111/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.1680 - accuracy: 0.9538 - val_loss: 0.2228 - val_accuracy: 0.9330\n",
      "Epoch 112/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.1677 - accuracy: 0.9538 - val_loss: 0.2215 - val_accuracy: 0.9329\n",
      "Epoch 113/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.1673 - accuracy: 0.9538 - val_loss: 0.2218 - val_accuracy: 0.9330\n",
      "Epoch 114/800\n",
      "1665/1665 [==============================] - 0s 129us/step - loss: 0.1670 - accuracy: 0.9538 - val_loss: 0.2220 - val_accuracy: 0.9334\n",
      "Epoch 115/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.1666 - accuracy: 0.9538 - val_loss: 0.2206 - val_accuracy: 0.9329\n",
      "Epoch 116/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.1662 - accuracy: 0.9538 - val_loss: 0.2207 - val_accuracy: 0.9328\n",
      "Epoch 117/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.1658 - accuracy: 0.9538 - val_loss: 0.2196 - val_accuracy: 0.9332\n",
      "Epoch 118/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.1654 - accuracy: 0.9539 - val_loss: 0.2197 - val_accuracy: 0.9326\n",
      "Epoch 119/800\n",
      "1665/1665 [==============================] - 0s 136us/step - loss: 0.1649 - accuracy: 0.9538 - val_loss: 0.2189 - val_accuracy: 0.9332\n",
      "Epoch 120/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.1646 - accuracy: 0.9539 - val_loss: 0.2178 - val_accuracy: 0.9334\n",
      "Epoch 121/800\n",
      "1665/1665 [==============================] - 0s 142us/step - loss: 0.1642 - accuracy: 0.9539 - val_loss: 0.2177 - val_accuracy: 0.9335\n",
      "Epoch 122/800\n",
      "1665/1665 [==============================] - 0s 145us/step - loss: 0.1638 - accuracy: 0.9539 - val_loss: 0.2179 - val_accuracy: 0.9331\n",
      "Epoch 123/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.1634 - accuracy: 0.9539 - val_loss: 0.2163 - val_accuracy: 0.9334\n",
      "Epoch 124/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.1629 - accuracy: 0.9539 - val_loss: 0.2164 - val_accuracy: 0.9334\n",
      "Epoch 125/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.1625 - accuracy: 0.9539 - val_loss: 0.2159 - val_accuracy: 0.9333\n",
      "Epoch 126/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1621 - accuracy: 0.9539 - val_loss: 0.2153 - val_accuracy: 0.9334\n",
      "Epoch 127/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.1617 - accuracy: 0.9540 - val_loss: 0.2148 - val_accuracy: 0.9335\n",
      "Epoch 128/800\n",
      "1665/1665 [==============================] - 0s 141us/step - loss: 0.1611 - accuracy: 0.9540 - val_loss: 0.2145 - val_accuracy: 0.9335\n",
      "Epoch 129/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.1607 - accuracy: 0.9539 - val_loss: 0.2131 - val_accuracy: 0.9335\n",
      "Epoch 130/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.1602 - accuracy: 0.9540 - val_loss: 0.2139 - val_accuracy: 0.9331\n",
      "Epoch 131/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1598 - accuracy: 0.9540 - val_loss: 0.2113 - val_accuracy: 0.9335\n",
      "Epoch 132/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.1593 - accuracy: 0.9540 - val_loss: 0.2114 - val_accuracy: 0.9336\n",
      "Epoch 133/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.1589 - accuracy: 0.9540 - val_loss: 0.2115 - val_accuracy: 0.9335\n",
      "Epoch 134/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.1585 - accuracy: 0.9540 - val_loss: 0.2100 - val_accuracy: 0.9335\n",
      "Epoch 135/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.1579 - accuracy: 0.9540 - val_loss: 0.2091 - val_accuracy: 0.9336\n",
      "Epoch 136/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.1574 - accuracy: 0.9540 - val_loss: 0.2112 - val_accuracy: 0.9327\n",
      "Epoch 137/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.1570 - accuracy: 0.9540 - val_loss: 0.2074 - val_accuracy: 0.9336\n",
      "Epoch 138/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.1565 - accuracy: 0.9541 - val_loss: 0.2074 - val_accuracy: 0.9336\n",
      "Epoch 139/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.1560 - accuracy: 0.9540 - val_loss: 0.2076 - val_accuracy: 0.9337\n",
      "Epoch 140/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.1554 - accuracy: 0.9540 - val_loss: 0.2064 - val_accuracy: 0.9339\n",
      "Epoch 141/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1549 - accuracy: 0.9541 - val_loss: 0.2065 - val_accuracy: 0.9341\n",
      "Epoch 142/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.1543 - accuracy: 0.9542 - val_loss: 0.2070 - val_accuracy: 0.9336\n",
      "Epoch 143/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1540 - accuracy: 0.9541 - val_loss: 0.2057 - val_accuracy: 0.9336\n",
      "Epoch 144/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.1534 - accuracy: 0.9541 - val_loss: 0.2040 - val_accuracy: 0.9338\n",
      "Epoch 145/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.1528 - accuracy: 0.9542 - val_loss: 0.2032 - val_accuracy: 0.9336\n",
      "Epoch 146/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.1522 - accuracy: 0.9542 - val_loss: 0.2025 - val_accuracy: 0.9338\n",
      "Epoch 147/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1517 - accuracy: 0.9542 - val_loss: 0.2019 - val_accuracy: 0.9342\n",
      "Epoch 148/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.1512 - accuracy: 0.9543 - val_loss: 0.2006 - val_accuracy: 0.9343\n",
      "Epoch 149/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.1506 - accuracy: 0.9543 - val_loss: 0.2002 - val_accuracy: 0.9342\n",
      "Epoch 150/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1500 - accuracy: 0.9543 - val_loss: 0.1989 - val_accuracy: 0.9342\n",
      "Epoch 151/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.1494 - accuracy: 0.9544 - val_loss: 0.1992 - val_accuracy: 0.9340\n",
      "Epoch 152/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.1488 - accuracy: 0.9544 - val_loss: 0.1976 - val_accuracy: 0.9342\n",
      "Epoch 153/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.1483 - accuracy: 0.9544 - val_loss: 0.1977 - val_accuracy: 0.9341\n",
      "Epoch 154/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.1477 - accuracy: 0.9545 - val_loss: 0.1960 - val_accuracy: 0.9341\n",
      "Epoch 155/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.1471 - accuracy: 0.9546 - val_loss: 0.1959 - val_accuracy: 0.9341\n",
      "Epoch 156/800\n",
      "1665/1665 [==============================] - 0s 126us/step - loss: 0.1465 - accuracy: 0.9545 - val_loss: 0.1950 - val_accuracy: 0.9347\n",
      "Epoch 157/800\n",
      "1665/1665 [==============================] - 0s 129us/step - loss: 0.1460 - accuracy: 0.9547 - val_loss: 0.1942 - val_accuracy: 0.9345\n",
      "Epoch 158/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.1455 - accuracy: 0.9546 - val_loss: 0.1934 - val_accuracy: 0.9342\n",
      "Epoch 159/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.1448 - accuracy: 0.9547 - val_loss: 0.1931 - val_accuracy: 0.9339\n",
      "Epoch 160/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1444 - accuracy: 0.9545 - val_loss: 0.1918 - val_accuracy: 0.9344\n",
      "Epoch 161/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.1437 - accuracy: 0.9547 - val_loss: 0.1919 - val_accuracy: 0.9344\n",
      "Epoch 162/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1431 - accuracy: 0.9549 - val_loss: 0.1910 - val_accuracy: 0.9352\n",
      "Epoch 163/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.1425 - accuracy: 0.9548 - val_loss: 0.1903 - val_accuracy: 0.9342\n",
      "Epoch 164/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.1420 - accuracy: 0.9549 - val_loss: 0.1894 - val_accuracy: 0.9347\n",
      "Epoch 165/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1415 - accuracy: 0.9550 - val_loss: 0.1891 - val_accuracy: 0.9345\n",
      "Epoch 166/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.1408 - accuracy: 0.9552 - val_loss: 0.1877 - val_accuracy: 0.9350\n",
      "Epoch 167/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.1403 - accuracy: 0.9552 - val_loss: 0.1874 - val_accuracy: 0.9344\n",
      "Epoch 168/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.1398 - accuracy: 0.9554 - val_loss: 0.1858 - val_accuracy: 0.9350\n",
      "Epoch 169/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.1392 - accuracy: 0.9555 - val_loss: 0.1856 - val_accuracy: 0.9354\n",
      "Epoch 170/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.1387 - accuracy: 0.9555 - val_loss: 0.1851 - val_accuracy: 0.9344\n",
      "Epoch 171/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.1381 - accuracy: 0.9556 - val_loss: 0.1842 - val_accuracy: 0.9348\n",
      "Epoch 172/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.1376 - accuracy: 0.9556 - val_loss: 0.1837 - val_accuracy: 0.9350\n",
      "Epoch 173/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.1371 - accuracy: 0.9559 - val_loss: 0.1878 - val_accuracy: 0.9326\n",
      "Epoch 174/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.1367 - accuracy: 0.9560 - val_loss: 0.1827 - val_accuracy: 0.9353\n",
      "Epoch 175/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1360 - accuracy: 0.9560 - val_loss: 0.1808 - val_accuracy: 0.9354\n",
      "Epoch 176/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.1355 - accuracy: 0.9563 - val_loss: 0.1814 - val_accuracy: 0.9356\n",
      "Epoch 177/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.1350 - accuracy: 0.9563 - val_loss: 0.1797 - val_accuracy: 0.9356\n",
      "Epoch 178/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.1345 - accuracy: 0.9563 - val_loss: 0.1797 - val_accuracy: 0.9366\n",
      "Epoch 179/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1340 - accuracy: 0.9565 - val_loss: 0.1791 - val_accuracy: 0.9366\n",
      "Epoch 180/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.1335 - accuracy: 0.9565 - val_loss: 0.1803 - val_accuracy: 0.9345\n",
      "Epoch 181/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.1332 - accuracy: 0.9566 - val_loss: 0.1781 - val_accuracy: 0.9360\n",
      "Epoch 182/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.1325 - accuracy: 0.9566 - val_loss: 0.1782 - val_accuracy: 0.9352\n",
      "Epoch 183/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1320 - accuracy: 0.9569 - val_loss: 0.1759 - val_accuracy: 0.9365\n",
      "Epoch 184/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.1315 - accuracy: 0.9567 - val_loss: 0.1757 - val_accuracy: 0.9371\n",
      "Epoch 185/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.1310 - accuracy: 0.9570 - val_loss: 0.1755 - val_accuracy: 0.9367\n",
      "Epoch 186/800\n",
      "1665/1665 [==============================] - 0s 141us/step - loss: 0.1305 - accuracy: 0.9568 - val_loss: 0.1747 - val_accuracy: 0.9366\n",
      "Epoch 187/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.1301 - accuracy: 0.9571 - val_loss: 0.1773 - val_accuracy: 0.9373\n",
      "Epoch 188/800\n",
      "1665/1665 [==============================] - 0s 143us/step - loss: 0.1298 - accuracy: 0.9571 - val_loss: 0.1749 - val_accuracy: 0.9369\n",
      "Epoch 189/800\n",
      "1665/1665 [==============================] - 0s 161us/step - loss: 0.1292 - accuracy: 0.9573 - val_loss: 0.1731 - val_accuracy: 0.9372\n",
      "Epoch 190/800\n",
      "1665/1665 [==============================] - 0s 126us/step - loss: 0.1287 - accuracy: 0.9572 - val_loss: 0.1730 - val_accuracy: 0.9371\n",
      "Epoch 191/800\n",
      "1665/1665 [==============================] - 0s 142us/step - loss: 0.1282 - accuracy: 0.9573 - val_loss: 0.1717 - val_accuracy: 0.9378\n",
      "Epoch 192/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.1278 - accuracy: 0.9575 - val_loss: 0.1714 - val_accuracy: 0.9376\n",
      "Epoch 193/800\n",
      "1665/1665 [==============================] - 0s 139us/step - loss: 0.1272 - accuracy: 0.9576 - val_loss: 0.1709 - val_accuracy: 0.9381\n",
      "Epoch 194/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.1267 - accuracy: 0.9577 - val_loss: 0.1700 - val_accuracy: 0.9376\n",
      "Epoch 195/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1264 - accuracy: 0.9576 - val_loss: 0.1703 - val_accuracy: 0.9384\n",
      "Epoch 196/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.1259 - accuracy: 0.9579 - val_loss: 0.1707 - val_accuracy: 0.9378\n",
      "Epoch 197/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.1256 - accuracy: 0.9581 - val_loss: 0.1694 - val_accuracy: 0.9387\n",
      "Epoch 198/800\n",
      "1665/1665 [==============================] - 0s 144us/step - loss: 0.1250 - accuracy: 0.9580 - val_loss: 0.1681 - val_accuracy: 0.9386\n",
      "Epoch 199/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.1246 - accuracy: 0.9580 - val_loss: 0.1680 - val_accuracy: 0.9383\n",
      "Epoch 200/800\n",
      "1665/1665 [==============================] - 0s 145us/step - loss: 0.1242 - accuracy: 0.9582 - val_loss: 0.1679 - val_accuracy: 0.9386\n",
      "Epoch 201/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.1238 - accuracy: 0.9583 - val_loss: 0.1668 - val_accuracy: 0.9387\n",
      "Epoch 202/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.1234 - accuracy: 0.9584 - val_loss: 0.1672 - val_accuracy: 0.9384\n",
      "Epoch 203/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.1230 - accuracy: 0.9584 - val_loss: 0.1657 - val_accuracy: 0.9385\n",
      "Epoch 204/800\n",
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.1226 - accuracy: 0.9586 - val_loss: 0.1661 - val_accuracy: 0.9386\n",
      "Epoch 205/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.1222 - accuracy: 0.9587 - val_loss: 0.1652 - val_accuracy: 0.9394\n",
      "Epoch 206/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.1218 - accuracy: 0.9591 - val_loss: 0.1650 - val_accuracy: 0.9383\n",
      "Epoch 207/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.1215 - accuracy: 0.9590 - val_loss: 0.1642 - val_accuracy: 0.9387\n",
      "Epoch 208/800\n",
      "1665/1665 [==============================] - 0s 140us/step - loss: 0.1212 - accuracy: 0.9591 - val_loss: 0.1631 - val_accuracy: 0.9393\n",
      "Epoch 209/800\n",
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.1207 - accuracy: 0.9592 - val_loss: 0.1634 - val_accuracy: 0.9398\n",
      "Epoch 210/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.1203 - accuracy: 0.9592 - val_loss: 0.1641 - val_accuracy: 0.9403\n",
      "Epoch 211/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.1201 - accuracy: 0.9593 - val_loss: 0.1634 - val_accuracy: 0.9397\n",
      "Epoch 212/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.1196 - accuracy: 0.9596 - val_loss: 0.1648 - val_accuracy: 0.9388\n",
      "Epoch 213/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.1194 - accuracy: 0.9594 - val_loss: 0.1633 - val_accuracy: 0.9394\n",
      "Epoch 214/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.1190 - accuracy: 0.9597 - val_loss: 0.1634 - val_accuracy: 0.9389\n",
      "Epoch 215/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.1187 - accuracy: 0.9596 - val_loss: 0.1618 - val_accuracy: 0.9409\n",
      "Epoch 216/800\n",
      "1665/1665 [==============================] - 0s 126us/step - loss: 0.1183 - accuracy: 0.9596 - val_loss: 0.1626 - val_accuracy: 0.9391\n",
      "Epoch 217/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.1181 - accuracy: 0.9598 - val_loss: 0.1610 - val_accuracy: 0.9403\n",
      "Epoch 218/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.1176 - accuracy: 0.9598 - val_loss: 0.1603 - val_accuracy: 0.9408\n",
      "Epoch 219/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.1173 - accuracy: 0.9600 - val_loss: 0.1602 - val_accuracy: 0.9402\n",
      "Epoch 220/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.1170 - accuracy: 0.9604 - val_loss: 0.1603 - val_accuracy: 0.9410\n",
      "Epoch 221/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.1167 - accuracy: 0.9602 - val_loss: 0.1611 - val_accuracy: 0.9394\n",
      "Epoch 222/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.1164 - accuracy: 0.9605 - val_loss: 0.1587 - val_accuracy: 0.9410\n",
      "Epoch 223/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.1160 - accuracy: 0.9606 - val_loss: 0.1593 - val_accuracy: 0.9408\n",
      "Epoch 224/800\n",
      "1665/1665 [==============================] - 0s 146us/step - loss: 0.1157 - accuracy: 0.9605 - val_loss: 0.1580 - val_accuracy: 0.9415\n",
      "Epoch 225/800\n",
      "1665/1665 [==============================] - 0s 129us/step - loss: 0.1154 - accuracy: 0.9606 - val_loss: 0.1577 - val_accuracy: 0.9414\n",
      "Epoch 226/800\n",
      "1665/1665 [==============================] - 0s 142us/step - loss: 0.1152 - accuracy: 0.9607 - val_loss: 0.1580 - val_accuracy: 0.9413\n",
      "Epoch 227/800\n",
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.1149 - accuracy: 0.9609 - val_loss: 0.1590 - val_accuracy: 0.9410\n",
      "Epoch 228/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.1145 - accuracy: 0.9608 - val_loss: 0.1572 - val_accuracy: 0.9420\n",
      "Epoch 229/800\n",
      "1665/1665 [==============================] - 0s 155us/step - loss: 0.1142 - accuracy: 0.9610 - val_loss: 0.1571 - val_accuracy: 0.9421\n",
      "Epoch 230/800\n",
      "1665/1665 [==============================] - 0s 159us/step - loss: 0.1139 - accuracy: 0.9611 - val_loss: 0.1565 - val_accuracy: 0.9423\n",
      "Epoch 231/800\n",
      "1665/1665 [==============================] - 0s 184us/step - loss: 0.1135 - accuracy: 0.9609 - val_loss: 0.1584 - val_accuracy: 0.9408\n",
      "Epoch 232/800\n",
      "1665/1665 [==============================] - 0s 154us/step - loss: 0.1134 - accuracy: 0.9611 - val_loss: 0.1563 - val_accuracy: 0.9424\n",
      "Epoch 233/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.1129 - accuracy: 0.9611 - val_loss: 0.1559 - val_accuracy: 0.9415\n",
      "Epoch 234/800\n",
      "1665/1665 [==============================] - 0s 176us/step - loss: 0.1127 - accuracy: 0.9613 - val_loss: 0.1563 - val_accuracy: 0.9422\n",
      "Epoch 235/800\n",
      "1665/1665 [==============================] - 0s 181us/step - loss: 0.1125 - accuracy: 0.9614 - val_loss: 0.1554 - val_accuracy: 0.9425\n",
      "Epoch 236/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.1121 - accuracy: 0.9616 - val_loss: 0.1555 - val_accuracy: 0.9413\n",
      "Epoch 237/800\n",
      "1665/1665 [==============================] - 0s 172us/step - loss: 0.1119 - accuracy: 0.9615 - val_loss: 0.1543 - val_accuracy: 0.9423\n",
      "Epoch 238/800\n",
      "1665/1665 [==============================] - 0s 157us/step - loss: 0.1116 - accuracy: 0.9618 - val_loss: 0.1560 - val_accuracy: 0.9406\n",
      "Epoch 239/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.1114 - accuracy: 0.9619 - val_loss: 0.1544 - val_accuracy: 0.9426\n",
      "Epoch 240/800\n",
      "1665/1665 [==============================] - 0s 129us/step - loss: 0.1110 - accuracy: 0.9620 - val_loss: 0.1539 - val_accuracy: 0.9425\n",
      "Epoch 241/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.1107 - accuracy: 0.9620 - val_loss: 0.1541 - val_accuracy: 0.9425\n",
      "Epoch 242/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1104 - accuracy: 0.9621 - val_loss: 0.1550 - val_accuracy: 0.9422\n",
      "Epoch 243/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.1102 - accuracy: 0.9624 - val_loss: 0.1545 - val_accuracy: 0.9424\n",
      "Epoch 244/800\n",
      "1665/1665 [==============================] - 0s 176us/step - loss: 0.1099 - accuracy: 0.9624 - val_loss: 0.1568 - val_accuracy: 0.9415\n",
      "Epoch 245/800\n",
      "1665/1665 [==============================] - 0s 165us/step - loss: 0.1099 - accuracy: 0.9622 - val_loss: 0.1532 - val_accuracy: 0.9430\n",
      "Epoch 246/800\n",
      "1665/1665 [==============================] - 0s 152us/step - loss: 0.1094 - accuracy: 0.9624 - val_loss: 0.1532 - val_accuracy: 0.9422\n",
      "Epoch 247/800\n",
      "1665/1665 [==============================] - 0s 141us/step - loss: 0.1092 - accuracy: 0.9625 - val_loss: 0.1535 - val_accuracy: 0.9422\n",
      "Epoch 248/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.1089 - accuracy: 0.9628 - val_loss: 0.1520 - val_accuracy: 0.9429\n",
      "Epoch 249/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.1085 - accuracy: 0.9625 - val_loss: 0.1528 - val_accuracy: 0.9429\n",
      "Epoch 250/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.1083 - accuracy: 0.9628 - val_loss: 0.1520 - val_accuracy: 0.9436\n",
      "Epoch 251/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.1079 - accuracy: 0.9629 - val_loss: 0.1509 - val_accuracy: 0.9433\n",
      "Epoch 252/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.1078 - accuracy: 0.9631 - val_loss: 0.1521 - val_accuracy: 0.9435\n",
      "Epoch 253/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.1075 - accuracy: 0.9631 - val_loss: 0.1508 - val_accuracy: 0.9440\n",
      "Epoch 254/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.1073 - accuracy: 0.9632 - val_loss: 0.1510 - val_accuracy: 0.9436\n",
      "Epoch 255/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.1069 - accuracy: 0.9632 - val_loss: 0.1592 - val_accuracy: 0.9418\n",
      "Epoch 256/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.1072 - accuracy: 0.9632 - val_loss: 0.1497 - val_accuracy: 0.9432\n",
      "Epoch 257/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.1065 - accuracy: 0.9632 - val_loss: 0.1551 - val_accuracy: 0.9411\n",
      "Epoch 258/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.1065 - accuracy: 0.9636 - val_loss: 0.1503 - val_accuracy: 0.9436\n",
      "Epoch 259/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.1060 - accuracy: 0.9637 - val_loss: 0.1583 - val_accuracy: 0.9398\n",
      "Epoch 260/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.1061 - accuracy: 0.9635 - val_loss: 0.1495 - val_accuracy: 0.9436\n",
      "Epoch 261/800\n",
      "1665/1665 [==============================] - 0s 151us/step - loss: 0.1054 - accuracy: 0.9638 - val_loss: 0.1492 - val_accuracy: 0.9442\n",
      "Epoch 262/800\n",
      "1665/1665 [==============================] - 0s 147us/step - loss: 0.1052 - accuracy: 0.9638 - val_loss: 0.1500 - val_accuracy: 0.9443\n",
      "Epoch 263/800\n",
      "1665/1665 [==============================] - 0s 129us/step - loss: 0.1050 - accuracy: 0.9640 - val_loss: 0.1487 - val_accuracy: 0.9442\n",
      "Epoch 264/800\n",
      "1665/1665 [==============================] - 0s 142us/step - loss: 0.1046 - accuracy: 0.9642 - val_loss: 0.1494 - val_accuracy: 0.9445\n",
      "Epoch 265/800\n",
      "1665/1665 [==============================] - 0s 172us/step - loss: 0.1044 - accuracy: 0.9641 - val_loss: 0.1494 - val_accuracy: 0.9442\n",
      "Epoch 266/800\n",
      "1665/1665 [==============================] - 0s 143us/step - loss: 0.1043 - accuracy: 0.9643 - val_loss: 0.1475 - val_accuracy: 0.9441\n",
      "Epoch 267/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.1039 - accuracy: 0.9642 - val_loss: 0.1476 - val_accuracy: 0.9448\n",
      "Epoch 268/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.1038 - accuracy: 0.9643 - val_loss: 0.1492 - val_accuracy: 0.9448\n",
      "Epoch 269/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.1035 - accuracy: 0.9646 - val_loss: 0.1479 - val_accuracy: 0.9443\n",
      "Epoch 270/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.1033 - accuracy: 0.9645 - val_loss: 0.1492 - val_accuracy: 0.9440\n",
      "Epoch 271/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.1030 - accuracy: 0.9646 - val_loss: 0.1462 - val_accuracy: 0.9446\n",
      "Epoch 272/800\n",
      "1665/1665 [==============================] - 0s 149us/step - loss: 0.1027 - accuracy: 0.9648 - val_loss: 0.1485 - val_accuracy: 0.9449\n",
      "Epoch 273/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.1026 - accuracy: 0.9647 - val_loss: 0.1569 - val_accuracy: 0.9416\n",
      "Epoch 274/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.1028 - accuracy: 0.9646 - val_loss: 0.1485 - val_accuracy: 0.9449\n",
      "Epoch 275/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.1021 - accuracy: 0.9647 - val_loss: 0.1473 - val_accuracy: 0.9445\n",
      "Epoch 276/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.1019 - accuracy: 0.9650 - val_loss: 0.1472 - val_accuracy: 0.9455\n",
      "Epoch 277/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.1016 - accuracy: 0.9650 - val_loss: 0.1467 - val_accuracy: 0.9452\n",
      "Epoch 278/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.1014 - accuracy: 0.9651 - val_loss: 0.1454 - val_accuracy: 0.9453\n",
      "Epoch 279/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.1011 - accuracy: 0.9651 - val_loss: 0.1601 - val_accuracy: 0.9377\n",
      "Epoch 280/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.1017 - accuracy: 0.9648 - val_loss: 0.1458 - val_accuracy: 0.9446\n",
      "Epoch 281/800\n",
      "1665/1665 [==============================] - 0s 157us/step - loss: 0.1008 - accuracy: 0.9653 - val_loss: 0.1452 - val_accuracy: 0.9452\n",
      "Epoch 282/800\n",
      "1665/1665 [==============================] - 0s 133us/step - loss: 0.1005 - accuracy: 0.9653 - val_loss: 0.1455 - val_accuracy: 0.9454\n",
      "Epoch 283/800\n",
      "1665/1665 [==============================] - 0s 159us/step - loss: 0.1003 - accuracy: 0.9653 - val_loss: 0.1465 - val_accuracy: 0.9454\n",
      "Epoch 284/800\n",
      "1665/1665 [==============================] - 0s 152us/step - loss: 0.1002 - accuracy: 0.9654 - val_loss: 0.1464 - val_accuracy: 0.9447\n",
      "Epoch 285/800\n",
      "1665/1665 [==============================] - 0s 157us/step - loss: 0.1000 - accuracy: 0.9655 - val_loss: 0.1447 - val_accuracy: 0.9447\n",
      "Epoch 286/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.0997 - accuracy: 0.9657 - val_loss: 0.1443 - val_accuracy: 0.9461\n",
      "Epoch 287/800\n",
      "1665/1665 [==============================] - 0s 152us/step - loss: 0.0995 - accuracy: 0.9657 - val_loss: 0.1492 - val_accuracy: 0.9446\n",
      "Epoch 288/800\n",
      "1665/1665 [==============================] - 0s 141us/step - loss: 0.0995 - accuracy: 0.9658 - val_loss: 0.1451 - val_accuracy: 0.9449\n",
      "Epoch 289/800\n",
      "1665/1665 [==============================] - 0s 133us/step - loss: 0.0991 - accuracy: 0.9660 - val_loss: 0.1447 - val_accuracy: 0.9462\n",
      "Epoch 290/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0988 - accuracy: 0.9660 - val_loss: 0.1433 - val_accuracy: 0.9455\n",
      "Epoch 291/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0986 - accuracy: 0.9662 - val_loss: 0.1436 - val_accuracy: 0.9451\n",
      "Epoch 292/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0985 - accuracy: 0.9661 - val_loss: 0.1449 - val_accuracy: 0.9462\n",
      "Epoch 293/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0980 - accuracy: 0.9662 - val_loss: 0.1437 - val_accuracy: 0.9455\n",
      "Epoch 294/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0979 - accuracy: 0.9664 - val_loss: 0.1437 - val_accuracy: 0.9462\n",
      "Epoch 295/800\n",
      "1665/1665 [==============================] - 0s 140us/step - loss: 0.0978 - accuracy: 0.9664 - val_loss: 0.1442 - val_accuracy: 0.9458\n",
      "Epoch 296/800\n",
      "1665/1665 [==============================] - 0s 145us/step - loss: 0.0975 - accuracy: 0.9664 - val_loss: 0.1441 - val_accuracy: 0.9450\n",
      "Epoch 297/800\n",
      "1665/1665 [==============================] - 0s 171us/step - loss: 0.0973 - accuracy: 0.9666 - val_loss: 0.1435 - val_accuracy: 0.9464\n",
      "Epoch 298/800\n",
      "1665/1665 [==============================] - 0s 165us/step - loss: 0.0970 - accuracy: 0.9668 - val_loss: 0.1429 - val_accuracy: 0.9458\n",
      "Epoch 299/800\n",
      "1665/1665 [==============================] - 0s 177us/step - loss: 0.0968 - accuracy: 0.9667 - val_loss: 0.1426 - val_accuracy: 0.9466\n",
      "Epoch 300/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.0968 - accuracy: 0.9664 - val_loss: 0.1448 - val_accuracy: 0.9439\n",
      "Epoch 301/800\n",
      "1665/1665 [==============================] - 0s 171us/step - loss: 0.0965 - accuracy: 0.9671 - val_loss: 0.1422 - val_accuracy: 0.9465\n",
      "Epoch 302/800\n",
      "1665/1665 [==============================] - 0s 142us/step - loss: 0.0961 - accuracy: 0.9668 - val_loss: 0.1418 - val_accuracy: 0.9463\n",
      "Epoch 303/800\n",
      "1665/1665 [==============================] - 0s 151us/step - loss: 0.0960 - accuracy: 0.9671 - val_loss: 0.1437 - val_accuracy: 0.9459\n",
      "Epoch 304/800\n",
      "1665/1665 [==============================] - 0s 159us/step - loss: 0.0958 - accuracy: 0.9670 - val_loss: 0.1427 - val_accuracy: 0.9457\n",
      "Epoch 305/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.0956 - accuracy: 0.9673 - val_loss: 0.1427 - val_accuracy: 0.9465\n",
      "Epoch 306/800\n",
      "1665/1665 [==============================] - 0s 161us/step - loss: 0.0955 - accuracy: 0.9672 - val_loss: 0.1407 - val_accuracy: 0.9463\n",
      "Epoch 307/800\n",
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.0954 - accuracy: 0.9672 - val_loss: 0.1429 - val_accuracy: 0.9460\n",
      "Epoch 308/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0951 - accuracy: 0.9673 - val_loss: 0.1449 - val_accuracy: 0.9451\n",
      "Epoch 309/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0949 - accuracy: 0.9674 - val_loss: 0.1414 - val_accuracy: 0.9461\n",
      "Epoch 310/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0947 - accuracy: 0.9672 - val_loss: 0.1419 - val_accuracy: 0.9461\n",
      "Epoch 311/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0945 - accuracy: 0.9674 - val_loss: 0.1407 - val_accuracy: 0.9464\n",
      "Epoch 312/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0944 - accuracy: 0.9674 - val_loss: 0.1457 - val_accuracy: 0.9440\n",
      "Epoch 313/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0944 - accuracy: 0.9676 - val_loss: 0.1420 - val_accuracy: 0.9464\n",
      "Epoch 314/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0938 - accuracy: 0.9679 - val_loss: 0.1399 - val_accuracy: 0.9471\n",
      "Epoch 315/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0937 - accuracy: 0.9681 - val_loss: 0.1416 - val_accuracy: 0.9463\n",
      "Epoch 316/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0935 - accuracy: 0.9680 - val_loss: 0.1410 - val_accuracy: 0.9472\n",
      "Epoch 317/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0933 - accuracy: 0.9679 - val_loss: 0.1413 - val_accuracy: 0.9469\n",
      "Epoch 318/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0931 - accuracy: 0.9681 - val_loss: 0.1455 - val_accuracy: 0.9443\n",
      "Epoch 319/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0932 - accuracy: 0.9678 - val_loss: 0.1401 - val_accuracy: 0.9470\n",
      "Epoch 320/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0928 - accuracy: 0.9681 - val_loss: 0.1395 - val_accuracy: 0.9471\n",
      "Epoch 321/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0926 - accuracy: 0.9681 - val_loss: 0.1387 - val_accuracy: 0.9472\n",
      "Epoch 322/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0924 - accuracy: 0.9683 - val_loss: 0.1397 - val_accuracy: 0.9473\n",
      "Epoch 323/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0923 - accuracy: 0.9682 - val_loss: 0.1397 - val_accuracy: 0.9475\n",
      "Epoch 324/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0919 - accuracy: 0.9683 - val_loss: 0.1413 - val_accuracy: 0.9466\n",
      "Epoch 325/800\n",
      "1665/1665 [==============================] - 0s 160us/step - loss: 0.0918 - accuracy: 0.9684 - val_loss: 0.1405 - val_accuracy: 0.9478\n",
      "Epoch 326/800\n",
      "1665/1665 [==============================] - 0s 192us/step - loss: 0.0916 - accuracy: 0.9686 - val_loss: 0.1403 - val_accuracy: 0.9464\n",
      "Epoch 327/800\n",
      "1665/1665 [==============================] - 0s 173us/step - loss: 0.0916 - accuracy: 0.9683 - val_loss: 0.1409 - val_accuracy: 0.9463\n",
      "Epoch 328/800\n",
      "1665/1665 [==============================] - 0s 168us/step - loss: 0.0915 - accuracy: 0.9686 - val_loss: 0.1576 - val_accuracy: 0.9396\n",
      "Epoch 329/800\n",
      "1665/1665 [==============================] - 0s 167us/step - loss: 0.0921 - accuracy: 0.9683 - val_loss: 0.1389 - val_accuracy: 0.9466\n",
      "Epoch 330/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1665/1665 [==============================] - 0s 194us/step - loss: 0.0911 - accuracy: 0.9685 - val_loss: 0.1385 - val_accuracy: 0.9481\n",
      "Epoch 331/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.0910 - accuracy: 0.9686 - val_loss: 0.1531 - val_accuracy: 0.9415\n",
      "Epoch 332/800\n",
      "1665/1665 [==============================] - 0s 140us/step - loss: 0.0914 - accuracy: 0.9686 - val_loss: 0.1370 - val_accuracy: 0.9473\n",
      "Epoch 333/800\n",
      "1665/1665 [==============================] - 0s 142us/step - loss: 0.0904 - accuracy: 0.9687 - val_loss: 0.1406 - val_accuracy: 0.9475\n",
      "Epoch 334/800\n",
      "1665/1665 [==============================] - 0s 140us/step - loss: 0.0903 - accuracy: 0.9686 - val_loss: 0.1391 - val_accuracy: 0.9477\n",
      "Epoch 335/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0901 - accuracy: 0.9690 - val_loss: 0.1377 - val_accuracy: 0.9477\n",
      "Epoch 336/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0899 - accuracy: 0.9692 - val_loss: 0.1399 - val_accuracy: 0.9481\n",
      "Epoch 337/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0898 - accuracy: 0.9690 - val_loss: 0.1377 - val_accuracy: 0.9477\n",
      "Epoch 338/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0896 - accuracy: 0.9692 - val_loss: 0.1367 - val_accuracy: 0.9477\n",
      "Epoch 339/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.0894 - accuracy: 0.9689 - val_loss: 0.1374 - val_accuracy: 0.9477\n",
      "Epoch 340/800\n",
      "1665/1665 [==============================] - 0s 156us/step - loss: 0.0892 - accuracy: 0.9693 - val_loss: 0.1362 - val_accuracy: 0.9486\n",
      "Epoch 341/800\n",
      "1665/1665 [==============================] - 0s 164us/step - loss: 0.0892 - accuracy: 0.9691 - val_loss: 0.1370 - val_accuracy: 0.9474\n",
      "Epoch 342/800\n",
      "1665/1665 [==============================] - 0s 177us/step - loss: 0.0890 - accuracy: 0.9693 - val_loss: 0.1391 - val_accuracy: 0.9484\n",
      "Epoch 343/800\n",
      "1665/1665 [==============================] - 0s 157us/step - loss: 0.0889 - accuracy: 0.9693 - val_loss: 0.1375 - val_accuracy: 0.9478\n",
      "Epoch 344/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.0887 - accuracy: 0.9694 - val_loss: 0.1377 - val_accuracy: 0.9481\n",
      "Epoch 345/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0885 - accuracy: 0.9694 - val_loss: 0.1373 - val_accuracy: 0.9489\n",
      "Epoch 346/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0883 - accuracy: 0.9696 - val_loss: 0.1368 - val_accuracy: 0.9487\n",
      "Epoch 347/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.0881 - accuracy: 0.9699 - val_loss: 0.1371 - val_accuracy: 0.9485\n",
      "Epoch 348/800\n",
      "1665/1665 [==============================] - 0s 136us/step - loss: 0.0881 - accuracy: 0.9698 - val_loss: 0.1360 - val_accuracy: 0.9487\n",
      "Epoch 349/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0879 - accuracy: 0.9696 - val_loss: 0.1374 - val_accuracy: 0.9479\n",
      "Epoch 350/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0879 - accuracy: 0.9696 - val_loss: 0.1361 - val_accuracy: 0.9491\n",
      "Epoch 351/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0876 - accuracy: 0.9695 - val_loss: 0.1375 - val_accuracy: 0.9484\n",
      "Epoch 352/800\n",
      "1665/1665 [==============================] - 0s 151us/step - loss: 0.0875 - accuracy: 0.9699 - val_loss: 0.1365 - val_accuracy: 0.9479\n",
      "Epoch 353/800\n",
      "1665/1665 [==============================] - 0s 148us/step - loss: 0.0875 - accuracy: 0.9698 - val_loss: 0.1360 - val_accuracy: 0.9491\n",
      "Epoch 354/800\n",
      "1665/1665 [==============================] - 0s 140us/step - loss: 0.0872 - accuracy: 0.9699 - val_loss: 0.1359 - val_accuracy: 0.9487\n",
      "Epoch 355/800\n",
      "1665/1665 [==============================] - 0s 129us/step - loss: 0.0871 - accuracy: 0.9701 - val_loss: 0.1505 - val_accuracy: 0.9433\n",
      "Epoch 356/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0877 - accuracy: 0.9699 - val_loss: 0.1376 - val_accuracy: 0.9488\n",
      "Epoch 357/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0867 - accuracy: 0.9703 - val_loss: 0.1374 - val_accuracy: 0.9479\n",
      "Epoch 358/800\n",
      "1665/1665 [==============================] - 0s 148us/step - loss: 0.0867 - accuracy: 0.9700 - val_loss: 0.1358 - val_accuracy: 0.9490\n",
      "Epoch 359/800\n",
      "1665/1665 [==============================] - 0s 165us/step - loss: 0.0865 - accuracy: 0.9699 - val_loss: 0.1368 - val_accuracy: 0.9494\n",
      "Epoch 360/800\n",
      "1665/1665 [==============================] - 0s 164us/step - loss: 0.0864 - accuracy: 0.9703 - val_loss: 0.1344 - val_accuracy: 0.9486\n",
      "Epoch 361/800\n",
      "1665/1665 [==============================] - 0s 167us/step - loss: 0.0863 - accuracy: 0.9700 - val_loss: 0.1367 - val_accuracy: 0.9490\n",
      "Epoch 362/800\n",
      "1665/1665 [==============================] - 0s 162us/step - loss: 0.0861 - accuracy: 0.9702 - val_loss: 0.1369 - val_accuracy: 0.9476\n",
      "Epoch 363/800\n",
      "1665/1665 [==============================] - 0s 156us/step - loss: 0.0861 - accuracy: 0.9700 - val_loss: 0.1378 - val_accuracy: 0.9477\n",
      "Epoch 364/800\n",
      "1665/1665 [==============================] - 0s 142us/step - loss: 0.0860 - accuracy: 0.9702 - val_loss: 0.1351 - val_accuracy: 0.9483\n",
      "Epoch 365/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.0859 - accuracy: 0.9702 - val_loss: 0.1343 - val_accuracy: 0.9499\n",
      "Epoch 366/800\n",
      "1665/1665 [==============================] - 0s 139us/step - loss: 0.0857 - accuracy: 0.9705 - val_loss: 0.1356 - val_accuracy: 0.9494\n",
      "Epoch 367/800\n",
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.0854 - accuracy: 0.9704 - val_loss: 0.1348 - val_accuracy: 0.9489\n",
      "Epoch 368/800\n",
      "1665/1665 [==============================] - 0s 133us/step - loss: 0.0854 - accuracy: 0.9702 - val_loss: 0.1341 - val_accuracy: 0.9491\n",
      "Epoch 369/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0853 - accuracy: 0.9705 - val_loss: 0.1343 - val_accuracy: 0.9494\n",
      "Epoch 370/800\n",
      "1665/1665 [==============================] - 0s 143us/step - loss: 0.0852 - accuracy: 0.9705 - val_loss: 0.1355 - val_accuracy: 0.9489\n",
      "Epoch 371/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.0849 - accuracy: 0.9704 - val_loss: 0.1347 - val_accuracy: 0.9501\n",
      "Epoch 372/800\n",
      "1665/1665 [==============================] - 0s 150us/step - loss: 0.0848 - accuracy: 0.9705 - val_loss: 0.1516 - val_accuracy: 0.9447\n",
      "Epoch 373/800\n",
      "1665/1665 [==============================] - 0s 160us/step - loss: 0.0853 - accuracy: 0.9704 - val_loss: 0.1343 - val_accuracy: 0.9491\n",
      "Epoch 374/800\n",
      "1665/1665 [==============================] - 0s 156us/step - loss: 0.0846 - accuracy: 0.9708 - val_loss: 0.1468 - val_accuracy: 0.9462\n",
      "Epoch 375/800\n",
      "1665/1665 [==============================] - 0s 139us/step - loss: 0.0851 - accuracy: 0.9706 - val_loss: 0.1337 - val_accuracy: 0.9491\n",
      "Epoch 376/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.0843 - accuracy: 0.9707 - val_loss: 0.1377 - val_accuracy: 0.9479\n",
      "Epoch 377/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.0842 - accuracy: 0.9710 - val_loss: 0.1334 - val_accuracy: 0.9494\n",
      "Epoch 378/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0840 - accuracy: 0.9708 - val_loss: 0.1358 - val_accuracy: 0.9485\n",
      "Epoch 379/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0841 - accuracy: 0.9708 - val_loss: 0.1340 - val_accuracy: 0.9494\n",
      "Epoch 380/800\n",
      "1665/1665 [==============================] - 0s 150us/step - loss: 0.0838 - accuracy: 0.9709 - val_loss: 0.1342 - val_accuracy: 0.9490\n",
      "Epoch 381/800\n",
      "1665/1665 [==============================] - 0s 143us/step - loss: 0.0836 - accuracy: 0.9707 - val_loss: 0.1340 - val_accuracy: 0.9505\n",
      "Epoch 382/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.0834 - accuracy: 0.9711 - val_loss: 0.1330 - val_accuracy: 0.9497\n",
      "Epoch 383/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0834 - accuracy: 0.9710 - val_loss: 0.1354 - val_accuracy: 0.9496\n",
      "Epoch 384/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.0833 - accuracy: 0.9708 - val_loss: 0.1349 - val_accuracy: 0.9487\n",
      "Epoch 385/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0832 - accuracy: 0.9710 - val_loss: 0.1345 - val_accuracy: 0.9493\n",
      "Epoch 386/800\n",
      "1665/1665 [==============================] - 0s 126us/step - loss: 0.0830 - accuracy: 0.9710 - val_loss: 0.1329 - val_accuracy: 0.9498\n",
      "Epoch 387/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.0828 - accuracy: 0.9710 - val_loss: 0.1319 - val_accuracy: 0.9496\n",
      "Epoch 388/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0829 - accuracy: 0.9708 - val_loss: 0.1345 - val_accuracy: 0.9497\n",
      "Epoch 389/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0827 - accuracy: 0.9710 - val_loss: 0.1333 - val_accuracy: 0.9494\n",
      "Epoch 390/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0826 - accuracy: 0.9712 - val_loss: 0.1382 - val_accuracy: 0.9474\n",
      "Epoch 391/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.0826 - accuracy: 0.9711 - val_loss: 0.1306 - val_accuracy: 0.9498\n",
      "Epoch 392/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0823 - accuracy: 0.9713 - val_loss: 0.1317 - val_accuracy: 0.9499\n",
      "Epoch 393/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0822 - accuracy: 0.9711 - val_loss: 0.1307 - val_accuracy: 0.9501\n",
      "Epoch 394/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0818 - accuracy: 0.9715 - val_loss: 0.1413 - val_accuracy: 0.9465\n",
      "Epoch 395/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0821 - accuracy: 0.9711 - val_loss: 0.1362 - val_accuracy: 0.9495\n",
      "Epoch 396/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0820 - accuracy: 0.9712 - val_loss: 0.1310 - val_accuracy: 0.9502\n",
      "Epoch 397/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.0817 - accuracy: 0.9713 - val_loss: 0.1348 - val_accuracy: 0.9506\n",
      "Epoch 398/800\n",
      "1665/1665 [==============================] - 0s 129us/step - loss: 0.0817 - accuracy: 0.9714 - val_loss: 0.1319 - val_accuracy: 0.9499\n",
      "Epoch 399/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0814 - accuracy: 0.9713 - val_loss: 0.1325 - val_accuracy: 0.9499\n",
      "Epoch 400/800\n",
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.0812 - accuracy: 0.9713 - val_loss: 0.1313 - val_accuracy: 0.9504\n",
      "Epoch 401/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0813 - accuracy: 0.9715 - val_loss: 0.1296 - val_accuracy: 0.9514\n",
      "Epoch 402/800\n",
      "1665/1665 [==============================] - 0s 126us/step - loss: 0.0811 - accuracy: 0.9716 - val_loss: 0.1303 - val_accuracy: 0.9508\n",
      "Epoch 403/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0809 - accuracy: 0.9715 - val_loss: 0.1311 - val_accuracy: 0.9505\n",
      "Epoch 404/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0807 - accuracy: 0.9718 - val_loss: 0.1326 - val_accuracy: 0.9504\n",
      "Epoch 405/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0807 - accuracy: 0.9715 - val_loss: 0.1303 - val_accuracy: 0.9515\n",
      "Epoch 406/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0806 - accuracy: 0.9714 - val_loss: 0.1467 - val_accuracy: 0.9434\n",
      "Epoch 407/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0812 - accuracy: 0.9714 - val_loss: 0.1305 - val_accuracy: 0.9505\n",
      "Epoch 408/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0807 - accuracy: 0.9715 - val_loss: 0.1304 - val_accuracy: 0.9514\n",
      "Epoch 409/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0803 - accuracy: 0.9717 - val_loss: 0.1316 - val_accuracy: 0.9502\n",
      "Epoch 410/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0802 - accuracy: 0.9715 - val_loss: 0.1434 - val_accuracy: 0.9468\n",
      "Epoch 411/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0809 - accuracy: 0.9713 - val_loss: 0.1335 - val_accuracy: 0.9504\n",
      "Epoch 412/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0800 - accuracy: 0.9717 - val_loss: 0.1291 - val_accuracy: 0.9503\n",
      "Epoch 413/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0799 - accuracy: 0.9716 - val_loss: 0.1285 - val_accuracy: 0.9520\n",
      "Epoch 414/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0797 - accuracy: 0.9721 - val_loss: 0.1326 - val_accuracy: 0.9503\n",
      "Epoch 415/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.0798 - accuracy: 0.9718 - val_loss: 0.1389 - val_accuracy: 0.9472\n",
      "Epoch 416/800\n",
      "1665/1665 [==============================] - 0s 126us/step - loss: 0.0801 - accuracy: 0.9717 - val_loss: 0.1314 - val_accuracy: 0.9504\n",
      "Epoch 417/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.0795 - accuracy: 0.9719 - val_loss: 0.1295 - val_accuracy: 0.9513\n",
      "Epoch 418/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0793 - accuracy: 0.9718 - val_loss: 0.1317 - val_accuracy: 0.9510\n",
      "Epoch 419/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.0793 - accuracy: 0.9720 - val_loss: 0.1305 - val_accuracy: 0.9507\n",
      "Epoch 420/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.0793 - accuracy: 0.9718 - val_loss: 0.1294 - val_accuracy: 0.9517\n",
      "Epoch 421/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.0790 - accuracy: 0.9722 - val_loss: 0.1318 - val_accuracy: 0.9512\n",
      "Epoch 422/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.0789 - accuracy: 0.9722 - val_loss: 0.1341 - val_accuracy: 0.9501\n",
      "Epoch 423/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0791 - accuracy: 0.9720 - val_loss: 0.1309 - val_accuracy: 0.9505\n",
      "Epoch 424/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0788 - accuracy: 0.9722 - val_loss: 0.1308 - val_accuracy: 0.9513\n",
      "Epoch 425/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0786 - accuracy: 0.9722 - val_loss: 0.1304 - val_accuracy: 0.9514\n",
      "Epoch 426/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0784 - accuracy: 0.9723 - val_loss: 0.1310 - val_accuracy: 0.9507\n",
      "Epoch 427/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0785 - accuracy: 0.9721 - val_loss: 0.1282 - val_accuracy: 0.9514\n",
      "Epoch 428/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0787 - accuracy: 0.9720 - val_loss: 0.1283 - val_accuracy: 0.9507\n",
      "Epoch 429/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0782 - accuracy: 0.9723 - val_loss: 0.1301 - val_accuracy: 0.9513\n",
      "Epoch 430/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0781 - accuracy: 0.9721 - val_loss: 0.1295 - val_accuracy: 0.9521\n",
      "Epoch 431/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0780 - accuracy: 0.9724 - val_loss: 0.1296 - val_accuracy: 0.9521\n",
      "Epoch 432/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0778 - accuracy: 0.9724 - val_loss: 0.1292 - val_accuracy: 0.9506\n",
      "Epoch 433/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0779 - accuracy: 0.9724 - val_loss: 0.1290 - val_accuracy: 0.9503\n",
      "Epoch 434/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0777 - accuracy: 0.9724 - val_loss: 0.1297 - val_accuracy: 0.9520\n",
      "Epoch 435/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0773 - accuracy: 0.9725 - val_loss: 0.1299 - val_accuracy: 0.9517\n",
      "Epoch 436/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0774 - accuracy: 0.9726 - val_loss: 0.1450 - val_accuracy: 0.9461\n",
      "Epoch 437/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0780 - accuracy: 0.9722 - val_loss: 0.1304 - val_accuracy: 0.9518\n",
      "Epoch 438/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0774 - accuracy: 0.9724 - val_loss: 0.1290 - val_accuracy: 0.9519\n",
      "Epoch 439/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0772 - accuracy: 0.9728 - val_loss: 0.1303 - val_accuracy: 0.9515\n",
      "Epoch 440/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1665/1665 [==============================] - 0s 145us/step - loss: 0.0772 - accuracy: 0.9724 - val_loss: 0.1291 - val_accuracy: 0.9519\n",
      "Epoch 441/800\n",
      "1665/1665 [==============================] - 0s 145us/step - loss: 0.0768 - accuracy: 0.9727 - val_loss: 0.1287 - val_accuracy: 0.9521\n",
      "Epoch 442/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.0768 - accuracy: 0.9724 - val_loss: 0.1309 - val_accuracy: 0.9525\n",
      "Epoch 443/800\n",
      "1665/1665 [==============================] - 0s 165us/step - loss: 0.0770 - accuracy: 0.9724 - val_loss: 0.1304 - val_accuracy: 0.9515\n",
      "Epoch 444/800\n",
      "1665/1665 [==============================] - 0s 147us/step - loss: 0.0767 - accuracy: 0.9727 - val_loss: 0.1281 - val_accuracy: 0.9517\n",
      "Epoch 445/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.0766 - accuracy: 0.9727 - val_loss: 0.1274 - val_accuracy: 0.9511\n",
      "Epoch 446/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0767 - accuracy: 0.9725 - val_loss: 0.1271 - val_accuracy: 0.9513\n",
      "Epoch 447/800\n",
      "1665/1665 [==============================] - 0s 133us/step - loss: 0.0765 - accuracy: 0.9728 - val_loss: 0.1297 - val_accuracy: 0.9524\n",
      "Epoch 448/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0763 - accuracy: 0.9728 - val_loss: 0.1278 - val_accuracy: 0.9509\n",
      "Epoch 449/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0765 - accuracy: 0.9727 - val_loss: 0.1267 - val_accuracy: 0.9521\n",
      "Epoch 450/800\n",
      "1665/1665 [==============================] - 0s 141us/step - loss: 0.0761 - accuracy: 0.9726 - val_loss: 0.1270 - val_accuracy: 0.9520\n",
      "Epoch 451/800\n",
      "1665/1665 [==============================] - 0s 145us/step - loss: 0.0760 - accuracy: 0.9730 - val_loss: 0.1274 - val_accuracy: 0.9523\n",
      "Epoch 452/800\n",
      "1665/1665 [==============================] - 0s 143us/step - loss: 0.0761 - accuracy: 0.9729 - val_loss: 0.1294 - val_accuracy: 0.9518\n",
      "Epoch 453/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.0760 - accuracy: 0.9729 - val_loss: 0.1274 - val_accuracy: 0.9526\n",
      "Epoch 454/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0761 - accuracy: 0.9728 - val_loss: 0.1264 - val_accuracy: 0.9521\n",
      "Epoch 455/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0756 - accuracy: 0.9728 - val_loss: 0.1284 - val_accuracy: 0.9525\n",
      "Epoch 456/800\n",
      "1665/1665 [==============================] - 0s 140us/step - loss: 0.0756 - accuracy: 0.9730 - val_loss: 0.1354 - val_accuracy: 0.9495\n",
      "Epoch 457/800\n",
      "1665/1665 [==============================] - 0s 144us/step - loss: 0.0758 - accuracy: 0.9728 - val_loss: 0.1396 - val_accuracy: 0.9490\n",
      "Epoch 458/800\n",
      "1665/1665 [==============================] - 0s 142us/step - loss: 0.0763 - accuracy: 0.9725 - val_loss: 0.1314 - val_accuracy: 0.9523\n",
      "Epoch 459/800\n",
      "1665/1665 [==============================] - 0s 144us/step - loss: 0.0755 - accuracy: 0.9731 - val_loss: 0.1280 - val_accuracy: 0.9519\n",
      "Epoch 460/800\n",
      "1665/1665 [==============================] - 0s 161us/step - loss: 0.0753 - accuracy: 0.9730 - val_loss: 0.1271 - val_accuracy: 0.9528\n",
      "Epoch 461/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.0751 - accuracy: 0.9731 - val_loss: 0.1260 - val_accuracy: 0.9528\n",
      "Epoch 462/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0749 - accuracy: 0.9733 - val_loss: 0.1275 - val_accuracy: 0.9512\n",
      "Epoch 463/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0753 - accuracy: 0.9729 - val_loss: 0.1262 - val_accuracy: 0.9520\n",
      "Epoch 464/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0748 - accuracy: 0.9730 - val_loss: 0.1279 - val_accuracy: 0.9519\n",
      "Epoch 465/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0748 - accuracy: 0.9728 - val_loss: 0.1264 - val_accuracy: 0.9536\n",
      "Epoch 466/800\n",
      "1665/1665 [==============================] - 0s 136us/step - loss: 0.0746 - accuracy: 0.9732 - val_loss: 0.1258 - val_accuracy: 0.9521\n",
      "Epoch 467/800\n",
      "1665/1665 [==============================] - 0s 143us/step - loss: 0.0745 - accuracy: 0.9733 - val_loss: 0.1262 - val_accuracy: 0.9524\n",
      "Epoch 468/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.0746 - accuracy: 0.9735 - val_loss: 0.1300 - val_accuracy: 0.9528\n",
      "Epoch 469/800\n",
      "1665/1665 [==============================] - 0s 140us/step - loss: 0.0745 - accuracy: 0.9734 - val_loss: 0.1264 - val_accuracy: 0.9524\n",
      "Epoch 470/800\n",
      "1665/1665 [==============================] - 0s 129us/step - loss: 0.0745 - accuracy: 0.9731 - val_loss: 0.1273 - val_accuracy: 0.9532\n",
      "Epoch 471/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0741 - accuracy: 0.9735 - val_loss: 0.1275 - val_accuracy: 0.9522\n",
      "Epoch 472/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0742 - accuracy: 0.9735 - val_loss: 0.1282 - val_accuracy: 0.9517\n",
      "Epoch 473/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0740 - accuracy: 0.9734 - val_loss: 0.1250 - val_accuracy: 0.9530\n",
      "Epoch 474/800\n",
      "1665/1665 [==============================] - 0s 140us/step - loss: 0.0738 - accuracy: 0.9735 - val_loss: 0.1278 - val_accuracy: 0.9527\n",
      "Epoch 475/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.0739 - accuracy: 0.9738 - val_loss: 0.1253 - val_accuracy: 0.9532\n",
      "Epoch 476/800\n",
      "1665/1665 [==============================] - 0s 146us/step - loss: 0.0737 - accuracy: 0.9734 - val_loss: 0.1266 - val_accuracy: 0.9525\n",
      "Epoch 477/800\n",
      "1665/1665 [==============================] - 0s 143us/step - loss: 0.0738 - accuracy: 0.9736 - val_loss: 0.1253 - val_accuracy: 0.9524\n",
      "Epoch 478/800\n",
      "1665/1665 [==============================] - 0s 141us/step - loss: 0.0737 - accuracy: 0.9733 - val_loss: 0.1264 - val_accuracy: 0.9525\n",
      "Epoch 479/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.0736 - accuracy: 0.9735 - val_loss: 0.1258 - val_accuracy: 0.9531\n",
      "Epoch 480/800\n",
      "1665/1665 [==============================] - 0s 148us/step - loss: 0.0736 - accuracy: 0.9734 - val_loss: 0.1239 - val_accuracy: 0.9534\n",
      "Epoch 481/800\n",
      "1665/1665 [==============================] - 0s 141us/step - loss: 0.0733 - accuracy: 0.9737 - val_loss: 0.1262 - val_accuracy: 0.9534\n",
      "Epoch 482/800\n",
      "1665/1665 [==============================] - 0s 139us/step - loss: 0.0732 - accuracy: 0.9734 - val_loss: 0.1250 - val_accuracy: 0.9537\n",
      "Epoch 483/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.0731 - accuracy: 0.9735 - val_loss: 0.1254 - val_accuracy: 0.9528\n",
      "Epoch 484/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0730 - accuracy: 0.9735 - val_loss: 0.1261 - val_accuracy: 0.9525\n",
      "Epoch 485/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0730 - accuracy: 0.9737 - val_loss: 0.1278 - val_accuracy: 0.9522\n",
      "Epoch 486/800\n",
      "1665/1665 [==============================] - 0s 143us/step - loss: 0.0731 - accuracy: 0.9735 - val_loss: 0.1272 - val_accuracy: 0.9520\n",
      "Epoch 487/800\n",
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.0732 - accuracy: 0.9734 - val_loss: 0.1318 - val_accuracy: 0.9499\n",
      "Epoch 488/800\n",
      "1665/1665 [==============================] - 0s 126us/step - loss: 0.0730 - accuracy: 0.9735 - val_loss: 0.1241 - val_accuracy: 0.9527\n",
      "Epoch 489/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0727 - accuracy: 0.9735 - val_loss: 0.1259 - val_accuracy: 0.9533\n",
      "Epoch 490/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0724 - accuracy: 0.9739 - val_loss: 0.1246 - val_accuracy: 0.9537\n",
      "Epoch 491/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0724 - accuracy: 0.9737 - val_loss: 0.1224 - val_accuracy: 0.9533\n",
      "Epoch 492/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0723 - accuracy: 0.9737 - val_loss: 0.1254 - val_accuracy: 0.9536\n",
      "Epoch 493/800\n",
      "1665/1665 [==============================] - 0s 133us/step - loss: 0.0724 - accuracy: 0.9734 - val_loss: 0.1268 - val_accuracy: 0.9534\n",
      "Epoch 494/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.0722 - accuracy: 0.9738 - val_loss: 0.1258 - val_accuracy: 0.9526\n",
      "Epoch 495/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.0723 - accuracy: 0.9738 - val_loss: 0.1250 - val_accuracy: 0.9537\n",
      "Epoch 496/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0718 - accuracy: 0.9737 - val_loss: 0.1297 - val_accuracy: 0.9537\n",
      "Epoch 497/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0721 - accuracy: 0.9740 - val_loss: 0.1275 - val_accuracy: 0.9534\n",
      "Epoch 498/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0717 - accuracy: 0.9739 - val_loss: 0.1267 - val_accuracy: 0.9533\n",
      "Epoch 499/800\n",
      "1665/1665 [==============================] - 0s 126us/step - loss: 0.0718 - accuracy: 0.9739 - val_loss: 0.1255 - val_accuracy: 0.9533\n",
      "Epoch 500/800\n",
      "1665/1665 [==============================] - 0s 143us/step - loss: 0.0716 - accuracy: 0.9741 - val_loss: 0.1237 - val_accuracy: 0.9538\n",
      "Epoch 501/800\n",
      "1665/1665 [==============================] - 0s 139us/step - loss: 0.0714 - accuracy: 0.9741 - val_loss: 0.1222 - val_accuracy: 0.9537\n",
      "Epoch 502/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.0715 - accuracy: 0.9740 - val_loss: 0.1265 - val_accuracy: 0.9536\n",
      "Epoch 503/800\n",
      "1665/1665 [==============================] - 0s 133us/step - loss: 0.0714 - accuracy: 0.9739 - val_loss: 0.1248 - val_accuracy: 0.9533\n",
      "Epoch 504/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0712 - accuracy: 0.9741 - val_loss: 0.1216 - val_accuracy: 0.9530\n",
      "Epoch 505/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.0714 - accuracy: 0.9739 - val_loss: 0.1283 - val_accuracy: 0.9535\n",
      "Epoch 506/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0713 - accuracy: 0.9740 - val_loss: 0.1240 - val_accuracy: 0.9536\n",
      "Epoch 507/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0710 - accuracy: 0.9741 - val_loss: 0.1235 - val_accuracy: 0.9535\n",
      "Epoch 508/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0710 - accuracy: 0.9739 - val_loss: 0.1236 - val_accuracy: 0.9542\n",
      "Epoch 509/800\n",
      "1665/1665 [==============================] - 0s 133us/step - loss: 0.0707 - accuracy: 0.9744 - val_loss: 0.1227 - val_accuracy: 0.9540\n",
      "Epoch 510/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.0710 - accuracy: 0.9743 - val_loss: 0.1555 - val_accuracy: 0.9439\n",
      "Epoch 511/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0721 - accuracy: 0.9737 - val_loss: 0.1273 - val_accuracy: 0.9530\n",
      "Epoch 512/800\n",
      "1665/1665 [==============================] - 0s 133us/step - loss: 0.0709 - accuracy: 0.9740 - val_loss: 0.1216 - val_accuracy: 0.9544\n",
      "Epoch 513/800\n",
      "1665/1665 [==============================] - 0s 151us/step - loss: 0.0704 - accuracy: 0.9743 - val_loss: 0.1234 - val_accuracy: 0.9540\n",
      "Epoch 514/800\n",
      "1665/1665 [==============================] - 0s 163us/step - loss: 0.0706 - accuracy: 0.9739 - val_loss: 0.1287 - val_accuracy: 0.9535\n",
      "Epoch 515/800\n",
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.0706 - accuracy: 0.9741 - val_loss: 0.1278 - val_accuracy: 0.9541\n",
      "Epoch 516/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.0705 - accuracy: 0.9743 - val_loss: 0.1245 - val_accuracy: 0.9542\n",
      "Epoch 517/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0702 - accuracy: 0.9742 - val_loss: 0.1253 - val_accuracy: 0.9535\n",
      "Epoch 518/800\n",
      "1665/1665 [==============================] - 0s 141us/step - loss: 0.0702 - accuracy: 0.9742 - val_loss: 0.1247 - val_accuracy: 0.9537\n",
      "Epoch 519/800\n",
      "1665/1665 [==============================] - 0s 140us/step - loss: 0.0702 - accuracy: 0.9742 - val_loss: 0.1238 - val_accuracy: 0.9541\n",
      "Epoch 520/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.0701 - accuracy: 0.9744 - val_loss: 0.1216 - val_accuracy: 0.9545\n",
      "Epoch 521/800\n",
      "1665/1665 [==============================] - 0s 126us/step - loss: 0.0697 - accuracy: 0.9746 - val_loss: 0.1243 - val_accuracy: 0.9544\n",
      "Epoch 522/800\n",
      "1665/1665 [==============================] - 0s 152us/step - loss: 0.0699 - accuracy: 0.9744 - val_loss: 0.1216 - val_accuracy: 0.9535\n",
      "Epoch 523/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.0698 - accuracy: 0.9746 - val_loss: 0.1242 - val_accuracy: 0.9531\n",
      "Epoch 524/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0698 - accuracy: 0.9741 - val_loss: 0.1218 - val_accuracy: 0.9545\n",
      "Epoch 525/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0694 - accuracy: 0.9744 - val_loss: 0.1250 - val_accuracy: 0.9544\n",
      "Epoch 526/800\n",
      "1665/1665 [==============================] - 0s 145us/step - loss: 0.0695 - accuracy: 0.9746 - val_loss: 0.1264 - val_accuracy: 0.9546\n",
      "Epoch 527/800\n",
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.0695 - accuracy: 0.9743 - val_loss: 0.1265 - val_accuracy: 0.9544\n",
      "Epoch 528/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0695 - accuracy: 0.9745 - val_loss: 0.1223 - val_accuracy: 0.9548\n",
      "Epoch 529/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0692 - accuracy: 0.9747 - val_loss: 0.1242 - val_accuracy: 0.9538\n",
      "Epoch 530/800\n",
      "1665/1665 [==============================] - 0s 139us/step - loss: 0.0691 - accuracy: 0.9746 - val_loss: 0.1263 - val_accuracy: 0.9551\n",
      "Epoch 531/800\n",
      "1665/1665 [==============================] - 0s 139us/step - loss: 0.0691 - accuracy: 0.9748 - val_loss: 0.1314 - val_accuracy: 0.9519\n",
      "Epoch 532/800\n",
      "1665/1665 [==============================] - 0s 133us/step - loss: 0.0694 - accuracy: 0.9746 - val_loss: 0.1275 - val_accuracy: 0.9534\n",
      "Epoch 533/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0691 - accuracy: 0.9746 - val_loss: 0.1209 - val_accuracy: 0.9549\n",
      "Epoch 534/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0689 - accuracy: 0.9747 - val_loss: 0.1233 - val_accuracy: 0.9543\n",
      "Epoch 535/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0687 - accuracy: 0.9749 - val_loss: 0.1248 - val_accuracy: 0.9546\n",
      "Epoch 536/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0688 - accuracy: 0.9746 - val_loss: 0.1213 - val_accuracy: 0.9552\n",
      "Epoch 537/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0686 - accuracy: 0.9750 - val_loss: 0.1245 - val_accuracy: 0.9536\n",
      "Epoch 538/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0688 - accuracy: 0.9751 - val_loss: 0.1224 - val_accuracy: 0.9542\n",
      "Epoch 539/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0685 - accuracy: 0.9743 - val_loss: 0.1206 - val_accuracy: 0.9555\n",
      "Epoch 540/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0684 - accuracy: 0.9748 - val_loss: 0.1200 - val_accuracy: 0.9561\n",
      "Epoch 541/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0681 - accuracy: 0.9749 - val_loss: 0.1236 - val_accuracy: 0.9539\n",
      "Epoch 542/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0683 - accuracy: 0.9747 - val_loss: 0.1216 - val_accuracy: 0.9553\n",
      "Epoch 543/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0681 - accuracy: 0.9751 - val_loss: 0.1189 - val_accuracy: 0.9554\n",
      "Epoch 544/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0681 - accuracy: 0.9748 - val_loss: 0.1432 - val_accuracy: 0.9482\n",
      "Epoch 545/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.0693 - accuracy: 0.9741 - val_loss: 0.1201 - val_accuracy: 0.9557\n",
      "Epoch 546/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0678 - accuracy: 0.9749 - val_loss: 0.1178 - val_accuracy: 0.9549\n",
      "Epoch 547/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0679 - accuracy: 0.9748 - val_loss: 0.1212 - val_accuracy: 0.9552\n",
      "Epoch 548/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0677 - accuracy: 0.9751 - val_loss: 0.1215 - val_accuracy: 0.9556\n",
      "Epoch 549/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.0677 - accuracy: 0.9751 - val_loss: 0.1209 - val_accuracy: 0.9542\n",
      "Epoch 550/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1665/1665 [==============================] - 0s 112us/step - loss: 0.0677 - accuracy: 0.9751 - val_loss: 0.1218 - val_accuracy: 0.9551\n",
      "Epoch 551/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0676 - accuracy: 0.9752 - val_loss: 0.1205 - val_accuracy: 0.9561\n",
      "Epoch 552/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.0676 - accuracy: 0.9752 - val_loss: 0.1242 - val_accuracy: 0.9549\n",
      "Epoch 553/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0676 - accuracy: 0.9751 - val_loss: 0.1240 - val_accuracy: 0.9547\n",
      "Epoch 554/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0676 - accuracy: 0.9750 - val_loss: 0.1175 - val_accuracy: 0.9558\n",
      "Epoch 555/800\n",
      "1665/1665 [==============================] - 0s 142us/step - loss: 0.0674 - accuracy: 0.9755 - val_loss: 0.1207 - val_accuracy: 0.9556\n",
      "Epoch 556/800\n",
      "1665/1665 [==============================] - 0s 142us/step - loss: 0.0671 - accuracy: 0.9752 - val_loss: 0.1204 - val_accuracy: 0.9560\n",
      "Epoch 557/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0671 - accuracy: 0.9753 - val_loss: 0.1255 - val_accuracy: 0.9554\n",
      "Epoch 558/800\n",
      "1665/1665 [==============================] - 0s 140us/step - loss: 0.0672 - accuracy: 0.9750 - val_loss: 0.1188 - val_accuracy: 0.9562\n",
      "Epoch 559/800\n",
      "1665/1665 [==============================] - 0s 141us/step - loss: 0.0669 - accuracy: 0.9752 - val_loss: 0.1239 - val_accuracy: 0.9552\n",
      "Epoch 560/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0671 - accuracy: 0.9752 - val_loss: 0.1202 - val_accuracy: 0.9561\n",
      "Epoch 561/800\n",
      "1665/1665 [==============================] - 0s 139us/step - loss: 0.0667 - accuracy: 0.9753 - val_loss: 0.1224 - val_accuracy: 0.9540\n",
      "Epoch 562/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0669 - accuracy: 0.9754 - val_loss: 0.1250 - val_accuracy: 0.9559\n",
      "Epoch 563/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.0669 - accuracy: 0.9752 - val_loss: 0.1222 - val_accuracy: 0.9556\n",
      "Epoch 564/800\n",
      "1665/1665 [==============================] - 0s 156us/step - loss: 0.0667 - accuracy: 0.9755 - val_loss: 0.1190 - val_accuracy: 0.9546\n",
      "Epoch 565/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0670 - accuracy: 0.9752 - val_loss: 0.1188 - val_accuracy: 0.9557\n",
      "Epoch 566/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.0666 - accuracy: 0.9753 - val_loss: 0.1193 - val_accuracy: 0.9562\n",
      "Epoch 567/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.0662 - accuracy: 0.9754 - val_loss: 0.1198 - val_accuracy: 0.9564\n",
      "Epoch 568/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.0663 - accuracy: 0.9755 - val_loss: 0.1178 - val_accuracy: 0.9560\n",
      "Epoch 569/800\n",
      "1665/1665 [==============================] - 0s 142us/step - loss: 0.0662 - accuracy: 0.9755 - val_loss: 0.1202 - val_accuracy: 0.9557\n",
      "Epoch 570/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.0664 - accuracy: 0.9754 - val_loss: 0.1212 - val_accuracy: 0.9560\n",
      "Epoch 571/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0664 - accuracy: 0.9753 - val_loss: 0.1176 - val_accuracy: 0.9562\n",
      "Epoch 572/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0660 - accuracy: 0.9754 - val_loss: 0.1481 - val_accuracy: 0.9489\n",
      "Epoch 573/800\n",
      "1665/1665 [==============================] - 0s 133us/step - loss: 0.0670 - accuracy: 0.9751 - val_loss: 0.1195 - val_accuracy: 0.9567\n",
      "Epoch 574/800\n",
      "1665/1665 [==============================] - 0s 144us/step - loss: 0.0660 - accuracy: 0.9756 - val_loss: 0.1181 - val_accuracy: 0.9559\n",
      "Epoch 575/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0659 - accuracy: 0.9754 - val_loss: 0.1284 - val_accuracy: 0.9526\n",
      "Epoch 576/800\n",
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.0661 - accuracy: 0.9756 - val_loss: 0.1237 - val_accuracy: 0.9554\n",
      "Epoch 577/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.0659 - accuracy: 0.9756 - val_loss: 0.1179 - val_accuracy: 0.9565\n",
      "Epoch 578/800\n",
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.0656 - accuracy: 0.9757 - val_loss: 0.1199 - val_accuracy: 0.9557\n",
      "Epoch 579/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0655 - accuracy: 0.9758 - val_loss: 0.1175 - val_accuracy: 0.9562\n",
      "Epoch 580/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.0656 - accuracy: 0.9754 - val_loss: 0.1302 - val_accuracy: 0.9523\n",
      "Epoch 581/800\n",
      "1665/1665 [==============================] - 0s 140us/step - loss: 0.0661 - accuracy: 0.9753 - val_loss: 0.1188 - val_accuracy: 0.9572\n",
      "Epoch 582/800\n",
      "1665/1665 [==============================] - 0s 159us/step - loss: 0.0655 - accuracy: 0.9756 - val_loss: 0.1269 - val_accuracy: 0.9526\n",
      "Epoch 583/800\n",
      "1665/1665 [==============================] - 0s 140us/step - loss: 0.0658 - accuracy: 0.9756 - val_loss: 0.1187 - val_accuracy: 0.9570\n",
      "Epoch 584/800\n",
      "1665/1665 [==============================] - 0s 154us/step - loss: 0.0652 - accuracy: 0.9757 - val_loss: 0.1177 - val_accuracy: 0.9574\n",
      "Epoch 585/800\n",
      "1665/1665 [==============================] - 0s 165us/step - loss: 0.0652 - accuracy: 0.9757 - val_loss: 0.1268 - val_accuracy: 0.9560\n",
      "Epoch 586/800\n",
      "1665/1665 [==============================] - 0s 154us/step - loss: 0.0655 - accuracy: 0.9756 - val_loss: 0.1211 - val_accuracy: 0.9558\n",
      "Epoch 587/800\n",
      "1665/1665 [==============================] - 0s 154us/step - loss: 0.0650 - accuracy: 0.9757 - val_loss: 0.1215 - val_accuracy: 0.9544\n",
      "Epoch 588/800\n",
      "1665/1665 [==============================] - 0s 155us/step - loss: 0.0653 - accuracy: 0.9754 - val_loss: 0.1201 - val_accuracy: 0.9563\n",
      "Epoch 589/800\n",
      "1665/1665 [==============================] - 0s 156us/step - loss: 0.0651 - accuracy: 0.9756 - val_loss: 0.1171 - val_accuracy: 0.9560\n",
      "Epoch 590/800\n",
      "1665/1665 [==============================] - 0s 146us/step - loss: 0.0649 - accuracy: 0.9759 - val_loss: 0.1222 - val_accuracy: 0.9549\n",
      "Epoch 591/800\n",
      "1665/1665 [==============================] - 0s 143us/step - loss: 0.0647 - accuracy: 0.9761 - val_loss: 0.1198 - val_accuracy: 0.9560\n",
      "Epoch 592/800\n",
      "1665/1665 [==============================] - 0s 142us/step - loss: 0.0649 - accuracy: 0.9758 - val_loss: 0.1197 - val_accuracy: 0.9562\n",
      "Epoch 593/800\n",
      "1665/1665 [==============================] - 0s 147us/step - loss: 0.0646 - accuracy: 0.9760 - val_loss: 0.1314 - val_accuracy: 0.9531\n",
      "Epoch 594/800\n",
      "1665/1665 [==============================] - 0s 143us/step - loss: 0.0651 - accuracy: 0.9762 - val_loss: 0.1199 - val_accuracy: 0.9562\n",
      "Epoch 595/800\n",
      "1665/1665 [==============================] - 0s 162us/step - loss: 0.0645 - accuracy: 0.9760 - val_loss: 0.1262 - val_accuracy: 0.9551\n",
      "Epoch 596/800\n",
      "1665/1665 [==============================] - 0s 141us/step - loss: 0.0645 - accuracy: 0.9760 - val_loss: 0.1149 - val_accuracy: 0.9572\n",
      "Epoch 597/800\n",
      "1665/1665 [==============================] - 0s 142us/step - loss: 0.0643 - accuracy: 0.9759 - val_loss: 0.1200 - val_accuracy: 0.9561\n",
      "Epoch 598/800\n",
      "1665/1665 [==============================] - 0s 155us/step - loss: 0.0645 - accuracy: 0.9758 - val_loss: 0.1243 - val_accuracy: 0.9563\n",
      "Epoch 599/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0644 - accuracy: 0.9759 - val_loss: 0.1179 - val_accuracy: 0.9569\n",
      "Epoch 600/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0644 - accuracy: 0.9760 - val_loss: 0.1309 - val_accuracy: 0.9530\n",
      "Epoch 601/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0647 - accuracy: 0.9759 - val_loss: 0.1179 - val_accuracy: 0.9564\n",
      "Epoch 602/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0640 - accuracy: 0.9762 - val_loss: 0.1162 - val_accuracy: 0.9567\n",
      "Epoch 603/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0640 - accuracy: 0.9760 - val_loss: 0.1174 - val_accuracy: 0.9566\n",
      "Epoch 604/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0639 - accuracy: 0.9763 - val_loss: 0.1177 - val_accuracy: 0.9571\n",
      "Epoch 605/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0638 - accuracy: 0.9762 - val_loss: 0.1201 - val_accuracy: 0.9561\n",
      "Epoch 606/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0640 - accuracy: 0.9759 - val_loss: 0.1188 - val_accuracy: 0.9567\n",
      "Epoch 607/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0639 - accuracy: 0.9762 - val_loss: 0.1176 - val_accuracy: 0.9569\n",
      "Epoch 608/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.0639 - accuracy: 0.9763 - val_loss: 0.1161 - val_accuracy: 0.9568\n",
      "Epoch 609/800\n",
      "1665/1665 [==============================] - 0s 147us/step - loss: 0.0635 - accuracy: 0.9763 - val_loss: 0.1169 - val_accuracy: 0.9575\n",
      "Epoch 610/800\n",
      "1665/1665 [==============================] - 0s 140us/step - loss: 0.0634 - accuracy: 0.9762 - val_loss: 0.1174 - val_accuracy: 0.9571\n",
      "Epoch 611/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0636 - accuracy: 0.9763 - val_loss: 0.1510 - val_accuracy: 0.9467\n",
      "Epoch 612/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0647 - accuracy: 0.9757 - val_loss: 0.1183 - val_accuracy: 0.9572\n",
      "Epoch 613/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0633 - accuracy: 0.9761 - val_loss: 0.1173 - val_accuracy: 0.9555\n",
      "Epoch 614/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0639 - accuracy: 0.9760 - val_loss: 0.1186 - val_accuracy: 0.9567\n",
      "Epoch 615/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.0633 - accuracy: 0.9763 - val_loss: 0.1158 - val_accuracy: 0.9575\n",
      "Epoch 616/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.0630 - accuracy: 0.9764 - val_loss: 0.1158 - val_accuracy: 0.9563\n",
      "Epoch 617/800\n",
      "1665/1665 [==============================] - 0s 136us/step - loss: 0.0632 - accuracy: 0.9763 - val_loss: 0.1173 - val_accuracy: 0.9568\n",
      "Epoch 618/800\n",
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.0633 - accuracy: 0.9762 - val_loss: 0.1178 - val_accuracy: 0.9570\n",
      "Epoch 619/800\n",
      "1665/1665 [==============================] - 0s 149us/step - loss: 0.0631 - accuracy: 0.9767 - val_loss: 0.1195 - val_accuracy: 0.9560\n",
      "Epoch 620/800\n",
      "1665/1665 [==============================] - 0s 150us/step - loss: 0.0630 - accuracy: 0.9763 - val_loss: 0.1160 - val_accuracy: 0.9580\n",
      "Epoch 621/800\n",
      "1665/1665 [==============================] - 0s 142us/step - loss: 0.0629 - accuracy: 0.9764 - val_loss: 0.1189 - val_accuracy: 0.9567\n",
      "Epoch 622/800\n",
      "1665/1665 [==============================] - 0s 145us/step - loss: 0.0629 - accuracy: 0.9764 - val_loss: 0.1144 - val_accuracy: 0.9574\n",
      "Epoch 623/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.0627 - accuracy: 0.9765 - val_loss: 0.1160 - val_accuracy: 0.9578\n",
      "Epoch 624/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0625 - accuracy: 0.9763 - val_loss: 0.1179 - val_accuracy: 0.9574\n",
      "Epoch 625/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.0625 - accuracy: 0.9765 - val_loss: 0.1187 - val_accuracy: 0.9578\n",
      "Epoch 626/800\n",
      "1665/1665 [==============================] - 0s 142us/step - loss: 0.0628 - accuracy: 0.9764 - val_loss: 0.1140 - val_accuracy: 0.9576\n",
      "Epoch 627/800\n",
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.0624 - accuracy: 0.9764 - val_loss: 0.1163 - val_accuracy: 0.9578\n",
      "Epoch 628/800\n",
      "1665/1665 [==============================] - 0s 158us/step - loss: 0.0625 - accuracy: 0.9765 - val_loss: 0.1148 - val_accuracy: 0.9580\n",
      "Epoch 629/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0623 - accuracy: 0.9767 - val_loss: 0.1176 - val_accuracy: 0.9565\n",
      "Epoch 630/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.0625 - accuracy: 0.9764 - val_loss: 0.1173 - val_accuracy: 0.9571\n",
      "Epoch 631/800\n",
      "1665/1665 [==============================] - 0s 153us/step - loss: 0.0623 - accuracy: 0.9764 - val_loss: 0.1167 - val_accuracy: 0.9575\n",
      "Epoch 632/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0627 - accuracy: 0.9765 - val_loss: 0.1174 - val_accuracy: 0.9575\n",
      "Epoch 633/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0622 - accuracy: 0.9765 - val_loss: 0.1440 - val_accuracy: 0.9475\n",
      "Epoch 634/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0632 - accuracy: 0.9761 - val_loss: 0.1186 - val_accuracy: 0.9566\n",
      "Epoch 635/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.0623 - accuracy: 0.9767 - val_loss: 0.1192 - val_accuracy: 0.9571\n",
      "Epoch 636/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0621 - accuracy: 0.9766 - val_loss: 0.1175 - val_accuracy: 0.9572\n",
      "Epoch 637/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0620 - accuracy: 0.9768 - val_loss: 0.1170 - val_accuracy: 0.9577\n",
      "Epoch 638/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0621 - accuracy: 0.9768 - val_loss: 0.1148 - val_accuracy: 0.9577\n",
      "Epoch 639/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.0618 - accuracy: 0.9765 - val_loss: 0.1221 - val_accuracy: 0.9561\n",
      "Epoch 640/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0621 - accuracy: 0.9767 - val_loss: 0.1144 - val_accuracy: 0.9581\n",
      "Epoch 641/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.0616 - accuracy: 0.9768 - val_loss: 0.1174 - val_accuracy: 0.9573\n",
      "Epoch 642/800\n",
      "1665/1665 [==============================] - 0s 126us/step - loss: 0.0620 - accuracy: 0.9765 - val_loss: 0.1257 - val_accuracy: 0.9533\n",
      "Epoch 643/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0622 - accuracy: 0.9766 - val_loss: 0.1135 - val_accuracy: 0.9586\n",
      "Epoch 644/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.0616 - accuracy: 0.9766 - val_loss: 0.1152 - val_accuracy: 0.9581\n",
      "Epoch 645/800\n",
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.0615 - accuracy: 0.9768 - val_loss: 0.1170 - val_accuracy: 0.9572\n",
      "Epoch 646/800\n",
      "1665/1665 [==============================] - 0s 147us/step - loss: 0.0615 - accuracy: 0.9768 - val_loss: 0.1147 - val_accuracy: 0.9588\n",
      "Epoch 647/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0613 - accuracy: 0.9769 - val_loss: 0.1157 - val_accuracy: 0.9584\n",
      "Epoch 648/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0612 - accuracy: 0.9770 - val_loss: 0.1159 - val_accuracy: 0.9589\n",
      "Epoch 649/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0612 - accuracy: 0.9769 - val_loss: 0.1168 - val_accuracy: 0.9572\n",
      "Epoch 650/800\n",
      "1665/1665 [==============================] - 0s 126us/step - loss: 0.0611 - accuracy: 0.9769 - val_loss: 0.1195 - val_accuracy: 0.9571\n",
      "Epoch 651/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.0616 - accuracy: 0.9768 - val_loss: 0.1141 - val_accuracy: 0.9580\n",
      "Epoch 652/800\n",
      "1665/1665 [==============================] - 0s 177us/step - loss: 0.0611 - accuracy: 0.9769 - val_loss: 0.1159 - val_accuracy: 0.9576\n",
      "Epoch 653/800\n",
      "1665/1665 [==============================] - 0s 147us/step - loss: 0.0610 - accuracy: 0.9768 - val_loss: 0.1147 - val_accuracy: 0.9586\n",
      "Epoch 654/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0608 - accuracy: 0.9769 - val_loss: 0.1174 - val_accuracy: 0.9569\n",
      "Epoch 655/800\n",
      "1665/1665 [==============================] - 0s 154us/step - loss: 0.0612 - accuracy: 0.9767 - val_loss: 0.1130 - val_accuracy: 0.9586\n",
      "Epoch 656/800\n",
      "1665/1665 [==============================] - 0s 181us/step - loss: 0.0608 - accuracy: 0.9771 - val_loss: 0.1150 - val_accuracy: 0.9583\n",
      "Epoch 657/800\n",
      "1665/1665 [==============================] - 0s 142us/step - loss: 0.0607 - accuracy: 0.9771 - val_loss: 0.1251 - val_accuracy: 0.9543\n",
      "Epoch 658/800\n",
      "1665/1665 [==============================] - 0s 153us/step - loss: 0.0614 - accuracy: 0.9771 - val_loss: 0.1187 - val_accuracy: 0.9574\n",
      "Epoch 659/800\n",
      "1665/1665 [==============================] - 0s 156us/step - loss: 0.0608 - accuracy: 0.9769 - val_loss: 0.1153 - val_accuracy: 0.9579\n",
      "Epoch 660/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.0606 - accuracy: 0.9769 - val_loss: 0.1308 - val_accuracy: 0.9538\n",
      "Epoch 661/800\n",
      "1665/1665 [==============================] - 0s 149us/step - loss: 0.0612 - accuracy: 0.9768 - val_loss: 0.1153 - val_accuracy: 0.9582\n",
      "Epoch 662/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.0607 - accuracy: 0.9771 - val_loss: 0.1211 - val_accuracy: 0.9561\n",
      "Epoch 663/800\n",
      "1665/1665 [==============================] - 0s 150us/step - loss: 0.0607 - accuracy: 0.9770 - val_loss: 0.1414 - val_accuracy: 0.9473\n",
      "Epoch 664/800\n",
      "1665/1665 [==============================] - 0s 133us/step - loss: 0.0623 - accuracy: 0.9762 - val_loss: 0.1362 - val_accuracy: 0.9517\n",
      "Epoch 665/800\n",
      "1665/1665 [==============================] - 0s 143us/step - loss: 0.0612 - accuracy: 0.9766 - val_loss: 0.1120 - val_accuracy: 0.9585\n",
      "Epoch 666/800\n",
      "1665/1665 [==============================] - 0s 133us/step - loss: 0.0605 - accuracy: 0.9770 - val_loss: 0.1124 - val_accuracy: 0.9578\n",
      "Epoch 667/800\n",
      "1665/1665 [==============================] - 0s 140us/step - loss: 0.0603 - accuracy: 0.9772 - val_loss: 0.1163 - val_accuracy: 0.9582\n",
      "Epoch 668/800\n",
      "1665/1665 [==============================] - 0s 145us/step - loss: 0.0601 - accuracy: 0.9769 - val_loss: 0.1120 - val_accuracy: 0.9587\n",
      "Epoch 669/800\n",
      "1665/1665 [==============================] - 0s 141us/step - loss: 0.0601 - accuracy: 0.9773 - val_loss: 0.1124 - val_accuracy: 0.9587\n",
      "Epoch 670/800\n",
      "1665/1665 [==============================] - 0s 144us/step - loss: 0.0601 - accuracy: 0.9771 - val_loss: 0.1157 - val_accuracy: 0.9579\n",
      "Epoch 671/800\n",
      "1665/1665 [==============================] - 0s 142us/step - loss: 0.0603 - accuracy: 0.9771 - val_loss: 0.1116 - val_accuracy: 0.9589\n",
      "Epoch 672/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0599 - accuracy: 0.9771 - val_loss: 0.1187 - val_accuracy: 0.9573\n",
      "Epoch 673/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0602 - accuracy: 0.9769 - val_loss: 0.1139 - val_accuracy: 0.9587\n",
      "Epoch 674/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0598 - accuracy: 0.9773 - val_loss: 0.1198 - val_accuracy: 0.9570\n",
      "Epoch 675/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0601 - accuracy: 0.9771 - val_loss: 0.1132 - val_accuracy: 0.9587\n",
      "Epoch 676/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0598 - accuracy: 0.9772 - val_loss: 0.1143 - val_accuracy: 0.9586\n",
      "Epoch 677/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0598 - accuracy: 0.9770 - val_loss: 0.1225 - val_accuracy: 0.9570\n",
      "Epoch 678/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.0600 - accuracy: 0.9771 - val_loss: 0.1125 - val_accuracy: 0.9598\n",
      "Epoch 679/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.0595 - accuracy: 0.9774 - val_loss: 0.1171 - val_accuracy: 0.9576\n",
      "Epoch 680/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.0597 - accuracy: 0.9774 - val_loss: 0.1129 - val_accuracy: 0.9593\n",
      "Epoch 681/800\n",
      "1665/1665 [==============================] - 0s 136us/step - loss: 0.0596 - accuracy: 0.9776 - val_loss: 0.1134 - val_accuracy: 0.9591\n",
      "Epoch 682/800\n",
      "1665/1665 [==============================] - 0s 141us/step - loss: 0.0593 - accuracy: 0.9773 - val_loss: 0.1139 - val_accuracy: 0.9592\n",
      "Epoch 683/800\n",
      "1665/1665 [==============================] - 0s 139us/step - loss: 0.0594 - accuracy: 0.9774 - val_loss: 0.1159 - val_accuracy: 0.9580\n",
      "Epoch 684/800\n",
      "1665/1665 [==============================] - 0s 129us/step - loss: 0.0596 - accuracy: 0.9773 - val_loss: 0.1116 - val_accuracy: 0.9594\n",
      "Epoch 685/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.0593 - accuracy: 0.9773 - val_loss: 0.1119 - val_accuracy: 0.9596\n",
      "Epoch 686/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0592 - accuracy: 0.9772 - val_loss: 0.1130 - val_accuracy: 0.9591\n",
      "Epoch 687/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0592 - accuracy: 0.9774 - val_loss: 0.1429 - val_accuracy: 0.9509\n",
      "Epoch 688/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0602 - accuracy: 0.9769 - val_loss: 0.1184 - val_accuracy: 0.9580\n",
      "Epoch 689/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0591 - accuracy: 0.9775 - val_loss: 0.1117 - val_accuracy: 0.9590\n",
      "Epoch 690/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0591 - accuracy: 0.9774 - val_loss: 0.1120 - val_accuracy: 0.9599\n",
      "Epoch 691/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0590 - accuracy: 0.9773 - val_loss: 0.1182 - val_accuracy: 0.9583\n",
      "Epoch 692/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0595 - accuracy: 0.9775 - val_loss: 0.1181 - val_accuracy: 0.9587\n",
      "Epoch 693/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0591 - accuracy: 0.9775 - val_loss: 0.1218 - val_accuracy: 0.9576\n",
      "Epoch 694/800\n",
      "1665/1665 [==============================] - 0s 129us/step - loss: 0.0597 - accuracy: 0.9770 - val_loss: 0.1152 - val_accuracy: 0.9585\n",
      "Epoch 695/800\n",
      "1665/1665 [==============================] - 0s 156us/step - loss: 0.0590 - accuracy: 0.9775 - val_loss: 0.1170 - val_accuracy: 0.9582\n",
      "Epoch 696/800\n",
      "1665/1665 [==============================] - 0s 126us/step - loss: 0.0589 - accuracy: 0.9777 - val_loss: 0.1126 - val_accuracy: 0.9593\n",
      "Epoch 697/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0586 - accuracy: 0.9777 - val_loss: 0.1144 - val_accuracy: 0.9585\n",
      "Epoch 698/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0587 - accuracy: 0.9774 - val_loss: 0.1146 - val_accuracy: 0.9581\n",
      "Epoch 699/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0589 - accuracy: 0.9774 - val_loss: 0.1132 - val_accuracy: 0.9592\n",
      "Epoch 700/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.0586 - accuracy: 0.9777 - val_loss: 0.1138 - val_accuracy: 0.9588\n",
      "Epoch 701/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0585 - accuracy: 0.9777 - val_loss: 0.1117 - val_accuracy: 0.9594\n",
      "Epoch 702/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0586 - accuracy: 0.9774 - val_loss: 0.1146 - val_accuracy: 0.9582\n",
      "Epoch 703/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.0584 - accuracy: 0.9777 - val_loss: 0.1500 - val_accuracy: 0.9476\n",
      "Epoch 704/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0601 - accuracy: 0.9768 - val_loss: 0.1125 - val_accuracy: 0.9601\n",
      "Epoch 705/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0584 - accuracy: 0.9776 - val_loss: 0.1155 - val_accuracy: 0.9580\n",
      "Epoch 706/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.0588 - accuracy: 0.9775 - val_loss: 0.1146 - val_accuracy: 0.9595\n",
      "Epoch 707/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0583 - accuracy: 0.9777 - val_loss: 0.1145 - val_accuracy: 0.9588\n",
      "Epoch 708/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0584 - accuracy: 0.9776 - val_loss: 0.1105 - val_accuracy: 0.9601\n",
      "Epoch 709/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0580 - accuracy: 0.9777 - val_loss: 0.1116 - val_accuracy: 0.9594\n",
      "Epoch 710/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0580 - accuracy: 0.9776 - val_loss: 0.1100 - val_accuracy: 0.9598\n",
      "Epoch 711/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0583 - accuracy: 0.9776 - val_loss: 0.1135 - val_accuracy: 0.9593\n",
      "Epoch 712/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.0580 - accuracy: 0.9777 - val_loss: 0.1108 - val_accuracy: 0.9594\n",
      "Epoch 713/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.0579 - accuracy: 0.9779 - val_loss: 0.1141 - val_accuracy: 0.9585\n",
      "Epoch 714/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0579 - accuracy: 0.9777 - val_loss: 0.1116 - val_accuracy: 0.9596\n",
      "Epoch 715/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1665/1665 [==============================] - 0s 133us/step - loss: 0.0577 - accuracy: 0.9781 - val_loss: 0.1136 - val_accuracy: 0.9590\n",
      "Epoch 716/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0579 - accuracy: 0.9779 - val_loss: 0.1149 - val_accuracy: 0.9579\n",
      "Epoch 717/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0581 - accuracy: 0.9777 - val_loss: 0.1078 - val_accuracy: 0.9601\n",
      "Epoch 718/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0576 - accuracy: 0.9778 - val_loss: 0.1140 - val_accuracy: 0.9587\n",
      "Epoch 719/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.0578 - accuracy: 0.9780 - val_loss: 0.1124 - val_accuracy: 0.9594\n",
      "Epoch 720/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.0579 - accuracy: 0.9778 - val_loss: 0.1112 - val_accuracy: 0.9597\n",
      "Epoch 721/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0578 - accuracy: 0.9778 - val_loss: 0.1115 - val_accuracy: 0.9586\n",
      "Epoch 722/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0581 - accuracy: 0.9776 - val_loss: 0.1107 - val_accuracy: 0.9604\n",
      "Epoch 723/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0575 - accuracy: 0.9781 - val_loss: 0.1132 - val_accuracy: 0.9594\n",
      "Epoch 724/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0575 - accuracy: 0.9779 - val_loss: 0.1104 - val_accuracy: 0.9597\n",
      "Epoch 725/800\n",
      "1665/1665 [==============================] - 0s 146us/step - loss: 0.0576 - accuracy: 0.9779 - val_loss: 0.1132 - val_accuracy: 0.9583\n",
      "Epoch 726/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.0576 - accuracy: 0.9779 - val_loss: 0.1133 - val_accuracy: 0.9596\n",
      "Epoch 727/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.0574 - accuracy: 0.9778 - val_loss: 0.1100 - val_accuracy: 0.9601\n",
      "Epoch 728/800\n",
      "1665/1665 [==============================] - 0s 151us/step - loss: 0.0574 - accuracy: 0.9781 - val_loss: 0.1118 - val_accuracy: 0.9598\n",
      "Epoch 729/800\n",
      "1665/1665 [==============================] - 0s 172us/step - loss: 0.0571 - accuracy: 0.9781 - val_loss: 0.1102 - val_accuracy: 0.9597\n",
      "Epoch 730/800\n",
      "1665/1665 [==============================] - 0s 143us/step - loss: 0.0571 - accuracy: 0.9778 - val_loss: 0.1089 - val_accuracy: 0.9603\n",
      "Epoch 731/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.0570 - accuracy: 0.9779 - val_loss: 0.1124 - val_accuracy: 0.9598\n",
      "Epoch 732/800\n",
      "1665/1665 [==============================] - 0s 145us/step - loss: 0.0571 - accuracy: 0.9780 - val_loss: 0.1123 - val_accuracy: 0.9594\n",
      "Epoch 733/800\n",
      "1665/1665 [==============================] - 0s 133us/step - loss: 0.0572 - accuracy: 0.9780 - val_loss: 0.1089 - val_accuracy: 0.9606\n",
      "Epoch 734/800\n",
      "1665/1665 [==============================] - 0s 150us/step - loss: 0.0569 - accuracy: 0.9783 - val_loss: 0.1136 - val_accuracy: 0.9591\n",
      "Epoch 735/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0571 - accuracy: 0.9779 - val_loss: 0.1111 - val_accuracy: 0.9604\n",
      "Epoch 736/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0569 - accuracy: 0.9781 - val_loss: 0.1194 - val_accuracy: 0.9563\n",
      "Epoch 737/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0571 - accuracy: 0.9780 - val_loss: 0.1095 - val_accuracy: 0.9605\n",
      "Epoch 738/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0567 - accuracy: 0.9782 - val_loss: 0.1136 - val_accuracy: 0.9598\n",
      "Epoch 739/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0568 - accuracy: 0.9781 - val_loss: 0.1167 - val_accuracy: 0.9578\n",
      "Epoch 740/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0568 - accuracy: 0.9781 - val_loss: 0.1187 - val_accuracy: 0.9583\n",
      "Epoch 741/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0570 - accuracy: 0.9780 - val_loss: 0.1117 - val_accuracy: 0.9599\n",
      "Epoch 742/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0566 - accuracy: 0.9782 - val_loss: 0.1131 - val_accuracy: 0.9591\n",
      "Epoch 743/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0566 - accuracy: 0.9782 - val_loss: 0.1132 - val_accuracy: 0.9604\n",
      "Epoch 744/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0566 - accuracy: 0.9782 - val_loss: 0.1122 - val_accuracy: 0.9592\n",
      "Epoch 745/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0565 - accuracy: 0.9783 - val_loss: 0.1081 - val_accuracy: 0.9609\n",
      "Epoch 746/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.0564 - accuracy: 0.9781 - val_loss: 0.1376 - val_accuracy: 0.9515\n",
      "Epoch 747/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0574 - accuracy: 0.9776 - val_loss: 0.1151 - val_accuracy: 0.9594\n",
      "Epoch 748/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0567 - accuracy: 0.9778 - val_loss: 0.1128 - val_accuracy: 0.9600\n",
      "Epoch 749/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0564 - accuracy: 0.9782 - val_loss: 0.1110 - val_accuracy: 0.9599\n",
      "Epoch 750/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0565 - accuracy: 0.9781 - val_loss: 0.1110 - val_accuracy: 0.9602\n",
      "Epoch 751/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0562 - accuracy: 0.9783 - val_loss: 0.1103 - val_accuracy: 0.9607\n",
      "Epoch 752/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0561 - accuracy: 0.9784 - val_loss: 0.1170 - val_accuracy: 0.9587\n",
      "Epoch 753/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0563 - accuracy: 0.9782 - val_loss: 0.1107 - val_accuracy: 0.9606\n",
      "Epoch 754/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0560 - accuracy: 0.9785 - val_loss: 0.1728 - val_accuracy: 0.9411\n",
      "Epoch 755/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0589 - accuracy: 0.9771 - val_loss: 0.1079 - val_accuracy: 0.9606\n",
      "Epoch 756/800\n",
      "1665/1665 [==============================] - 0s 106us/step - loss: 0.0563 - accuracy: 0.9783 - val_loss: 0.1083 - val_accuracy: 0.9605\n",
      "Epoch 757/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0561 - accuracy: 0.9781 - val_loss: 0.1110 - val_accuracy: 0.9597\n",
      "Epoch 758/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0562 - accuracy: 0.9786 - val_loss: 0.1092 - val_accuracy: 0.9600\n",
      "Epoch 759/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0558 - accuracy: 0.9783 - val_loss: 0.1145 - val_accuracy: 0.9590\n",
      "Epoch 760/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0563 - accuracy: 0.9782 - val_loss: 0.1348 - val_accuracy: 0.9542\n",
      "Epoch 761/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0573 - accuracy: 0.9774 - val_loss: 0.1103 - val_accuracy: 0.9603\n",
      "Epoch 762/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0557 - accuracy: 0.9785 - val_loss: 0.1430 - val_accuracy: 0.9488\n",
      "Epoch 763/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0579 - accuracy: 0.9777 - val_loss: 0.1077 - val_accuracy: 0.9610\n",
      "Epoch 764/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.0558 - accuracy: 0.9782 - val_loss: 0.1108 - val_accuracy: 0.9599\n",
      "Epoch 765/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0561 - accuracy: 0.9783 - val_loss: 0.1086 - val_accuracy: 0.9607\n",
      "Epoch 766/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0557 - accuracy: 0.9784 - val_loss: 0.1286 - val_accuracy: 0.9530\n",
      "Epoch 767/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0564 - accuracy: 0.9780 - val_loss: 0.1113 - val_accuracy: 0.9607\n",
      "Epoch 768/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0555 - accuracy: 0.9784 - val_loss: 0.1108 - val_accuracy: 0.9603\n",
      "Epoch 769/800\n",
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.0556 - accuracy: 0.9783 - val_loss: 0.1140 - val_accuracy: 0.9607\n",
      "Epoch 770/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1665/1665 [==============================] - 0s 144us/step - loss: 0.0555 - accuracy: 0.9786 - val_loss: 0.1109 - val_accuracy: 0.9602\n",
      "Epoch 771/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.0555 - accuracy: 0.9784 - val_loss: 0.1087 - val_accuracy: 0.9612\n",
      "Epoch 772/800\n",
      "1665/1665 [==============================] - 0s 143us/step - loss: 0.0552 - accuracy: 0.9786 - val_loss: 0.1103 - val_accuracy: 0.9597\n",
      "Epoch 773/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0555 - accuracy: 0.9786 - val_loss: 0.1305 - val_accuracy: 0.9502\n",
      "Epoch 774/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.0565 - accuracy: 0.9782 - val_loss: 0.1130 - val_accuracy: 0.9596\n",
      "Epoch 775/800\n",
      "1665/1665 [==============================] - 0s 143us/step - loss: 0.0554 - accuracy: 0.9787 - val_loss: 0.1234 - val_accuracy: 0.9573\n",
      "Epoch 776/800\n",
      "1665/1665 [==============================] - 0s 143us/step - loss: 0.0557 - accuracy: 0.9785 - val_loss: 0.1159 - val_accuracy: 0.9592\n",
      "Epoch 777/800\n",
      "1665/1665 [==============================] - 0s 142us/step - loss: 0.0554 - accuracy: 0.9784 - val_loss: 0.1189 - val_accuracy: 0.9593\n",
      "Epoch 778/800\n",
      "1665/1665 [==============================] - 0s 149us/step - loss: 0.0556 - accuracy: 0.9783 - val_loss: 0.1134 - val_accuracy: 0.9593\n",
      "Epoch 779/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0554 - accuracy: 0.9785 - val_loss: 0.1058 - val_accuracy: 0.9615\n",
      "Epoch 780/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0551 - accuracy: 0.9785 - val_loss: 0.1095 - val_accuracy: 0.9607\n",
      "Epoch 781/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0549 - accuracy: 0.9787 - val_loss: 0.1088 - val_accuracy: 0.9605\n",
      "Epoch 782/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0550 - accuracy: 0.9783 - val_loss: 0.1127 - val_accuracy: 0.9592\n",
      "Epoch 783/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0550 - accuracy: 0.9786 - val_loss: 0.1105 - val_accuracy: 0.9602\n",
      "Epoch 784/800\n",
      "1665/1665 [==============================] - 0s 152us/step - loss: 0.0548 - accuracy: 0.9785 - val_loss: 0.1084 - val_accuracy: 0.9613\n",
      "Epoch 785/800\n",
      "1665/1665 [==============================] - 0s 151us/step - loss: 0.0547 - accuracy: 0.9787 - val_loss: 0.1068 - val_accuracy: 0.9619\n",
      "Epoch 786/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.0547 - accuracy: 0.9787 - val_loss: 0.1099 - val_accuracy: 0.9610\n",
      "Epoch 787/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0548 - accuracy: 0.9788 - val_loss: 0.1106 - val_accuracy: 0.9603\n",
      "Epoch 788/800\n",
      "1665/1665 [==============================] - 0s 140us/step - loss: 0.0549 - accuracy: 0.9786 - val_loss: 0.1138 - val_accuracy: 0.9593\n",
      "Epoch 789/800\n",
      "1665/1665 [==============================] - 0s 147us/step - loss: 0.0549 - accuracy: 0.9786 - val_loss: 0.1097 - val_accuracy: 0.9608\n",
      "Epoch 790/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.0548 - accuracy: 0.9786 - val_loss: 0.1106 - val_accuracy: 0.9595\n",
      "Epoch 791/800\n",
      "1665/1665 [==============================] - 0s 150us/step - loss: 0.0549 - accuracy: 0.9785 - val_loss: 0.1623 - val_accuracy: 0.9470\n",
      "Epoch 792/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.0570 - accuracy: 0.9781 - val_loss: 0.1081 - val_accuracy: 0.9604\n",
      "Epoch 793/800\n",
      "1665/1665 [==============================] - 0s 141us/step - loss: 0.0545 - accuracy: 0.9787 - val_loss: 0.1091 - val_accuracy: 0.9610\n",
      "Epoch 794/800\n",
      "1665/1665 [==============================] - 0s 143us/step - loss: 0.0544 - accuracy: 0.9788 - val_loss: 0.1106 - val_accuracy: 0.9605\n",
      "Epoch 795/800\n",
      "1665/1665 [==============================] - 0s 155us/step - loss: 0.0548 - accuracy: 0.9787 - val_loss: 0.1097 - val_accuracy: 0.9613\n",
      "Epoch 796/800\n",
      "1665/1665 [==============================] - 0s 151us/step - loss: 0.0545 - accuracy: 0.9786 - val_loss: 0.1122 - val_accuracy: 0.9600\n",
      "Epoch 797/800\n",
      "1665/1665 [==============================] - 0s 162us/step - loss: 0.0543 - accuracy: 0.9788 - val_loss: 0.1280 - val_accuracy: 0.9559\n",
      "Epoch 798/800\n",
      "1665/1665 [==============================] - 0s 165us/step - loss: 0.0559 - accuracy: 0.9783 - val_loss: 0.1090 - val_accuracy: 0.9605\n",
      "Epoch 799/800\n",
      "1665/1665 [==============================] - 0s 166us/step - loss: 0.0547 - accuracy: 0.9786 - val_loss: 0.1291 - val_accuracy: 0.9565\n",
      "Epoch 800/800\n",
      "1665/1665 [==============================] - 0s 153us/step - loss: 0.0552 - accuracy: 0.9787 - val_loss: 0.1090 - val_accuracy: 0.9607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clair\\Anaconda3\\envs\\CNNs\\lib\\site-packages\\ipykernel_launcher.py:125: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "C:\\Users\\clair\\Anaconda3\\envs\\CNNs\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:808: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Train on 1665 samples, validate on 556 samples\n",
      "Epoch 1/800\n",
      "1665/1665 [==============================] - 0s 219us/step - loss: 0.6624 - accuracy: 0.7564 - val_loss: 0.6224 - val_accuracy: 0.8901\n",
      "Epoch 2/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.4543 - accuracy: 0.9344 - val_loss: 0.2821 - val_accuracy: 0.9331\n",
      "Epoch 3/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.1966 - accuracy: 0.9539 - val_loss: 0.2508 - val_accuracy: 0.9331\n",
      "Epoch 4/800\n",
      "1665/1665 [==============================] - 0s 140us/step - loss: 0.1878 - accuracy: 0.9539 - val_loss: 0.2502 - val_accuracy: 0.9331\n",
      "Epoch 5/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.1869 - accuracy: 0.9539 - val_loss: 0.2499 - val_accuracy: 0.9331\n",
      "Epoch 6/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.1864 - accuracy: 0.9539 - val_loss: 0.2501 - val_accuracy: 0.9331\n",
      "Epoch 7/800\n",
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.1863 - accuracy: 0.9539 - val_loss: 0.2499 - val_accuracy: 0.9331\n",
      "Epoch 8/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.1861 - accuracy: 0.9539 - val_loss: 0.2495 - val_accuracy: 0.9331\n",
      "Epoch 9/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.1860 - accuracy: 0.9539 - val_loss: 0.2498 - val_accuracy: 0.9331\n",
      "Epoch 10/800\n",
      "1665/1665 [==============================] - 0s 133us/step - loss: 0.1859 - accuracy: 0.9539 - val_loss: 0.2494 - val_accuracy: 0.9331\n",
      "Epoch 11/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.1858 - accuracy: 0.9539 - val_loss: 0.2491 - val_accuracy: 0.9331\n",
      "Epoch 12/800\n",
      "1665/1665 [==============================] - 0s 166us/step - loss: 0.1857 - accuracy: 0.9539 - val_loss: 0.2489 - val_accuracy: 0.9331\n",
      "Epoch 13/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.1857 - accuracy: 0.9539 - val_loss: 0.2490 - val_accuracy: 0.9331\n",
      "Epoch 14/800\n",
      "1665/1665 [==============================] - 0s 136us/step - loss: 0.1856 - accuracy: 0.9539 - val_loss: 0.2487 - val_accuracy: 0.9331\n",
      "Epoch 15/800\n",
      "1665/1665 [==============================] - 0s 144us/step - loss: 0.1855 - accuracy: 0.9539 - val_loss: 0.2488 - val_accuracy: 0.9331\n",
      "Epoch 16/800\n",
      "1665/1665 [==============================] - 0s 139us/step - loss: 0.1854 - accuracy: 0.9539 - val_loss: 0.2491 - val_accuracy: 0.9331\n",
      "Epoch 17/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.1854 - accuracy: 0.9539 - val_loss: 0.2493 - val_accuracy: 0.9331\n",
      "Epoch 18/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.1853 - accuracy: 0.9539 - val_loss: 0.2487 - val_accuracy: 0.9331\n",
      "Epoch 19/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.1853 - accuracy: 0.9539 - val_loss: 0.2475 - val_accuracy: 0.9331\n",
      "Epoch 20/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.1852 - accuracy: 0.9539 - val_loss: 0.2485 - val_accuracy: 0.9331\n",
      "Epoch 21/800\n",
      "1665/1665 [==============================] - 0s 146us/step - loss: 0.1851 - accuracy: 0.9539 - val_loss: 0.2479 - val_accuracy: 0.9331\n",
      "Epoch 22/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.1851 - accuracy: 0.9539 - val_loss: 0.2480 - val_accuracy: 0.9331\n",
      "Epoch 23/800\n",
      "1665/1665 [==============================] - 0s 143us/step - loss: 0.1850 - accuracy: 0.9539 - val_loss: 0.2478 - val_accuracy: 0.9331\n",
      "Epoch 24/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.1849 - accuracy: 0.9539 - val_loss: 0.2478 - val_accuracy: 0.9331\n",
      "Epoch 25/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.1848 - accuracy: 0.9539 - val_loss: 0.2481 - val_accuracy: 0.9331\n",
      "Epoch 26/800\n",
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.1848 - accuracy: 0.9539 - val_loss: 0.2476 - val_accuracy: 0.9331\n",
      "Epoch 27/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.1847 - accuracy: 0.9539 - val_loss: 0.2477 - val_accuracy: 0.9331\n",
      "Epoch 28/800\n",
      "1665/1665 [==============================] - 0s 142us/step - loss: 0.1847 - accuracy: 0.9539 - val_loss: 0.2477 - val_accuracy: 0.9331\n",
      "Epoch 29/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1846 - accuracy: 0.9539 - val_loss: 0.2478 - val_accuracy: 0.9331\n",
      "Epoch 30/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.1846 - accuracy: 0.9539 - val_loss: 0.2480 - val_accuracy: 0.9331\n",
      "Epoch 31/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.1845 - accuracy: 0.9539 - val_loss: 0.2473 - val_accuracy: 0.9331\n",
      "Epoch 32/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.1844 - accuracy: 0.9539 - val_loss: 0.2472 - val_accuracy: 0.9331\n",
      "Epoch 33/800\n",
      "1665/1665 [==============================] - 0s 145us/step - loss: 0.1843 - accuracy: 0.9539 - val_loss: 0.2467 - val_accuracy: 0.9331\n",
      "Epoch 34/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.1842 - accuracy: 0.9539 - val_loss: 0.2472 - val_accuracy: 0.9331\n",
      "Epoch 35/800\n",
      "1665/1665 [==============================] - 0s 143us/step - loss: 0.1842 - accuracy: 0.9539 - val_loss: 0.2465 - val_accuracy: 0.9331\n",
      "Epoch 36/800\n",
      "1665/1665 [==============================] - 0s 142us/step - loss: 0.1841 - accuracy: 0.9539 - val_loss: 0.2465 - val_accuracy: 0.9331\n",
      "Epoch 37/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.1840 - accuracy: 0.9539 - val_loss: 0.2461 - val_accuracy: 0.9331\n",
      "Epoch 38/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.1840 - accuracy: 0.9539 - val_loss: 0.2465 - val_accuracy: 0.9331\n",
      "Epoch 39/800\n",
      "1665/1665 [==============================] - 0s 141us/step - loss: 0.1839 - accuracy: 0.9539 - val_loss: 0.2460 - val_accuracy: 0.9331\n",
      "Epoch 40/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.1838 - accuracy: 0.9539 - val_loss: 0.2458 - val_accuracy: 0.9331\n",
      "Epoch 41/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.1837 - accuracy: 0.9539 - val_loss: 0.2457 - val_accuracy: 0.9331\n",
      "Epoch 42/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.1836 - accuracy: 0.9539 - val_loss: 0.2456 - val_accuracy: 0.9331\n",
      "Epoch 43/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.1835 - accuracy: 0.9539 - val_loss: 0.2461 - val_accuracy: 0.9331\n",
      "Epoch 44/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.1834 - accuracy: 0.9539 - val_loss: 0.2458 - val_accuracy: 0.9331\n",
      "Epoch 45/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.1833 - accuracy: 0.9539 - val_loss: 0.2450 - val_accuracy: 0.9331\n",
      "Epoch 46/800\n",
      "1665/1665 [==============================] - 0s 142us/step - loss: 0.1832 - accuracy: 0.9539 - val_loss: 0.2449 - val_accuracy: 0.9331\n",
      "Epoch 47/800\n",
      "1665/1665 [==============================] - 0s 144us/step - loss: 0.1832 - accuracy: 0.9539 - val_loss: 0.2451 - val_accuracy: 0.9331\n",
      "Epoch 48/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.1830 - accuracy: 0.9539 - val_loss: 0.2448 - val_accuracy: 0.9331\n",
      "Epoch 49/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.1829 - accuracy: 0.9539 - val_loss: 0.2449 - val_accuracy: 0.9331\n",
      "Epoch 50/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.1828 - accuracy: 0.9539 - val_loss: 0.2449 - val_accuracy: 0.9331\n",
      "Epoch 51/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.1827 - accuracy: 0.9539 - val_loss: 0.2450 - val_accuracy: 0.9331\n",
      "Epoch 52/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.1825 - accuracy: 0.9539 - val_loss: 0.2442 - val_accuracy: 0.9331\n",
      "Epoch 53/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1824 - accuracy: 0.9539 - val_loss: 0.2440 - val_accuracy: 0.9331\n",
      "Epoch 54/800\n",
      "1665/1665 [==============================] - 0s 112us/step - loss: 0.1823 - accuracy: 0.9539 - val_loss: 0.2440 - val_accuracy: 0.9331\n",
      "Epoch 55/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.1822 - accuracy: 0.9539 - val_loss: 0.2434 - val_accuracy: 0.9331\n",
      "Epoch 56/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.1820 - accuracy: 0.9539 - val_loss: 0.2435 - val_accuracy: 0.9331\n",
      "Epoch 57/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1819 - accuracy: 0.9539 - val_loss: 0.2432 - val_accuracy: 0.9331\n",
      "Epoch 58/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.1817 - accuracy: 0.9539 - val_loss: 0.2434 - val_accuracy: 0.9331\n",
      "Epoch 59/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.1816 - accuracy: 0.9539 - val_loss: 0.2430 - val_accuracy: 0.9331\n",
      "Epoch 60/800\n",
      "1665/1665 [==============================] - 0s 156us/step - loss: 0.1814 - accuracy: 0.9539 - val_loss: 0.2432 - val_accuracy: 0.9331\n",
      "Epoch 61/800\n",
      "1665/1665 [==============================] - 0s 165us/step - loss: 0.1813 - accuracy: 0.9539 - val_loss: 0.2425 - val_accuracy: 0.9331\n",
      "Epoch 62/800\n",
      "1665/1665 [==============================] - 0s 140us/step - loss: 0.1812 - accuracy: 0.9539 - val_loss: 0.2426 - val_accuracy: 0.9331\n",
      "Epoch 63/800\n",
      "1665/1665 [==============================] - 0s 160us/step - loss: 0.1809 - accuracy: 0.9539 - val_loss: 0.2417 - val_accuracy: 0.9331\n",
      "Epoch 64/800\n",
      "1665/1665 [==============================] - 0s 153us/step - loss: 0.1808 - accuracy: 0.9539 - val_loss: 0.2424 - val_accuracy: 0.9331\n",
      "Epoch 65/800\n",
      "1665/1665 [==============================] - 0s 139us/step - loss: 0.1807 - accuracy: 0.9539 - val_loss: 0.2419 - val_accuracy: 0.9331\n",
      "Epoch 66/800\n",
      "1665/1665 [==============================] - 0s 151us/step - loss: 0.1804 - accuracy: 0.9539 - val_loss: 0.2419 - val_accuracy: 0.9331\n",
      "Epoch 67/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.1803 - accuracy: 0.9539 - val_loss: 0.2411 - val_accuracy: 0.9331\n",
      "Epoch 68/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.1800 - accuracy: 0.9539 - val_loss: 0.2406 - val_accuracy: 0.9331\n",
      "Epoch 69/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.1798 - accuracy: 0.9539 - val_loss: 0.2403 - val_accuracy: 0.9331\n",
      "Epoch 70/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1796 - accuracy: 0.9539 - val_loss: 0.2404 - val_accuracy: 0.9331\n",
      "Epoch 71/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.1793 - accuracy: 0.9539 - val_loss: 0.2398 - val_accuracy: 0.9331\n",
      "Epoch 72/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.1791 - accuracy: 0.9539 - val_loss: 0.2392 - val_accuracy: 0.9331\n",
      "Epoch 73/800\n",
      "1665/1665 [==============================] - 0s 129us/step - loss: 0.1788 - accuracy: 0.9539 - val_loss: 0.2384 - val_accuracy: 0.9331\n",
      "Epoch 74/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.1786 - accuracy: 0.9539 - val_loss: 0.2386 - val_accuracy: 0.9331\n",
      "Epoch 75/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.1783 - accuracy: 0.9539 - val_loss: 0.2378 - val_accuracy: 0.9331\n",
      "Epoch 76/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.1781 - accuracy: 0.9539 - val_loss: 0.2373 - val_accuracy: 0.9331\n",
      "Epoch 77/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.1777 - accuracy: 0.9539 - val_loss: 0.2368 - val_accuracy: 0.9331\n",
      "Epoch 78/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.1774 - accuracy: 0.9539 - val_loss: 0.2367 - val_accuracy: 0.9331\n",
      "Epoch 79/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.1771 - accuracy: 0.9539 - val_loss: 0.2361 - val_accuracy: 0.9331\n",
      "Epoch 80/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.1767 - accuracy: 0.9539 - val_loss: 0.2356 - val_accuracy: 0.9331\n",
      "Epoch 81/800\n",
      "1665/1665 [==============================] - 0s 126us/step - loss: 0.1763 - accuracy: 0.9539 - val_loss: 0.2349 - val_accuracy: 0.9331\n",
      "Epoch 82/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.1759 - accuracy: 0.9539 - val_loss: 0.2344 - val_accuracy: 0.9331\n",
      "Epoch 83/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.1755 - accuracy: 0.9539 - val_loss: 0.2336 - val_accuracy: 0.9331\n",
      "Epoch 84/800\n",
      "1665/1665 [==============================] - 0s 126us/step - loss: 0.1751 - accuracy: 0.9539 - val_loss: 0.2337 - val_accuracy: 0.9331\n",
      "Epoch 85/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1747 - accuracy: 0.9539 - val_loss: 0.2329 - val_accuracy: 0.9331\n",
      "Epoch 86/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.1743 - accuracy: 0.9539 - val_loss: 0.2318 - val_accuracy: 0.9331\n",
      "Epoch 87/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1739 - accuracy: 0.9539 - val_loss: 0.2319 - val_accuracy: 0.9331\n",
      "Epoch 88/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.1734 - accuracy: 0.9539 - val_loss: 0.2304 - val_accuracy: 0.9331\n",
      "Epoch 89/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.1730 - accuracy: 0.9539 - val_loss: 0.2304 - val_accuracy: 0.9331\n",
      "Epoch 90/800\n",
      "1665/1665 [==============================] - 0s 112us/step - loss: 0.1725 - accuracy: 0.9539 - val_loss: 0.2305 - val_accuracy: 0.9331\n",
      "Epoch 91/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.1721 - accuracy: 0.9539 - val_loss: 0.2295 - val_accuracy: 0.9331\n",
      "Epoch 92/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.1716 - accuracy: 0.9539 - val_loss: 0.2283 - val_accuracy: 0.9331\n",
      "Epoch 93/800\n",
      "1665/1665 [==============================] - 0s 133us/step - loss: 0.1711 - accuracy: 0.9539 - val_loss: 0.2277 - val_accuracy: 0.9331\n",
      "Epoch 94/800\n",
      "1665/1665 [==============================] - 0s 126us/step - loss: 0.1706 - accuracy: 0.9539 - val_loss: 0.2266 - val_accuracy: 0.9331\n",
      "Epoch 95/800\n",
      "1665/1665 [==============================] - 0s 152us/step - loss: 0.1701 - accuracy: 0.9539 - val_loss: 0.2257 - val_accuracy: 0.9331\n",
      "Epoch 96/800\n",
      "1665/1665 [==============================] - 0s 129us/step - loss: 0.1695 - accuracy: 0.9539 - val_loss: 0.2259 - val_accuracy: 0.9331\n",
      "Epoch 97/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.1690 - accuracy: 0.9539 - val_loss: 0.2239 - val_accuracy: 0.9331\n",
      "Epoch 98/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1684 - accuracy: 0.9539 - val_loss: 0.2236 - val_accuracy: 0.9331\n",
      "Epoch 99/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.1679 - accuracy: 0.9539 - val_loss: 0.2236 - val_accuracy: 0.9331\n",
      "Epoch 100/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.1675 - accuracy: 0.9539 - val_loss: 0.2217 - val_accuracy: 0.9331\n",
      "Epoch 101/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.1670 - accuracy: 0.9539 - val_loss: 0.2217 - val_accuracy: 0.9331\n",
      "Epoch 102/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.1665 - accuracy: 0.9539 - val_loss: 0.2213 - val_accuracy: 0.9331\n",
      "Epoch 103/800\n",
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.1659 - accuracy: 0.9539 - val_loss: 0.2206 - val_accuracy: 0.9331\n",
      "Epoch 104/800\n",
      "1665/1665 [==============================] - 0s 136us/step - loss: 0.1655 - accuracy: 0.9539 - val_loss: 0.2207 - val_accuracy: 0.9331\n",
      "Epoch 105/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.1649 - accuracy: 0.9539 - val_loss: 0.2202 - val_accuracy: 0.9331\n",
      "Epoch 106/800\n",
      "1665/1665 [==============================] - 0s 141us/step - loss: 0.1643 - accuracy: 0.9539 - val_loss: 0.2185 - val_accuracy: 0.9331\n",
      "Epoch 107/800\n",
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.1638 - accuracy: 0.9539 - val_loss: 0.2182 - val_accuracy: 0.9331\n",
      "Epoch 108/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.1634 - accuracy: 0.9539 - val_loss: 0.2172 - val_accuracy: 0.9331\n",
      "Epoch 109/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.1628 - accuracy: 0.9539 - val_loss: 0.2163 - val_accuracy: 0.9331\n",
      "Epoch 110/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.1622 - accuracy: 0.9539 - val_loss: 0.2165 - val_accuracy: 0.9333\n",
      "Epoch 111/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.1616 - accuracy: 0.9539 - val_loss: 0.2150 - val_accuracy: 0.9338\n",
      "Epoch 112/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1611 - accuracy: 0.9540 - val_loss: 0.2151 - val_accuracy: 0.9335\n",
      "Epoch 113/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.1605 - accuracy: 0.9539 - val_loss: 0.2136 - val_accuracy: 0.9332\n",
      "Epoch 114/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.1600 - accuracy: 0.9539 - val_loss: 0.2131 - val_accuracy: 0.9338\n",
      "Epoch 115/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.1594 - accuracy: 0.9540 - val_loss: 0.2127 - val_accuracy: 0.9335\n",
      "Epoch 116/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.1588 - accuracy: 0.9540 - val_loss: 0.2123 - val_accuracy: 0.9336\n",
      "Epoch 117/800\n",
      "1665/1665 [==============================] - 0s 129us/step - loss: 0.1582 - accuracy: 0.9540 - val_loss: 0.2100 - val_accuracy: 0.9335\n",
      "Epoch 118/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.1575 - accuracy: 0.9540 - val_loss: 0.2110 - val_accuracy: 0.9335\n",
      "Epoch 119/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1569 - accuracy: 0.9540 - val_loss: 0.2101 - val_accuracy: 0.9339\n",
      "Epoch 120/800\n",
      "1665/1665 [==============================] - 0s 133us/step - loss: 0.1564 - accuracy: 0.9541 - val_loss: 0.2086 - val_accuracy: 0.9338\n",
      "Epoch 121/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.1557 - accuracy: 0.9540 - val_loss: 0.2076 - val_accuracy: 0.9338\n",
      "Epoch 122/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1552 - accuracy: 0.9542 - val_loss: 0.2065 - val_accuracy: 0.9336\n",
      "Epoch 123/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.1545 - accuracy: 0.9542 - val_loss: 0.2065 - val_accuracy: 0.9338\n",
      "Epoch 124/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.1538 - accuracy: 0.9541 - val_loss: 0.2054 - val_accuracy: 0.9339\n",
      "Epoch 125/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.1532 - accuracy: 0.9542 - val_loss: 0.2047 - val_accuracy: 0.9343\n",
      "Epoch 126/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.1526 - accuracy: 0.9542 - val_loss: 0.2044 - val_accuracy: 0.9337\n",
      "Epoch 127/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.1520 - accuracy: 0.9542 - val_loss: 0.2030 - val_accuracy: 0.9341\n",
      "Epoch 128/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1513 - accuracy: 0.9543 - val_loss: 0.2036 - val_accuracy: 0.9345\n",
      "Epoch 129/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.1509 - accuracy: 0.9544 - val_loss: 0.2027 - val_accuracy: 0.9344\n",
      "Epoch 130/800\n",
      "1665/1665 [==============================] - 0s 112us/step - loss: 0.1502 - accuracy: 0.9543 - val_loss: 0.2001 - val_accuracy: 0.9348\n",
      "Epoch 131/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.1496 - accuracy: 0.9544 - val_loss: 0.2005 - val_accuracy: 0.9348\n",
      "Epoch 132/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.1491 - accuracy: 0.9545 - val_loss: 0.1995 - val_accuracy: 0.9349\n",
      "Epoch 133/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.1484 - accuracy: 0.9545 - val_loss: 0.1985 - val_accuracy: 0.9349\n",
      "Epoch 134/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.1478 - accuracy: 0.9544 - val_loss: 0.1984 - val_accuracy: 0.9351\n",
      "Epoch 135/800\n",
      "1665/1665 [==============================] - 0s 129us/step - loss: 0.1473 - accuracy: 0.9546 - val_loss: 0.1982 - val_accuracy: 0.9352\n",
      "Epoch 136/800\n",
      "1665/1665 [==============================] - 0s 151us/step - loss: 0.1467 - accuracy: 0.9546 - val_loss: 0.1971 - val_accuracy: 0.9349\n",
      "Epoch 137/800\n",
      "1665/1665 [==============================] - 0s 154us/step - loss: 0.1460 - accuracy: 0.9546 - val_loss: 0.1966 - val_accuracy: 0.9351\n",
      "Epoch 138/800\n",
      "1665/1665 [==============================] - 0s 164us/step - loss: 0.1456 - accuracy: 0.9546 - val_loss: 0.1961 - val_accuracy: 0.9350\n",
      "Epoch 139/800\n",
      "1665/1665 [==============================] - 0s 136us/step - loss: 0.1450 - accuracy: 0.9548 - val_loss: 0.1948 - val_accuracy: 0.9357\n",
      "Epoch 140/800\n",
      "1665/1665 [==============================] - 0s 141us/step - loss: 0.1444 - accuracy: 0.9548 - val_loss: 0.1966 - val_accuracy: 0.9353\n",
      "Epoch 141/800\n",
      "1665/1665 [==============================] - 0s 141us/step - loss: 0.1440 - accuracy: 0.9549 - val_loss: 0.1949 - val_accuracy: 0.9355\n",
      "Epoch 142/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.1434 - accuracy: 0.9548 - val_loss: 0.1928 - val_accuracy: 0.9358\n",
      "Epoch 143/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.1428 - accuracy: 0.9548 - val_loss: 0.1929 - val_accuracy: 0.9350\n",
      "Epoch 144/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.1423 - accuracy: 0.9548 - val_loss: 0.1926 - val_accuracy: 0.9356\n",
      "Epoch 145/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.1418 - accuracy: 0.9552 - val_loss: 0.1906 - val_accuracy: 0.9362\n",
      "Epoch 146/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.1413 - accuracy: 0.9552 - val_loss: 0.1933 - val_accuracy: 0.9358\n",
      "Epoch 147/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.1409 - accuracy: 0.9551 - val_loss: 0.1901 - val_accuracy: 0.9368\n",
      "Epoch 148/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.1404 - accuracy: 0.9552 - val_loss: 0.1897 - val_accuracy: 0.9362\n",
      "Epoch 149/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.1397 - accuracy: 0.9553 - val_loss: 0.1888 - val_accuracy: 0.9364\n",
      "Epoch 150/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.1393 - accuracy: 0.9553 - val_loss: 0.1884 - val_accuracy: 0.9363\n",
      "Epoch 151/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.1387 - accuracy: 0.9555 - val_loss: 0.1884 - val_accuracy: 0.9366\n",
      "Epoch 152/800\n",
      "1665/1665 [==============================] - 0s 112us/step - loss: 0.1382 - accuracy: 0.9557 - val_loss: 0.1864 - val_accuracy: 0.9368\n",
      "Epoch 153/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.1377 - accuracy: 0.9555 - val_loss: 0.1924 - val_accuracy: 0.9350\n",
      "Epoch 154/800\n",
      "1665/1665 [==============================] - 0s 106us/step - loss: 0.1375 - accuracy: 0.9558 - val_loss: 0.1860 - val_accuracy: 0.9371\n",
      "Epoch 155/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.1366 - accuracy: 0.9559 - val_loss: 0.1849 - val_accuracy: 0.9371\n",
      "Epoch 156/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.1360 - accuracy: 0.9559 - val_loss: 0.1854 - val_accuracy: 0.9366\n",
      "Epoch 157/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.1355 - accuracy: 0.9561 - val_loss: 0.1846 - val_accuracy: 0.9372\n",
      "Epoch 158/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.1351 - accuracy: 0.9562 - val_loss: 0.1851 - val_accuracy: 0.9372\n",
      "Epoch 159/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.1345 - accuracy: 0.9563 - val_loss: 0.1858 - val_accuracy: 0.9368\n",
      "Epoch 160/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.1341 - accuracy: 0.9564 - val_loss: 0.1831 - val_accuracy: 0.9371\n",
      "Epoch 161/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.1335 - accuracy: 0.9563 - val_loss: 0.1820 - val_accuracy: 0.9373\n",
      "Epoch 162/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.1329 - accuracy: 0.9565 - val_loss: 0.1822 - val_accuracy: 0.9376\n",
      "Epoch 163/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.1325 - accuracy: 0.9564 - val_loss: 0.1828 - val_accuracy: 0.9377\n",
      "Epoch 164/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1320 - accuracy: 0.9569 - val_loss: 0.1817 - val_accuracy: 0.9375\n",
      "Epoch 165/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.1313 - accuracy: 0.9570 - val_loss: 0.1798 - val_accuracy: 0.9384\n",
      "Epoch 166/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.1307 - accuracy: 0.9570 - val_loss: 0.1798 - val_accuracy: 0.9385\n",
      "Epoch 167/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1304 - accuracy: 0.9572 - val_loss: 0.1835 - val_accuracy: 0.9342\n",
      "Epoch 168/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1301 - accuracy: 0.9573 - val_loss: 0.1798 - val_accuracy: 0.9380\n",
      "Epoch 169/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.1295 - accuracy: 0.9573 - val_loss: 0.1783 - val_accuracy: 0.9379\n",
      "Epoch 170/800\n",
      "1665/1665 [==============================] - 0s 109us/step - loss: 0.1287 - accuracy: 0.9574 - val_loss: 0.1777 - val_accuracy: 0.9387\n",
      "Epoch 171/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.1284 - accuracy: 0.9576 - val_loss: 0.1773 - val_accuracy: 0.9378\n",
      "Epoch 172/800\n",
      "1665/1665 [==============================] - 0s 143us/step - loss: 0.1279 - accuracy: 0.9576 - val_loss: 0.1777 - val_accuracy: 0.9382\n",
      "Epoch 173/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.1275 - accuracy: 0.9579 - val_loss: 0.1765 - val_accuracy: 0.9388\n",
      "Epoch 174/800\n",
      "1665/1665 [==============================] - 0s 129us/step - loss: 0.1270 - accuracy: 0.9579 - val_loss: 0.1751 - val_accuracy: 0.9384\n",
      "Epoch 175/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.1265 - accuracy: 0.9582 - val_loss: 0.1749 - val_accuracy: 0.9385\n",
      "Epoch 176/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.1260 - accuracy: 0.9581 - val_loss: 0.1744 - val_accuracy: 0.9384\n",
      "Epoch 177/800\n",
      "1665/1665 [==============================] - 0s 133us/step - loss: 0.1255 - accuracy: 0.9583 - val_loss: 0.1790 - val_accuracy: 0.9373\n",
      "Epoch 178/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.1252 - accuracy: 0.9583 - val_loss: 0.1732 - val_accuracy: 0.9387\n",
      "Epoch 179/800\n",
      "1665/1665 [==============================] - 0s 109us/step - loss: 0.1246 - accuracy: 0.9586 - val_loss: 0.1727 - val_accuracy: 0.9390\n",
      "Epoch 180/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.1241 - accuracy: 0.9584 - val_loss: 0.1715 - val_accuracy: 0.9389\n",
      "Epoch 181/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.1238 - accuracy: 0.9585 - val_loss: 0.1719 - val_accuracy: 0.9392\n",
      "Epoch 182/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1231 - accuracy: 0.9585 - val_loss: 0.1735 - val_accuracy: 0.9385\n",
      "Epoch 183/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.1230 - accuracy: 0.9588 - val_loss: 0.1699 - val_accuracy: 0.9396\n",
      "Epoch 184/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.1224 - accuracy: 0.9587 - val_loss: 0.1715 - val_accuracy: 0.9390\n",
      "Epoch 185/800\n",
      "1665/1665 [==============================] - 0s 105us/step - loss: 0.1220 - accuracy: 0.9588 - val_loss: 0.1724 - val_accuracy: 0.9388\n",
      "Epoch 186/800\n",
      "1665/1665 [==============================] - 0s 136us/step - loss: 0.1216 - accuracy: 0.9589 - val_loss: 0.1693 - val_accuracy: 0.9398\n",
      "Epoch 187/800\n",
      "1665/1665 [==============================] - 0s 129us/step - loss: 0.1211 - accuracy: 0.9591 - val_loss: 0.1714 - val_accuracy: 0.9392\n",
      "Epoch 188/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1208 - accuracy: 0.9590 - val_loss: 0.1702 - val_accuracy: 0.9393\n",
      "Epoch 189/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.1204 - accuracy: 0.9591 - val_loss: 0.1677 - val_accuracy: 0.9402\n",
      "Epoch 190/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1198 - accuracy: 0.9592 - val_loss: 0.1676 - val_accuracy: 0.9404\n",
      "Epoch 191/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.1194 - accuracy: 0.9594 - val_loss: 0.1714 - val_accuracy: 0.9386\n",
      "Epoch 192/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1192 - accuracy: 0.9593 - val_loss: 0.1672 - val_accuracy: 0.9391\n",
      "Epoch 193/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.1186 - accuracy: 0.9595 - val_loss: 0.1666 - val_accuracy: 0.9400\n",
      "Epoch 194/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.1181 - accuracy: 0.9596 - val_loss: 0.1661 - val_accuracy: 0.9404\n",
      "Epoch 195/800\n",
      "1665/1665 [==============================] - 0s 109us/step - loss: 0.1179 - accuracy: 0.9595 - val_loss: 0.1662 - val_accuracy: 0.9415\n",
      "Epoch 196/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.1175 - accuracy: 0.9597 - val_loss: 0.1666 - val_accuracy: 0.9392\n",
      "Epoch 197/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.1171 - accuracy: 0.9599 - val_loss: 0.1666 - val_accuracy: 0.9391\n",
      "Epoch 198/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.1168 - accuracy: 0.9598 - val_loss: 0.1655 - val_accuracy: 0.9410\n",
      "Epoch 199/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.1163 - accuracy: 0.9598 - val_loss: 0.1647 - val_accuracy: 0.9406\n",
      "Epoch 200/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.1161 - accuracy: 0.9600 - val_loss: 0.1647 - val_accuracy: 0.9410\n",
      "Epoch 201/800\n",
      "1665/1665 [==============================] - 0s 126us/step - loss: 0.1155 - accuracy: 0.9603 - val_loss: 0.1676 - val_accuracy: 0.9393\n",
      "Epoch 202/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.1154 - accuracy: 0.9601 - val_loss: 0.1631 - val_accuracy: 0.9416\n",
      "Epoch 203/800\n",
      "1665/1665 [==============================] - 0s 136us/step - loss: 0.1148 - accuracy: 0.9605 - val_loss: 0.1623 - val_accuracy: 0.9412\n",
      "Epoch 204/800\n",
      "1665/1665 [==============================] - 0s 159us/step - loss: 0.1145 - accuracy: 0.9604 - val_loss: 0.1637 - val_accuracy: 0.9407\n",
      "Epoch 205/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.1143 - accuracy: 0.9603 - val_loss: 0.1640 - val_accuracy: 0.9408\n",
      "Epoch 206/800\n",
      "1665/1665 [==============================] - 0s 174us/step - loss: 0.1140 - accuracy: 0.9606 - val_loss: 0.1622 - val_accuracy: 0.9407\n",
      "Epoch 207/800\n",
      "1665/1665 [==============================] - 0s 133us/step - loss: 0.1136 - accuracy: 0.9605 - val_loss: 0.1633 - val_accuracy: 0.9406\n",
      "Epoch 208/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.1133 - accuracy: 0.9605 - val_loss: 0.1623 - val_accuracy: 0.9410\n",
      "Epoch 209/800\n",
      "1665/1665 [==============================] - 0s 133us/step - loss: 0.1129 - accuracy: 0.9607 - val_loss: 0.1606 - val_accuracy: 0.9418\n",
      "Epoch 210/800\n",
      "1665/1665 [==============================] - 0s 152us/step - loss: 0.1124 - accuracy: 0.9608 - val_loss: 0.1593 - val_accuracy: 0.9425\n",
      "Epoch 211/800\n",
      "1665/1665 [==============================] - 0s 142us/step - loss: 0.1120 - accuracy: 0.9610 - val_loss: 0.1605 - val_accuracy: 0.9415\n",
      "Epoch 212/800\n",
      "1665/1665 [==============================] - 0s 142us/step - loss: 0.1119 - accuracy: 0.9611 - val_loss: 0.1596 - val_accuracy: 0.9423\n",
      "Epoch 213/800\n",
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.1115 - accuracy: 0.9608 - val_loss: 0.1598 - val_accuracy: 0.9407\n",
      "Epoch 214/800\n",
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.1112 - accuracy: 0.9612 - val_loss: 0.1586 - val_accuracy: 0.9420\n",
      "Epoch 215/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.1108 - accuracy: 0.9613 - val_loss: 0.1575 - val_accuracy: 0.9423\n",
      "Epoch 216/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.1105 - accuracy: 0.9614 - val_loss: 0.1587 - val_accuracy: 0.9422\n",
      "Epoch 217/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.1103 - accuracy: 0.9614 - val_loss: 0.1793 - val_accuracy: 0.9343\n",
      "Epoch 218/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1107 - accuracy: 0.9614 - val_loss: 0.1573 - val_accuracy: 0.9433\n",
      "Epoch 219/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1095 - accuracy: 0.9618 - val_loss: 0.1567 - val_accuracy: 0.9422\n",
      "Epoch 220/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1093 - accuracy: 0.9618 - val_loss: 0.1578 - val_accuracy: 0.9420\n",
      "Epoch 221/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1089 - accuracy: 0.9622 - val_loss: 0.1570 - val_accuracy: 0.9417\n",
      "Epoch 222/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.1086 - accuracy: 0.9622 - val_loss: 0.1777 - val_accuracy: 0.9346\n",
      "Epoch 223/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.1091 - accuracy: 0.9617 - val_loss: 0.1560 - val_accuracy: 0.9429\n",
      "Epoch 224/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.1082 - accuracy: 0.9621 - val_loss: 0.1553 - val_accuracy: 0.9421\n",
      "Epoch 225/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1079 - accuracy: 0.9623 - val_loss: 0.1555 - val_accuracy: 0.9416\n",
      "Epoch 226/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.1074 - accuracy: 0.9624 - val_loss: 0.1548 - val_accuracy: 0.9430\n",
      "Epoch 227/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.1070 - accuracy: 0.9626 - val_loss: 0.1579 - val_accuracy: 0.9414\n",
      "Epoch 228/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.1070 - accuracy: 0.9625 - val_loss: 0.1554 - val_accuracy: 0.9430\n",
      "Epoch 229/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.1065 - accuracy: 0.9628 - val_loss: 0.1679 - val_accuracy: 0.9364\n",
      "Epoch 230/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1066 - accuracy: 0.9624 - val_loss: 0.1528 - val_accuracy: 0.9433\n",
      "Epoch 231/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.1060 - accuracy: 0.9630 - val_loss: 0.1519 - val_accuracy: 0.9440\n",
      "Epoch 232/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.1055 - accuracy: 0.9632 - val_loss: 0.1643 - val_accuracy: 0.9380\n",
      "Epoch 233/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.1059 - accuracy: 0.9627 - val_loss: 0.1530 - val_accuracy: 0.9438\n",
      "Epoch 234/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1051 - accuracy: 0.9632 - val_loss: 0.1529 - val_accuracy: 0.9439\n",
      "Epoch 235/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.1047 - accuracy: 0.9633 - val_loss: 0.1538 - val_accuracy: 0.9439\n",
      "Epoch 236/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.1044 - accuracy: 0.9634 - val_loss: 0.1528 - val_accuracy: 0.9440\n",
      "Epoch 237/800\n",
      "1665/1665 [==============================] - 0s 143us/step - loss: 0.1042 - accuracy: 0.9633 - val_loss: 0.1521 - val_accuracy: 0.9445\n",
      "Epoch 238/800\n",
      "1665/1665 [==============================] - 0s 139us/step - loss: 0.1039 - accuracy: 0.9636 - val_loss: 0.1528 - val_accuracy: 0.9437\n",
      "Epoch 239/800\n",
      "1665/1665 [==============================] - 0s 144us/step - loss: 0.1036 - accuracy: 0.9636 - val_loss: 0.1659 - val_accuracy: 0.9371\n",
      "Epoch 240/800\n",
      "1665/1665 [==============================] - 0s 139us/step - loss: 0.1039 - accuracy: 0.9634 - val_loss: 0.1528 - val_accuracy: 0.9431\n",
      "Epoch 241/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.1031 - accuracy: 0.9639 - val_loss: 0.1631 - val_accuracy: 0.9382\n",
      "Epoch 242/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1032 - accuracy: 0.9636 - val_loss: 0.1502 - val_accuracy: 0.9451\n",
      "Epoch 243/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1023 - accuracy: 0.9641 - val_loss: 0.1500 - val_accuracy: 0.9444\n",
      "Epoch 244/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.1022 - accuracy: 0.9642 - val_loss: 0.1502 - val_accuracy: 0.9445\n",
      "Epoch 245/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.1018 - accuracy: 0.9643 - val_loss: 0.1498 - val_accuracy: 0.9446\n",
      "Epoch 246/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.1015 - accuracy: 0.9645 - val_loss: 0.1481 - val_accuracy: 0.9449\n",
      "Epoch 247/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.1011 - accuracy: 0.9648 - val_loss: 0.1501 - val_accuracy: 0.9444\n",
      "Epoch 248/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.1010 - accuracy: 0.9647 - val_loss: 0.1487 - val_accuracy: 0.9444\n",
      "Epoch 249/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1006 - accuracy: 0.9649 - val_loss: 0.1638 - val_accuracy: 0.9375\n",
      "Epoch 250/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1010 - accuracy: 0.9645 - val_loss: 0.1490 - val_accuracy: 0.9455\n",
      "Epoch 251/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.1001 - accuracy: 0.9648 - val_loss: 0.1475 - val_accuracy: 0.9457\n",
      "Epoch 252/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0999 - accuracy: 0.9652 - val_loss: 0.1485 - val_accuracy: 0.9443\n",
      "Epoch 253/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0995 - accuracy: 0.9653 - val_loss: 0.1490 - val_accuracy: 0.9448\n",
      "Epoch 254/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0991 - accuracy: 0.9656 - val_loss: 0.1487 - val_accuracy: 0.9452\n",
      "Epoch 255/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0990 - accuracy: 0.9657 - val_loss: 0.1467 - val_accuracy: 0.9460\n",
      "Epoch 256/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0986 - accuracy: 0.9657 - val_loss: 0.1465 - val_accuracy: 0.9454\n",
      "Epoch 257/800\n",
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.0983 - accuracy: 0.9656 - val_loss: 0.1461 - val_accuracy: 0.9457\n",
      "Epoch 258/800\n",
      "1665/1665 [==============================] - 0s 143us/step - loss: 0.0981 - accuracy: 0.9659 - val_loss: 0.1548 - val_accuracy: 0.9420\n",
      "Epoch 259/800\n",
      "1665/1665 [==============================] - 0s 154us/step - loss: 0.0981 - accuracy: 0.9661 - val_loss: 0.1467 - val_accuracy: 0.9460\n",
      "Epoch 260/800\n",
      "1665/1665 [==============================] - 0s 136us/step - loss: 0.0976 - accuracy: 0.9661 - val_loss: 0.1459 - val_accuracy: 0.9461\n",
      "Epoch 261/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0972 - accuracy: 0.9661 - val_loss: 0.1451 - val_accuracy: 0.9467\n",
      "Epoch 262/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.0970 - accuracy: 0.9665 - val_loss: 0.1447 - val_accuracy: 0.9466\n",
      "Epoch 263/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0965 - accuracy: 0.9664 - val_loss: 0.1443 - val_accuracy: 0.9466\n",
      "Epoch 264/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0963 - accuracy: 0.9664 - val_loss: 0.1449 - val_accuracy: 0.9460\n",
      "Epoch 265/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0962 - accuracy: 0.9667 - val_loss: 0.1531 - val_accuracy: 0.9422\n",
      "Epoch 266/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0962 - accuracy: 0.9666 - val_loss: 0.1428 - val_accuracy: 0.9474\n",
      "Epoch 267/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0956 - accuracy: 0.9668 - val_loss: 0.1441 - val_accuracy: 0.9469\n",
      "Epoch 268/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0954 - accuracy: 0.9671 - val_loss: 0.1442 - val_accuracy: 0.9465\n",
      "Epoch 269/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0950 - accuracy: 0.9670 - val_loss: 0.1425 - val_accuracy: 0.9466\n",
      "Epoch 270/800\n",
      "1665/1665 [==============================] - 0s 133us/step - loss: 0.0947 - accuracy: 0.9674 - val_loss: 0.1416 - val_accuracy: 0.9471\n",
      "Epoch 271/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.0945 - accuracy: 0.9674 - val_loss: 0.1422 - val_accuracy: 0.9465\n",
      "Epoch 272/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0942 - accuracy: 0.9676 - val_loss: 0.1428 - val_accuracy: 0.9469\n",
      "Epoch 273/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0940 - accuracy: 0.9674 - val_loss: 0.1422 - val_accuracy: 0.9470\n",
      "Epoch 274/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.0938 - accuracy: 0.9676 - val_loss: 0.1413 - val_accuracy: 0.9478\n",
      "Epoch 275/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0934 - accuracy: 0.9679 - val_loss: 0.1405 - val_accuracy: 0.9476\n",
      "Epoch 276/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.0932 - accuracy: 0.9677 - val_loss: 0.1404 - val_accuracy: 0.9474\n",
      "Epoch 277/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.0929 - accuracy: 0.9678 - val_loss: 0.1521 - val_accuracy: 0.9424\n",
      "Epoch 278/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0931 - accuracy: 0.9677 - val_loss: 0.1406 - val_accuracy: 0.9472\n",
      "Epoch 279/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.0924 - accuracy: 0.9682 - val_loss: 0.1523 - val_accuracy: 0.9433\n",
      "Epoch 280/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0926 - accuracy: 0.9678 - val_loss: 0.1400 - val_accuracy: 0.9487\n",
      "Epoch 281/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0918 - accuracy: 0.9684 - val_loss: 0.1386 - val_accuracy: 0.9481\n",
      "Epoch 282/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0917 - accuracy: 0.9684 - val_loss: 0.1391 - val_accuracy: 0.9485\n",
      "Epoch 283/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.0914 - accuracy: 0.9686 - val_loss: 0.1392 - val_accuracy: 0.9484\n",
      "Epoch 284/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0912 - accuracy: 0.9685 - val_loss: 0.1378 - val_accuracy: 0.9484\n",
      "Epoch 285/800\n",
      "1665/1665 [==============================] - 0s 163us/step - loss: 0.0909 - accuracy: 0.9688 - val_loss: 0.1380 - val_accuracy: 0.9487\n",
      "Epoch 286/800\n",
      "1665/1665 [==============================] - 0s 155us/step - loss: 0.0907 - accuracy: 0.9688 - val_loss: 0.1380 - val_accuracy: 0.9483\n",
      "Epoch 287/800\n",
      "1665/1665 [==============================] - 0s 158us/step - loss: 0.0905 - accuracy: 0.9687 - val_loss: 0.1385 - val_accuracy: 0.9485\n",
      "Epoch 288/800\n",
      "1665/1665 [==============================] - 0s 139us/step - loss: 0.0902 - accuracy: 0.9690 - val_loss: 0.1437 - val_accuracy: 0.9459\n",
      "Epoch 289/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.0903 - accuracy: 0.9689 - val_loss: 0.1379 - val_accuracy: 0.9493\n",
      "Epoch 290/800\n",
      "1665/1665 [==============================] - 0s 145us/step - loss: 0.0898 - accuracy: 0.9692 - val_loss: 0.1380 - val_accuracy: 0.9481\n",
      "Epoch 291/800\n",
      "1665/1665 [==============================] - 0s 148us/step - loss: 0.0897 - accuracy: 0.9692 - val_loss: 0.1543 - val_accuracy: 0.9417\n",
      "Epoch 292/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.0901 - accuracy: 0.9687 - val_loss: 0.1394 - val_accuracy: 0.9482\n",
      "Epoch 293/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.0893 - accuracy: 0.9691 - val_loss: 0.1389 - val_accuracy: 0.9480\n",
      "Epoch 294/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.0890 - accuracy: 0.9691 - val_loss: 0.1378 - val_accuracy: 0.9483\n",
      "Epoch 295/800\n",
      "1665/1665 [==============================] - 0s 129us/step - loss: 0.0887 - accuracy: 0.9692 - val_loss: 0.1370 - val_accuracy: 0.9491\n",
      "Epoch 296/800\n",
      "1665/1665 [==============================] - 0s 139us/step - loss: 0.0885 - accuracy: 0.9690 - val_loss: 0.1374 - val_accuracy: 0.9492\n",
      "Epoch 297/800\n",
      "1665/1665 [==============================] - 0s 140us/step - loss: 0.0882 - accuracy: 0.9695 - val_loss: 0.1371 - val_accuracy: 0.9501\n",
      "Epoch 298/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.0883 - accuracy: 0.9692 - val_loss: 0.1353 - val_accuracy: 0.9497\n",
      "Epoch 299/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0879 - accuracy: 0.9695 - val_loss: 0.1351 - val_accuracy: 0.9500\n",
      "Epoch 300/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0878 - accuracy: 0.9697 - val_loss: 0.1383 - val_accuracy: 0.9494\n",
      "Epoch 301/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.0876 - accuracy: 0.9697 - val_loss: 0.1351 - val_accuracy: 0.9498\n",
      "Epoch 302/800\n",
      "1665/1665 [==============================] - 0s 133us/step - loss: 0.0873 - accuracy: 0.9698 - val_loss: 0.1350 - val_accuracy: 0.9506\n",
      "Epoch 303/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.0870 - accuracy: 0.9698 - val_loss: 0.1369 - val_accuracy: 0.9487\n",
      "Epoch 304/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0873 - accuracy: 0.9694 - val_loss: 0.1348 - val_accuracy: 0.9498\n",
      "Epoch 305/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0869 - accuracy: 0.9698 - val_loss: 0.1345 - val_accuracy: 0.9500\n",
      "Epoch 306/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0867 - accuracy: 0.9700 - val_loss: 0.1354 - val_accuracy: 0.9490\n",
      "Epoch 307/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0866 - accuracy: 0.9700 - val_loss: 0.1341 - val_accuracy: 0.9502\n",
      "Epoch 308/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0864 - accuracy: 0.9698 - val_loss: 0.1347 - val_accuracy: 0.9495\n",
      "Epoch 309/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0863 - accuracy: 0.9699 - val_loss: 0.1350 - val_accuracy: 0.9502\n",
      "Epoch 310/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0859 - accuracy: 0.9697 - val_loss: 0.1333 - val_accuracy: 0.9511\n",
      "Epoch 311/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0858 - accuracy: 0.9701 - val_loss: 0.1346 - val_accuracy: 0.9506\n",
      "Epoch 312/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0857 - accuracy: 0.9702 - val_loss: 0.1330 - val_accuracy: 0.9508\n",
      "Epoch 313/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0856 - accuracy: 0.9703 - val_loss: 0.1350 - val_accuracy: 0.9498\n",
      "Epoch 314/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.0854 - accuracy: 0.9703 - val_loss: 0.1444 - val_accuracy: 0.9461\n",
      "Epoch 315/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0856 - accuracy: 0.9702 - val_loss: 0.1413 - val_accuracy: 0.9473\n",
      "Epoch 316/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0853 - accuracy: 0.9702 - val_loss: 0.1318 - val_accuracy: 0.9506\n",
      "Epoch 317/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0850 - accuracy: 0.9706 - val_loss: 0.1402 - val_accuracy: 0.9482\n",
      "Epoch 318/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0850 - accuracy: 0.9703 - val_loss: 0.1344 - val_accuracy: 0.9502\n",
      "Epoch 319/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0846 - accuracy: 0.9705 - val_loss: 0.1316 - val_accuracy: 0.9518\n",
      "Epoch 320/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.0843 - accuracy: 0.9707 - val_loss: 0.1350 - val_accuracy: 0.9499\n",
      "Epoch 321/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0844 - accuracy: 0.9703 - val_loss: 0.1338 - val_accuracy: 0.9502\n",
      "Epoch 322/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0842 - accuracy: 0.9707 - val_loss: 0.1314 - val_accuracy: 0.9511\n",
      "Epoch 323/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0838 - accuracy: 0.9707 - val_loss: 0.1312 - val_accuracy: 0.9510\n",
      "Epoch 324/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0837 - accuracy: 0.9709 - val_loss: 0.1329 - val_accuracy: 0.9509\n",
      "Epoch 325/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0836 - accuracy: 0.9708 - val_loss: 0.1484 - val_accuracy: 0.9456\n",
      "Epoch 326/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.0841 - accuracy: 0.9707 - val_loss: 0.1336 - val_accuracy: 0.9513\n",
      "Epoch 327/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0834 - accuracy: 0.9707 - val_loss: 0.1309 - val_accuracy: 0.9516\n",
      "Epoch 328/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0831 - accuracy: 0.9709 - val_loss: 0.1314 - val_accuracy: 0.9514\n",
      "Epoch 329/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0828 - accuracy: 0.9710 - val_loss: 0.1325 - val_accuracy: 0.9512\n",
      "Epoch 330/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.0830 - accuracy: 0.9709 - val_loss: 0.1313 - val_accuracy: 0.9508\n",
      "Epoch 331/800\n",
      "1665/1665 [==============================] - 0s 107us/step - loss: 0.0828 - accuracy: 0.9710 - val_loss: 0.1334 - val_accuracy: 0.9509\n",
      "Epoch 332/800\n",
      "1665/1665 [==============================] - 0s 106us/step - loss: 0.0827 - accuracy: 0.9708 - val_loss: 0.1497 - val_accuracy: 0.9453\n",
      "Epoch 333/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0833 - accuracy: 0.9707 - val_loss: 0.1323 - val_accuracy: 0.9516\n",
      "Epoch 334/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0824 - accuracy: 0.9711 - val_loss: 0.1315 - val_accuracy: 0.9507\n",
      "Epoch 335/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.0823 - accuracy: 0.9709 - val_loss: 0.1313 - val_accuracy: 0.9510\n",
      "Epoch 336/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.0822 - accuracy: 0.9710 - val_loss: 0.1285 - val_accuracy: 0.9512\n",
      "Epoch 337/800\n",
      "1665/1665 [==============================] - 0s 106us/step - loss: 0.0820 - accuracy: 0.9709 - val_loss: 0.1311 - val_accuracy: 0.9523\n",
      "Epoch 338/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0818 - accuracy: 0.9711 - val_loss: 0.1617 - val_accuracy: 0.9403\n",
      "Epoch 339/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0829 - accuracy: 0.9706 - val_loss: 0.1488 - val_accuracy: 0.9458\n",
      "Epoch 340/800\n",
      "1665/1665 [==============================] - 0s 107us/step - loss: 0.0824 - accuracy: 0.9711 - val_loss: 0.1303 - val_accuracy: 0.9514\n",
      "Epoch 341/800\n",
      "1665/1665 [==============================] - 0s 104us/step - loss: 0.0815 - accuracy: 0.9712 - val_loss: 0.1295 - val_accuracy: 0.9524\n",
      "Epoch 342/800\n",
      "1665/1665 [==============================] - 0s 107us/step - loss: 0.0812 - accuracy: 0.9715 - val_loss: 0.1367 - val_accuracy: 0.9503\n",
      "Epoch 343/800\n",
      "1665/1665 [==============================] - 0s 107us/step - loss: 0.0815 - accuracy: 0.9713 - val_loss: 0.1290 - val_accuracy: 0.9525\n",
      "Epoch 344/800\n",
      "1665/1665 [==============================] - 0s 107us/step - loss: 0.0809 - accuracy: 0.9714 - val_loss: 0.1439 - val_accuracy: 0.9465\n",
      "Epoch 345/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0813 - accuracy: 0.9711 - val_loss: 0.1310 - val_accuracy: 0.9516\n",
      "Epoch 346/800\n",
      "1665/1665 [==============================] - 0s 106us/step - loss: 0.0808 - accuracy: 0.9715 - val_loss: 0.1286 - val_accuracy: 0.9521\n",
      "Epoch 347/800\n",
      "1665/1665 [==============================] - 0s 104us/step - loss: 0.0806 - accuracy: 0.9715 - val_loss: 0.1322 - val_accuracy: 0.9517\n",
      "Epoch 348/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.0807 - accuracy: 0.9716 - val_loss: 0.1297 - val_accuracy: 0.9527\n",
      "Epoch 349/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.0804 - accuracy: 0.9718 - val_loss: 0.1289 - val_accuracy: 0.9525\n",
      "Epoch 350/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0803 - accuracy: 0.9716 - val_loss: 0.1288 - val_accuracy: 0.9523\n",
      "Epoch 351/800\n",
      "1665/1665 [==============================] - 0s 112us/step - loss: 0.0802 - accuracy: 0.9717 - val_loss: 0.1299 - val_accuracy: 0.9527\n",
      "Epoch 352/800\n",
      "1665/1665 [==============================] - 0s 107us/step - loss: 0.0801 - accuracy: 0.9715 - val_loss: 0.1285 - val_accuracy: 0.9525\n",
      "Epoch 353/800\n",
      "1665/1665 [==============================] - 0s 107us/step - loss: 0.0799 - accuracy: 0.9716 - val_loss: 0.1304 - val_accuracy: 0.9514\n",
      "Epoch 354/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.0799 - accuracy: 0.9716 - val_loss: 0.1349 - val_accuracy: 0.9496\n",
      "Epoch 355/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.0798 - accuracy: 0.9717 - val_loss: 0.1272 - val_accuracy: 0.9530\n",
      "Epoch 356/800\n",
      "1665/1665 [==============================] - 0s 112us/step - loss: 0.0793 - accuracy: 0.9716 - val_loss: 0.1310 - val_accuracy: 0.9522\n",
      "Epoch 357/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.0794 - accuracy: 0.9718 - val_loss: 0.1290 - val_accuracy: 0.9527\n",
      "Epoch 358/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0793 - accuracy: 0.9717 - val_loss: 0.1286 - val_accuracy: 0.9524\n",
      "Epoch 359/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0792 - accuracy: 0.9718 - val_loss: 0.1289 - val_accuracy: 0.9532\n",
      "Epoch 360/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0790 - accuracy: 0.9720 - val_loss: 0.1265 - val_accuracy: 0.9528\n",
      "Epoch 361/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0788 - accuracy: 0.9721 - val_loss: 0.1284 - val_accuracy: 0.9533\n",
      "Epoch 362/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0787 - accuracy: 0.9720 - val_loss: 0.1270 - val_accuracy: 0.9528\n",
      "Epoch 363/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.0785 - accuracy: 0.9721 - val_loss: 0.1266 - val_accuracy: 0.9533\n",
      "Epoch 364/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0783 - accuracy: 0.9722 - val_loss: 0.1294 - val_accuracy: 0.9518\n",
      "Epoch 365/800\n",
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.0785 - accuracy: 0.9721 - val_loss: 0.1378 - val_accuracy: 0.9482\n",
      "Epoch 366/800\n",
      "1665/1665 [==============================] - 0s 148us/step - loss: 0.0786 - accuracy: 0.9722 - val_loss: 0.1558 - val_accuracy: 0.9422\n",
      "Epoch 367/800\n",
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.0793 - accuracy: 0.9721 - val_loss: 0.1260 - val_accuracy: 0.9534\n",
      "Epoch 368/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0780 - accuracy: 0.9721 - val_loss: 0.1263 - val_accuracy: 0.9529\n",
      "Epoch 369/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0778 - accuracy: 0.9723 - val_loss: 0.1276 - val_accuracy: 0.9535\n",
      "Epoch 370/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0779 - accuracy: 0.9724 - val_loss: 0.1284 - val_accuracy: 0.9531\n",
      "Epoch 371/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0777 - accuracy: 0.9722 - val_loss: 0.1263 - val_accuracy: 0.9529\n",
      "Epoch 372/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0776 - accuracy: 0.9722 - val_loss: 0.1469 - val_accuracy: 0.9453\n",
      "Epoch 373/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.0781 - accuracy: 0.9722 - val_loss: 0.1317 - val_accuracy: 0.9510\n",
      "Epoch 374/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0777 - accuracy: 0.9721 - val_loss: 0.1295 - val_accuracy: 0.9527\n",
      "Epoch 375/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0773 - accuracy: 0.9728 - val_loss: 0.1256 - val_accuracy: 0.9540\n",
      "Epoch 376/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.0770 - accuracy: 0.9725 - val_loss: 0.1284 - val_accuracy: 0.9528\n",
      "Epoch 377/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.0771 - accuracy: 0.9725 - val_loss: 0.1256 - val_accuracy: 0.9541\n",
      "Epoch 378/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.0767 - accuracy: 0.9725 - val_loss: 0.1253 - val_accuracy: 0.9535\n",
      "Epoch 379/800\n",
      "1665/1665 [==============================] - 0s 107us/step - loss: 0.0766 - accuracy: 0.9726 - val_loss: 0.1251 - val_accuracy: 0.9535\n",
      "Epoch 380/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.0766 - accuracy: 0.9726 - val_loss: 0.1263 - val_accuracy: 0.9538\n",
      "Epoch 381/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0763 - accuracy: 0.9726 - val_loss: 0.1263 - val_accuracy: 0.9532\n",
      "Epoch 382/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0764 - accuracy: 0.9726 - val_loss: 0.1258 - val_accuracy: 0.9534\n",
      "Epoch 383/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0763 - accuracy: 0.9729 - val_loss: 0.1392 - val_accuracy: 0.9471\n",
      "Epoch 384/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0765 - accuracy: 0.9727 - val_loss: 0.1250 - val_accuracy: 0.9539\n",
      "Epoch 385/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0759 - accuracy: 0.9730 - val_loss: 0.1250 - val_accuracy: 0.9536\n",
      "Epoch 386/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0758 - accuracy: 0.9728 - val_loss: 0.1246 - val_accuracy: 0.9542\n",
      "Epoch 387/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0758 - accuracy: 0.9730 - val_loss: 0.1260 - val_accuracy: 0.9533\n",
      "Epoch 388/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0758 - accuracy: 0.9727 - val_loss: 0.1235 - val_accuracy: 0.9542\n",
      "Epoch 389/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.0753 - accuracy: 0.9729 - val_loss: 0.1258 - val_accuracy: 0.9520\n",
      "Epoch 390/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0755 - accuracy: 0.9730 - val_loss: 0.1262 - val_accuracy: 0.9548\n",
      "Epoch 391/800\n",
      "1665/1665 [==============================] - 0s 142us/step - loss: 0.0753 - accuracy: 0.9732 - val_loss: 0.1250 - val_accuracy: 0.9546\n",
      "Epoch 392/800\n",
      "1665/1665 [==============================] - 0s 129us/step - loss: 0.0752 - accuracy: 0.9732 - val_loss: 0.1235 - val_accuracy: 0.9546\n",
      "Epoch 393/800\n",
      "1665/1665 [==============================] - 0s 136us/step - loss: 0.0750 - accuracy: 0.9729 - val_loss: 0.1239 - val_accuracy: 0.9548\n",
      "Epoch 394/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0748 - accuracy: 0.9734 - val_loss: 0.1225 - val_accuracy: 0.9544\n",
      "Epoch 395/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.0747 - accuracy: 0.9732 - val_loss: 0.1242 - val_accuracy: 0.9544\n",
      "Epoch 396/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0747 - accuracy: 0.9733 - val_loss: 0.1236 - val_accuracy: 0.9545\n",
      "Epoch 397/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0745 - accuracy: 0.9734 - val_loss: 0.1252 - val_accuracy: 0.9539\n",
      "Epoch 398/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0747 - accuracy: 0.9729 - val_loss: 0.1245 - val_accuracy: 0.9544\n",
      "Epoch 399/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0745 - accuracy: 0.9733 - val_loss: 0.1352 - val_accuracy: 0.9497\n",
      "Epoch 400/800\n",
      "1665/1665 [==============================] - 0s 142us/step - loss: 0.0747 - accuracy: 0.9735 - val_loss: 0.1238 - val_accuracy: 0.9532\n",
      "Epoch 401/800\n",
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.0741 - accuracy: 0.9733 - val_loss: 0.1401 - val_accuracy: 0.9477\n",
      "Epoch 402/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0750 - accuracy: 0.9730 - val_loss: 0.1402 - val_accuracy: 0.9472\n",
      "Epoch 403/800\n",
      "1665/1665 [==============================] - 0s 129us/step - loss: 0.0750 - accuracy: 0.9730 - val_loss: 0.1241 - val_accuracy: 0.9540\n",
      "Epoch 404/800\n",
      "1665/1665 [==============================] - 0s 152us/step - loss: 0.0740 - accuracy: 0.9737 - val_loss: 0.1239 - val_accuracy: 0.9542\n",
      "Epoch 405/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0738 - accuracy: 0.9735 - val_loss: 0.1243 - val_accuracy: 0.9541\n",
      "Epoch 406/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0739 - accuracy: 0.9735 - val_loss: 0.1242 - val_accuracy: 0.9534\n",
      "Epoch 407/800\n",
      "1665/1665 [==============================] - 0s 129us/step - loss: 0.0737 - accuracy: 0.9734 - val_loss: 0.1227 - val_accuracy: 0.9534\n",
      "Epoch 408/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0735 - accuracy: 0.9735 - val_loss: 0.1215 - val_accuracy: 0.9549\n",
      "Epoch 409/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.0732 - accuracy: 0.9736 - val_loss: 0.1242 - val_accuracy: 0.9533\n",
      "Epoch 410/800\n",
      "1665/1665 [==============================] - 0s 109us/step - loss: 0.0734 - accuracy: 0.9735 - val_loss: 0.1222 - val_accuracy: 0.9540\n",
      "Epoch 411/800\n",
      "1665/1665 [==============================] - 0s 126us/step - loss: 0.0732 - accuracy: 0.9735 - val_loss: 0.1446 - val_accuracy: 0.9464\n",
      "Epoch 412/800\n",
      "1665/1665 [==============================] - 0s 112us/step - loss: 0.0743 - accuracy: 0.9731 - val_loss: 0.1210 - val_accuracy: 0.9540\n",
      "Epoch 413/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.0731 - accuracy: 0.9739 - val_loss: 0.1224 - val_accuracy: 0.9549\n",
      "Epoch 414/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.0729 - accuracy: 0.9736 - val_loss: 0.1222 - val_accuracy: 0.9549\n",
      "Epoch 415/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0728 - accuracy: 0.9736 - val_loss: 0.1227 - val_accuracy: 0.9546\n",
      "Epoch 416/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.0727 - accuracy: 0.9737 - val_loss: 0.1234 - val_accuracy: 0.9538\n",
      "Epoch 417/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.0726 - accuracy: 0.9736 - val_loss: 0.1218 - val_accuracy: 0.9549\n",
      "Epoch 418/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.0724 - accuracy: 0.9739 - val_loss: 0.1231 - val_accuracy: 0.9548\n",
      "Epoch 419/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.0723 - accuracy: 0.9736 - val_loss: 0.1439 - val_accuracy: 0.9461\n",
      "Epoch 420/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.0734 - accuracy: 0.9734 - val_loss: 0.1221 - val_accuracy: 0.9551\n",
      "Epoch 421/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0721 - accuracy: 0.9739 - val_loss: 0.1243 - val_accuracy: 0.9539\n",
      "Epoch 422/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.0722 - accuracy: 0.9737 - val_loss: 0.1209 - val_accuracy: 0.9550\n",
      "Epoch 423/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.0719 - accuracy: 0.9741 - val_loss: 0.1216 - val_accuracy: 0.9546\n",
      "Epoch 424/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.0719 - accuracy: 0.9740 - val_loss: 0.1204 - val_accuracy: 0.9550\n",
      "Epoch 425/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0716 - accuracy: 0.9739 - val_loss: 0.1222 - val_accuracy: 0.9555\n",
      "Epoch 426/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0717 - accuracy: 0.9741 - val_loss: 0.1213 - val_accuracy: 0.9553\n",
      "Epoch 427/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.0714 - accuracy: 0.9741 - val_loss: 0.1240 - val_accuracy: 0.9544\n",
      "Epoch 428/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0716 - accuracy: 0.9741 - val_loss: 0.1230 - val_accuracy: 0.9551\n",
      "Epoch 429/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0713 - accuracy: 0.9739 - val_loss: 0.1211 - val_accuracy: 0.9557\n",
      "Epoch 430/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0713 - accuracy: 0.9740 - val_loss: 0.1225 - val_accuracy: 0.9545\n",
      "Epoch 431/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.0713 - accuracy: 0.9737 - val_loss: 0.1218 - val_accuracy: 0.9556\n",
      "Epoch 432/800\n",
      "1665/1665 [==============================] - 0s 139us/step - loss: 0.0711 - accuracy: 0.9739 - val_loss: 0.1217 - val_accuracy: 0.9546\n",
      "Epoch 433/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.0711 - accuracy: 0.9742 - val_loss: 0.1201 - val_accuracy: 0.9545\n",
      "Epoch 434/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.0708 - accuracy: 0.9740 - val_loss: 0.1221 - val_accuracy: 0.9548\n",
      "Epoch 435/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0709 - accuracy: 0.9741 - val_loss: 0.1203 - val_accuracy: 0.9556\n",
      "Epoch 436/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0707 - accuracy: 0.9743 - val_loss: 0.1213 - val_accuracy: 0.9559\n",
      "Epoch 437/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.0705 - accuracy: 0.9743 - val_loss: 0.1202 - val_accuracy: 0.9554\n",
      "Epoch 438/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0706 - accuracy: 0.9740 - val_loss: 0.1212 - val_accuracy: 0.9557\n",
      "Epoch 439/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.0704 - accuracy: 0.9746 - val_loss: 0.1192 - val_accuracy: 0.9556\n",
      "Epoch 440/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1665/1665 [==============================] - 0s 162us/step - loss: 0.0704 - accuracy: 0.9743 - val_loss: 0.1207 - val_accuracy: 0.9560\n",
      "Epoch 441/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.0702 - accuracy: 0.9747 - val_loss: 0.1224 - val_accuracy: 0.9558\n",
      "Epoch 442/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0702 - accuracy: 0.9746 - val_loss: 0.1205 - val_accuracy: 0.9558\n",
      "Epoch 443/800\n",
      "1665/1665 [==============================] - 0s 146us/step - loss: 0.0699 - accuracy: 0.9743 - val_loss: 0.1198 - val_accuracy: 0.9555\n",
      "Epoch 444/800\n",
      "1665/1665 [==============================] - 0s 154us/step - loss: 0.0700 - accuracy: 0.9746 - val_loss: 0.1219 - val_accuracy: 0.9561\n",
      "Epoch 445/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0697 - accuracy: 0.9747 - val_loss: 0.1214 - val_accuracy: 0.9553\n",
      "Epoch 446/800\n",
      "1665/1665 [==============================] - 0s 129us/step - loss: 0.0699 - accuracy: 0.9745 - val_loss: 0.1288 - val_accuracy: 0.9521\n",
      "Epoch 447/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0701 - accuracy: 0.9743 - val_loss: 0.1205 - val_accuracy: 0.9556\n",
      "Epoch 448/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.0696 - accuracy: 0.9744 - val_loss: 0.1205 - val_accuracy: 0.9555\n",
      "Epoch 449/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0694 - accuracy: 0.9744 - val_loss: 0.1209 - val_accuracy: 0.9558\n",
      "Epoch 450/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0694 - accuracy: 0.9746 - val_loss: 0.1248 - val_accuracy: 0.9548\n",
      "Epoch 451/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0698 - accuracy: 0.9745 - val_loss: 0.1363 - val_accuracy: 0.9487\n",
      "Epoch 452/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0702 - accuracy: 0.9742 - val_loss: 0.1236 - val_accuracy: 0.9542\n",
      "Epoch 453/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0693 - accuracy: 0.9746 - val_loss: 0.1200 - val_accuracy: 0.9551\n",
      "Epoch 454/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.0692 - accuracy: 0.9746 - val_loss: 0.1209 - val_accuracy: 0.9559\n",
      "Epoch 455/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0690 - accuracy: 0.9746 - val_loss: 0.1206 - val_accuracy: 0.9550\n",
      "Epoch 456/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0692 - accuracy: 0.9748 - val_loss: 0.1210 - val_accuracy: 0.9556\n",
      "Epoch 457/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0690 - accuracy: 0.9746 - val_loss: 0.1183 - val_accuracy: 0.9563\n",
      "Epoch 458/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.0687 - accuracy: 0.9746 - val_loss: 0.1235 - val_accuracy: 0.9545\n",
      "Epoch 459/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0689 - accuracy: 0.9746 - val_loss: 0.1182 - val_accuracy: 0.9559\n",
      "Epoch 460/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0686 - accuracy: 0.9745 - val_loss: 0.1195 - val_accuracy: 0.9560\n",
      "Epoch 461/800\n",
      "1665/1665 [==============================] - 0s 109us/step - loss: 0.0685 - accuracy: 0.9748 - val_loss: 0.1187 - val_accuracy: 0.9562\n",
      "Epoch 462/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.0683 - accuracy: 0.9750 - val_loss: 0.1181 - val_accuracy: 0.9556\n",
      "Epoch 463/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.0684 - accuracy: 0.9750 - val_loss: 0.1196 - val_accuracy: 0.9561\n",
      "Epoch 464/800\n",
      "1665/1665 [==============================] - 0s 112us/step - loss: 0.0685 - accuracy: 0.9745 - val_loss: 0.1178 - val_accuracy: 0.9561\n",
      "Epoch 465/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0683 - accuracy: 0.9750 - val_loss: 0.1326 - val_accuracy: 0.9497\n",
      "Epoch 466/800\n",
      "1665/1665 [==============================] - 0s 107us/step - loss: 0.0688 - accuracy: 0.9746 - val_loss: 0.1213 - val_accuracy: 0.9561\n",
      "Epoch 467/800\n",
      "1665/1665 [==============================] - 0s 101us/step - loss: 0.0681 - accuracy: 0.9749 - val_loss: 0.1213 - val_accuracy: 0.9566\n",
      "Epoch 468/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.0681 - accuracy: 0.9749 - val_loss: 0.1176 - val_accuracy: 0.9566\n",
      "Epoch 469/800\n",
      "1665/1665 [==============================] - 0s 106us/step - loss: 0.0678 - accuracy: 0.9751 - val_loss: 0.1268 - val_accuracy: 0.9539\n",
      "Epoch 470/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0682 - accuracy: 0.9748 - val_loss: 0.1392 - val_accuracy: 0.9489\n",
      "Epoch 471/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0688 - accuracy: 0.9748 - val_loss: 0.1206 - val_accuracy: 0.9560\n",
      "Epoch 472/800\n",
      "1665/1665 [==============================] - 0s 109us/step - loss: 0.0678 - accuracy: 0.9749 - val_loss: 0.1176 - val_accuracy: 0.9559\n",
      "Epoch 473/800\n",
      "1665/1665 [==============================] - 0s 105us/step - loss: 0.0676 - accuracy: 0.9751 - val_loss: 0.1171 - val_accuracy: 0.9565\n",
      "Epoch 474/800\n",
      "1665/1665 [==============================] - 0s 105us/step - loss: 0.0675 - accuracy: 0.9750 - val_loss: 0.1193 - val_accuracy: 0.9570\n",
      "Epoch 475/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.0674 - accuracy: 0.9754 - val_loss: 0.1188 - val_accuracy: 0.9563\n",
      "Epoch 476/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0673 - accuracy: 0.9753 - val_loss: 0.1178 - val_accuracy: 0.9568\n",
      "Epoch 477/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.0673 - accuracy: 0.9753 - val_loss: 0.1193 - val_accuracy: 0.9550\n",
      "Epoch 478/800\n",
      "1665/1665 [==============================] - 0s 107us/step - loss: 0.0675 - accuracy: 0.9750 - val_loss: 0.1181 - val_accuracy: 0.9562\n",
      "Epoch 479/800\n",
      "1665/1665 [==============================] - 0s 107us/step - loss: 0.0672 - accuracy: 0.9752 - val_loss: 0.1197 - val_accuracy: 0.9561\n",
      "Epoch 480/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0671 - accuracy: 0.9752 - val_loss: 0.1353 - val_accuracy: 0.9506\n",
      "Epoch 481/800\n",
      "1665/1665 [==============================] - 0s 140us/step - loss: 0.0675 - accuracy: 0.9747 - val_loss: 0.1169 - val_accuracy: 0.9558\n",
      "Epoch 482/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0670 - accuracy: 0.9752 - val_loss: 0.1199 - val_accuracy: 0.9562\n",
      "Epoch 483/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0668 - accuracy: 0.9752 - val_loss: 0.1279 - val_accuracy: 0.9521\n",
      "Epoch 484/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0673 - accuracy: 0.9751 - val_loss: 0.1182 - val_accuracy: 0.9570\n",
      "Epoch 485/800\n",
      "1665/1665 [==============================] - 0s 129us/step - loss: 0.0667 - accuracy: 0.9754 - val_loss: 0.1209 - val_accuracy: 0.9562\n",
      "Epoch 486/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.0668 - accuracy: 0.9755 - val_loss: 0.1168 - val_accuracy: 0.9566\n",
      "Epoch 487/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0665 - accuracy: 0.9752 - val_loss: 0.1180 - val_accuracy: 0.9574\n",
      "Epoch 488/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0663 - accuracy: 0.9755 - val_loss: 0.1168 - val_accuracy: 0.9569\n",
      "Epoch 489/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0664 - accuracy: 0.9756 - val_loss: 0.1185 - val_accuracy: 0.9563\n",
      "Epoch 490/800\n",
      "1665/1665 [==============================] - 0s 141us/step - loss: 0.0666 - accuracy: 0.9754 - val_loss: 0.1215 - val_accuracy: 0.9557\n",
      "Epoch 491/800\n",
      "1665/1665 [==============================] - 0s 129us/step - loss: 0.0664 - accuracy: 0.9755 - val_loss: 0.1199 - val_accuracy: 0.9562\n",
      "Epoch 492/800\n",
      "1665/1665 [==============================] - 0s 142us/step - loss: 0.0664 - accuracy: 0.9753 - val_loss: 0.1171 - val_accuracy: 0.9570\n",
      "Epoch 493/800\n",
      "1665/1665 [==============================] - 0s 158us/step - loss: 0.0661 - accuracy: 0.9755 - val_loss: 0.1164 - val_accuracy: 0.9573\n",
      "Epoch 494/800\n",
      "1665/1665 [==============================] - 0s 150us/step - loss: 0.0660 - accuracy: 0.9757 - val_loss: 0.1166 - val_accuracy: 0.9574\n",
      "Epoch 495/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1665/1665 [==============================] - 0s 139us/step - loss: 0.0661 - accuracy: 0.9756 - val_loss: 0.1156 - val_accuracy: 0.9571\n",
      "Epoch 496/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.0658 - accuracy: 0.9758 - val_loss: 0.1180 - val_accuracy: 0.9566\n",
      "Epoch 497/800\n",
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.0660 - accuracy: 0.9756 - val_loss: 0.1250 - val_accuracy: 0.9541\n",
      "Epoch 498/800\n",
      "1665/1665 [==============================] - 0s 151us/step - loss: 0.0662 - accuracy: 0.9755 - val_loss: 0.1180 - val_accuracy: 0.9569\n",
      "Epoch 499/800\n",
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.0658 - accuracy: 0.9758 - val_loss: 0.1170 - val_accuracy: 0.9576\n",
      "Epoch 500/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0657 - accuracy: 0.9757 - val_loss: 0.1162 - val_accuracy: 0.9574\n",
      "Epoch 501/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.0656 - accuracy: 0.9758 - val_loss: 0.1161 - val_accuracy: 0.9574\n",
      "Epoch 502/800\n",
      "1665/1665 [==============================] - 0s 143us/step - loss: 0.0653 - accuracy: 0.9759 - val_loss: 0.1177 - val_accuracy: 0.9581\n",
      "Epoch 503/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0653 - accuracy: 0.9759 - val_loss: 0.1157 - val_accuracy: 0.9574\n",
      "Epoch 504/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.0653 - accuracy: 0.9757 - val_loss: 0.1161 - val_accuracy: 0.9571\n",
      "Epoch 505/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.0653 - accuracy: 0.9759 - val_loss: 0.1156 - val_accuracy: 0.9573\n",
      "Epoch 506/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.0652 - accuracy: 0.9757 - val_loss: 0.1162 - val_accuracy: 0.9574\n",
      "Epoch 507/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0650 - accuracy: 0.9760 - val_loss: 0.1170 - val_accuracy: 0.9573\n",
      "Epoch 508/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.0650 - accuracy: 0.9759 - val_loss: 0.1163 - val_accuracy: 0.9569\n",
      "Epoch 509/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.0650 - accuracy: 0.9758 - val_loss: 0.1181 - val_accuracy: 0.9569\n",
      "Epoch 510/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0650 - accuracy: 0.9758 - val_loss: 0.1165 - val_accuracy: 0.9573\n",
      "Epoch 511/800\n",
      "1665/1665 [==============================] - 0s 105us/step - loss: 0.0648 - accuracy: 0.9759 - val_loss: 0.1542 - val_accuracy: 0.9454\n",
      "Epoch 512/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0662 - accuracy: 0.9756 - val_loss: 0.1156 - val_accuracy: 0.9575\n",
      "Epoch 513/800\n",
      "1665/1665 [==============================] - 0s 107us/step - loss: 0.0647 - accuracy: 0.9759 - val_loss: 0.1158 - val_accuracy: 0.9573\n",
      "Epoch 514/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0647 - accuracy: 0.9763 - val_loss: 0.1158 - val_accuracy: 0.9571\n",
      "Epoch 515/800\n",
      "1665/1665 [==============================] - 0s 129us/step - loss: 0.0646 - accuracy: 0.9763 - val_loss: 0.1171 - val_accuracy: 0.9569\n",
      "Epoch 516/800\n",
      "1665/1665 [==============================] - 0s 141us/step - loss: 0.0646 - accuracy: 0.9760 - val_loss: 0.1161 - val_accuracy: 0.9579\n",
      "Epoch 517/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.0644 - accuracy: 0.9759 - val_loss: 0.1330 - val_accuracy: 0.9503\n",
      "Epoch 518/800\n",
      "1665/1665 [==============================] - 0s 142us/step - loss: 0.0649 - accuracy: 0.9759 - val_loss: 0.1158 - val_accuracy: 0.9579\n",
      "Epoch 519/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.0642 - accuracy: 0.9761 - val_loss: 0.1179 - val_accuracy: 0.9560\n",
      "Epoch 520/800\n",
      "1665/1665 [==============================] - 0s 154us/step - loss: 0.0646 - accuracy: 0.9759 - val_loss: 0.1165 - val_accuracy: 0.9570\n",
      "Epoch 521/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.0644 - accuracy: 0.9761 - val_loss: 0.1172 - val_accuracy: 0.9575\n",
      "Epoch 522/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.0642 - accuracy: 0.9760 - val_loss: 0.1150 - val_accuracy: 0.9582\n",
      "Epoch 523/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.0640 - accuracy: 0.9762 - val_loss: 0.1341 - val_accuracy: 0.9500\n",
      "Epoch 524/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.0652 - accuracy: 0.9758 - val_loss: 0.1156 - val_accuracy: 0.9578\n",
      "Epoch 525/800\n",
      "1665/1665 [==============================] - 0s 136us/step - loss: 0.0639 - accuracy: 0.9763 - val_loss: 0.1151 - val_accuracy: 0.9584\n",
      "Epoch 526/800\n",
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.0638 - accuracy: 0.9767 - val_loss: 0.1143 - val_accuracy: 0.9582\n",
      "Epoch 527/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.0638 - accuracy: 0.9765 - val_loss: 0.1165 - val_accuracy: 0.9581\n",
      "Epoch 528/800\n",
      "1665/1665 [==============================] - 0s 145us/step - loss: 0.0638 - accuracy: 0.9764 - val_loss: 0.1173 - val_accuracy: 0.9577\n",
      "Epoch 529/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0636 - accuracy: 0.9764 - val_loss: 0.1146 - val_accuracy: 0.9587\n",
      "Epoch 530/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0635 - accuracy: 0.9764 - val_loss: 0.1165 - val_accuracy: 0.9578\n",
      "Epoch 531/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0637 - accuracy: 0.9764 - val_loss: 0.1144 - val_accuracy: 0.9586\n",
      "Epoch 532/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.0634 - accuracy: 0.9765 - val_loss: 0.1149 - val_accuracy: 0.9584\n",
      "Epoch 533/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0633 - accuracy: 0.9765 - val_loss: 0.1130 - val_accuracy: 0.9585\n",
      "Epoch 534/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.0633 - accuracy: 0.9764 - val_loss: 0.1166 - val_accuracy: 0.9579\n",
      "Epoch 535/800\n",
      "1665/1665 [==============================] - 0s 139us/step - loss: 0.0634 - accuracy: 0.9765 - val_loss: 0.1157 - val_accuracy: 0.9582\n",
      "Epoch 536/800\n",
      "1665/1665 [==============================] - 0s 140us/step - loss: 0.0633 - accuracy: 0.9769 - val_loss: 0.1141 - val_accuracy: 0.9583\n",
      "Epoch 537/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0633 - accuracy: 0.9767 - val_loss: 0.1178 - val_accuracy: 0.9581\n",
      "Epoch 538/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0633 - accuracy: 0.9765 - val_loss: 0.1146 - val_accuracy: 0.9581\n",
      "Epoch 539/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0631 - accuracy: 0.9765 - val_loss: 0.1158 - val_accuracy: 0.9587\n",
      "Epoch 540/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.0630 - accuracy: 0.9764 - val_loss: 0.1157 - val_accuracy: 0.9581\n",
      "Epoch 541/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.0630 - accuracy: 0.9765 - val_loss: 0.1137 - val_accuracy: 0.9586\n",
      "Epoch 542/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.0631 - accuracy: 0.9766 - val_loss: 0.1203 - val_accuracy: 0.9567\n",
      "Epoch 543/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.0632 - accuracy: 0.9766 - val_loss: 0.1283 - val_accuracy: 0.9546\n",
      "Epoch 544/800\n",
      "1665/1665 [==============================] - 0s 129us/step - loss: 0.0636 - accuracy: 0.9762 - val_loss: 0.1145 - val_accuracy: 0.9578\n",
      "Epoch 545/800\n",
      "1665/1665 [==============================] - 0s 141us/step - loss: 0.0629 - accuracy: 0.9766 - val_loss: 0.1141 - val_accuracy: 0.9582\n",
      "Epoch 546/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.0627 - accuracy: 0.9765 - val_loss: 0.1181 - val_accuracy: 0.9593\n",
      "Epoch 547/800\n",
      "1665/1665 [==============================] - 0s 126us/step - loss: 0.0630 - accuracy: 0.9766 - val_loss: 0.1154 - val_accuracy: 0.9576\n",
      "Epoch 548/800\n",
      "1665/1665 [==============================] - 0s 133us/step - loss: 0.0628 - accuracy: 0.9767 - val_loss: 0.1220 - val_accuracy: 0.9549\n",
      "Epoch 549/800\n",
      "1665/1665 [==============================] - 0s 136us/step - loss: 0.0631 - accuracy: 0.9762 - val_loss: 0.1165 - val_accuracy: 0.9582\n",
      "Epoch 550/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0626 - accuracy: 0.9767 - val_loss: 0.1351 - val_accuracy: 0.9529\n",
      "Epoch 551/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.0634 - accuracy: 0.9763 - val_loss: 0.1148 - val_accuracy: 0.9582\n",
      "Epoch 552/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.0623 - accuracy: 0.9769 - val_loss: 0.1323 - val_accuracy: 0.9520\n",
      "Epoch 553/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0631 - accuracy: 0.9763 - val_loss: 0.1140 - val_accuracy: 0.9588\n",
      "Epoch 554/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0623 - accuracy: 0.9765 - val_loss: 0.1124 - val_accuracy: 0.9592\n",
      "Epoch 555/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0621 - accuracy: 0.9768 - val_loss: 0.1138 - val_accuracy: 0.9594\n",
      "Epoch 556/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.0620 - accuracy: 0.9768 - val_loss: 0.1164 - val_accuracy: 0.9570\n",
      "Epoch 557/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0624 - accuracy: 0.9769 - val_loss: 0.1160 - val_accuracy: 0.9589\n",
      "Epoch 558/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0622 - accuracy: 0.9766 - val_loss: 0.1154 - val_accuracy: 0.9580\n",
      "Epoch 559/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0622 - accuracy: 0.9767 - val_loss: 0.1204 - val_accuracy: 0.9547\n",
      "Epoch 560/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0626 - accuracy: 0.9766 - val_loss: 0.1160 - val_accuracy: 0.9578\n",
      "Epoch 561/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.0621 - accuracy: 0.9769 - val_loss: 0.1127 - val_accuracy: 0.9590\n",
      "Epoch 562/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.0619 - accuracy: 0.9770 - val_loss: 0.1244 - val_accuracy: 0.9549\n",
      "Epoch 563/800\n",
      "1665/1665 [==============================] - 0s 133us/step - loss: 0.0624 - accuracy: 0.9767 - val_loss: 0.1147 - val_accuracy: 0.9587\n",
      "Epoch 564/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.0618 - accuracy: 0.9767 - val_loss: 0.1132 - val_accuracy: 0.9592\n",
      "Epoch 565/800\n",
      "1665/1665 [==============================] - 0s 129us/step - loss: 0.0618 - accuracy: 0.9768 - val_loss: 0.1133 - val_accuracy: 0.9597\n",
      "Epoch 566/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0617 - accuracy: 0.9768 - val_loss: 0.1145 - val_accuracy: 0.9596\n",
      "Epoch 567/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0615 - accuracy: 0.9772 - val_loss: 0.1191 - val_accuracy: 0.9590\n",
      "Epoch 568/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0618 - accuracy: 0.9768 - val_loss: 0.1141 - val_accuracy: 0.9587\n",
      "Epoch 569/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.0615 - accuracy: 0.9770 - val_loss: 0.1163 - val_accuracy: 0.9592\n",
      "Epoch 570/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0615 - accuracy: 0.9768 - val_loss: 0.1131 - val_accuracy: 0.9596\n",
      "Epoch 571/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0614 - accuracy: 0.9770 - val_loss: 0.1178 - val_accuracy: 0.9584\n",
      "Epoch 572/800\n",
      "1665/1665 [==============================] - 0s 105us/step - loss: 0.0616 - accuracy: 0.9768 - val_loss: 0.1112 - val_accuracy: 0.9597\n",
      "Epoch 573/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0614 - accuracy: 0.9771 - val_loss: 0.1137 - val_accuracy: 0.9593\n",
      "Epoch 574/800\n",
      "1665/1665 [==============================] - 0s 109us/step - loss: 0.0613 - accuracy: 0.9772 - val_loss: 0.1148 - val_accuracy: 0.9584\n",
      "Epoch 575/800\n",
      "1665/1665 [==============================] - 0s 104us/step - loss: 0.0612 - accuracy: 0.9769 - val_loss: 0.1134 - val_accuracy: 0.9600\n",
      "Epoch 576/800\n",
      "1665/1665 [==============================] - 0s 105us/step - loss: 0.0610 - accuracy: 0.9772 - val_loss: 0.1166 - val_accuracy: 0.9583\n",
      "Epoch 577/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.0614 - accuracy: 0.9771 - val_loss: 0.1143 - val_accuracy: 0.9594\n",
      "Epoch 578/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.0611 - accuracy: 0.9770 - val_loss: 0.1139 - val_accuracy: 0.9589\n",
      "Epoch 579/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0613 - accuracy: 0.9771 - val_loss: 0.1138 - val_accuracy: 0.9591\n",
      "Epoch 580/800\n",
      "1665/1665 [==============================] - 0s 103us/step - loss: 0.0610 - accuracy: 0.9774 - val_loss: 0.1125 - val_accuracy: 0.9599\n",
      "Epoch 581/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0607 - accuracy: 0.9773 - val_loss: 0.1274 - val_accuracy: 0.9538\n",
      "Epoch 582/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0614 - accuracy: 0.9768 - val_loss: 0.1121 - val_accuracy: 0.9593\n",
      "Epoch 583/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.0608 - accuracy: 0.9772 - val_loss: 0.1140 - val_accuracy: 0.9587\n",
      "Epoch 584/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0608 - accuracy: 0.9772 - val_loss: 0.1123 - val_accuracy: 0.9596\n",
      "Epoch 585/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0607 - accuracy: 0.9771 - val_loss: 0.1136 - val_accuracy: 0.9597\n",
      "Epoch 586/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0607 - accuracy: 0.9775 - val_loss: 0.1130 - val_accuracy: 0.9598\n",
      "Epoch 587/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0606 - accuracy: 0.9772 - val_loss: 0.1147 - val_accuracy: 0.9582\n",
      "Epoch 588/800\n",
      "1665/1665 [==============================] - 0s 136us/step - loss: 0.0606 - accuracy: 0.9773 - val_loss: 0.1128 - val_accuracy: 0.9600\n",
      "Epoch 589/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0604 - accuracy: 0.9772 - val_loss: 0.1117 - val_accuracy: 0.9604\n",
      "Epoch 590/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.0602 - accuracy: 0.9772 - val_loss: 0.1188 - val_accuracy: 0.9583\n",
      "Epoch 591/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.0606 - accuracy: 0.9775 - val_loss: 0.1130 - val_accuracy: 0.9596\n",
      "Epoch 592/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.0603 - accuracy: 0.9775 - val_loss: 0.1153 - val_accuracy: 0.9590\n",
      "Epoch 593/800\n",
      "1665/1665 [==============================] - 0s 152us/step - loss: 0.0605 - accuracy: 0.9768 - val_loss: 0.1127 - val_accuracy: 0.9604\n",
      "Epoch 594/800\n",
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.0602 - accuracy: 0.9775 - val_loss: 0.1129 - val_accuracy: 0.9602\n",
      "Epoch 595/800\n",
      "1665/1665 [==============================] - 0s 148us/step - loss: 0.0600 - accuracy: 0.9775 - val_loss: 0.1164 - val_accuracy: 0.9592\n",
      "Epoch 596/800\n",
      "1665/1665 [==============================] - 0s 136us/step - loss: 0.0603 - accuracy: 0.9771 - val_loss: 0.1130 - val_accuracy: 0.9600\n",
      "Epoch 597/800\n",
      "1665/1665 [==============================] - 0s 151us/step - loss: 0.0600 - accuracy: 0.9773 - val_loss: 0.1137 - val_accuracy: 0.9598\n",
      "Epoch 598/800\n",
      "1665/1665 [==============================] - 0s 153us/step - loss: 0.0601 - accuracy: 0.9772 - val_loss: 0.1136 - val_accuracy: 0.9591\n",
      "Epoch 599/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0602 - accuracy: 0.9775 - val_loss: 0.1119 - val_accuracy: 0.9602\n",
      "Epoch 600/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0598 - accuracy: 0.9774 - val_loss: 0.1135 - val_accuracy: 0.9599\n",
      "Epoch 601/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0599 - accuracy: 0.9774 - val_loss: 0.1113 - val_accuracy: 0.9601\n",
      "Epoch 602/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0599 - accuracy: 0.9774 - val_loss: 0.1126 - val_accuracy: 0.9604\n",
      "Epoch 603/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0596 - accuracy: 0.9777 - val_loss: 0.1397 - val_accuracy: 0.9495\n",
      "Epoch 604/800\n",
      "1665/1665 [==============================] - 0s 133us/step - loss: 0.0614 - accuracy: 0.9769 - val_loss: 0.1133 - val_accuracy: 0.9596\n",
      "Epoch 605/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0599 - accuracy: 0.9774 - val_loss: 0.1112 - val_accuracy: 0.9607\n",
      "Epoch 606/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.0597 - accuracy: 0.9777 - val_loss: 0.1124 - val_accuracy: 0.9594\n",
      "Epoch 607/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0597 - accuracy: 0.9775 - val_loss: 0.1109 - val_accuracy: 0.9607\n",
      "Epoch 608/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.0597 - accuracy: 0.9775 - val_loss: 0.1113 - val_accuracy: 0.9608\n",
      "Epoch 609/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0592 - accuracy: 0.9776 - val_loss: 0.1120 - val_accuracy: 0.9602\n",
      "Epoch 610/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0595 - accuracy: 0.9774 - val_loss: 0.1138 - val_accuracy: 0.9597\n",
      "Epoch 611/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0594 - accuracy: 0.9776 - val_loss: 0.1127 - val_accuracy: 0.9610\n",
      "Epoch 612/800\n",
      "1665/1665 [==============================] - 0s 157us/step - loss: 0.0593 - accuracy: 0.9776 - val_loss: 0.1262 - val_accuracy: 0.9549\n",
      "Epoch 613/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0602 - accuracy: 0.9773 - val_loss: 0.1144 - val_accuracy: 0.9593\n",
      "Epoch 614/800\n",
      "1665/1665 [==============================] - 0s 126us/step - loss: 0.0592 - accuracy: 0.9777 - val_loss: 0.1139 - val_accuracy: 0.9603\n",
      "Epoch 615/800\n",
      "1665/1665 [==============================] - 0s 112us/step - loss: 0.0592 - accuracy: 0.9777 - val_loss: 0.1139 - val_accuracy: 0.9605\n",
      "Epoch 616/800\n",
      "1665/1665 [==============================] - 0s 109us/step - loss: 0.0596 - accuracy: 0.9775 - val_loss: 0.1123 - val_accuracy: 0.9605\n",
      "Epoch 617/800\n",
      "1665/1665 [==============================] - 0s 109us/step - loss: 0.0592 - accuracy: 0.9778 - val_loss: 0.1122 - val_accuracy: 0.9597\n",
      "Epoch 618/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0591 - accuracy: 0.9777 - val_loss: 0.1123 - val_accuracy: 0.9616\n",
      "Epoch 619/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0589 - accuracy: 0.9779 - val_loss: 0.1112 - val_accuracy: 0.9611\n",
      "Epoch 620/800\n",
      "1665/1665 [==============================] - 0s 105us/step - loss: 0.0590 - accuracy: 0.9776 - val_loss: 0.1276 - val_accuracy: 0.9554\n",
      "Epoch 621/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.0596 - accuracy: 0.9775 - val_loss: 0.1358 - val_accuracy: 0.9515\n",
      "Epoch 622/800\n",
      "1665/1665 [==============================] - 0s 109us/step - loss: 0.0602 - accuracy: 0.9776 - val_loss: 0.1122 - val_accuracy: 0.9607\n",
      "Epoch 623/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0589 - accuracy: 0.9777 - val_loss: 0.1127 - val_accuracy: 0.9608\n",
      "Epoch 624/800\n",
      "1665/1665 [==============================] - 0s 106us/step - loss: 0.0590 - accuracy: 0.9777 - val_loss: 0.1105 - val_accuracy: 0.9615\n",
      "Epoch 625/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.0587 - accuracy: 0.9779 - val_loss: 0.1465 - val_accuracy: 0.9484\n",
      "Epoch 626/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.0606 - accuracy: 0.9771 - val_loss: 0.1113 - val_accuracy: 0.9607\n",
      "Epoch 627/800\n",
      "1665/1665 [==============================] - 0s 142us/step - loss: 0.0587 - accuracy: 0.9780 - val_loss: 0.1101 - val_accuracy: 0.9606\n",
      "Epoch 628/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.0586 - accuracy: 0.9779 - val_loss: 0.1132 - val_accuracy: 0.9584\n",
      "Epoch 629/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0588 - accuracy: 0.9778 - val_loss: 0.1117 - val_accuracy: 0.9598\n",
      "Epoch 630/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.0585 - accuracy: 0.9777 - val_loss: 0.1120 - val_accuracy: 0.9601\n",
      "Epoch 631/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.0586 - accuracy: 0.9780 - val_loss: 0.1135 - val_accuracy: 0.9596\n",
      "Epoch 632/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.0588 - accuracy: 0.9780 - val_loss: 0.1122 - val_accuracy: 0.9603\n",
      "Epoch 633/800\n",
      "1665/1665 [==============================] - 0s 129us/step - loss: 0.0582 - accuracy: 0.9777 - val_loss: 0.1122 - val_accuracy: 0.9606\n",
      "Epoch 634/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.0583 - accuracy: 0.9780 - val_loss: 0.1114 - val_accuracy: 0.9607\n",
      "Epoch 635/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.0582 - accuracy: 0.9780 - val_loss: 0.1121 - val_accuracy: 0.9607\n",
      "Epoch 636/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.0583 - accuracy: 0.9780 - val_loss: 0.1116 - val_accuracy: 0.9611\n",
      "Epoch 637/800\n",
      "1665/1665 [==============================] - 0s 141us/step - loss: 0.0580 - accuracy: 0.9782 - val_loss: 0.1113 - val_accuracy: 0.9612\n",
      "Epoch 638/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.0580 - accuracy: 0.9780 - val_loss: 0.1124 - val_accuracy: 0.9595\n",
      "Epoch 639/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0582 - accuracy: 0.9782 - val_loss: 0.1107 - val_accuracy: 0.9617\n",
      "Epoch 640/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0582 - accuracy: 0.9780 - val_loss: 0.1099 - val_accuracy: 0.9609\n",
      "Epoch 641/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0578 - accuracy: 0.9781 - val_loss: 0.1104 - val_accuracy: 0.9615\n",
      "Epoch 642/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.0578 - accuracy: 0.9782 - val_loss: 0.1116 - val_accuracy: 0.9604\n",
      "Epoch 643/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0581 - accuracy: 0.9780 - val_loss: 0.1119 - val_accuracy: 0.9611\n",
      "Epoch 644/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0579 - accuracy: 0.9782 - val_loss: 0.1097 - val_accuracy: 0.9614\n",
      "Epoch 645/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0577 - accuracy: 0.9782 - val_loss: 0.1122 - val_accuracy: 0.9612\n",
      "Epoch 646/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0577 - accuracy: 0.9782 - val_loss: 0.1112 - val_accuracy: 0.9604\n",
      "Epoch 647/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0578 - accuracy: 0.9783 - val_loss: 0.1119 - val_accuracy: 0.9601\n",
      "Epoch 648/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.0579 - accuracy: 0.9783 - val_loss: 0.1113 - val_accuracy: 0.9613\n",
      "Epoch 649/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0576 - accuracy: 0.9784 - val_loss: 0.1116 - val_accuracy: 0.9609\n",
      "Epoch 650/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0576 - accuracy: 0.9783 - val_loss: 0.1091 - val_accuracy: 0.9618\n",
      "Epoch 651/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.0574 - accuracy: 0.9782 - val_loss: 0.1116 - val_accuracy: 0.9603\n",
      "Epoch 652/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0575 - accuracy: 0.9780 - val_loss: 0.1080 - val_accuracy: 0.9618\n",
      "Epoch 653/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.0573 - accuracy: 0.9784 - val_loss: 0.1108 - val_accuracy: 0.9607\n",
      "Epoch 654/800\n",
      "1665/1665 [==============================] - 0s 112us/step - loss: 0.0575 - accuracy: 0.9784 - val_loss: 0.1129 - val_accuracy: 0.9606\n",
      "Epoch 655/800\n",
      "1665/1665 [==============================] - 0s 129us/step - loss: 0.0575 - accuracy: 0.9782 - val_loss: 0.1127 - val_accuracy: 0.9605\n",
      "Epoch 656/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.0574 - accuracy: 0.9784 - val_loss: 0.1098 - val_accuracy: 0.9607\n",
      "Epoch 657/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0574 - accuracy: 0.9779 - val_loss: 0.1100 - val_accuracy: 0.9613\n",
      "Epoch 658/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0572 - accuracy: 0.9784 - val_loss: 0.1182 - val_accuracy: 0.9574\n",
      "Epoch 659/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0578 - accuracy: 0.9780 - val_loss: 0.1101 - val_accuracy: 0.9616\n",
      "Epoch 660/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0571 - accuracy: 0.9784 - val_loss: 0.1126 - val_accuracy: 0.9609\n",
      "Epoch 661/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.0572 - accuracy: 0.9782 - val_loss: 0.1096 - val_accuracy: 0.9613\n",
      "Epoch 662/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.0569 - accuracy: 0.9783 - val_loss: 0.1293 - val_accuracy: 0.9552\n",
      "Epoch 663/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0581 - accuracy: 0.9777 - val_loss: 0.1118 - val_accuracy: 0.9612\n",
      "Epoch 664/800\n",
      "1665/1665 [==============================] - 0s 112us/step - loss: 0.0572 - accuracy: 0.9783 - val_loss: 0.1461 - val_accuracy: 0.9493\n",
      "Epoch 665/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.0591 - accuracy: 0.9777 - val_loss: 0.1112 - val_accuracy: 0.9610\n",
      "Epoch 666/800\n",
      "1665/1665 [==============================] - 0s 102us/step - loss: 0.0571 - accuracy: 0.9784 - val_loss: 0.1105 - val_accuracy: 0.9614\n",
      "Epoch 667/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.0569 - accuracy: 0.9782 - val_loss: 0.1126 - val_accuracy: 0.9609\n",
      "Epoch 668/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.0570 - accuracy: 0.9782 - val_loss: 0.1115 - val_accuracy: 0.9616\n",
      "Epoch 669/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0570 - accuracy: 0.9784 - val_loss: 0.1102 - val_accuracy: 0.9614\n",
      "Epoch 670/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.0566 - accuracy: 0.9785 - val_loss: 0.1209 - val_accuracy: 0.9572\n",
      "Epoch 671/800\n",
      "1665/1665 [==============================] - 0s 143us/step - loss: 0.0574 - accuracy: 0.9783 - val_loss: 0.1120 - val_accuracy: 0.9600\n",
      "Epoch 672/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0568 - accuracy: 0.9786 - val_loss: 0.1134 - val_accuracy: 0.9604\n",
      "Epoch 673/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0567 - accuracy: 0.9786 - val_loss: 0.1081 - val_accuracy: 0.9627\n",
      "Epoch 674/800\n",
      "1665/1665 [==============================] - 0s 136us/step - loss: 0.0564 - accuracy: 0.9785 - val_loss: 0.1123 - val_accuracy: 0.9600\n",
      "Epoch 675/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0568 - accuracy: 0.9786 - val_loss: 0.1098 - val_accuracy: 0.9622\n",
      "Epoch 676/800\n",
      "1665/1665 [==============================] - 0s 145us/step - loss: 0.0565 - accuracy: 0.9785 - val_loss: 0.1093 - val_accuracy: 0.9621\n",
      "Epoch 677/800\n",
      "1665/1665 [==============================] - 0s 146us/step - loss: 0.0567 - accuracy: 0.9784 - val_loss: 0.1092 - val_accuracy: 0.9615\n",
      "Epoch 678/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0563 - accuracy: 0.9786 - val_loss: 0.1107 - val_accuracy: 0.9617\n",
      "Epoch 679/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.0563 - accuracy: 0.9787 - val_loss: 0.1124 - val_accuracy: 0.9612\n",
      "Epoch 680/800\n",
      "1665/1665 [==============================] - 0s 106us/step - loss: 0.0565 - accuracy: 0.9785 - val_loss: 0.1125 - val_accuracy: 0.9600\n",
      "Epoch 681/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0568 - accuracy: 0.9783 - val_loss: 0.1099 - val_accuracy: 0.9617\n",
      "Epoch 682/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.0562 - accuracy: 0.9786 - val_loss: 0.1100 - val_accuracy: 0.9615\n",
      "Epoch 683/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0562 - accuracy: 0.9786 - val_loss: 0.1508 - val_accuracy: 0.9477\n",
      "Epoch 684/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0579 - accuracy: 0.9780 - val_loss: 0.1088 - val_accuracy: 0.9613\n",
      "Epoch 685/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.0562 - accuracy: 0.9786 - val_loss: 0.1243 - val_accuracy: 0.9589\n",
      "Epoch 686/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.0570 - accuracy: 0.9787 - val_loss: 0.1115 - val_accuracy: 0.9610\n",
      "Epoch 687/800\n",
      "1665/1665 [==============================] - 0s 126us/step - loss: 0.0563 - accuracy: 0.9789 - val_loss: 0.1074 - val_accuracy: 0.9622\n",
      "Epoch 688/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0559 - accuracy: 0.9787 - val_loss: 0.1344 - val_accuracy: 0.9533\n",
      "Epoch 689/800\n",
      "1665/1665 [==============================] - 0s 141us/step - loss: 0.0577 - accuracy: 0.9784 - val_loss: 0.1105 - val_accuracy: 0.9620\n",
      "Epoch 690/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0560 - accuracy: 0.9788 - val_loss: 0.1088 - val_accuracy: 0.9617\n",
      "Epoch 691/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.0560 - accuracy: 0.9788 - val_loss: 0.1092 - val_accuracy: 0.9622\n",
      "Epoch 692/800\n",
      "1665/1665 [==============================] - 0s 136us/step - loss: 0.0557 - accuracy: 0.9789 - val_loss: 0.1094 - val_accuracy: 0.9619\n",
      "Epoch 693/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0557 - accuracy: 0.9791 - val_loss: 0.1096 - val_accuracy: 0.9617\n",
      "Epoch 694/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0557 - accuracy: 0.9789 - val_loss: 0.1104 - val_accuracy: 0.9614\n",
      "Epoch 695/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.0559 - accuracy: 0.9787 - val_loss: 0.1141 - val_accuracy: 0.9610\n",
      "Epoch 696/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0558 - accuracy: 0.9787 - val_loss: 0.1081 - val_accuracy: 0.9626\n",
      "Epoch 697/800\n",
      "1665/1665 [==============================] - 0s 154us/step - loss: 0.0556 - accuracy: 0.9788 - val_loss: 0.1089 - val_accuracy: 0.9623\n",
      "Epoch 698/800\n",
      "1665/1665 [==============================] - 0s 146us/step - loss: 0.0555 - accuracy: 0.9789 - val_loss: 0.1096 - val_accuracy: 0.9622\n",
      "Epoch 699/800\n",
      "1665/1665 [==============================] - 0s 161us/step - loss: 0.0555 - accuracy: 0.9790 - val_loss: 0.1197 - val_accuracy: 0.9576\n",
      "Epoch 700/800\n",
      "1665/1665 [==============================] - 0s 126us/step - loss: 0.0562 - accuracy: 0.9787 - val_loss: 0.1112 - val_accuracy: 0.9623\n",
      "Epoch 701/800\n",
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.0557 - accuracy: 0.9788 - val_loss: 0.1077 - val_accuracy: 0.9625\n",
      "Epoch 702/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.0554 - accuracy: 0.9786 - val_loss: 0.1093 - val_accuracy: 0.9627\n",
      "Epoch 703/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0553 - accuracy: 0.9789 - val_loss: 0.1070 - val_accuracy: 0.9627\n",
      "Epoch 704/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0555 - accuracy: 0.9789 - val_loss: 0.1088 - val_accuracy: 0.9618\n",
      "Epoch 705/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0554 - accuracy: 0.9786 - val_loss: 0.1105 - val_accuracy: 0.9601\n",
      "Epoch 706/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0555 - accuracy: 0.9790 - val_loss: 0.1301 - val_accuracy: 0.9544\n",
      "Epoch 707/800\n",
      "1665/1665 [==============================] - 0s 143us/step - loss: 0.0560 - accuracy: 0.9787 - val_loss: 0.1089 - val_accuracy: 0.9622\n",
      "Epoch 708/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0552 - accuracy: 0.9792 - val_loss: 0.1090 - val_accuracy: 0.9612\n",
      "Epoch 709/800\n",
      "1665/1665 [==============================] - 0s 140us/step - loss: 0.0550 - accuracy: 0.9790 - val_loss: 0.1092 - val_accuracy: 0.9619\n",
      "Epoch 710/800\n",
      "1665/1665 [==============================] - 0s 129us/step - loss: 0.0550 - accuracy: 0.9792 - val_loss: 0.1101 - val_accuracy: 0.9616\n",
      "Epoch 711/800\n",
      "1665/1665 [==============================] - 0s 129us/step - loss: 0.0550 - accuracy: 0.9791 - val_loss: 0.1089 - val_accuracy: 0.9623\n",
      "Epoch 712/800\n",
      "1665/1665 [==============================] - 0s 107us/step - loss: 0.0551 - accuracy: 0.9790 - val_loss: 0.1086 - val_accuracy: 0.9621\n",
      "Epoch 713/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0550 - accuracy: 0.9791 - val_loss: 0.1099 - val_accuracy: 0.9615\n",
      "Epoch 714/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0551 - accuracy: 0.9788 - val_loss: 0.1097 - val_accuracy: 0.9624\n",
      "Epoch 715/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0552 - accuracy: 0.9790 - val_loss: 0.1112 - val_accuracy: 0.9615\n",
      "Epoch 716/800\n",
      "1665/1665 [==============================] - 0s 133us/step - loss: 0.0549 - accuracy: 0.9792 - val_loss: 0.1091 - val_accuracy: 0.9619\n",
      "Epoch 717/800\n",
      "1665/1665 [==============================] - 0s 112us/step - loss: 0.0549 - accuracy: 0.9791 - val_loss: 0.1084 - val_accuracy: 0.9616\n",
      "Epoch 718/800\n",
      "1665/1665 [==============================] - 0s 107us/step - loss: 0.0550 - accuracy: 0.9789 - val_loss: 0.1093 - val_accuracy: 0.9622\n",
      "Epoch 719/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0549 - accuracy: 0.9790 - val_loss: 0.1518 - val_accuracy: 0.9476\n",
      "Epoch 720/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.0567 - accuracy: 0.9784 - val_loss: 0.1127 - val_accuracy: 0.9619\n",
      "Epoch 721/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.0550 - accuracy: 0.9789 - val_loss: 0.1069 - val_accuracy: 0.9626\n",
      "Epoch 722/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0546 - accuracy: 0.9790 - val_loss: 0.1082 - val_accuracy: 0.9627\n",
      "Epoch 723/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0547 - accuracy: 0.9791 - val_loss: 0.1106 - val_accuracy: 0.9618\n",
      "Epoch 724/800\n",
      "1665/1665 [==============================] - 0s 106us/step - loss: 0.0546 - accuracy: 0.9792 - val_loss: 0.1087 - val_accuracy: 0.9622\n",
      "Epoch 725/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.0547 - accuracy: 0.9791 - val_loss: 0.1281 - val_accuracy: 0.9541\n",
      "Epoch 726/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0554 - accuracy: 0.9788 - val_loss: 0.1105 - val_accuracy: 0.9617\n",
      "Epoch 727/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0546 - accuracy: 0.9791 - val_loss: 0.1078 - val_accuracy: 0.9624\n",
      "Epoch 728/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0543 - accuracy: 0.9793 - val_loss: 0.1096 - val_accuracy: 0.9615\n",
      "Epoch 729/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.0545 - accuracy: 0.9792 - val_loss: 0.1094 - val_accuracy: 0.9625\n",
      "Epoch 730/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0545 - accuracy: 0.9792 - val_loss: 0.1092 - val_accuracy: 0.9620\n",
      "Epoch 731/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0545 - accuracy: 0.9791 - val_loss: 0.1104 - val_accuracy: 0.9622\n",
      "Epoch 732/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0544 - accuracy: 0.9793 - val_loss: 0.1072 - val_accuracy: 0.9627\n",
      "Epoch 733/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.0542 - accuracy: 0.9790 - val_loss: 0.1098 - val_accuracy: 0.9618\n",
      "Epoch 734/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.0542 - accuracy: 0.9793 - val_loss: 0.1080 - val_accuracy: 0.9621\n",
      "Epoch 735/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.0542 - accuracy: 0.9791 - val_loss: 0.1085 - val_accuracy: 0.9626\n",
      "Epoch 736/800\n",
      "1665/1665 [==============================] - 0s 133us/step - loss: 0.0539 - accuracy: 0.9795 - val_loss: 0.1111 - val_accuracy: 0.9621\n",
      "Epoch 737/800\n",
      "1665/1665 [==============================] - 0s 143us/step - loss: 0.0541 - accuracy: 0.9791 - val_loss: 0.1090 - val_accuracy: 0.9624\n",
      "Epoch 738/800\n",
      "1665/1665 [==============================] - 0s 144us/step - loss: 0.0538 - accuracy: 0.9793 - val_loss: 0.1405 - val_accuracy: 0.9551\n",
      "Epoch 739/800\n",
      "1665/1665 [==============================] - 0s 136us/step - loss: 0.0555 - accuracy: 0.9786 - val_loss: 0.1091 - val_accuracy: 0.9622\n",
      "Epoch 740/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0539 - accuracy: 0.9792 - val_loss: 0.1087 - val_accuracy: 0.9619\n",
      "Epoch 741/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0541 - accuracy: 0.9794 - val_loss: 0.1077 - val_accuracy: 0.9629\n",
      "Epoch 742/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0537 - accuracy: 0.9794 - val_loss: 0.1107 - val_accuracy: 0.9611\n",
      "Epoch 743/800\n",
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.0543 - accuracy: 0.9790 - val_loss: 0.1091 - val_accuracy: 0.9621\n",
      "Epoch 744/800\n",
      "1665/1665 [==============================] - 0s 140us/step - loss: 0.0539 - accuracy: 0.9792 - val_loss: 0.1089 - val_accuracy: 0.9626\n",
      "Epoch 745/800\n",
      "1665/1665 [==============================] - 0s 146us/step - loss: 0.0540 - accuracy: 0.9794 - val_loss: 0.1086 - val_accuracy: 0.9621\n",
      "Epoch 746/800\n",
      "1665/1665 [==============================] - 0s 133us/step - loss: 0.0536 - accuracy: 0.9795 - val_loss: 0.1100 - val_accuracy: 0.9617\n",
      "Epoch 747/800\n",
      "1665/1665 [==============================] - 0s 141us/step - loss: 0.0537 - accuracy: 0.9795 - val_loss: 0.1114 - val_accuracy: 0.9625\n",
      "Epoch 748/800\n",
      "1665/1665 [==============================] - 0s 149us/step - loss: 0.0539 - accuracy: 0.9792 - val_loss: 0.1094 - val_accuracy: 0.9624\n",
      "Epoch 749/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.0536 - accuracy: 0.9795 - val_loss: 0.1060 - val_accuracy: 0.9631\n",
      "Epoch 750/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.0536 - accuracy: 0.9797 - val_loss: 0.1087 - val_accuracy: 0.9627\n",
      "Epoch 751/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0535 - accuracy: 0.9796 - val_loss: 0.1160 - val_accuracy: 0.9618\n",
      "Epoch 752/800\n",
      "1665/1665 [==============================] - 0s 126us/step - loss: 0.0537 - accuracy: 0.9793 - val_loss: 0.1080 - val_accuracy: 0.9625\n",
      "Epoch 753/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0534 - accuracy: 0.9796 - val_loss: 0.1098 - val_accuracy: 0.9623\n",
      "Epoch 754/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0534 - accuracy: 0.9795 - val_loss: 0.1261 - val_accuracy: 0.9570\n",
      "Epoch 755/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0546 - accuracy: 0.9793 - val_loss: 0.1298 - val_accuracy: 0.9556\n",
      "Epoch 756/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0549 - accuracy: 0.9786 - val_loss: 0.1107 - val_accuracy: 0.9617\n",
      "Epoch 757/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0536 - accuracy: 0.9795 - val_loss: 0.1095 - val_accuracy: 0.9622\n",
      "Epoch 758/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0533 - accuracy: 0.9797 - val_loss: 0.1092 - val_accuracy: 0.9614\n",
      "Epoch 759/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0534 - accuracy: 0.9793 - val_loss: 0.1258 - val_accuracy: 0.9561\n",
      "Epoch 760/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0540 - accuracy: 0.9792 - val_loss: 0.1148 - val_accuracy: 0.9628\n",
      "Epoch 761/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0535 - accuracy: 0.9795 - val_loss: 0.1216 - val_accuracy: 0.9577\n",
      "Epoch 762/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0541 - accuracy: 0.9790 - val_loss: 0.1078 - val_accuracy: 0.9631\n",
      "Epoch 763/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0531 - accuracy: 0.9798 - val_loss: 0.1084 - val_accuracy: 0.9631\n",
      "Epoch 764/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0531 - accuracy: 0.9798 - val_loss: 0.1075 - val_accuracy: 0.9630\n",
      "Epoch 765/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0529 - accuracy: 0.9798 - val_loss: 0.1068 - val_accuracy: 0.9627\n",
      "Epoch 766/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0529 - accuracy: 0.9798 - val_loss: 0.1071 - val_accuracy: 0.9628\n",
      "Epoch 767/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0528 - accuracy: 0.9796 - val_loss: 0.1071 - val_accuracy: 0.9628\n",
      "Epoch 768/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0530 - accuracy: 0.9794 - val_loss: 0.1136 - val_accuracy: 0.9615\n",
      "Epoch 769/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0532 - accuracy: 0.9795 - val_loss: 0.1091 - val_accuracy: 0.9627\n",
      "Epoch 770/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0529 - accuracy: 0.9797 - val_loss: 0.1080 - val_accuracy: 0.9633\n",
      "Epoch 771/800\n",
      "1665/1665 [==============================] - ETA: 0s - loss: 0.0520 - accuracy: 0.98 - 0s 110us/step - loss: 0.0527 - accuracy: 0.9797 - val_loss: 0.1064 - val_accuracy: 0.9636\n",
      "Epoch 772/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0526 - accuracy: 0.9798 - val_loss: 0.1079 - val_accuracy: 0.9628\n",
      "Epoch 773/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0528 - accuracy: 0.9795 - val_loss: 0.1090 - val_accuracy: 0.9625\n",
      "Epoch 774/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0531 - accuracy: 0.9795 - val_loss: 0.1106 - val_accuracy: 0.9624\n",
      "Epoch 775/800\n",
      "1665/1665 [==============================] - 0s 112us/step - loss: 0.0529 - accuracy: 0.9798 - val_loss: 0.1061 - val_accuracy: 0.9631\n",
      "Epoch 776/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.0527 - accuracy: 0.9797 - val_loss: 0.1080 - val_accuracy: 0.9627\n",
      "Epoch 777/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0527 - accuracy: 0.9797 - val_loss: 0.1073 - val_accuracy: 0.9630\n",
      "Epoch 778/800\n",
      "1665/1665 [==============================] - 0s 109us/step - loss: 0.0525 - accuracy: 0.9797 - val_loss: 0.1069 - val_accuracy: 0.9634\n",
      "Epoch 779/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0529 - accuracy: 0.9796 - val_loss: 0.1068 - val_accuracy: 0.9636\n",
      "Epoch 780/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0524 - accuracy: 0.9797 - val_loss: 0.1105 - val_accuracy: 0.9629\n",
      "Epoch 781/800\n",
      "1665/1665 [==============================] - 0s 112us/step - loss: 0.0526 - accuracy: 0.9796 - val_loss: 0.1096 - val_accuracy: 0.9628\n",
      "Epoch 782/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0526 - accuracy: 0.9798 - val_loss: 0.1074 - val_accuracy: 0.9632\n",
      "Epoch 783/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0526 - accuracy: 0.9798 - val_loss: 0.1080 - val_accuracy: 0.9629\n",
      "Epoch 784/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.0522 - accuracy: 0.9799 - val_loss: 0.1064 - val_accuracy: 0.9629\n",
      "Epoch 785/800\n",
      "1665/1665 [==============================] - 0s 147us/step - loss: 0.0524 - accuracy: 0.9797 - val_loss: 0.1133 - val_accuracy: 0.9622\n",
      "Epoch 786/800\n",
      "1665/1665 [==============================] - 0s 133us/step - loss: 0.0526 - accuracy: 0.9799 - val_loss: 0.1088 - val_accuracy: 0.9623\n",
      "Epoch 787/800\n",
      "1665/1665 [==============================] - 0s 148us/step - loss: 0.0523 - accuracy: 0.9798 - val_loss: 0.1098 - val_accuracy: 0.9631\n",
      "Epoch 788/800\n",
      "1665/1665 [==============================] - 0s 168us/step - loss: 0.0525 - accuracy: 0.9798 - val_loss: 0.1092 - val_accuracy: 0.9621\n",
      "Epoch 789/800\n",
      "1665/1665 [==============================] - 0s 146us/step - loss: 0.0523 - accuracy: 0.9797 - val_loss: 0.1127 - val_accuracy: 0.9609\n",
      "Epoch 790/800\n",
      "1665/1665 [==============================] - 0s 154us/step - loss: 0.0527 - accuracy: 0.9801 - val_loss: 0.1274 - val_accuracy: 0.9561\n",
      "Epoch 791/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.0532 - accuracy: 0.9794 - val_loss: 0.1080 - val_accuracy: 0.9633\n",
      "Epoch 792/800\n",
      "1665/1665 [==============================] - 0s 171us/step - loss: 0.0522 - accuracy: 0.9797 - val_loss: 0.1082 - val_accuracy: 0.9633\n",
      "Epoch 793/800\n",
      "1665/1665 [==============================] - 0s 165us/step - loss: 0.0519 - accuracy: 0.9799 - val_loss: 0.1078 - val_accuracy: 0.9629\n",
      "Epoch 794/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.0522 - accuracy: 0.9799 - val_loss: 0.1100 - val_accuracy: 0.9625\n",
      "Epoch 795/800\n",
      "1665/1665 [==============================] - 0s 147us/step - loss: 0.0523 - accuracy: 0.9797 - val_loss: 0.1088 - val_accuracy: 0.9631\n",
      "Epoch 796/800\n",
      "1665/1665 [==============================] - 0s 192us/step - loss: 0.0520 - accuracy: 0.9798 - val_loss: 0.1081 - val_accuracy: 0.9636\n",
      "Epoch 797/800\n",
      "1665/1665 [==============================] - 0s 152us/step - loss: 0.0518 - accuracy: 0.9801 - val_loss: 0.1081 - val_accuracy: 0.9635\n",
      "Epoch 798/800\n",
      "1665/1665 [==============================] - ETA: 0s - loss: 0.0521 - accuracy: 0.97 - 0s 169us/step - loss: 0.0518 - accuracy: 0.9801 - val_loss: 0.1084 - val_accuracy: 0.9634\n",
      "Epoch 799/800\n",
      "1665/1665 [==============================] - 0s 172us/step - loss: 0.0517 - accuracy: 0.9799 - val_loss: 0.1083 - val_accuracy: 0.9625\n",
      "Epoch 800/800\n",
      "1665/1665 [==============================] - 0s 176us/step - loss: 0.0521 - accuracy: 0.9798 - val_loss: 0.1095 - val_accuracy: 0.9629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clair\\Anaconda3\\envs\\CNNs\\lib\\site-packages\\ipykernel_launcher.py:125: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "C:\\Users\\clair\\Anaconda3\\envs\\CNNs\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:808: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Train on 1665 samples, validate on 556 samples\n",
      "Epoch 1/800\n",
      "1665/1665 [==============================] - 0s 181us/step - loss: 0.6545 - accuracy: 0.7385 - val_loss: 0.5980 - val_accuracy: 0.8639\n",
      "Epoch 2/800\n",
      "1665/1665 [==============================] - 0s 98us/step - loss: 0.3927 - accuracy: 0.9236 - val_loss: 0.2626 - val_accuracy: 0.9324\n",
      "Epoch 3/800\n",
      "1665/1665 [==============================] - 0s 109us/step - loss: 0.1936 - accuracy: 0.9536 - val_loss: 0.2531 - val_accuracy: 0.9324\n",
      "Epoch 4/800\n",
      "1665/1665 [==============================] - 0s 107us/step - loss: 0.1886 - accuracy: 0.9536 - val_loss: 0.2522 - val_accuracy: 0.9324\n",
      "Epoch 5/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1877 - accuracy: 0.9536 - val_loss: 0.2511 - val_accuracy: 0.9324\n",
      "Epoch 6/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.1874 - accuracy: 0.9536 - val_loss: 0.2512 - val_accuracy: 0.9324\n",
      "Epoch 7/800\n",
      "1665/1665 [==============================] - ETA: 0s - loss: 0.1876 - accuracy: 0.95 - 0s 108us/step - loss: 0.1872 - accuracy: 0.9536 - val_loss: 0.2517 - val_accuracy: 0.9324\n",
      "Epoch 8/800\n",
      "1665/1665 [==============================] - 0s 107us/step - loss: 0.1871 - accuracy: 0.9536 - val_loss: 0.2515 - val_accuracy: 0.9324\n",
      "Epoch 9/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.1870 - accuracy: 0.9536 - val_loss: 0.2510 - val_accuracy: 0.9324\n",
      "Epoch 10/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.1869 - accuracy: 0.9536 - val_loss: 0.2511 - val_accuracy: 0.9324\n",
      "Epoch 11/800\n",
      "1665/1665 [==============================] - 0s 112us/step - loss: 0.1868 - accuracy: 0.9536 - val_loss: 0.2506 - val_accuracy: 0.9324\n",
      "Epoch 12/800\n",
      "1665/1665 [==============================] - 0s 109us/step - loss: 0.1867 - accuracy: 0.9536 - val_loss: 0.2508 - val_accuracy: 0.9324\n",
      "Epoch 13/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.1867 - accuracy: 0.9536 - val_loss: 0.2504 - val_accuracy: 0.9324\n",
      "Epoch 14/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1866 - accuracy: 0.9536 - val_loss: 0.2501 - val_accuracy: 0.9324\n",
      "Epoch 15/800\n",
      "1665/1665 [==============================] - 0s 107us/step - loss: 0.1864 - accuracy: 0.9536 - val_loss: 0.2499 - val_accuracy: 0.9324\n",
      "Epoch 16/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.1863 - accuracy: 0.9536 - val_loss: 0.2498 - val_accuracy: 0.9324\n",
      "Epoch 17/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.1863 - accuracy: 0.9536 - val_loss: 0.2499 - val_accuracy: 0.9324\n",
      "Epoch 18/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1862 - accuracy: 0.9536 - val_loss: 0.2498 - val_accuracy: 0.9324\n",
      "Epoch 19/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.1861 - accuracy: 0.9536 - val_loss: 0.2500 - val_accuracy: 0.9324\n",
      "Epoch 20/800\n",
      "1665/1665 [==============================] - 0s 106us/step - loss: 0.1860 - accuracy: 0.9536 - val_loss: 0.2496 - val_accuracy: 0.9324\n",
      "Epoch 21/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.1859 - accuracy: 0.9536 - val_loss: 0.2498 - val_accuracy: 0.9324\n",
      "Epoch 22/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.1859 - accuracy: 0.9536 - val_loss: 0.2493 - val_accuracy: 0.9324\n",
      "Epoch 23/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.1857 - accuracy: 0.9536 - val_loss: 0.2494 - val_accuracy: 0.9324\n",
      "Epoch 24/800\n",
      "1665/1665 [==============================] - 0s 109us/step - loss: 0.1856 - accuracy: 0.9536 - val_loss: 0.2490 - val_accuracy: 0.9324\n",
      "Epoch 25/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.1855 - accuracy: 0.9536 - val_loss: 0.2491 - val_accuracy: 0.9324\n",
      "Epoch 26/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.1855 - accuracy: 0.9536 - val_loss: 0.2487 - val_accuracy: 0.9324\n",
      "Epoch 27/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.1854 - accuracy: 0.9536 - val_loss: 0.2489 - val_accuracy: 0.9324\n",
      "Epoch 28/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.1852 - accuracy: 0.9536 - val_loss: 0.2484 - val_accuracy: 0.9324\n",
      "Epoch 29/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.1852 - accuracy: 0.9536 - val_loss: 0.2484 - val_accuracy: 0.9324\n",
      "Epoch 30/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.1851 - accuracy: 0.9536 - val_loss: 0.2481 - val_accuracy: 0.9324\n",
      "Epoch 31/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.1850 - accuracy: 0.9536 - val_loss: 0.2478 - val_accuracy: 0.9324\n",
      "Epoch 32/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.1849 - accuracy: 0.9536 - val_loss: 0.2475 - val_accuracy: 0.9324\n",
      "Epoch 33/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.1848 - accuracy: 0.9536 - val_loss: 0.2476 - val_accuracy: 0.9324\n",
      "Epoch 34/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.1847 - accuracy: 0.9536 - val_loss: 0.2467 - val_accuracy: 0.9324\n",
      "Epoch 35/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.1845 - accuracy: 0.9536 - val_loss: 0.2469 - val_accuracy: 0.9324\n",
      "Epoch 36/800\n",
      "1665/1665 [==============================] - 0s 126us/step - loss: 0.1844 - accuracy: 0.9536 - val_loss: 0.2471 - val_accuracy: 0.9324\n",
      "Epoch 37/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1843 - accuracy: 0.9536 - val_loss: 0.2464 - val_accuracy: 0.9324\n",
      "Epoch 38/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1842 - accuracy: 0.9536 - val_loss: 0.2460 - val_accuracy: 0.9324\n",
      "Epoch 39/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.1841 - accuracy: 0.9536 - val_loss: 0.2460 - val_accuracy: 0.9324\n",
      "Epoch 40/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.1840 - accuracy: 0.9536 - val_loss: 0.2452 - val_accuracy: 0.9324\n",
      "Epoch 41/800\n",
      "1665/1665 [==============================] - 0s 103us/step - loss: 0.1838 - accuracy: 0.9536 - val_loss: 0.2455 - val_accuracy: 0.9324\n",
      "Epoch 42/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.1837 - accuracy: 0.9536 - val_loss: 0.2457 - val_accuracy: 0.9324\n",
      "Epoch 43/800\n",
      "1665/1665 [==============================] - 0s 109us/step - loss: 0.1836 - accuracy: 0.9536 - val_loss: 0.2452 - val_accuracy: 0.9324\n",
      "Epoch 44/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.1834 - accuracy: 0.9536 - val_loss: 0.2459 - val_accuracy: 0.9324\n",
      "Epoch 45/800\n",
      "1665/1665 [==============================] - 0s 106us/step - loss: 0.1833 - accuracy: 0.9536 - val_loss: 0.2457 - val_accuracy: 0.9324\n",
      "Epoch 46/800\n",
      "1665/1665 [==============================] - 0s 107us/step - loss: 0.1831 - accuracy: 0.9536 - val_loss: 0.2448 - val_accuracy: 0.9324\n",
      "Epoch 47/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.1830 - accuracy: 0.9536 - val_loss: 0.2447 - val_accuracy: 0.9324\n",
      "Epoch 48/800\n",
      "1665/1665 [==============================] - 0s 112us/step - loss: 0.1828 - accuracy: 0.9536 - val_loss: 0.2441 - val_accuracy: 0.9324\n",
      "Epoch 49/800\n",
      "1665/1665 [==============================] - 0s 109us/step - loss: 0.1827 - accuracy: 0.9536 - val_loss: 0.2442 - val_accuracy: 0.9324\n",
      "Epoch 50/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.1825 - accuracy: 0.9536 - val_loss: 0.2439 - val_accuracy: 0.9324\n",
      "Epoch 51/800\n",
      "1665/1665 [==============================] - 0s 107us/step - loss: 0.1823 - accuracy: 0.9536 - val_loss: 0.2429 - val_accuracy: 0.9324\n",
      "Epoch 52/800\n",
      "1665/1665 [==============================] - 0s 109us/step - loss: 0.1822 - accuracy: 0.9536 - val_loss: 0.2433 - val_accuracy: 0.9324\n",
      "Epoch 53/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1820 - accuracy: 0.9536 - val_loss: 0.2426 - val_accuracy: 0.9324\n",
      "Epoch 54/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1818 - accuracy: 0.9536 - val_loss: 0.2429 - val_accuracy: 0.9324\n",
      "Epoch 55/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.1815 - accuracy: 0.9536 - val_loss: 0.2429 - val_accuracy: 0.9324\n",
      "Epoch 56/800\n",
      "1665/1665 [==============================] - 0s 106us/step - loss: 0.1813 - accuracy: 0.9536 - val_loss: 0.2413 - val_accuracy: 0.9324\n",
      "Epoch 57/800\n",
      "1665/1665 [==============================] - 0s 103us/step - loss: 0.1811 - accuracy: 0.9536 - val_loss: 0.2421 - val_accuracy: 0.9324\n",
      "Epoch 58/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.1809 - accuracy: 0.9536 - val_loss: 0.2417 - val_accuracy: 0.9324\n",
      "Epoch 59/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.1807 - accuracy: 0.9536 - val_loss: 0.2412 - val_accuracy: 0.9324\n",
      "Epoch 60/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.1804 - accuracy: 0.9536 - val_loss: 0.2404 - val_accuracy: 0.9324\n",
      "Epoch 61/800\n",
      "1665/1665 [==============================] - 0s 103us/step - loss: 0.1801 - accuracy: 0.9536 - val_loss: 0.2401 - val_accuracy: 0.9324\n",
      "Epoch 62/800\n",
      "1665/1665 [==============================] - 0s 106us/step - loss: 0.1799 - accuracy: 0.9536 - val_loss: 0.2399 - val_accuracy: 0.9324\n",
      "Epoch 63/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.1796 - accuracy: 0.9536 - val_loss: 0.2394 - val_accuracy: 0.9324\n",
      "Epoch 64/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1793 - accuracy: 0.9536 - val_loss: 0.2385 - val_accuracy: 0.9324\n",
      "Epoch 65/800\n",
      "1665/1665 [==============================] - 0s 105us/step - loss: 0.1790 - accuracy: 0.9536 - val_loss: 0.2379 - val_accuracy: 0.9324\n",
      "Epoch 66/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.1787 - accuracy: 0.9536 - val_loss: 0.2379 - val_accuracy: 0.9324\n",
      "Epoch 67/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.1783 - accuracy: 0.9536 - val_loss: 0.2371 - val_accuracy: 0.9324\n",
      "Epoch 68/800\n",
      "1665/1665 [==============================] - 0s 106us/step - loss: 0.1780 - accuracy: 0.9536 - val_loss: 0.2368 - val_accuracy: 0.9324\n",
      "Epoch 69/800\n",
      "1665/1665 [==============================] - 0s 107us/step - loss: 0.1776 - accuracy: 0.9536 - val_loss: 0.2361 - val_accuracy: 0.9324\n",
      "Epoch 70/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.1772 - accuracy: 0.9536 - val_loss: 0.2355 - val_accuracy: 0.9324\n",
      "Epoch 71/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.1768 - accuracy: 0.9536 - val_loss: 0.2348 - val_accuracy: 0.9324\n",
      "Epoch 72/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.1764 - accuracy: 0.9536 - val_loss: 0.2344 - val_accuracy: 0.9324\n",
      "Epoch 73/800\n",
      "1665/1665 [==============================] - 0s 102us/step - loss: 0.1760 - accuracy: 0.9536 - val_loss: 0.2333 - val_accuracy: 0.9324\n",
      "Epoch 74/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1756 - accuracy: 0.9536 - val_loss: 0.2329 - val_accuracy: 0.9324\n",
      "Epoch 75/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1751 - accuracy: 0.9536 - val_loss: 0.2321 - val_accuracy: 0.9324\n",
      "Epoch 76/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1747 - accuracy: 0.9536 - val_loss: 0.2312 - val_accuracy: 0.9324\n",
      "Epoch 77/800\n",
      "1665/1665 [==============================] - 0s 105us/step - loss: 0.1743 - accuracy: 0.9536 - val_loss: 0.2300 - val_accuracy: 0.9324\n",
      "Epoch 78/800\n",
      "1665/1665 [==============================] - 0s 107us/step - loss: 0.1737 - accuracy: 0.9536 - val_loss: 0.2293 - val_accuracy: 0.9324\n",
      "Epoch 79/800\n",
      "1665/1665 [==============================] - 0s 109us/step - loss: 0.1732 - accuracy: 0.9536 - val_loss: 0.2286 - val_accuracy: 0.9324\n",
      "Epoch 80/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.1726 - accuracy: 0.9536 - val_loss: 0.2278 - val_accuracy: 0.9324\n",
      "Epoch 81/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.1721 - accuracy: 0.9536 - val_loss: 0.2274 - val_accuracy: 0.9324\n",
      "Epoch 82/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.1715 - accuracy: 0.9536 - val_loss: 0.2268 - val_accuracy: 0.9324\n",
      "Epoch 83/800\n",
      "1665/1665 [==============================] - 0s 106us/step - loss: 0.1711 - accuracy: 0.9536 - val_loss: 0.2252 - val_accuracy: 0.9324\n",
      "Epoch 84/800\n",
      "1665/1665 [==============================] - 0s 106us/step - loss: 0.1704 - accuracy: 0.9536 - val_loss: 0.2243 - val_accuracy: 0.9324\n",
      "Epoch 85/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.1699 - accuracy: 0.9536 - val_loss: 0.2239 - val_accuracy: 0.9324\n",
      "Epoch 86/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.1694 - accuracy: 0.9536 - val_loss: 0.2227 - val_accuracy: 0.9324\n",
      "Epoch 87/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1687 - accuracy: 0.9536 - val_loss: 0.2220 - val_accuracy: 0.9324\n",
      "Epoch 88/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.1681 - accuracy: 0.9536 - val_loss: 0.2212 - val_accuracy: 0.9324\n",
      "Epoch 89/800\n",
      "1665/1665 [==============================] - 0s 107us/step - loss: 0.1675 - accuracy: 0.9536 - val_loss: 0.2203 - val_accuracy: 0.9324\n",
      "Epoch 90/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.1669 - accuracy: 0.9536 - val_loss: 0.2194 - val_accuracy: 0.9324\n",
      "Epoch 91/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1662 - accuracy: 0.9536 - val_loss: 0.2180 - val_accuracy: 0.9324\n",
      "Epoch 92/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.1657 - accuracy: 0.9536 - val_loss: 0.2171 - val_accuracy: 0.9324\n",
      "Epoch 93/800\n",
      "1665/1665 [==============================] - 0s 105us/step - loss: 0.1650 - accuracy: 0.9536 - val_loss: 0.2168 - val_accuracy: 0.9324\n",
      "Epoch 94/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.1644 - accuracy: 0.9536 - val_loss: 0.2161 - val_accuracy: 0.9324\n",
      "Epoch 95/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1639 - accuracy: 0.9536 - val_loss: 0.2154 - val_accuracy: 0.9325\n",
      "Epoch 96/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.1633 - accuracy: 0.9536 - val_loss: 0.2140 - val_accuracy: 0.9324\n",
      "Epoch 97/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1626 - accuracy: 0.9536 - val_loss: 0.2136 - val_accuracy: 0.9324\n",
      "Epoch 98/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.1620 - accuracy: 0.9536 - val_loss: 0.2119 - val_accuracy: 0.9324\n",
      "Epoch 99/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.1614 - accuracy: 0.9536 - val_loss: 0.2113 - val_accuracy: 0.9324\n",
      "Epoch 100/800\n",
      "1665/1665 [==============================] - 0s 104us/step - loss: 0.1609 - accuracy: 0.9536 - val_loss: 0.2101 - val_accuracy: 0.9324\n",
      "Epoch 101/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.1602 - accuracy: 0.9536 - val_loss: 0.2093 - val_accuracy: 0.9325\n",
      "Epoch 102/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.1596 - accuracy: 0.9536 - val_loss: 0.2087 - val_accuracy: 0.9327\n",
      "Epoch 103/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1591 - accuracy: 0.9537 - val_loss: 0.2082 - val_accuracy: 0.9328\n",
      "Epoch 104/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.1585 - accuracy: 0.9537 - val_loss: 0.2070 - val_accuracy: 0.9327\n",
      "Epoch 105/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.1579 - accuracy: 0.9537 - val_loss: 0.2059 - val_accuracy: 0.9327\n",
      "Epoch 106/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.1573 - accuracy: 0.9537 - val_loss: 0.2053 - val_accuracy: 0.9328\n",
      "Epoch 107/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.1567 - accuracy: 0.9538 - val_loss: 0.2044 - val_accuracy: 0.9329\n",
      "Epoch 108/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.1561 - accuracy: 0.9538 - val_loss: 0.2035 - val_accuracy: 0.9329\n",
      "Epoch 109/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.1555 - accuracy: 0.9538 - val_loss: 0.2030 - val_accuracy: 0.9329\n",
      "Epoch 110/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1665/1665 [==============================] - 0s 106us/step - loss: 0.1549 - accuracy: 0.9539 - val_loss: 0.2022 - val_accuracy: 0.9332\n",
      "Epoch 111/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.1543 - accuracy: 0.9540 - val_loss: 0.2022 - val_accuracy: 0.9332\n",
      "Epoch 112/800\n",
      "1665/1665 [==============================] - 0s 126us/step - loss: 0.1537 - accuracy: 0.9539 - val_loss: 0.2011 - val_accuracy: 0.9333\n",
      "Epoch 113/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.1532 - accuracy: 0.9540 - val_loss: 0.2004 - val_accuracy: 0.9333\n",
      "Epoch 114/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.1526 - accuracy: 0.9540 - val_loss: 0.1987 - val_accuracy: 0.9336\n",
      "Epoch 115/800\n",
      "1665/1665 [==============================] - 0s 126us/step - loss: 0.1520 - accuracy: 0.9541 - val_loss: 0.1980 - val_accuracy: 0.9336\n",
      "Epoch 116/800\n",
      "1665/1665 [==============================] - 0s 126us/step - loss: 0.1514 - accuracy: 0.9541 - val_loss: 0.1976 - val_accuracy: 0.9335\n",
      "Epoch 117/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.1508 - accuracy: 0.9541 - val_loss: 0.1967 - val_accuracy: 0.9337\n",
      "Epoch 118/800\n",
      "1665/1665 [==============================] - 0s 129us/step - loss: 0.1502 - accuracy: 0.9541 - val_loss: 0.1950 - val_accuracy: 0.9337\n",
      "Epoch 119/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.1496 - accuracy: 0.9542 - val_loss: 0.1949 - val_accuracy: 0.9337\n",
      "Epoch 120/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.1489 - accuracy: 0.9542 - val_loss: 0.1939 - val_accuracy: 0.9339\n",
      "Epoch 121/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.1483 - accuracy: 0.9543 - val_loss: 0.1931 - val_accuracy: 0.9337\n",
      "Epoch 122/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.1477 - accuracy: 0.9543 - val_loss: 0.1921 - val_accuracy: 0.9342\n",
      "Epoch 123/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.1472 - accuracy: 0.9543 - val_loss: 0.1911 - val_accuracy: 0.9342\n",
      "Epoch 124/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.1465 - accuracy: 0.9544 - val_loss: 0.1911 - val_accuracy: 0.9339\n",
      "Epoch 125/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.1458 - accuracy: 0.9543 - val_loss: 0.1898 - val_accuracy: 0.9339\n",
      "Epoch 126/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.1452 - accuracy: 0.9544 - val_loss: 0.1896 - val_accuracy: 0.9342\n",
      "Epoch 127/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.1447 - accuracy: 0.9544 - val_loss: 0.1882 - val_accuracy: 0.9345\n",
      "Epoch 128/800\n",
      "1665/1665 [==============================] - 0s 126us/step - loss: 0.1439 - accuracy: 0.9545 - val_loss: 0.1872 - val_accuracy: 0.9347\n",
      "Epoch 129/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.1433 - accuracy: 0.9547 - val_loss: 0.1863 - val_accuracy: 0.9339\n",
      "Epoch 130/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.1427 - accuracy: 0.9544 - val_loss: 0.1856 - val_accuracy: 0.9347\n",
      "Epoch 131/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.1421 - accuracy: 0.9546 - val_loss: 0.1843 - val_accuracy: 0.9348\n",
      "Epoch 132/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.1415 - accuracy: 0.9548 - val_loss: 0.1834 - val_accuracy: 0.9345\n",
      "Epoch 133/800\n",
      "1665/1665 [==============================] - 0s 166us/step - loss: 0.1409 - accuracy: 0.9548 - val_loss: 0.1827 - val_accuracy: 0.9347\n",
      "Epoch 134/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.1403 - accuracy: 0.9549 - val_loss: 0.1823 - val_accuracy: 0.9347\n",
      "Epoch 135/800\n",
      "1665/1665 [==============================] - 0s 112us/step - loss: 0.1396 - accuracy: 0.9548 - val_loss: 0.1833 - val_accuracy: 0.9345\n",
      "Epoch 136/800\n",
      "1665/1665 [==============================] - 0s 133us/step - loss: 0.1392 - accuracy: 0.9551 - val_loss: 0.1814 - val_accuracy: 0.9355\n",
      "Epoch 137/800\n",
      "1665/1665 [==============================] - 0s 167us/step - loss: 0.1386 - accuracy: 0.9552 - val_loss: 0.1833 - val_accuracy: 0.9345\n",
      "Epoch 138/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.1381 - accuracy: 0.9553 - val_loss: 0.1800 - val_accuracy: 0.9355\n",
      "Epoch 139/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.1374 - accuracy: 0.9554 - val_loss: 0.1787 - val_accuracy: 0.9358\n",
      "Epoch 140/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1369 - accuracy: 0.9554 - val_loss: 0.1797 - val_accuracy: 0.9353\n",
      "Epoch 141/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.1364 - accuracy: 0.9553 - val_loss: 0.1782 - val_accuracy: 0.9357\n",
      "Epoch 142/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.1357 - accuracy: 0.9556 - val_loss: 0.1770 - val_accuracy: 0.9356\n",
      "Epoch 143/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.1353 - accuracy: 0.9558 - val_loss: 0.1777 - val_accuracy: 0.9359\n",
      "Epoch 144/800\n",
      "1665/1665 [==============================] - 0s 107us/step - loss: 0.1348 - accuracy: 0.9558 - val_loss: 0.1790 - val_accuracy: 0.9348\n",
      "Epoch 145/800\n",
      "1665/1665 [==============================] - 0s 107us/step - loss: 0.1343 - accuracy: 0.9558 - val_loss: 0.1752 - val_accuracy: 0.9361\n",
      "Epoch 146/800\n",
      "1665/1665 [==============================] - 0s 112us/step - loss: 0.1338 - accuracy: 0.9559 - val_loss: 0.1759 - val_accuracy: 0.9359\n",
      "Epoch 147/800\n",
      "1665/1665 [==============================] - 0s 104us/step - loss: 0.1333 - accuracy: 0.9560 - val_loss: 0.1771 - val_accuracy: 0.9353\n",
      "Epoch 148/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1328 - accuracy: 0.9563 - val_loss: 0.1738 - val_accuracy: 0.9366\n",
      "Epoch 149/800\n",
      "1665/1665 [==============================] - 0s 107us/step - loss: 0.1322 - accuracy: 0.9565 - val_loss: 0.1737 - val_accuracy: 0.9365\n",
      "Epoch 150/800\n",
      "1665/1665 [==============================] - 0s 107us/step - loss: 0.1318 - accuracy: 0.9563 - val_loss: 0.1720 - val_accuracy: 0.9370\n",
      "Epoch 151/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1312 - accuracy: 0.9565 - val_loss: 0.1725 - val_accuracy: 0.9362\n",
      "Epoch 152/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.1308 - accuracy: 0.9566 - val_loss: 0.1716 - val_accuracy: 0.9379\n",
      "Epoch 153/800\n",
      "1665/1665 [==============================] - 0s 109us/step - loss: 0.1303 - accuracy: 0.9569 - val_loss: 0.1710 - val_accuracy: 0.9374\n",
      "Epoch 154/800\n",
      "1665/1665 [==============================] - 0s 107us/step - loss: 0.1299 - accuracy: 0.9569 - val_loss: 0.1706 - val_accuracy: 0.9364\n",
      "Epoch 155/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.1294 - accuracy: 0.9568 - val_loss: 0.1699 - val_accuracy: 0.9376\n",
      "Epoch 156/800\n",
      "1665/1665 [==============================] - 0s 107us/step - loss: 0.1289 - accuracy: 0.9572 - val_loss: 0.1694 - val_accuracy: 0.9373\n",
      "Epoch 157/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.1285 - accuracy: 0.9573 - val_loss: 0.1693 - val_accuracy: 0.9378\n",
      "Epoch 158/800\n",
      "1665/1665 [==============================] - 0s 105us/step - loss: 0.1280 - accuracy: 0.9575 - val_loss: 0.1679 - val_accuracy: 0.9379\n",
      "Epoch 159/800\n",
      "1665/1665 [==============================] - 0s 109us/step - loss: 0.1275 - accuracy: 0.9574 - val_loss: 0.1680 - val_accuracy: 0.9375\n",
      "Epoch 160/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.1272 - accuracy: 0.9575 - val_loss: 0.1738 - val_accuracy: 0.9355\n",
      "Epoch 161/800\n",
      "1665/1665 [==============================] - 0s 105us/step - loss: 0.1268 - accuracy: 0.9576 - val_loss: 0.1674 - val_accuracy: 0.9383\n",
      "Epoch 162/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.1262 - accuracy: 0.9579 - val_loss: 0.1671 - val_accuracy: 0.9384\n",
      "Epoch 163/800\n",
      "1665/1665 [==============================] - 0s 112us/step - loss: 0.1257 - accuracy: 0.9583 - val_loss: 0.1662 - val_accuracy: 0.9384\n",
      "Epoch 164/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.1253 - accuracy: 0.9582 - val_loss: 0.1662 - val_accuracy: 0.9379\n",
      "Epoch 165/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.1248 - accuracy: 0.9584 - val_loss: 0.1664 - val_accuracy: 0.9382\n",
      "Epoch 166/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.1244 - accuracy: 0.9586 - val_loss: 0.1653 - val_accuracy: 0.9383\n",
      "Epoch 167/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.1239 - accuracy: 0.9586 - val_loss: 0.1653 - val_accuracy: 0.9385\n",
      "Epoch 168/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.1235 - accuracy: 0.9587 - val_loss: 0.1645 - val_accuracy: 0.9390\n",
      "Epoch 169/800\n",
      "1665/1665 [==============================] - 0s 103us/step - loss: 0.1230 - accuracy: 0.9589 - val_loss: 0.1635 - val_accuracy: 0.9395\n",
      "Epoch 170/800\n",
      "1665/1665 [==============================] - 0s 109us/step - loss: 0.1225 - accuracy: 0.9591 - val_loss: 0.1624 - val_accuracy: 0.9396\n",
      "Epoch 171/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.1222 - accuracy: 0.9592 - val_loss: 0.1626 - val_accuracy: 0.9397\n",
      "Epoch 172/800\n",
      "1665/1665 [==============================] - 0s 107us/step - loss: 0.1217 - accuracy: 0.9591 - val_loss: 0.1662 - val_accuracy: 0.9361\n",
      "Epoch 173/800\n",
      "1665/1665 [==============================] - ETA: 0s - loss: 0.1212 - accuracy: 0.95 - 0s 110us/step - loss: 0.1214 - accuracy: 0.9595 - val_loss: 0.1616 - val_accuracy: 0.9397\n",
      "Epoch 174/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.1208 - accuracy: 0.9595 - val_loss: 0.1606 - val_accuracy: 0.9397\n",
      "Epoch 175/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.1204 - accuracy: 0.9595 - val_loss: 0.1617 - val_accuracy: 0.9401\n",
      "Epoch 176/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.1201 - accuracy: 0.9596 - val_loss: 0.1601 - val_accuracy: 0.9403\n",
      "Epoch 177/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.1196 - accuracy: 0.9597 - val_loss: 0.1592 - val_accuracy: 0.9407\n",
      "Epoch 178/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.1191 - accuracy: 0.9597 - val_loss: 0.1612 - val_accuracy: 0.9402\n",
      "Epoch 179/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.1188 - accuracy: 0.9601 - val_loss: 0.1605 - val_accuracy: 0.9394\n",
      "Epoch 180/800\n",
      "1665/1665 [==============================] - 0s 112us/step - loss: 0.1183 - accuracy: 0.9603 - val_loss: 0.1588 - val_accuracy: 0.9402\n",
      "Epoch 181/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.1179 - accuracy: 0.9603 - val_loss: 0.1578 - val_accuracy: 0.9404\n",
      "Epoch 182/800\n",
      "1665/1665 [==============================] - 0s 141us/step - loss: 0.1175 - accuracy: 0.9603 - val_loss: 0.1576 - val_accuracy: 0.9408\n",
      "Epoch 183/800\n",
      "1665/1665 [==============================] - 0s 133us/step - loss: 0.1171 - accuracy: 0.9605 - val_loss: 0.1561 - val_accuracy: 0.9408\n",
      "Epoch 184/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.1167 - accuracy: 0.9607 - val_loss: 0.1563 - val_accuracy: 0.9413\n",
      "Epoch 185/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.1164 - accuracy: 0.9607 - val_loss: 0.1559 - val_accuracy: 0.9421\n",
      "Epoch 186/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.1160 - accuracy: 0.9608 - val_loss: 0.1564 - val_accuracy: 0.9408\n",
      "Epoch 187/800\n",
      "1665/1665 [==============================] - 0s 179us/step - loss: 0.1156 - accuracy: 0.9608 - val_loss: 0.1553 - val_accuracy: 0.9413\n",
      "Epoch 188/800\n",
      "1665/1665 [==============================] - 0s 188us/step - loss: 0.1152 - accuracy: 0.9608 - val_loss: 0.1555 - val_accuracy: 0.9410\n",
      "Epoch 189/800\n",
      "1665/1665 [==============================] - 0s 180us/step - loss: 0.1149 - accuracy: 0.9612 - val_loss: 0.1559 - val_accuracy: 0.9410\n",
      "Epoch 190/800\n",
      "1665/1665 [==============================] - 0s 202us/step - loss: 0.1146 - accuracy: 0.9614 - val_loss: 0.1554 - val_accuracy: 0.9417\n",
      "Epoch 191/800\n",
      "1665/1665 [==============================] - 0s 166us/step - loss: 0.1140 - accuracy: 0.9612 - val_loss: 0.1552 - val_accuracy: 0.9419\n",
      "Epoch 192/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.1137 - accuracy: 0.9617 - val_loss: 0.1547 - val_accuracy: 0.9421\n",
      "Epoch 193/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.1134 - accuracy: 0.9616 - val_loss: 0.1553 - val_accuracy: 0.9423\n",
      "Epoch 194/800\n",
      "1665/1665 [==============================] - 0s 155us/step - loss: 0.1130 - accuracy: 0.9620 - val_loss: 0.1545 - val_accuracy: 0.9418\n",
      "Epoch 195/800\n",
      "1665/1665 [==============================] - 0s 204us/step - loss: 0.1127 - accuracy: 0.9619 - val_loss: 0.1533 - val_accuracy: 0.9423\n",
      "Epoch 196/800\n",
      "1665/1665 [==============================] - 0s 160us/step - loss: 0.1124 - accuracy: 0.9621 - val_loss: 0.1573 - val_accuracy: 0.9417\n",
      "Epoch 197/800\n",
      "1665/1665 [==============================] - 0s 139us/step - loss: 0.1121 - accuracy: 0.9620 - val_loss: 0.1542 - val_accuracy: 0.9425\n",
      "Epoch 198/800\n",
      "1665/1665 [==============================] - 0s 144us/step - loss: 0.1117 - accuracy: 0.9619 - val_loss: 0.1525 - val_accuracy: 0.9428\n",
      "Epoch 199/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.1112 - accuracy: 0.9622 - val_loss: 0.1521 - val_accuracy: 0.9427\n",
      "Epoch 200/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.1110 - accuracy: 0.9622 - val_loss: 0.1527 - val_accuracy: 0.9429\n",
      "Epoch 201/800\n",
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.1106 - accuracy: 0.9624 - val_loss: 0.1530 - val_accuracy: 0.9418\n",
      "Epoch 202/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.1102 - accuracy: 0.9628 - val_loss: 0.1576 - val_accuracy: 0.9400\n",
      "Epoch 203/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.1103 - accuracy: 0.9623 - val_loss: 0.1517 - val_accuracy: 0.9427\n",
      "Epoch 204/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.1095 - accuracy: 0.9628 - val_loss: 0.1504 - val_accuracy: 0.9430\n",
      "Epoch 205/800\n",
      "1665/1665 [==============================] - 0s 109us/step - loss: 0.1092 - accuracy: 0.9626 - val_loss: 0.1506 - val_accuracy: 0.9436\n",
      "Epoch 206/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.1087 - accuracy: 0.9631 - val_loss: 0.1505 - val_accuracy: 0.9431\n",
      "Epoch 207/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.1085 - accuracy: 0.9629 - val_loss: 0.1497 - val_accuracy: 0.9440\n",
      "Epoch 208/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.1081 - accuracy: 0.9631 - val_loss: 0.1497 - val_accuracy: 0.9437\n",
      "Epoch 209/800\n",
      "1665/1665 [==============================] - 0s 163us/step - loss: 0.1078 - accuracy: 0.9632 - val_loss: 0.1489 - val_accuracy: 0.9439\n",
      "Epoch 210/800\n",
      "1665/1665 [==============================] - 0s 167us/step - loss: 0.1074 - accuracy: 0.9633 - val_loss: 0.1661 - val_accuracy: 0.9356\n",
      "Epoch 211/800\n",
      "1665/1665 [==============================] - 0s 154us/step - loss: 0.1081 - accuracy: 0.9631 - val_loss: 0.1493 - val_accuracy: 0.9445\n",
      "Epoch 212/800\n",
      "1665/1665 [==============================] - 0s 163us/step - loss: 0.1068 - accuracy: 0.9635 - val_loss: 0.1523 - val_accuracy: 0.9433\n",
      "Epoch 213/800\n",
      "1665/1665 [==============================] - 0s 158us/step - loss: 0.1067 - accuracy: 0.9634 - val_loss: 0.1472 - val_accuracy: 0.9450\n",
      "Epoch 214/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.1062 - accuracy: 0.9639 - val_loss: 0.1470 - val_accuracy: 0.9444\n",
      "Epoch 215/800\n",
      "1665/1665 [==============================] - 0s 129us/step - loss: 0.1060 - accuracy: 0.9636 - val_loss: 0.1474 - val_accuracy: 0.9446\n",
      "Epoch 216/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.1055 - accuracy: 0.9638 - val_loss: 0.1460 - val_accuracy: 0.9446\n",
      "Epoch 217/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.1052 - accuracy: 0.9642 - val_loss: 0.1459 - val_accuracy: 0.9451\n",
      "Epoch 218/800\n",
      "1665/1665 [==============================] - 0s 136us/step - loss: 0.1048 - accuracy: 0.9641 - val_loss: 0.1466 - val_accuracy: 0.9456\n",
      "Epoch 219/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.1047 - accuracy: 0.9641 - val_loss: 0.1499 - val_accuracy: 0.9425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 220/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.1047 - accuracy: 0.9640 - val_loss: 0.1466 - val_accuracy: 0.9455\n",
      "Epoch 221/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.1041 - accuracy: 0.9642 - val_loss: 0.1463 - val_accuracy: 0.9445\n",
      "Epoch 222/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.1039 - accuracy: 0.9645 - val_loss: 0.1453 - val_accuracy: 0.9457\n",
      "Epoch 223/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1034 - accuracy: 0.9644 - val_loss: 0.1496 - val_accuracy: 0.9434\n",
      "Epoch 224/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.1035 - accuracy: 0.9646 - val_loss: 0.1459 - val_accuracy: 0.9448\n",
      "Epoch 225/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.1030 - accuracy: 0.9647 - val_loss: 0.1450 - val_accuracy: 0.9461\n",
      "Epoch 226/800\n",
      "1665/1665 [==============================] - 0s 112us/step - loss: 0.1027 - accuracy: 0.9648 - val_loss: 0.1451 - val_accuracy: 0.9459\n",
      "Epoch 227/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.1027 - accuracy: 0.9651 - val_loss: 0.1438 - val_accuracy: 0.9466\n",
      "Epoch 228/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.1021 - accuracy: 0.9649 - val_loss: 0.1433 - val_accuracy: 0.9459\n",
      "Epoch 229/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.1019 - accuracy: 0.9649 - val_loss: 0.1437 - val_accuracy: 0.9466\n",
      "Epoch 230/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.1016 - accuracy: 0.9653 - val_loss: 0.1445 - val_accuracy: 0.9455\n",
      "Epoch 231/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.1015 - accuracy: 0.9652 - val_loss: 0.1447 - val_accuracy: 0.9461\n",
      "Epoch 232/800\n",
      "1665/1665 [==============================] - 0s 133us/step - loss: 0.1011 - accuracy: 0.9654 - val_loss: 0.1432 - val_accuracy: 0.9463\n",
      "Epoch 233/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.1010 - accuracy: 0.9657 - val_loss: 0.1436 - val_accuracy: 0.9468\n",
      "Epoch 234/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.1006 - accuracy: 0.9655 - val_loss: 0.1454 - val_accuracy: 0.9450\n",
      "Epoch 235/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1007 - accuracy: 0.9656 - val_loss: 0.1427 - val_accuracy: 0.9463\n",
      "Epoch 236/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.1003 - accuracy: 0.9657 - val_loss: 0.1514 - val_accuracy: 0.9418\n",
      "Epoch 237/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.1003 - accuracy: 0.9658 - val_loss: 0.1428 - val_accuracy: 0.9462\n",
      "Epoch 238/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.0996 - accuracy: 0.9660 - val_loss: 0.1431 - val_accuracy: 0.9466\n",
      "Epoch 239/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.0995 - accuracy: 0.9660 - val_loss: 0.1516 - val_accuracy: 0.9427\n",
      "Epoch 240/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0996 - accuracy: 0.9659 - val_loss: 0.1419 - val_accuracy: 0.9463\n",
      "Epoch 241/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.0990 - accuracy: 0.9660 - val_loss: 0.1415 - val_accuracy: 0.9466\n",
      "Epoch 242/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0989 - accuracy: 0.9663 - val_loss: 0.1411 - val_accuracy: 0.9469\n",
      "Epoch 243/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.0986 - accuracy: 0.9664 - val_loss: 0.1427 - val_accuracy: 0.9475\n",
      "Epoch 244/800\n",
      "1665/1665 [==============================] - 0s 173us/step - loss: 0.0984 - accuracy: 0.9666 - val_loss: 0.1545 - val_accuracy: 0.9416\n",
      "Epoch 245/800\n",
      "1665/1665 [==============================] - 0s 160us/step - loss: 0.0987 - accuracy: 0.9662 - val_loss: 0.1397 - val_accuracy: 0.9478\n",
      "Epoch 246/800\n",
      "1665/1665 [==============================] - 0s 159us/step - loss: 0.0978 - accuracy: 0.9666 - val_loss: 0.1411 - val_accuracy: 0.9473\n",
      "Epoch 247/800\n",
      "1665/1665 [==============================] - 0s 168us/step - loss: 0.0978 - accuracy: 0.9666 - val_loss: 0.1411 - val_accuracy: 0.9472\n",
      "Epoch 248/800\n",
      "1665/1665 [==============================] - 0s 155us/step - loss: 0.0977 - accuracy: 0.9668 - val_loss: 0.1411 - val_accuracy: 0.9475\n",
      "Epoch 249/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.0973 - accuracy: 0.9665 - val_loss: 0.1403 - val_accuracy: 0.9477\n",
      "Epoch 250/800\n",
      "1665/1665 [==============================] - 0s 174us/step - loss: 0.0970 - accuracy: 0.9668 - val_loss: 0.1400 - val_accuracy: 0.9481\n",
      "Epoch 251/800\n",
      "1665/1665 [==============================] - 0s 237us/step - loss: 0.0969 - accuracy: 0.9672 - val_loss: 0.1407 - val_accuracy: 0.9481\n",
      "Epoch 252/800\n",
      "1665/1665 [==============================] - 0s 219us/step - loss: 0.0967 - accuracy: 0.9672 - val_loss: 0.1406 - val_accuracy: 0.9478\n",
      "Epoch 253/800\n",
      "1665/1665 [==============================] - 0s 159us/step - loss: 0.0967 - accuracy: 0.9669 - val_loss: 0.1398 - val_accuracy: 0.9474\n",
      "Epoch 254/800\n",
      "1665/1665 [==============================] - 0s 174us/step - loss: 0.0963 - accuracy: 0.9671 - val_loss: 0.1400 - val_accuracy: 0.9477\n",
      "Epoch 255/800\n",
      "1665/1665 [==============================] - 0s 154us/step - loss: 0.0962 - accuracy: 0.9673 - val_loss: 0.1385 - val_accuracy: 0.9481\n",
      "Epoch 256/800\n",
      "1665/1665 [==============================] - 0s 174us/step - loss: 0.0958 - accuracy: 0.9674 - val_loss: 0.1414 - val_accuracy: 0.9477\n",
      "Epoch 257/800\n",
      "1665/1665 [==============================] - 0s 160us/step - loss: 0.0958 - accuracy: 0.9674 - val_loss: 0.1408 - val_accuracy: 0.9479\n",
      "Epoch 258/800\n",
      "1665/1665 [==============================] - 0s 201us/step - loss: 0.0957 - accuracy: 0.9674 - val_loss: 0.1404 - val_accuracy: 0.9473\n",
      "Epoch 259/800\n",
      "1665/1665 [==============================] - 0s 205us/step - loss: 0.0953 - accuracy: 0.9675 - val_loss: 0.1403 - val_accuracy: 0.9479\n",
      "Epoch 260/800\n",
      "1665/1665 [==============================] - 0s 203us/step - loss: 0.0952 - accuracy: 0.9678 - val_loss: 0.1384 - val_accuracy: 0.9482\n",
      "Epoch 261/800\n",
      "1665/1665 [==============================] - 0s 161us/step - loss: 0.0950 - accuracy: 0.9679 - val_loss: 0.1385 - val_accuracy: 0.9478\n",
      "Epoch 262/800\n",
      "1665/1665 [==============================] - 0s 170us/step - loss: 0.0948 - accuracy: 0.9677 - val_loss: 0.1402 - val_accuracy: 0.9471\n",
      "Epoch 263/800\n",
      "1665/1665 [==============================] - 0s 178us/step - loss: 0.0946 - accuracy: 0.9678 - val_loss: 0.1396 - val_accuracy: 0.9483\n",
      "Epoch 264/800\n",
      "1665/1665 [==============================] - 0s 133us/step - loss: 0.0944 - accuracy: 0.9678 - val_loss: 0.1487 - val_accuracy: 0.9443\n",
      "Epoch 265/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.0947 - accuracy: 0.9676 - val_loss: 0.1380 - val_accuracy: 0.9477\n",
      "Epoch 266/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.0941 - accuracy: 0.9681 - val_loss: 0.1387 - val_accuracy: 0.9482\n",
      "Epoch 267/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0940 - accuracy: 0.9681 - val_loss: 0.1383 - val_accuracy: 0.9485\n",
      "Epoch 268/800\n",
      "1665/1665 [==============================] - 0s 141us/step - loss: 0.0937 - accuracy: 0.9680 - val_loss: 0.1378 - val_accuracy: 0.9482\n",
      "Epoch 269/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0936 - accuracy: 0.9683 - val_loss: 0.1463 - val_accuracy: 0.9473\n",
      "Epoch 270/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0936 - accuracy: 0.9679 - val_loss: 0.1379 - val_accuracy: 0.9487\n",
      "Epoch 271/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0931 - accuracy: 0.9685 - val_loss: 0.1379 - val_accuracy: 0.9483\n",
      "Epoch 272/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0931 - accuracy: 0.9682 - val_loss: 0.1388 - val_accuracy: 0.9488\n",
      "Epoch 273/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0928 - accuracy: 0.9684 - val_loss: 0.1418 - val_accuracy: 0.9471\n",
      "Epoch 274/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0929 - accuracy: 0.9683 - val_loss: 0.1370 - val_accuracy: 0.9487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 275/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0926 - accuracy: 0.9686 - val_loss: 0.1370 - val_accuracy: 0.9489\n",
      "Epoch 276/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0924 - accuracy: 0.9684 - val_loss: 0.1447 - val_accuracy: 0.9453\n",
      "Epoch 277/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0926 - accuracy: 0.9687 - val_loss: 0.1379 - val_accuracy: 0.9490\n",
      "Epoch 278/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0922 - accuracy: 0.9684 - val_loss: 0.1374 - val_accuracy: 0.9488\n",
      "Epoch 279/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0919 - accuracy: 0.9686 - val_loss: 0.1364 - val_accuracy: 0.9488\n",
      "Epoch 280/800\n",
      "1665/1665 [==============================] - 0s 141us/step - loss: 0.0916 - accuracy: 0.9685 - val_loss: 0.1380 - val_accuracy: 0.9494\n",
      "Epoch 281/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0915 - accuracy: 0.9688 - val_loss: 0.1381 - val_accuracy: 0.9488\n",
      "Epoch 282/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0915 - accuracy: 0.9689 - val_loss: 0.1377 - val_accuracy: 0.9490\n",
      "Epoch 283/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0912 - accuracy: 0.9689 - val_loss: 0.1356 - val_accuracy: 0.9489\n",
      "Epoch 284/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0910 - accuracy: 0.9693 - val_loss: 0.1366 - val_accuracy: 0.9492\n",
      "Epoch 285/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.0909 - accuracy: 0.9691 - val_loss: 0.1362 - val_accuracy: 0.9488\n",
      "Epoch 286/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0907 - accuracy: 0.9690 - val_loss: 0.1366 - val_accuracy: 0.9492\n",
      "Epoch 287/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0906 - accuracy: 0.9693 - val_loss: 0.1362 - val_accuracy: 0.9495\n",
      "Epoch 288/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0903 - accuracy: 0.9692 - val_loss: 0.1530 - val_accuracy: 0.9448\n",
      "Epoch 289/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0908 - accuracy: 0.9690 - val_loss: 0.1364 - val_accuracy: 0.9492\n",
      "Epoch 290/800\n",
      "1665/1665 [==============================] - 0s 107us/step - loss: 0.0901 - accuracy: 0.9694 - val_loss: 0.1367 - val_accuracy: 0.9487\n",
      "Epoch 291/800\n",
      "1665/1665 [==============================] - 0s 109us/step - loss: 0.0898 - accuracy: 0.9694 - val_loss: 0.1349 - val_accuracy: 0.9492\n",
      "Epoch 292/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0897 - accuracy: 0.9694 - val_loss: 0.1421 - val_accuracy: 0.9489\n",
      "Epoch 293/800\n",
      "1665/1665 [==============================] - 0s 107us/step - loss: 0.0898 - accuracy: 0.9693 - val_loss: 0.1347 - val_accuracy: 0.9502\n",
      "Epoch 294/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0893 - accuracy: 0.9697 - val_loss: 0.1364 - val_accuracy: 0.9490\n",
      "Epoch 295/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.0893 - accuracy: 0.9697 - val_loss: 0.1370 - val_accuracy: 0.9486\n",
      "Epoch 296/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0892 - accuracy: 0.9698 - val_loss: 0.1354 - val_accuracy: 0.9500\n",
      "Epoch 297/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.0890 - accuracy: 0.9697 - val_loss: 0.1342 - val_accuracy: 0.9500\n",
      "Epoch 298/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.0889 - accuracy: 0.9697 - val_loss: 0.1345 - val_accuracy: 0.9493\n",
      "Epoch 299/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.0887 - accuracy: 0.9697 - val_loss: 0.1340 - val_accuracy: 0.9501\n",
      "Epoch 300/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0885 - accuracy: 0.9699 - val_loss: 0.1355 - val_accuracy: 0.9498\n",
      "Epoch 301/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.0884 - accuracy: 0.9699 - val_loss: 0.1352 - val_accuracy: 0.9491\n",
      "Epoch 302/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.0883 - accuracy: 0.9701 - val_loss: 0.1358 - val_accuracy: 0.9492\n",
      "Epoch 303/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0881 - accuracy: 0.9701 - val_loss: 0.1348 - val_accuracy: 0.9499\n",
      "Epoch 304/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.0879 - accuracy: 0.9701 - val_loss: 0.1351 - val_accuracy: 0.9502\n",
      "Epoch 305/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0877 - accuracy: 0.9700 - val_loss: 0.1326 - val_accuracy: 0.9505\n",
      "Epoch 306/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0876 - accuracy: 0.9701 - val_loss: 0.1342 - val_accuracy: 0.9504\n",
      "Epoch 307/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.0874 - accuracy: 0.9702 - val_loss: 0.1409 - val_accuracy: 0.9474\n",
      "Epoch 308/800\n",
      "1665/1665 [==============================] - 0s 109us/step - loss: 0.0877 - accuracy: 0.9701 - val_loss: 0.1346 - val_accuracy: 0.9505\n",
      "Epoch 309/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.0871 - accuracy: 0.9702 - val_loss: 0.1361 - val_accuracy: 0.9506\n",
      "Epoch 310/800\n",
      "1665/1665 [==============================] - 0s 109us/step - loss: 0.0871 - accuracy: 0.9703 - val_loss: 0.1356 - val_accuracy: 0.9500\n",
      "Epoch 311/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0869 - accuracy: 0.9704 - val_loss: 0.1336 - val_accuracy: 0.9501\n",
      "Epoch 312/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.0868 - accuracy: 0.9703 - val_loss: 0.1341 - val_accuracy: 0.9494\n",
      "Epoch 313/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.0867 - accuracy: 0.9704 - val_loss: 0.1345 - val_accuracy: 0.9499\n",
      "Epoch 314/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0865 - accuracy: 0.9706 - val_loss: 0.1350 - val_accuracy: 0.9500\n",
      "Epoch 315/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0865 - accuracy: 0.9704 - val_loss: 0.1347 - val_accuracy: 0.9495\n",
      "Epoch 316/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0862 - accuracy: 0.9706 - val_loss: 0.1345 - val_accuracy: 0.9505\n",
      "Epoch 317/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0863 - accuracy: 0.9704 - val_loss: 0.1331 - val_accuracy: 0.9511\n",
      "Epoch 318/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0860 - accuracy: 0.9707 - val_loss: 0.1344 - val_accuracy: 0.9508\n",
      "Epoch 319/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.0858 - accuracy: 0.9706 - val_loss: 0.1326 - val_accuracy: 0.9508\n",
      "Epoch 320/800\n",
      "1665/1665 [==============================] - 0s 109us/step - loss: 0.0857 - accuracy: 0.9708 - val_loss: 0.1329 - val_accuracy: 0.9509\n",
      "Epoch 321/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0856 - accuracy: 0.9708 - val_loss: 0.1366 - val_accuracy: 0.9499\n",
      "Epoch 322/800\n",
      "1665/1665 [==============================] - 0s 109us/step - loss: 0.0856 - accuracy: 0.9707 - val_loss: 0.1335 - val_accuracy: 0.9499\n",
      "Epoch 323/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.0854 - accuracy: 0.9708 - val_loss: 0.1317 - val_accuracy: 0.9509\n",
      "Epoch 324/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0851 - accuracy: 0.9711 - val_loss: 0.1347 - val_accuracy: 0.9482\n",
      "Epoch 325/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0854 - accuracy: 0.9708 - val_loss: 0.1329 - val_accuracy: 0.9508\n",
      "Epoch 326/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0850 - accuracy: 0.9709 - val_loss: 0.1408 - val_accuracy: 0.9474\n",
      "Epoch 327/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.0853 - accuracy: 0.9710 - val_loss: 0.1324 - val_accuracy: 0.9513\n",
      "Epoch 328/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.0846 - accuracy: 0.9712 - val_loss: 0.1328 - val_accuracy: 0.9509\n",
      "Epoch 329/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0846 - accuracy: 0.9713 - val_loss: 0.1331 - val_accuracy: 0.9508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 330/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0844 - accuracy: 0.9708 - val_loss: 0.1316 - val_accuracy: 0.9508\n",
      "Epoch 331/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.0842 - accuracy: 0.9711 - val_loss: 0.1330 - val_accuracy: 0.9514\n",
      "Epoch 332/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0843 - accuracy: 0.9711 - val_loss: 0.1324 - val_accuracy: 0.9505\n",
      "Epoch 333/800\n",
      "1665/1665 [==============================] - 0s 112us/step - loss: 0.0841 - accuracy: 0.9713 - val_loss: 0.1310 - val_accuracy: 0.9514\n",
      "Epoch 334/800\n",
      "1665/1665 [==============================] - 0s 109us/step - loss: 0.0838 - accuracy: 0.9711 - val_loss: 0.1321 - val_accuracy: 0.9517\n",
      "Epoch 335/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.0837 - accuracy: 0.9712 - val_loss: 0.1323 - val_accuracy: 0.9508\n",
      "Epoch 336/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.0837 - accuracy: 0.9710 - val_loss: 0.1326 - val_accuracy: 0.9515\n",
      "Epoch 337/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0836 - accuracy: 0.9713 - val_loss: 0.1340 - val_accuracy: 0.9508\n",
      "Epoch 338/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.0834 - accuracy: 0.9715 - val_loss: 0.1326 - val_accuracy: 0.9509\n",
      "Epoch 339/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.0833 - accuracy: 0.9714 - val_loss: 0.1313 - val_accuracy: 0.9513\n",
      "Epoch 340/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0834 - accuracy: 0.9717 - val_loss: 0.1494 - val_accuracy: 0.9455\n",
      "Epoch 341/800\n",
      "1665/1665 [==============================] - 0s 129us/step - loss: 0.0839 - accuracy: 0.9710 - val_loss: 0.1453 - val_accuracy: 0.9460\n",
      "Epoch 342/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0837 - accuracy: 0.9714 - val_loss: 0.1318 - val_accuracy: 0.9506\n",
      "Epoch 343/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0830 - accuracy: 0.9716 - val_loss: 0.1314 - val_accuracy: 0.9501\n",
      "Epoch 344/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0829 - accuracy: 0.9712 - val_loss: 0.1312 - val_accuracy: 0.9514\n",
      "Epoch 345/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0826 - accuracy: 0.9716 - val_loss: 0.1540 - val_accuracy: 0.9450\n",
      "Epoch 346/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0836 - accuracy: 0.9714 - val_loss: 0.1334 - val_accuracy: 0.9509\n",
      "Epoch 347/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.0827 - accuracy: 0.9717 - val_loss: 0.1311 - val_accuracy: 0.9511\n",
      "Epoch 348/800\n",
      "1665/1665 [==============================] - 0s 109us/step - loss: 0.0822 - accuracy: 0.9718 - val_loss: 0.1315 - val_accuracy: 0.9503\n",
      "Epoch 349/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.0822 - accuracy: 0.9718 - val_loss: 0.1324 - val_accuracy: 0.9520\n",
      "Epoch 350/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0820 - accuracy: 0.9720 - val_loss: 0.1301 - val_accuracy: 0.9519\n",
      "Epoch 351/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.0819 - accuracy: 0.9719 - val_loss: 0.1314 - val_accuracy: 0.9513\n",
      "Epoch 352/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.0819 - accuracy: 0.9718 - val_loss: 0.1310 - val_accuracy: 0.9510\n",
      "Epoch 353/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0816 - accuracy: 0.9717 - val_loss: 0.1297 - val_accuracy: 0.9515\n",
      "Epoch 354/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.0817 - accuracy: 0.9720 - val_loss: 0.1297 - val_accuracy: 0.9513\n",
      "Epoch 355/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0814 - accuracy: 0.9720 - val_loss: 0.1316 - val_accuracy: 0.9512\n",
      "Epoch 356/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0813 - accuracy: 0.9719 - val_loss: 0.1302 - val_accuracy: 0.9512\n",
      "Epoch 357/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0811 - accuracy: 0.9721 - val_loss: 0.1362 - val_accuracy: 0.9485\n",
      "Epoch 358/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0815 - accuracy: 0.9718 - val_loss: 0.1297 - val_accuracy: 0.9513\n",
      "Epoch 359/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.0810 - accuracy: 0.9718 - val_loss: 0.1329 - val_accuracy: 0.9525\n",
      "Epoch 360/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0811 - accuracy: 0.9722 - val_loss: 0.1297 - val_accuracy: 0.9517\n",
      "Epoch 361/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.0809 - accuracy: 0.9721 - val_loss: 0.1468 - val_accuracy: 0.9449\n",
      "Epoch 362/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.0815 - accuracy: 0.9720 - val_loss: 0.1301 - val_accuracy: 0.9516\n",
      "Epoch 363/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.0807 - accuracy: 0.9723 - val_loss: 0.1298 - val_accuracy: 0.9513\n",
      "Epoch 364/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0807 - accuracy: 0.9725 - val_loss: 0.1303 - val_accuracy: 0.9523\n",
      "Epoch 365/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.0805 - accuracy: 0.9723 - val_loss: 0.1317 - val_accuracy: 0.9520\n",
      "Epoch 366/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.0805 - accuracy: 0.9719 - val_loss: 0.1298 - val_accuracy: 0.9523\n",
      "Epoch 367/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0803 - accuracy: 0.9723 - val_loss: 0.1313 - val_accuracy: 0.9514\n",
      "Epoch 368/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.0804 - accuracy: 0.9723 - val_loss: 0.1309 - val_accuracy: 0.9514\n",
      "Epoch 369/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0801 - accuracy: 0.9725 - val_loss: 0.1289 - val_accuracy: 0.9515\n",
      "Epoch 370/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.0800 - accuracy: 0.9724 - val_loss: 0.1301 - val_accuracy: 0.9516\n",
      "Epoch 371/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.0799 - accuracy: 0.9722 - val_loss: 0.1392 - val_accuracy: 0.9486\n",
      "Epoch 372/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0802 - accuracy: 0.9720 - val_loss: 0.1290 - val_accuracy: 0.9526\n",
      "Epoch 373/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.0796 - accuracy: 0.9726 - val_loss: 0.1301 - val_accuracy: 0.9513\n",
      "Epoch 374/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0797 - accuracy: 0.9726 - val_loss: 0.1291 - val_accuracy: 0.9526\n",
      "Epoch 375/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.0794 - accuracy: 0.9727 - val_loss: 0.1286 - val_accuracy: 0.9519\n",
      "Epoch 376/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.0794 - accuracy: 0.9724 - val_loss: 0.1284 - val_accuracy: 0.9514\n",
      "Epoch 377/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.0794 - accuracy: 0.9725 - val_loss: 0.1267 - val_accuracy: 0.9527\n",
      "Epoch 378/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0793 - accuracy: 0.9728 - val_loss: 0.1283 - val_accuracy: 0.9523\n",
      "Epoch 379/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.0790 - accuracy: 0.9728 - val_loss: 0.1289 - val_accuracy: 0.9518\n",
      "Epoch 380/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0789 - accuracy: 0.9725 - val_loss: 0.1281 - val_accuracy: 0.9530\n",
      "Epoch 381/800\n",
      "1665/1665 [==============================] - 0s 112us/step - loss: 0.0787 - accuracy: 0.9727 - val_loss: 0.1281 - val_accuracy: 0.9521\n",
      "Epoch 382/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0788 - accuracy: 0.9727 - val_loss: 0.1280 - val_accuracy: 0.9523\n",
      "Epoch 383/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0787 - accuracy: 0.9727 - val_loss: 0.1298 - val_accuracy: 0.9523\n",
      "Epoch 384/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.0786 - accuracy: 0.9725 - val_loss: 0.1310 - val_accuracy: 0.9522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 385/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0785 - accuracy: 0.9727 - val_loss: 0.1324 - val_accuracy: 0.9497\n",
      "Epoch 386/800\n",
      "1665/1665 [==============================] - 0s 112us/step - loss: 0.0788 - accuracy: 0.9730 - val_loss: 0.1283 - val_accuracy: 0.9520\n",
      "Epoch 387/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.0783 - accuracy: 0.9730 - val_loss: 0.1286 - val_accuracy: 0.9530\n",
      "Epoch 388/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0782 - accuracy: 0.9730 - val_loss: 0.1277 - val_accuracy: 0.9530\n",
      "Epoch 389/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0782 - accuracy: 0.9728 - val_loss: 0.1289 - val_accuracy: 0.9522\n",
      "Epoch 390/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0779 - accuracy: 0.9729 - val_loss: 0.1304 - val_accuracy: 0.9522\n",
      "Epoch 391/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.0782 - accuracy: 0.9731 - val_loss: 0.1273 - val_accuracy: 0.9530\n",
      "Epoch 392/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0778 - accuracy: 0.9730 - val_loss: 0.1279 - val_accuracy: 0.9534\n",
      "Epoch 393/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0777 - accuracy: 0.9732 - val_loss: 0.1273 - val_accuracy: 0.9534\n",
      "Epoch 394/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0778 - accuracy: 0.9731 - val_loss: 0.1352 - val_accuracy: 0.9497\n",
      "Epoch 395/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.0779 - accuracy: 0.9730 - val_loss: 0.1491 - val_accuracy: 0.9438\n",
      "Epoch 396/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.0785 - accuracy: 0.9727 - val_loss: 0.1281 - val_accuracy: 0.9525\n",
      "Epoch 397/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0777 - accuracy: 0.9731 - val_loss: 0.1299 - val_accuracy: 0.9519\n",
      "Epoch 398/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0775 - accuracy: 0.9730 - val_loss: 0.1269 - val_accuracy: 0.9531\n",
      "Epoch 399/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.0774 - accuracy: 0.9731 - val_loss: 0.1287 - val_accuracy: 0.9520\n",
      "Epoch 400/800\n",
      "1665/1665 [==============================] - 0s 112us/step - loss: 0.0773 - accuracy: 0.9734 - val_loss: 0.1288 - val_accuracy: 0.9528\n",
      "Epoch 401/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.0771 - accuracy: 0.9731 - val_loss: 0.1272 - val_accuracy: 0.9533\n",
      "Epoch 402/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0771 - accuracy: 0.9733 - val_loss: 0.1280 - val_accuracy: 0.9531\n",
      "Epoch 403/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0770 - accuracy: 0.9736 - val_loss: 0.1302 - val_accuracy: 0.9519\n",
      "Epoch 404/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0772 - accuracy: 0.9732 - val_loss: 0.1283 - val_accuracy: 0.9519\n",
      "Epoch 405/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0768 - accuracy: 0.9731 - val_loss: 0.1300 - val_accuracy: 0.9512\n",
      "Epoch 406/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.0771 - accuracy: 0.9729 - val_loss: 0.1285 - val_accuracy: 0.9524\n",
      "Epoch 407/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.0767 - accuracy: 0.9732 - val_loss: 0.1276 - val_accuracy: 0.9523\n",
      "Epoch 408/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0767 - accuracy: 0.9734 - val_loss: 0.1285 - val_accuracy: 0.9526\n",
      "Epoch 409/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0766 - accuracy: 0.9734 - val_loss: 0.1276 - val_accuracy: 0.9527\n",
      "Epoch 410/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.0765 - accuracy: 0.9733 - val_loss: 0.1268 - val_accuracy: 0.9537\n",
      "Epoch 411/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.0766 - accuracy: 0.9732 - val_loss: 0.1262 - val_accuracy: 0.9533\n",
      "Epoch 412/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0762 - accuracy: 0.9735 - val_loss: 0.1294 - val_accuracy: 0.9518\n",
      "Epoch 413/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0765 - accuracy: 0.9732 - val_loss: 0.1278 - val_accuracy: 0.9528\n",
      "Epoch 414/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0761 - accuracy: 0.9734 - val_loss: 0.1272 - val_accuracy: 0.9536\n",
      "Epoch 415/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0760 - accuracy: 0.9735 - val_loss: 0.1305 - val_accuracy: 0.9517\n",
      "Epoch 416/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0762 - accuracy: 0.9737 - val_loss: 0.1269 - val_accuracy: 0.9532\n",
      "Epoch 417/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0758 - accuracy: 0.9734 - val_loss: 0.1260 - val_accuracy: 0.9534\n",
      "Epoch 418/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.0756 - accuracy: 0.9736 - val_loss: 0.1356 - val_accuracy: 0.9517\n",
      "Epoch 419/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.0761 - accuracy: 0.9733 - val_loss: 0.1284 - val_accuracy: 0.9534\n",
      "Epoch 420/800\n",
      "1665/1665 [==============================] - 0s 126us/step - loss: 0.0757 - accuracy: 0.9737 - val_loss: 0.1280 - val_accuracy: 0.9531\n",
      "Epoch 421/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0757 - accuracy: 0.9736 - val_loss: 0.1250 - val_accuracy: 0.9539\n",
      "Epoch 422/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.0755 - accuracy: 0.9738 - val_loss: 0.1262 - val_accuracy: 0.9537\n",
      "Epoch 423/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0753 - accuracy: 0.9738 - val_loss: 0.1263 - val_accuracy: 0.9537\n",
      "Epoch 424/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0755 - accuracy: 0.9737 - val_loss: 0.1262 - val_accuracy: 0.9538\n",
      "Epoch 425/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.0751 - accuracy: 0.9738 - val_loss: 0.1267 - val_accuracy: 0.9531\n",
      "Epoch 426/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.0752 - accuracy: 0.9737 - val_loss: 0.1255 - val_accuracy: 0.9540\n",
      "Epoch 427/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0750 - accuracy: 0.9736 - val_loss: 0.1324 - val_accuracy: 0.9500\n",
      "Epoch 428/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0755 - accuracy: 0.9735 - val_loss: 0.1329 - val_accuracy: 0.9515\n",
      "Epoch 429/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0752 - accuracy: 0.9741 - val_loss: 0.1257 - val_accuracy: 0.9541\n",
      "Epoch 430/800\n",
      "1665/1665 [==============================] - 0s 112us/step - loss: 0.0749 - accuracy: 0.9740 - val_loss: 0.1257 - val_accuracy: 0.9539\n",
      "Epoch 431/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.0747 - accuracy: 0.9738 - val_loss: 0.1251 - val_accuracy: 0.9537\n",
      "Epoch 432/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0748 - accuracy: 0.9737 - val_loss: 0.1278 - val_accuracy: 0.9547\n",
      "Epoch 433/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0747 - accuracy: 0.9740 - val_loss: 0.1270 - val_accuracy: 0.9537\n",
      "Epoch 434/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0746 - accuracy: 0.9738 - val_loss: 0.1262 - val_accuracy: 0.9539\n",
      "Epoch 435/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.0745 - accuracy: 0.9741 - val_loss: 0.1345 - val_accuracy: 0.9528\n",
      "Epoch 436/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.0748 - accuracy: 0.9740 - val_loss: 0.1270 - val_accuracy: 0.9534\n",
      "Epoch 437/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0744 - accuracy: 0.9736 - val_loss: 0.1256 - val_accuracy: 0.9537\n",
      "Epoch 438/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.0742 - accuracy: 0.9740 - val_loss: 0.1293 - val_accuracy: 0.9524\n",
      "Epoch 439/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0745 - accuracy: 0.9738 - val_loss: 0.1239 - val_accuracy: 0.9549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 440/800\n",
      "1665/1665 [==============================] - 0s 142us/step - loss: 0.0740 - accuracy: 0.9740 - val_loss: 0.1266 - val_accuracy: 0.9540\n",
      "Epoch 441/800\n",
      "1665/1665 [==============================] - 0s 185us/step - loss: 0.0740 - accuracy: 0.9739 - val_loss: 0.1256 - val_accuracy: 0.9534\n",
      "Epoch 442/800\n",
      "1665/1665 [==============================] - 0s 156us/step - loss: 0.0740 - accuracy: 0.9741 - val_loss: 0.1248 - val_accuracy: 0.9542\n",
      "Epoch 443/800\n",
      "1665/1665 [==============================] - 0s 164us/step - loss: 0.0738 - accuracy: 0.9742 - val_loss: 0.1251 - val_accuracy: 0.9545\n",
      "Epoch 444/800\n",
      "1665/1665 [==============================] - 0s 153us/step - loss: 0.0740 - accuracy: 0.9739 - val_loss: 0.1254 - val_accuracy: 0.9532\n",
      "Epoch 445/800\n",
      "1665/1665 [==============================] - 0s 129us/step - loss: 0.0739 - accuracy: 0.9740 - val_loss: 0.1249 - val_accuracy: 0.9531\n",
      "Epoch 446/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0737 - accuracy: 0.9742 - val_loss: 0.1257 - val_accuracy: 0.9536\n",
      "Epoch 447/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0735 - accuracy: 0.9741 - val_loss: 0.1280 - val_accuracy: 0.9544\n",
      "Epoch 448/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0737 - accuracy: 0.9741 - val_loss: 0.1268 - val_accuracy: 0.9538\n",
      "Epoch 449/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0733 - accuracy: 0.9740 - val_loss: 0.1257 - val_accuracy: 0.9541\n",
      "Epoch 450/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0735 - accuracy: 0.9741 - val_loss: 0.1253 - val_accuracy: 0.9539\n",
      "Epoch 451/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.0733 - accuracy: 0.9742 - val_loss: 0.1246 - val_accuracy: 0.9543\n",
      "Epoch 452/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0731 - accuracy: 0.9743 - val_loss: 0.1245 - val_accuracy: 0.9545\n",
      "Epoch 453/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0730 - accuracy: 0.9742 - val_loss: 0.1239 - val_accuracy: 0.9551\n",
      "Epoch 454/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.0731 - accuracy: 0.9738 - val_loss: 0.1251 - val_accuracy: 0.9541\n",
      "Epoch 455/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0730 - accuracy: 0.9742 - val_loss: 0.1250 - val_accuracy: 0.9546\n",
      "Epoch 456/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0729 - accuracy: 0.9742 - val_loss: 0.1257 - val_accuracy: 0.9542\n",
      "Epoch 457/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0729 - accuracy: 0.9741 - val_loss: 0.1311 - val_accuracy: 0.9505\n",
      "Epoch 458/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0731 - accuracy: 0.9739 - val_loss: 0.1317 - val_accuracy: 0.9518\n",
      "Epoch 459/800\n",
      "1665/1665 [==============================] - 0s 108us/step - loss: 0.0729 - accuracy: 0.9742 - val_loss: 0.1289 - val_accuracy: 0.9532\n",
      "Epoch 460/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.0728 - accuracy: 0.9742 - val_loss: 0.1251 - val_accuracy: 0.9535\n",
      "Epoch 461/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0727 - accuracy: 0.9745 - val_loss: 0.1241 - val_accuracy: 0.9545\n",
      "Epoch 462/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0726 - accuracy: 0.9745 - val_loss: 0.1249 - val_accuracy: 0.9541\n",
      "Epoch 463/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0724 - accuracy: 0.9745 - val_loss: 0.1241 - val_accuracy: 0.9548\n",
      "Epoch 464/800\n",
      "1665/1665 [==============================] - 0s 109us/step - loss: 0.0722 - accuracy: 0.9745 - val_loss: 0.1254 - val_accuracy: 0.9536\n",
      "Epoch 465/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.0725 - accuracy: 0.9739 - val_loss: 0.1241 - val_accuracy: 0.9544\n",
      "Epoch 466/800\n",
      "1665/1665 [==============================] - 0s 112us/step - loss: 0.0724 - accuracy: 0.9744 - val_loss: 0.1234 - val_accuracy: 0.9545\n",
      "Epoch 467/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0721 - accuracy: 0.9744 - val_loss: 0.1252 - val_accuracy: 0.9549\n",
      "Epoch 468/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0719 - accuracy: 0.9744 - val_loss: 0.1251 - val_accuracy: 0.9542\n",
      "Epoch 469/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0720 - accuracy: 0.9744 - val_loss: 0.1244 - val_accuracy: 0.9539\n",
      "Epoch 470/800\n",
      "1665/1665 [==============================] - 0s 112us/step - loss: 0.0718 - accuracy: 0.9744 - val_loss: 0.1236 - val_accuracy: 0.9545\n",
      "Epoch 471/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0718 - accuracy: 0.9742 - val_loss: 0.1236 - val_accuracy: 0.9551\n",
      "Epoch 472/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0717 - accuracy: 0.9745 - val_loss: 0.1258 - val_accuracy: 0.9537\n",
      "Epoch 473/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0717 - accuracy: 0.9744 - val_loss: 0.1310 - val_accuracy: 0.9528\n",
      "Epoch 474/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0720 - accuracy: 0.9744 - val_loss: 0.1248 - val_accuracy: 0.9544\n",
      "Epoch 475/800\n",
      "1665/1665 [==============================] - 0s 109us/step - loss: 0.0715 - accuracy: 0.9746 - val_loss: 0.1237 - val_accuracy: 0.9546\n",
      "Epoch 476/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0716 - accuracy: 0.9744 - val_loss: 0.1273 - val_accuracy: 0.9532\n",
      "Epoch 477/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0717 - accuracy: 0.9744 - val_loss: 0.1236 - val_accuracy: 0.9547\n",
      "Epoch 478/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.0712 - accuracy: 0.9746 - val_loss: 0.1230 - val_accuracy: 0.9540\n",
      "Epoch 479/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0713 - accuracy: 0.9746 - val_loss: 0.1247 - val_accuracy: 0.9548\n",
      "Epoch 480/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0712 - accuracy: 0.9745 - val_loss: 0.1248 - val_accuracy: 0.9542\n",
      "Epoch 481/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.0712 - accuracy: 0.9746 - val_loss: 0.1247 - val_accuracy: 0.9544\n",
      "Epoch 482/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0712 - accuracy: 0.9748 - val_loss: 0.1335 - val_accuracy: 0.9534\n",
      "Epoch 483/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0714 - accuracy: 0.9744 - val_loss: 0.1244 - val_accuracy: 0.9540\n",
      "Epoch 484/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0709 - accuracy: 0.9744 - val_loss: 0.1366 - val_accuracy: 0.9502\n",
      "Epoch 485/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0716 - accuracy: 0.9744 - val_loss: 0.1471 - val_accuracy: 0.9469\n",
      "Epoch 486/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.0717 - accuracy: 0.9743 - val_loss: 0.1241 - val_accuracy: 0.9542\n",
      "Epoch 487/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.0707 - accuracy: 0.9745 - val_loss: 0.1231 - val_accuracy: 0.9542\n",
      "Epoch 488/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0708 - accuracy: 0.9747 - val_loss: 0.1319 - val_accuracy: 0.9515\n",
      "Epoch 489/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0712 - accuracy: 0.9744 - val_loss: 0.1286 - val_accuracy: 0.9525\n",
      "Epoch 490/800\n",
      "1665/1665 [==============================] - 0s 145us/step - loss: 0.0710 - accuracy: 0.9745 - val_loss: 0.1343 - val_accuracy: 0.9502\n",
      "Epoch 491/800\n",
      "1665/1665 [==============================] - 0s 165us/step - loss: 0.0710 - accuracy: 0.9743 - val_loss: 0.1230 - val_accuracy: 0.9552\n",
      "Epoch 492/800\n",
      "1665/1665 [==============================] - 0s 147us/step - loss: 0.0705 - accuracy: 0.9746 - val_loss: 0.1230 - val_accuracy: 0.9546\n",
      "Epoch 493/800\n",
      "1665/1665 [==============================] - 0s 140us/step - loss: 0.0703 - accuracy: 0.9748 - val_loss: 0.1239 - val_accuracy: 0.9543\n",
      "Epoch 494/800\n",
      "1665/1665 [==============================] - 0s 161us/step - loss: 0.0703 - accuracy: 0.9747 - val_loss: 0.1235 - val_accuracy: 0.9551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 495/800\n",
      "1665/1665 [==============================] - 0s 168us/step - loss: 0.0703 - accuracy: 0.9749 - val_loss: 0.1252 - val_accuracy: 0.9542\n",
      "Epoch 496/800\n",
      "1665/1665 [==============================] - 0s 155us/step - loss: 0.0704 - accuracy: 0.9747 - val_loss: 0.1214 - val_accuracy: 0.9551\n",
      "Epoch 497/800\n",
      "1665/1665 [==============================] - 0s 139us/step - loss: 0.0701 - accuracy: 0.9746 - val_loss: 0.1234 - val_accuracy: 0.9553\n",
      "Epoch 498/800\n",
      "1665/1665 [==============================] - 0s 148us/step - loss: 0.0700 - accuracy: 0.9749 - val_loss: 0.1240 - val_accuracy: 0.9544\n",
      "Epoch 499/800\n",
      "1665/1665 [==============================] - 0s 175us/step - loss: 0.0699 - accuracy: 0.9749 - val_loss: 0.1206 - val_accuracy: 0.9556\n",
      "Epoch 500/800\n",
      "1665/1665 [==============================] - 0s 166us/step - loss: 0.0700 - accuracy: 0.9749 - val_loss: 0.1238 - val_accuracy: 0.9546\n",
      "Epoch 501/800\n",
      "1665/1665 [==============================] - 0s 165us/step - loss: 0.0698 - accuracy: 0.9748 - val_loss: 0.1238 - val_accuracy: 0.9542\n",
      "Epoch 502/800\n",
      "1665/1665 [==============================] - 0s 164us/step - loss: 0.0697 - accuracy: 0.9749 - val_loss: 0.1232 - val_accuracy: 0.9554\n",
      "Epoch 503/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.0699 - accuracy: 0.9751 - val_loss: 0.1232 - val_accuracy: 0.9551\n",
      "Epoch 504/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.0696 - accuracy: 0.9749 - val_loss: 0.1245 - val_accuracy: 0.9538\n",
      "Epoch 505/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0696 - accuracy: 0.9748 - val_loss: 0.1408 - val_accuracy: 0.9512\n",
      "Epoch 506/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0705 - accuracy: 0.9748 - val_loss: 0.1345 - val_accuracy: 0.9519\n",
      "Epoch 507/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0700 - accuracy: 0.9747 - val_loss: 0.1237 - val_accuracy: 0.9553\n",
      "Epoch 508/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0695 - accuracy: 0.9750 - val_loss: 0.1233 - val_accuracy: 0.9545\n",
      "Epoch 509/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0695 - accuracy: 0.9751 - val_loss: 0.1258 - val_accuracy: 0.9535\n",
      "Epoch 510/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0696 - accuracy: 0.9749 - val_loss: 0.1209 - val_accuracy: 0.9550\n",
      "Epoch 511/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0692 - accuracy: 0.9749 - val_loss: 0.1223 - val_accuracy: 0.9549\n",
      "Epoch 512/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0692 - accuracy: 0.9750 - val_loss: 0.1223 - val_accuracy: 0.9555\n",
      "Epoch 513/800\n",
      "1665/1665 [==============================] - 0s 112us/step - loss: 0.0694 - accuracy: 0.9752 - val_loss: 0.1256 - val_accuracy: 0.9542\n",
      "Epoch 514/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0689 - accuracy: 0.9748 - val_loss: 0.1277 - val_accuracy: 0.9537\n",
      "Epoch 515/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0690 - accuracy: 0.9747 - val_loss: 0.1234 - val_accuracy: 0.9549\n",
      "Epoch 516/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0688 - accuracy: 0.9753 - val_loss: 0.1236 - val_accuracy: 0.9550\n",
      "Epoch 517/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0687 - accuracy: 0.9750 - val_loss: 0.1239 - val_accuracy: 0.9551\n",
      "Epoch 518/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0690 - accuracy: 0.9752 - val_loss: 0.1234 - val_accuracy: 0.9550\n",
      "Epoch 519/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0687 - accuracy: 0.9750 - val_loss: 0.1272 - val_accuracy: 0.9528\n",
      "Epoch 520/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0689 - accuracy: 0.9749 - val_loss: 0.1224 - val_accuracy: 0.9552\n",
      "Epoch 521/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0684 - accuracy: 0.9754 - val_loss: 0.1240 - val_accuracy: 0.9548\n",
      "Epoch 522/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0685 - accuracy: 0.9750 - val_loss: 0.1224 - val_accuracy: 0.9552\n",
      "Epoch 523/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0684 - accuracy: 0.9753 - val_loss: 0.1234 - val_accuracy: 0.9551\n",
      "Epoch 524/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.0685 - accuracy: 0.9752 - val_loss: 0.1277 - val_accuracy: 0.9541\n",
      "Epoch 525/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0687 - accuracy: 0.9750 - val_loss: 0.1217 - val_accuracy: 0.9552\n",
      "Epoch 526/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.0681 - accuracy: 0.9752 - val_loss: 0.1242 - val_accuracy: 0.9552\n",
      "Epoch 527/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0681 - accuracy: 0.9752 - val_loss: 0.1223 - val_accuracy: 0.9561\n",
      "Epoch 528/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0681 - accuracy: 0.9752 - val_loss: 0.1220 - val_accuracy: 0.9555\n",
      "Epoch 529/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0680 - accuracy: 0.9750 - val_loss: 0.1206 - val_accuracy: 0.9560\n",
      "Epoch 530/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.0678 - accuracy: 0.9753 - val_loss: 0.1223 - val_accuracy: 0.9551\n",
      "Epoch 531/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0678 - accuracy: 0.9754 - val_loss: 0.1227 - val_accuracy: 0.9549\n",
      "Epoch 532/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0680 - accuracy: 0.9752 - val_loss: 0.1221 - val_accuracy: 0.9557\n",
      "Epoch 533/800\n",
      "1665/1665 [==============================] - 0s 185us/step - loss: 0.0677 - accuracy: 0.9752 - val_loss: 0.1245 - val_accuracy: 0.9543\n",
      "Epoch 534/800\n",
      "1665/1665 [==============================] - 0s 196us/step - loss: 0.0678 - accuracy: 0.9754 - val_loss: 0.1209 - val_accuracy: 0.9562\n",
      "Epoch 535/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.0676 - accuracy: 0.9753 - val_loss: 0.1222 - val_accuracy: 0.9554\n",
      "Epoch 536/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0675 - accuracy: 0.9754 - val_loss: 0.1246 - val_accuracy: 0.9546\n",
      "Epoch 537/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0677 - accuracy: 0.9754 - val_loss: 0.1267 - val_accuracy: 0.9550\n",
      "Epoch 538/800\n",
      "1665/1665 [==============================] - 0s 158us/step - loss: 0.0675 - accuracy: 0.9755 - val_loss: 0.1229 - val_accuracy: 0.9547\n",
      "Epoch 539/800\n",
      "1665/1665 [==============================] - 0s 159us/step - loss: 0.0675 - accuracy: 0.9754 - val_loss: 0.1219 - val_accuracy: 0.9560\n",
      "Epoch 540/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0672 - accuracy: 0.9755 - val_loss: 0.1212 - val_accuracy: 0.9553\n",
      "Epoch 541/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0671 - accuracy: 0.9755 - val_loss: 0.1447 - val_accuracy: 0.9497\n",
      "Epoch 542/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0687 - accuracy: 0.9748 - val_loss: 0.1203 - val_accuracy: 0.9554\n",
      "Epoch 543/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0672 - accuracy: 0.9753 - val_loss: 0.1215 - val_accuracy: 0.9554\n",
      "Epoch 544/800\n",
      "1665/1665 [==============================] - 0s 112us/step - loss: 0.0670 - accuracy: 0.9756 - val_loss: 0.1226 - val_accuracy: 0.9560\n",
      "Epoch 545/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0670 - accuracy: 0.9757 - val_loss: 0.1221 - val_accuracy: 0.9546\n",
      "Epoch 546/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0671 - accuracy: 0.9755 - val_loss: 0.1214 - val_accuracy: 0.9557\n",
      "Epoch 547/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0670 - accuracy: 0.9756 - val_loss: 0.1252 - val_accuracy: 0.9542\n",
      "Epoch 548/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0669 - accuracy: 0.9756 - val_loss: 0.1209 - val_accuracy: 0.9555\n",
      "Epoch 549/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.0668 - accuracy: 0.9754 - val_loss: 0.1215 - val_accuracy: 0.9558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 550/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0666 - accuracy: 0.9756 - val_loss: 0.1210 - val_accuracy: 0.9561\n",
      "Epoch 551/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0667 - accuracy: 0.9756 - val_loss: 0.1206 - val_accuracy: 0.9558\n",
      "Epoch 552/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0664 - accuracy: 0.9758 - val_loss: 0.1202 - val_accuracy: 0.9557\n",
      "Epoch 553/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0663 - accuracy: 0.9756 - val_loss: 0.1219 - val_accuracy: 0.9562\n",
      "Epoch 554/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.0665 - accuracy: 0.9758 - val_loss: 0.1215 - val_accuracy: 0.9558\n",
      "Epoch 555/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0662 - accuracy: 0.9759 - val_loss: 0.1207 - val_accuracy: 0.9552\n",
      "Epoch 556/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0663 - accuracy: 0.9758 - val_loss: 0.1220 - val_accuracy: 0.9551\n",
      "Epoch 557/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0663 - accuracy: 0.9758 - val_loss: 0.1233 - val_accuracy: 0.9549\n",
      "Epoch 558/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0664 - accuracy: 0.9755 - val_loss: 0.1227 - val_accuracy: 0.9547\n",
      "Epoch 559/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0663 - accuracy: 0.9760 - val_loss: 0.1232 - val_accuracy: 0.9559\n",
      "Epoch 560/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0664 - accuracy: 0.9758 - val_loss: 0.1214 - val_accuracy: 0.9544\n",
      "Epoch 561/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0662 - accuracy: 0.9757 - val_loss: 0.1222 - val_accuracy: 0.9556\n",
      "Epoch 562/800\n",
      "1665/1665 [==============================] - 0s 161us/step - loss: 0.0659 - accuracy: 0.9759 - val_loss: 0.1277 - val_accuracy: 0.9531\n",
      "Epoch 563/800\n",
      "1665/1665 [==============================] - 0s 152us/step - loss: 0.0664 - accuracy: 0.9759 - val_loss: 0.1206 - val_accuracy: 0.9551\n",
      "Epoch 564/800\n",
      "1665/1665 [==============================] - 0s 162us/step - loss: 0.0659 - accuracy: 0.9756 - val_loss: 0.1304 - val_accuracy: 0.9530\n",
      "Epoch 565/800\n",
      "1665/1665 [==============================] - 0s 157us/step - loss: 0.0662 - accuracy: 0.9758 - val_loss: 0.1214 - val_accuracy: 0.9550\n",
      "Epoch 566/800\n",
      "1665/1665 [==============================] - 0s 142us/step - loss: 0.0658 - accuracy: 0.9760 - val_loss: 0.1190 - val_accuracy: 0.9563\n",
      "Epoch 567/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0656 - accuracy: 0.9759 - val_loss: 0.1216 - val_accuracy: 0.9550\n",
      "Epoch 568/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0655 - accuracy: 0.9759 - val_loss: 0.1194 - val_accuracy: 0.9556\n",
      "Epoch 569/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0658 - accuracy: 0.9759 - val_loss: 0.1214 - val_accuracy: 0.9556\n",
      "Epoch 570/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.0654 - accuracy: 0.9761 - val_loss: 0.1215 - val_accuracy: 0.9560\n",
      "Epoch 571/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.0655 - accuracy: 0.9761 - val_loss: 0.1201 - val_accuracy: 0.9559\n",
      "Epoch 572/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.0654 - accuracy: 0.9758 - val_loss: 0.1303 - val_accuracy: 0.9524\n",
      "Epoch 573/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.0658 - accuracy: 0.9759 - val_loss: 0.1227 - val_accuracy: 0.9546\n",
      "Epoch 574/800\n",
      "1665/1665 [==============================] - 0s 136us/step - loss: 0.0653 - accuracy: 0.9760 - val_loss: 0.1178 - val_accuracy: 0.9561\n",
      "Epoch 575/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.0651 - accuracy: 0.9762 - val_loss: 0.1216 - val_accuracy: 0.9561\n",
      "Epoch 576/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0651 - accuracy: 0.9758 - val_loss: 0.1324 - val_accuracy: 0.9512\n",
      "Epoch 577/800\n",
      "1665/1665 [==============================] - 0s 157us/step - loss: 0.0657 - accuracy: 0.9758 - val_loss: 0.1194 - val_accuracy: 0.9556\n",
      "Epoch 578/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.0651 - accuracy: 0.9760 - val_loss: 0.1225 - val_accuracy: 0.9534\n",
      "Epoch 579/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.0652 - accuracy: 0.9760 - val_loss: 0.1206 - val_accuracy: 0.9562\n",
      "Epoch 580/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0650 - accuracy: 0.9758 - val_loss: 0.1194 - val_accuracy: 0.9564\n",
      "Epoch 581/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.0647 - accuracy: 0.9760 - val_loss: 0.1202 - val_accuracy: 0.9558\n",
      "Epoch 582/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.0648 - accuracy: 0.9760 - val_loss: 0.1210 - val_accuracy: 0.9555\n",
      "Epoch 583/800\n",
      "1665/1665 [==============================] - 0s 169us/step - loss: 0.0648 - accuracy: 0.9761 - val_loss: 0.1227 - val_accuracy: 0.9547\n",
      "Epoch 584/800\n",
      "1665/1665 [==============================] - 0s 198us/step - loss: 0.0647 - accuracy: 0.9762 - val_loss: 0.1202 - val_accuracy: 0.9566\n",
      "Epoch 585/800\n",
      "1665/1665 [==============================] - 0s 150us/step - loss: 0.0643 - accuracy: 0.9764 - val_loss: 0.1213 - val_accuracy: 0.9556\n",
      "Epoch 586/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.0645 - accuracy: 0.9760 - val_loss: 0.1206 - val_accuracy: 0.9563\n",
      "Epoch 587/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0644 - accuracy: 0.9760 - val_loss: 0.1197 - val_accuracy: 0.9569\n",
      "Epoch 588/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0642 - accuracy: 0.9761 - val_loss: 0.1186 - val_accuracy: 0.9567\n",
      "Epoch 589/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0642 - accuracy: 0.9764 - val_loss: 0.1212 - val_accuracy: 0.9556\n",
      "Epoch 590/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0644 - accuracy: 0.9763 - val_loss: 0.1212 - val_accuracy: 0.9564\n",
      "Epoch 591/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0646 - accuracy: 0.9762 - val_loss: 0.1195 - val_accuracy: 0.9568\n",
      "Epoch 592/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0642 - accuracy: 0.9761 - val_loss: 0.1228 - val_accuracy: 0.9555\n",
      "Epoch 593/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0645 - accuracy: 0.9763 - val_loss: 0.1204 - val_accuracy: 0.9566\n",
      "Epoch 594/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0639 - accuracy: 0.9761 - val_loss: 0.1202 - val_accuracy: 0.9567\n",
      "Epoch 595/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0639 - accuracy: 0.9763 - val_loss: 0.1195 - val_accuracy: 0.9568\n",
      "Epoch 596/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0637 - accuracy: 0.9764 - val_loss: 0.1187 - val_accuracy: 0.9570\n",
      "Epoch 597/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0637 - accuracy: 0.9763 - val_loss: 0.1191 - val_accuracy: 0.9570\n",
      "Epoch 598/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.0636 - accuracy: 0.9764 - val_loss: 0.1206 - val_accuracy: 0.9566\n",
      "Epoch 599/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0636 - accuracy: 0.9767 - val_loss: 0.1203 - val_accuracy: 0.9567\n",
      "Epoch 600/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0636 - accuracy: 0.9766 - val_loss: 0.1202 - val_accuracy: 0.9560\n",
      "Epoch 601/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0637 - accuracy: 0.9764 - val_loss: 0.1187 - val_accuracy: 0.9572\n",
      "Epoch 602/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0636 - accuracy: 0.9766 - val_loss: 0.1205 - val_accuracy: 0.9554\n",
      "Epoch 603/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.0636 - accuracy: 0.9765 - val_loss: 0.1214 - val_accuracy: 0.9566\n",
      "Epoch 604/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.0638 - accuracy: 0.9764 - val_loss: 0.1200 - val_accuracy: 0.9558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 605/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.0634 - accuracy: 0.9767 - val_loss: 0.1195 - val_accuracy: 0.9567\n",
      "Epoch 606/800\n",
      "1665/1665 [==============================] - 0s 126us/step - loss: 0.0633 - accuracy: 0.9767 - val_loss: 0.1163 - val_accuracy: 0.9563\n",
      "Epoch 607/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0633 - accuracy: 0.9766 - val_loss: 0.1179 - val_accuracy: 0.9575\n",
      "Epoch 608/800\n",
      "1665/1665 [==============================] - 0s 133us/step - loss: 0.0631 - accuracy: 0.9766 - val_loss: 0.1380 - val_accuracy: 0.9501\n",
      "Epoch 609/800\n",
      "1665/1665 [==============================] - 0s 160us/step - loss: 0.0640 - accuracy: 0.9764 - val_loss: 0.1182 - val_accuracy: 0.9569\n",
      "Epoch 610/800\n",
      "1665/1665 [==============================] - 0s 179us/step - loss: 0.0632 - accuracy: 0.9765 - val_loss: 0.1174 - val_accuracy: 0.9571\n",
      "Epoch 611/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.0632 - accuracy: 0.9763 - val_loss: 0.1182 - val_accuracy: 0.9564\n",
      "Epoch 612/800\n",
      "1665/1665 [==============================] - 0s 154us/step - loss: 0.0631 - accuracy: 0.9763 - val_loss: 0.1219 - val_accuracy: 0.9556\n",
      "Epoch 613/800\n",
      "1665/1665 [==============================] - 0s 151us/step - loss: 0.0633 - accuracy: 0.9767 - val_loss: 0.1206 - val_accuracy: 0.9570\n",
      "Epoch 614/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.0629 - accuracy: 0.9765 - val_loss: 0.1190 - val_accuracy: 0.9573\n",
      "Epoch 615/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0630 - accuracy: 0.9767 - val_loss: 0.1198 - val_accuracy: 0.9566\n",
      "Epoch 616/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0628 - accuracy: 0.9766 - val_loss: 0.1211 - val_accuracy: 0.9556\n",
      "Epoch 617/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0631 - accuracy: 0.9766 - val_loss: 0.1242 - val_accuracy: 0.9550\n",
      "Epoch 618/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0627 - accuracy: 0.9768 - val_loss: 0.1186 - val_accuracy: 0.9575\n",
      "Epoch 619/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0625 - accuracy: 0.9770 - val_loss: 0.1190 - val_accuracy: 0.9569\n",
      "Epoch 620/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0626 - accuracy: 0.9768 - val_loss: 0.1407 - val_accuracy: 0.9519\n",
      "Epoch 621/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0635 - accuracy: 0.9765 - val_loss: 0.1203 - val_accuracy: 0.9569\n",
      "Epoch 622/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0624 - accuracy: 0.9766 - val_loss: 0.1196 - val_accuracy: 0.9566\n",
      "Epoch 623/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0624 - accuracy: 0.9769 - val_loss: 0.1184 - val_accuracy: 0.9564\n",
      "Epoch 624/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0625 - accuracy: 0.9768 - val_loss: 0.1192 - val_accuracy: 0.9576\n",
      "Epoch 625/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.0623 - accuracy: 0.9768 - val_loss: 0.1400 - val_accuracy: 0.9526\n",
      "Epoch 626/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0634 - accuracy: 0.9767 - val_loss: 0.1256 - val_accuracy: 0.9545\n",
      "Epoch 627/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0625 - accuracy: 0.9768 - val_loss: 0.1194 - val_accuracy: 0.9566\n",
      "Epoch 628/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0621 - accuracy: 0.9769 - val_loss: 0.1184 - val_accuracy: 0.9570\n",
      "Epoch 629/800\n",
      "1665/1665 [==============================] - 0s 109us/step - loss: 0.0619 - accuracy: 0.9771 - val_loss: 0.1197 - val_accuracy: 0.9572\n",
      "Epoch 630/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0618 - accuracy: 0.9771 - val_loss: 0.1182 - val_accuracy: 0.9575\n",
      "Epoch 631/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0619 - accuracy: 0.9768 - val_loss: 0.1217 - val_accuracy: 0.9560\n",
      "Epoch 632/800\n",
      "1665/1665 [==============================] - 0s 110us/step - loss: 0.0621 - accuracy: 0.9767 - val_loss: 0.1236 - val_accuracy: 0.9567\n",
      "Epoch 633/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0618 - accuracy: 0.9769 - val_loss: 0.1174 - val_accuracy: 0.9569\n",
      "Epoch 634/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0619 - accuracy: 0.9769 - val_loss: 0.1178 - val_accuracy: 0.9570\n",
      "Epoch 635/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0615 - accuracy: 0.9770 - val_loss: 0.1196 - val_accuracy: 0.9571\n",
      "Epoch 636/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0615 - accuracy: 0.9769 - val_loss: 0.1297 - val_accuracy: 0.9524\n",
      "Epoch 637/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0622 - accuracy: 0.9767 - val_loss: 0.1196 - val_accuracy: 0.9572\n",
      "Epoch 638/800\n",
      "1665/1665 [==============================] - 0s 107us/step - loss: 0.0616 - accuracy: 0.9769 - val_loss: 0.1183 - val_accuracy: 0.9575\n",
      "Epoch 639/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0613 - accuracy: 0.9768 - val_loss: 0.1208 - val_accuracy: 0.9564\n",
      "Epoch 640/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0615 - accuracy: 0.9769 - val_loss: 0.1185 - val_accuracy: 0.9575\n",
      "Epoch 641/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0613 - accuracy: 0.9770 - val_loss: 0.1165 - val_accuracy: 0.9579\n",
      "Epoch 642/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0611 - accuracy: 0.9772 - val_loss: 0.1235 - val_accuracy: 0.9568\n",
      "Epoch 643/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0612 - accuracy: 0.9769 - val_loss: 0.1203 - val_accuracy: 0.9574\n",
      "Epoch 644/800\n",
      "1665/1665 [==============================] - 0s 112us/step - loss: 0.0611 - accuracy: 0.9772 - val_loss: 0.1192 - val_accuracy: 0.9576\n",
      "Epoch 645/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.0610 - accuracy: 0.9770 - val_loss: 0.1209 - val_accuracy: 0.9573\n",
      "Epoch 646/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0610 - accuracy: 0.9774 - val_loss: 0.1176 - val_accuracy: 0.9581\n",
      "Epoch 647/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.0610 - accuracy: 0.9770 - val_loss: 0.1158 - val_accuracy: 0.9580\n",
      "Epoch 648/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.0611 - accuracy: 0.9772 - val_loss: 0.1306 - val_accuracy: 0.9531\n",
      "Epoch 649/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.0615 - accuracy: 0.9769 - val_loss: 0.1256 - val_accuracy: 0.9560\n",
      "Epoch 650/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0613 - accuracy: 0.9770 - val_loss: 0.1177 - val_accuracy: 0.9573\n",
      "Epoch 651/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.0606 - accuracy: 0.9773 - val_loss: 0.1185 - val_accuracy: 0.9572\n",
      "Epoch 652/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0607 - accuracy: 0.9774 - val_loss: 0.1181 - val_accuracy: 0.9572\n",
      "Epoch 653/800\n",
      "1665/1665 [==============================] - 0s 129us/step - loss: 0.0607 - accuracy: 0.9772 - val_loss: 0.1177 - val_accuracy: 0.9566\n",
      "Epoch 654/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0606 - accuracy: 0.9771 - val_loss: 0.1185 - val_accuracy: 0.9570\n",
      "Epoch 655/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0606 - accuracy: 0.9771 - val_loss: 0.1172 - val_accuracy: 0.9577\n",
      "Epoch 656/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0605 - accuracy: 0.9773 - val_loss: 0.1198 - val_accuracy: 0.9569\n",
      "Epoch 657/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0605 - accuracy: 0.9771 - val_loss: 0.1187 - val_accuracy: 0.9571\n",
      "Epoch 658/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0604 - accuracy: 0.9774 - val_loss: 0.1377 - val_accuracy: 0.9506\n",
      "Epoch 659/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0618 - accuracy: 0.9770 - val_loss: 0.1219 - val_accuracy: 0.9578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 660/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0607 - accuracy: 0.9775 - val_loss: 0.1230 - val_accuracy: 0.9563\n",
      "Epoch 661/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0606 - accuracy: 0.9770 - val_loss: 0.1183 - val_accuracy: 0.9579\n",
      "Epoch 662/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0601 - accuracy: 0.9774 - val_loss: 0.1183 - val_accuracy: 0.9573\n",
      "Epoch 663/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0602 - accuracy: 0.9774 - val_loss: 0.1212 - val_accuracy: 0.9568\n",
      "Epoch 664/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.0601 - accuracy: 0.9774 - val_loss: 0.1179 - val_accuracy: 0.9573\n",
      "Epoch 665/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0601 - accuracy: 0.9774 - val_loss: 0.1162 - val_accuracy: 0.9582\n",
      "Epoch 666/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0600 - accuracy: 0.9776 - val_loss: 0.1176 - val_accuracy: 0.9567\n",
      "Epoch 667/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0598 - accuracy: 0.9774 - val_loss: 0.1244 - val_accuracy: 0.9560\n",
      "Epoch 668/800\n",
      "1665/1665 [==============================] - 0s 136us/step - loss: 0.0602 - accuracy: 0.9773 - val_loss: 0.1158 - val_accuracy: 0.9578\n",
      "Epoch 669/800\n",
      "1665/1665 [==============================] - 0s 129us/step - loss: 0.0596 - accuracy: 0.9776 - val_loss: 0.1197 - val_accuracy: 0.9559\n",
      "Epoch 670/800\n",
      "1665/1665 [==============================] - 0s 140us/step - loss: 0.0602 - accuracy: 0.9773 - val_loss: 0.1163 - val_accuracy: 0.9581\n",
      "Epoch 671/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0596 - accuracy: 0.9774 - val_loss: 0.1157 - val_accuracy: 0.9587\n",
      "Epoch 672/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0596 - accuracy: 0.9773 - val_loss: 0.1194 - val_accuracy: 0.9576\n",
      "Epoch 673/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.0597 - accuracy: 0.9775 - val_loss: 0.1174 - val_accuracy: 0.9576\n",
      "Epoch 674/800\n",
      "1665/1665 [==============================] - 0s 144us/step - loss: 0.0594 - accuracy: 0.9775 - val_loss: 0.1193 - val_accuracy: 0.9570\n",
      "Epoch 675/800\n",
      "1665/1665 [==============================] - 0s 156us/step - loss: 0.0597 - accuracy: 0.9776 - val_loss: 0.1157 - val_accuracy: 0.9577\n",
      "Epoch 676/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0594 - accuracy: 0.9777 - val_loss: 0.1198 - val_accuracy: 0.9581\n",
      "Epoch 677/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0595 - accuracy: 0.9773 - val_loss: 0.1197 - val_accuracy: 0.9565\n",
      "Epoch 678/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0597 - accuracy: 0.9776 - val_loss: 0.1167 - val_accuracy: 0.9573\n",
      "Epoch 679/800\n",
      "1665/1665 [==============================] - 0s 148us/step - loss: 0.0593 - accuracy: 0.9775 - val_loss: 0.1225 - val_accuracy: 0.9560\n",
      "Epoch 680/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.0596 - accuracy: 0.9776 - val_loss: 0.1225 - val_accuracy: 0.9558\n",
      "Epoch 681/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0597 - accuracy: 0.9774 - val_loss: 0.1183 - val_accuracy: 0.9569\n",
      "Epoch 682/800\n",
      "1665/1665 [==============================] - 0s 144us/step - loss: 0.0592 - accuracy: 0.9777 - val_loss: 0.1476 - val_accuracy: 0.9480\n",
      "Epoch 683/800\n",
      "1665/1665 [==============================] - 0s 201us/step - loss: 0.0607 - accuracy: 0.9770 - val_loss: 0.1177 - val_accuracy: 0.9579\n",
      "Epoch 684/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.0592 - accuracy: 0.9776 - val_loss: 0.1179 - val_accuracy: 0.9578\n",
      "Epoch 685/800\n",
      "1665/1665 [==============================] - 0s 129us/step - loss: 0.0591 - accuracy: 0.9775 - val_loss: 0.1197 - val_accuracy: 0.9564\n",
      "Epoch 686/800\n",
      "1665/1665 [==============================] - 0s 166us/step - loss: 0.0592 - accuracy: 0.9774 - val_loss: 0.1184 - val_accuracy: 0.9574\n",
      "Epoch 687/800\n",
      "1665/1665 [==============================] - 0s 140us/step - loss: 0.0589 - accuracy: 0.9777 - val_loss: 0.1184 - val_accuracy: 0.9578\n",
      "Epoch 688/800\n",
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.0589 - accuracy: 0.9780 - val_loss: 0.1178 - val_accuracy: 0.9578\n",
      "Epoch 689/800\n",
      "1665/1665 [==============================] - 0s 164us/step - loss: 0.0587 - accuracy: 0.9778 - val_loss: 0.1158 - val_accuracy: 0.9582\n",
      "Epoch 690/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.0585 - accuracy: 0.9779 - val_loss: 0.1166 - val_accuracy: 0.9578\n",
      "Epoch 691/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.0590 - accuracy: 0.9779 - val_loss: 0.1191 - val_accuracy: 0.9582\n",
      "Epoch 692/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0588 - accuracy: 0.9776 - val_loss: 0.1191 - val_accuracy: 0.9562\n",
      "Epoch 693/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0589 - accuracy: 0.9779 - val_loss: 0.1165 - val_accuracy: 0.9581\n",
      "Epoch 694/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0587 - accuracy: 0.9780 - val_loss: 0.1167 - val_accuracy: 0.9570\n",
      "Epoch 695/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0585 - accuracy: 0.9778 - val_loss: 0.1172 - val_accuracy: 0.9581\n",
      "Epoch 696/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0583 - accuracy: 0.9780 - val_loss: 0.1167 - val_accuracy: 0.9583\n",
      "Epoch 697/800\n",
      "1665/1665 [==============================] - 0s 126us/step - loss: 0.0583 - accuracy: 0.9778 - val_loss: 0.1159 - val_accuracy: 0.9584\n",
      "Epoch 698/800\n",
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.0583 - accuracy: 0.9781 - val_loss: 0.1187 - val_accuracy: 0.9580\n",
      "Epoch 699/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0584 - accuracy: 0.9781 - val_loss: 0.1144 - val_accuracy: 0.9590\n",
      "Epoch 700/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0581 - accuracy: 0.9779 - val_loss: 0.1239 - val_accuracy: 0.9570\n",
      "Epoch 701/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0582 - accuracy: 0.9779 - val_loss: 0.1160 - val_accuracy: 0.9590\n",
      "Epoch 702/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0582 - accuracy: 0.9779 - val_loss: 0.1186 - val_accuracy: 0.9576\n",
      "Epoch 703/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0583 - accuracy: 0.9780 - val_loss: 0.1176 - val_accuracy: 0.9579\n",
      "Epoch 704/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0582 - accuracy: 0.9780 - val_loss: 0.1152 - val_accuracy: 0.9582\n",
      "Epoch 705/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0580 - accuracy: 0.9778 - val_loss: 0.1199 - val_accuracy: 0.9574\n",
      "Epoch 706/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0584 - accuracy: 0.9779 - val_loss: 0.1164 - val_accuracy: 0.9575\n",
      "Epoch 707/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0581 - accuracy: 0.9780 - val_loss: 0.1230 - val_accuracy: 0.9545\n",
      "Epoch 708/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0582 - accuracy: 0.9780 - val_loss: 0.1206 - val_accuracy: 0.9571\n",
      "Epoch 709/800\n",
      "1665/1665 [==============================] - 0s 128us/step - loss: 0.0581 - accuracy: 0.9780 - val_loss: 0.1185 - val_accuracy: 0.9574\n",
      "Epoch 710/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0577 - accuracy: 0.9780 - val_loss: 0.1188 - val_accuracy: 0.9575\n",
      "Epoch 711/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0578 - accuracy: 0.9778 - val_loss: 0.1231 - val_accuracy: 0.9566\n",
      "Epoch 712/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0577 - accuracy: 0.9780 - val_loss: 0.1165 - val_accuracy: 0.9582\n",
      "Epoch 713/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0574 - accuracy: 0.9780 - val_loss: 0.1166 - val_accuracy: 0.9582\n",
      "Epoch 714/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0574 - accuracy: 0.9781 - val_loss: 0.1167 - val_accuracy: 0.9577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 715/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0576 - accuracy: 0.9781 - val_loss: 0.1207 - val_accuracy: 0.9580\n",
      "Epoch 716/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.0575 - accuracy: 0.9781 - val_loss: 0.1146 - val_accuracy: 0.9584\n",
      "Epoch 717/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0571 - accuracy: 0.9781 - val_loss: 0.1143 - val_accuracy: 0.9588\n",
      "Epoch 718/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.0573 - accuracy: 0.9782 - val_loss: 0.1347 - val_accuracy: 0.9518\n",
      "Epoch 719/800\n",
      "1665/1665 [==============================] - 0s 131us/step - loss: 0.0582 - accuracy: 0.9778 - val_loss: 0.1137 - val_accuracy: 0.9587\n",
      "Epoch 720/800\n",
      "1665/1665 [==============================] - 0s 129us/step - loss: 0.0572 - accuracy: 0.9783 - val_loss: 0.1163 - val_accuracy: 0.9582\n",
      "Epoch 721/800\n",
      "1665/1665 [==============================] - 0s 142us/step - loss: 0.0572 - accuracy: 0.9783 - val_loss: 0.1154 - val_accuracy: 0.9583\n",
      "Epoch 722/800\n",
      "1665/1665 [==============================] - 0s 147us/step - loss: 0.0571 - accuracy: 0.9781 - val_loss: 0.1189 - val_accuracy: 0.9569\n",
      "Epoch 723/800\n",
      "1665/1665 [==============================] - 0s 140us/step - loss: 0.0576 - accuracy: 0.9783 - val_loss: 0.1176 - val_accuracy: 0.9572\n",
      "Epoch 724/800\n",
      "1665/1665 [==============================] - 0s 136us/step - loss: 0.0573 - accuracy: 0.9779 - val_loss: 0.1154 - val_accuracy: 0.9584\n",
      "Epoch 725/800\n",
      "1665/1665 [==============================] - 0s 133us/step - loss: 0.0569 - accuracy: 0.9783 - val_loss: 0.1148 - val_accuracy: 0.9591\n",
      "Epoch 726/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.0567 - accuracy: 0.9781 - val_loss: 0.1124 - val_accuracy: 0.9595\n",
      "Epoch 727/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.0568 - accuracy: 0.9783 - val_loss: 0.1144 - val_accuracy: 0.9587\n",
      "Epoch 728/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0566 - accuracy: 0.9784 - val_loss: 0.1156 - val_accuracy: 0.9589\n",
      "Epoch 729/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0567 - accuracy: 0.9783 - val_loss: 0.1151 - val_accuracy: 0.9592\n",
      "Epoch 730/800\n",
      "1665/1665 [==============================] - 0s 122us/step - loss: 0.0565 - accuracy: 0.9784 - val_loss: 0.1178 - val_accuracy: 0.9580\n",
      "Epoch 731/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0567 - accuracy: 0.9783 - val_loss: 0.1174 - val_accuracy: 0.9593\n",
      "Epoch 732/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0570 - accuracy: 0.9783 - val_loss: 0.1164 - val_accuracy: 0.9590\n",
      "Epoch 733/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0567 - accuracy: 0.9784 - val_loss: 0.1173 - val_accuracy: 0.9580\n",
      "Epoch 734/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0566 - accuracy: 0.9785 - val_loss: 0.1160 - val_accuracy: 0.9580\n",
      "Epoch 735/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0563 - accuracy: 0.9784 - val_loss: 0.1638 - val_accuracy: 0.9448\n",
      "Epoch 736/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0585 - accuracy: 0.9779 - val_loss: 0.1166 - val_accuracy: 0.9585\n",
      "Epoch 737/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0566 - accuracy: 0.9785 - val_loss: 0.1155 - val_accuracy: 0.9585\n",
      "Epoch 738/800\n",
      "1665/1665 [==============================] - 0s 113us/step - loss: 0.0565 - accuracy: 0.9784 - val_loss: 0.1220 - val_accuracy: 0.9581\n",
      "Epoch 739/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0565 - accuracy: 0.9784 - val_loss: 0.1128 - val_accuracy: 0.9591\n",
      "Epoch 740/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0562 - accuracy: 0.9786 - val_loss: 0.1153 - val_accuracy: 0.9589\n",
      "Epoch 741/800\n",
      "1665/1665 [==============================] - 0s 111us/step - loss: 0.0561 - accuracy: 0.9784 - val_loss: 0.1196 - val_accuracy: 0.9593\n",
      "Epoch 742/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0563 - accuracy: 0.9785 - val_loss: 0.1137 - val_accuracy: 0.9589\n",
      "Epoch 743/800\n",
      "1665/1665 [==============================] - 0s 114us/step - loss: 0.0560 - accuracy: 0.9786 - val_loss: 0.1120 - val_accuracy: 0.9593\n",
      "Epoch 744/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0563 - accuracy: 0.9785 - val_loss: 0.1168 - val_accuracy: 0.9578\n",
      "Epoch 745/800\n",
      "1665/1665 [==============================] - 0s 130us/step - loss: 0.0564 - accuracy: 0.9784 - val_loss: 0.1131 - val_accuracy: 0.9597\n",
      "Epoch 746/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.0559 - accuracy: 0.9786 - val_loss: 0.1156 - val_accuracy: 0.9590\n",
      "Epoch 747/800\n",
      "1665/1665 [==============================] - 0s 143us/step - loss: 0.0559 - accuracy: 0.9786 - val_loss: 0.1161 - val_accuracy: 0.9590\n",
      "Epoch 748/800\n",
      "1665/1665 [==============================] - 0s 136us/step - loss: 0.0558 - accuracy: 0.9785 - val_loss: 0.1129 - val_accuracy: 0.9597\n",
      "Epoch 749/800\n",
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.0556 - accuracy: 0.9788 - val_loss: 0.1163 - val_accuracy: 0.9587\n",
      "Epoch 750/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0560 - accuracy: 0.9786 - val_loss: 0.1164 - val_accuracy: 0.9587\n",
      "Epoch 751/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0558 - accuracy: 0.9784 - val_loss: 0.1218 - val_accuracy: 0.9570\n",
      "Epoch 752/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.0560 - accuracy: 0.9787 - val_loss: 0.1134 - val_accuracy: 0.9590\n",
      "Epoch 753/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0558 - accuracy: 0.9785 - val_loss: 0.1166 - val_accuracy: 0.9593\n",
      "Epoch 754/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0556 - accuracy: 0.9787 - val_loss: 0.1165 - val_accuracy: 0.9588\n",
      "Epoch 755/800\n",
      "1665/1665 [==============================] - 0s 136us/step - loss: 0.0556 - accuracy: 0.9786 - val_loss: 0.1174 - val_accuracy: 0.9589\n",
      "Epoch 756/800\n",
      "1665/1665 [==============================] - 0s 139us/step - loss: 0.0555 - accuracy: 0.9788 - val_loss: 0.1163 - val_accuracy: 0.9588\n",
      "Epoch 757/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.0555 - accuracy: 0.9787 - val_loss: 0.1126 - val_accuracy: 0.9596\n",
      "Epoch 758/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0554 - accuracy: 0.9787 - val_loss: 0.1167 - val_accuracy: 0.9582\n",
      "Epoch 759/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0554 - accuracy: 0.9789 - val_loss: 0.1175 - val_accuracy: 0.9585\n",
      "Epoch 760/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0553 - accuracy: 0.9788 - val_loss: 0.1131 - val_accuracy: 0.9598\n",
      "Epoch 761/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0552 - accuracy: 0.9788 - val_loss: 0.1179 - val_accuracy: 0.9588\n",
      "Epoch 762/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0552 - accuracy: 0.9787 - val_loss: 0.1129 - val_accuracy: 0.9600\n",
      "Epoch 763/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.0550 - accuracy: 0.9789 - val_loss: 0.1128 - val_accuracy: 0.9592\n",
      "Epoch 764/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0549 - accuracy: 0.9788 - val_loss: 0.1172 - val_accuracy: 0.9586\n",
      "Epoch 765/800\n",
      "1665/1665 [==============================] - 0s 117us/step - loss: 0.0551 - accuracy: 0.9787 - val_loss: 0.1165 - val_accuracy: 0.9586\n",
      "Epoch 766/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0551 - accuracy: 0.9788 - val_loss: 0.1153 - val_accuracy: 0.9594\n",
      "Epoch 767/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0550 - accuracy: 0.9788 - val_loss: 0.1126 - val_accuracy: 0.9599\n",
      "Epoch 768/800\n",
      "1665/1665 [==============================] - 0s 160us/step - loss: 0.0548 - accuracy: 0.9789 - val_loss: 0.1155 - val_accuracy: 0.9595\n",
      "Epoch 769/800\n",
      "1665/1665 [==============================] - 0s 129us/step - loss: 0.0549 - accuracy: 0.9787 - val_loss: 0.1132 - val_accuracy: 0.9605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 770/800\n",
      "1665/1665 [==============================] - 0s 115us/step - loss: 0.0546 - accuracy: 0.9788 - val_loss: 0.1142 - val_accuracy: 0.9601\n",
      "Epoch 771/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0548 - accuracy: 0.9789 - val_loss: 0.1138 - val_accuracy: 0.9597\n",
      "Epoch 772/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0545 - accuracy: 0.9787 - val_loss: 0.1131 - val_accuracy: 0.9598\n",
      "Epoch 773/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0545 - accuracy: 0.9789 - val_loss: 0.1149 - val_accuracy: 0.9590\n",
      "Epoch 774/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0547 - accuracy: 0.9789 - val_loss: 0.1487 - val_accuracy: 0.9509\n",
      "Epoch 775/800\n",
      "1665/1665 [==============================] - 0s 127us/step - loss: 0.0567 - accuracy: 0.9784 - val_loss: 0.1155 - val_accuracy: 0.9592\n",
      "Epoch 776/800\n",
      "1665/1665 [==============================] - 0s 139us/step - loss: 0.0545 - accuracy: 0.9791 - val_loss: 0.1130 - val_accuracy: 0.9591\n",
      "Epoch 777/800\n",
      "1665/1665 [==============================] - 0s 140us/step - loss: 0.0547 - accuracy: 0.9788 - val_loss: 0.1158 - val_accuracy: 0.9582\n",
      "Epoch 778/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.0546 - accuracy: 0.9789 - val_loss: 0.1211 - val_accuracy: 0.9580\n",
      "Epoch 779/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0549 - accuracy: 0.9788 - val_loss: 0.1152 - val_accuracy: 0.9593\n",
      "Epoch 780/800\n",
      "1665/1665 [==============================] - 0s 124us/step - loss: 0.0541 - accuracy: 0.9792 - val_loss: 0.1155 - val_accuracy: 0.9598\n",
      "Epoch 781/800\n",
      "1665/1665 [==============================] - 0s 142us/step - loss: 0.0542 - accuracy: 0.9790 - val_loss: 0.1130 - val_accuracy: 0.9604\n",
      "Epoch 782/800\n",
      "1665/1665 [==============================] - 0s 139us/step - loss: 0.0542 - accuracy: 0.9791 - val_loss: 0.1127 - val_accuracy: 0.9602\n",
      "Epoch 783/800\n",
      "1665/1665 [==============================] - 0s 139us/step - loss: 0.0540 - accuracy: 0.9792 - val_loss: 0.1256 - val_accuracy: 0.9580\n",
      "Epoch 784/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.0545 - accuracy: 0.9788 - val_loss: 0.1144 - val_accuracy: 0.9601\n",
      "Epoch 785/800\n",
      "1665/1665 [==============================] - 0s 125us/step - loss: 0.0542 - accuracy: 0.9791 - val_loss: 0.1140 - val_accuracy: 0.9600\n",
      "Epoch 786/800\n",
      "1665/1665 [==============================] - 0s 116us/step - loss: 0.0538 - accuracy: 0.9791 - val_loss: 0.1120 - val_accuracy: 0.9606\n",
      "Epoch 787/800\n",
      "1665/1665 [==============================] - 0s 121us/step - loss: 0.0541 - accuracy: 0.9789 - val_loss: 0.1165 - val_accuracy: 0.9600\n",
      "Epoch 788/800\n",
      "1665/1665 [==============================] - 0s 120us/step - loss: 0.0539 - accuracy: 0.9793 - val_loss: 0.1116 - val_accuracy: 0.9604\n",
      "Epoch 789/800\n",
      "1665/1665 [==============================] - 0s 119us/step - loss: 0.0538 - accuracy: 0.9792 - val_loss: 0.1581 - val_accuracy: 0.9475\n",
      "Epoch 790/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0563 - accuracy: 0.9780 - val_loss: 0.1171 - val_accuracy: 0.9593\n",
      "Epoch 791/800\n",
      "1665/1665 [==============================] - 0s 123us/step - loss: 0.0542 - accuracy: 0.9788 - val_loss: 0.1207 - val_accuracy: 0.9587\n",
      "Epoch 792/800\n",
      "1665/1665 [==============================] - 0s 133us/step - loss: 0.0541 - accuracy: 0.9791 - val_loss: 0.1127 - val_accuracy: 0.9607\n",
      "Epoch 793/800\n",
      "1665/1665 [==============================] - 0s 118us/step - loss: 0.0537 - accuracy: 0.9790 - val_loss: 0.1125 - val_accuracy: 0.9596\n",
      "Epoch 794/800\n",
      "1665/1665 [==============================] - 0s 132us/step - loss: 0.0538 - accuracy: 0.9793 - val_loss: 0.1552 - val_accuracy: 0.9484\n",
      "Epoch 795/800\n",
      "1665/1665 [==============================] - 0s 138us/step - loss: 0.0561 - accuracy: 0.9787 - val_loss: 0.1265 - val_accuracy: 0.9546\n",
      "Epoch 796/800\n",
      "1665/1665 [==============================] - 0s 137us/step - loss: 0.0546 - accuracy: 0.9789 - val_loss: 0.1172 - val_accuracy: 0.9587\n",
      "Epoch 797/800\n",
      "1665/1665 [==============================] - 0s 134us/step - loss: 0.0540 - accuracy: 0.9790 - val_loss: 0.1174 - val_accuracy: 0.9590\n",
      "Epoch 798/800\n",
      "1665/1665 [==============================] - 0s 135us/step - loss: 0.0537 - accuracy: 0.9791 - val_loss: 0.1421 - val_accuracy: 0.9515\n",
      "Epoch 799/800\n",
      "1665/1665 [==============================] - 0s 144us/step - loss: 0.0554 - accuracy: 0.9785 - val_loss: 0.1146 - val_accuracy: 0.9600\n",
      "Epoch 800/800\n",
      "1665/1665 [==============================] - 0s 154us/step - loss: 0.0537 - accuracy: 0.9794 - val_loss: 0.1140 - val_accuracy: 0.9605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clair\\Anaconda3\\envs\\CNNs\\lib\\site-packages\\ipykernel_launcher.py:125: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[554, 0]\n",
      "[0, 1]\n",
      "both b and c are zero\n",
      "[554, 0]\n",
      "[0, 1]\n",
      "both b and c are zero\n",
      "[521, 3]\n",
      "[28, 3]\n",
      "[507, 17]\n",
      "[5, 26]\n",
      "[529, 3]\n",
      "[20, 3]\n",
      "[524, 8]\n",
      "[3, 20]\n",
      "[541, 1]\n",
      "[12, 1]\n",
      "[528, 14]\n",
      "[6, 7]\n",
      "[545, 2]\n",
      "[8, 0]\n",
      "[526, 21]\n",
      "[1, 7]\n",
      "[509, 18]\n",
      "[16, 12]\n",
      "[510, 17]\n",
      "[5, 23]\n",
      "[506, 24]\n",
      "[18, 7]\n",
      "[501, 29]\n",
      "[4, 21]\n",
      "[536, 7]\n",
      "[9, 3]\n",
      "[530, 13]\n",
      "[6, 6]\n",
      "[540, 3]\n",
      "[9, 3]\n",
      "[529, 14]\n",
      "[7, 5]\n",
      "[540, 4]\n",
      "[9, 2]\n",
      "[525, 19]\n",
      "[6, 5]\n",
      "[522, 6]\n",
      "[7, 20]\n",
      "[514, 14]\n",
      "[7, 20]\n",
      "[514, 10]\n",
      "[14, 17]\n",
      "[504, 20]\n",
      "[9, 22]\n",
      "[535, 10]\n",
      "[8, 2]\n",
      "[525, 20]\n",
      "[5, 5]\n",
      "[536, 5]\n",
      "[11, 3]\n",
      "[524, 17]\n",
      "[6, 8]\n",
      "[538, 7]\n",
      "[9, 1]\n",
      "[531, 14]\n",
      "[7, 3]\n",
      "[501, 28]\n",
      "[17, 9]\n",
      "[508, 21]\n",
      "[7, 19]\n",
      "[513, 16]\n",
      "[15, 11]\n",
      "[512, 17]\n",
      "[5, 21]\n",
      "[531, 9]\n",
      "[13, 2]\n",
      "[521, 19]\n",
      "[11, 4]\n",
      "[525, 4]\n",
      "[23, 3]\n",
      "[510, 19]\n",
      "[18, 8]\n",
      "[538, 6]\n",
      "[8, 3]\n",
      "[522, 22]\n",
      "[5, 6]\n",
      "[512, 16]\n",
      "[9, 18]\n",
      "[509, 19]\n",
      "[7, 20]\n",
      "[521, 9]\n",
      "[5, 20]\n",
      "[512, 18]\n",
      "[2, 23]\n",
      "[539, 2]\n",
      "[13, 1]\n",
      "[523, 18]\n",
      "[4, 10]\n",
      "[540, 2]\n",
      "[12, 1]\n",
      "[526, 16]\n",
      "[5, 8]\n",
      "[542, 3]\n",
      "[8, 2]\n",
      "[528, 17]\n",
      "[8, 2]\n",
      "[543, 0]\n",
      "[7, 5]\n",
      "[534, 9]\n",
      "[3, 9]\n",
      "[505, 22]\n",
      "[19, 9]\n",
      "[520, 7]\n",
      "[8, 20]\n",
      "[515, 9]\n",
      "[11, 20]\n",
      "[509, 15]\n",
      "[7, 24]\n",
      "[507, 24]\n",
      "[18, 6]\n",
      "[513, 18]\n",
      "[2, 22]\n",
      "[516, 11]\n",
      "[9, 19]\n",
      "[513, 14]\n",
      "[2, 26]\n",
      "[540, 2]\n",
      "[11, 2]\n",
      "[524, 18]\n",
      "[8, 5]\n",
      "[545, 2]\n",
      "[7, 1]\n",
      "[527, 20]\n",
      "[1, 7]\n",
      "[533, 9]\n",
      "[11, 2]\n",
      "[523, 19]\n",
      "[8, 5]\n",
      "[531, 7]\n",
      "[14, 3]\n",
      "[527, 11]\n",
      "[6, 11]\n",
      "[536, 4]\n",
      "[13, 2]\n",
      "[526, 14]\n",
      "[2, 13]\n",
      "[546, 2]\n",
      "[5, 2]\n",
      "[535, 13]\n",
      "[2, 5]\n",
      "[544, 2]\n",
      "[8, 1]\n",
      "[530, 16]\n",
      "[0, 9]\n",
      "[535, 5]\n",
      "[12, 3]\n",
      "[526, 14]\n",
      "[0, 15]\n",
      "[534, 4]\n",
      "[14, 3]\n",
      "[527, 11]\n",
      "[1, 16]\n",
      "[545, 2]\n",
      "[7, 1]\n",
      "[532, 15]\n",
      "[2, 6]\n",
      "[545, 1]\n",
      "[9, 0]\n",
      "[526, 20]\n",
      "[2, 7]\n",
      "[540, 3]\n",
      "[11, 1]\n",
      "[526, 17]\n",
      "[8, 4]\n",
      "[536, 5]\n",
      "[10, 4]\n",
      "[526, 15]\n",
      "[3, 11]\n",
      "[532, 6]\n",
      "[13, 4]\n",
      "[521, 17]\n",
      "[4, 13]\n",
      "[539, 5]\n",
      "[10, 1]\n",
      "[529, 15]\n",
      "[2, 9]\n",
      "[547, 0]\n",
      "[7, 1]\n",
      "[525, 22]\n",
      "[2, 6]\n",
      "[538, 3]\n",
      "[13, 1]\n",
      "[528, 13]\n",
      "[3, 11]\n",
      "[507, 20]\n",
      "[16, 12]\n",
      "[508, 19]\n",
      "[4, 24]\n",
      "[515, 10]\n",
      "[13, 17]\n",
      "[511, 14]\n",
      "[8, 22]\n",
      "[520, 14]\n",
      "[17, 4]\n",
      "[522, 12]\n",
      "[3, 18]\n",
      "[531, 7]\n",
      "[5, 12]\n",
      "[525, 13]\n",
      "[3, 14]\n",
      "------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds of MWPM:\n",
      "> Accuracy: 0.9767687281017772 (+- 0.0006198545890794175)\n",
      "> F1: 0.7736534273282992(+- 0.00837649158200533)\n",
      "> Time: 30.024095519999992 (+- 2.0261333842192273)\n",
      "##############################################################################\n",
      "Average scores for all folds of PLUT:\n",
      "> Accuracy: 0.9492707519199228 (+- 0.000461264585676839)\n",
      "> F1: 0.2772809984582137(+- 0.008498698099636923)\n",
      "> Time: 0.01717756 (+- 0.002997952821910312)\n",
      "##############################################################################\n",
      "Average scores for all folds of NN:\n",
      "> Accuracy: 0.9691756674151929 (+- 0.0010909450288739428)\n",
      "> F1: 0.6769403116212669(+- 0.012596533561949995)\n",
      "> Time: 0.109608 (+- 0.021575717264925402)\n",
      "##############################################################################\n",
      "> AUC for class : 0.46750902527075816 (+- 0.0)\n",
      "X^2 for MWPM and NN: 0\n",
      "X^2 for PLUT and NN: 0\n",
      "> AUC for class X00: 0.741534526546337 (+- 0.051125798043353717)\n",
      "X^2 for MWPM and NN: 21.806451612903224\n",
      "X^2 for PLUT and NN: 5.5\n",
      "> AUC for class X01: 0.9433159325882071 (+- 0.02488999400742718)\n",
      "X^2 for MWPM and NN: 14.08695652173913\n",
      "X^2 for PLUT and NN: 1.4545454545454546\n",
      "> AUC for class X02: 0.9906031048874662 (+- 0.0017728898273044367)\n",
      "X^2 for MWPM and NN: 11.076923076923077\n",
      "X^2 for PLUT and NN: 2.45\n",
      "> AUC for class X03: 0.9911697328569392 (+- 0.0032547508550469003)\n",
      "X^2 for MWPM and NN: 4.9\n",
      "X^2 for PLUT and NN: 16.40909090909091\n",
      "> AUC for class X04: 0.9523196298066534 (+- 0.016277733086732925)\n",
      "X^2 for MWPM and NN: 0.029411764705882353\n",
      "X^2 for PLUT and NN: 5.5\n",
      "> AUC for class X10: 0.9276393882669876 (+- 0.02612172432359588)\n",
      "X^2 for MWPM and NN: 0.5952380952380952\n",
      "X^2 for PLUT and NN: 17.454545454545453\n",
      "> AUC for class X11: 0.9824948508136087 (+- 0.00422037048489771)\n",
      "X^2 for MWPM and NN: 0.5625\n",
      "X^2 for PLUT and NN: 1.894736842105263\n",
      "> AUC for class X12: 0.9897583195434645 (+- 0.003809817320522272)\n",
      "X^2 for MWPM and NN: 4.083333333333333\n",
      "X^2 for PLUT and NN: 1.7142857142857142\n",
      "> AUC for class X13: 0.9844088832491907 (+- 0.007196836542527629)\n",
      "X^2 for MWPM and NN: 2.769230769230769\n",
      "X^2 for PLUT and NN: 5.76\n",
      "> AUC for class X14: 0.943549341432709 (+- 0.013769744143897998)\n",
      "X^2 for MWPM and NN: 0.3076923076923077\n",
      "X^2 for PLUT and NN: 1.7142857142857142\n",
      "> AUC for class X20: 0.9384315196385637 (+- 0.016538634789616355)\n",
      "X^2 for MWPM and NN: 1.0416666666666667\n",
      "X^2 for PLUT and NN: 3.4482758620689653\n",
      "> AUC for class X21: 0.9852303614617369 (+- 0.012096699664153384)\n",
      "X^2 for MWPM and NN: 0.05555555555555555\n",
      "X^2 for PLUT and NN: 7.84\n",
      "> AUC for class X22: 0.9869068205862772 (+- 0.009300567343138437)\n",
      "X^2 for MWPM and NN: 3.0625\n",
      "X^2 for PLUT and NN: 4.3478260869565215\n",
      "> AUC for class X23: 0.9880503910322604 (+- 0.0035021084698691803)\n",
      "X^2 for MWPM and NN: 0.5625\n",
      "X^2 for PLUT and NN: 1.7142857142857142\n",
      "> AUC for class X24: 0.9432042418335568 (+- 0.010729463110947858)\n",
      "X^2 for MWPM and NN: 2.2222222222222223\n",
      "X^2 for PLUT and NN: 6.035714285714286\n",
      "> AUC for class X30: 0.9360437390333182 (+- 0.013581929590313005)\n",
      "X^2 for MWPM and NN: 0.0\n",
      "X^2 for PLUT and NN: 5.5\n",
      "> AUC for class X31: 0.985664188576628 (+- 0.004994434621315567)\n",
      "X^2 for MWPM and NN: 1.1363636363636365\n",
      "X^2 for PLUT and NN: 1.6333333333333333\n",
      "> AUC for class X32: 0.9848433433941487 (+- 0.00991007212625007)\n",
      "X^2 for MWPM and NN: 14.814814814814815\n",
      "X^2 for PLUT and NN: 0.0\n",
      "> AUC for class X33: 0.984555021137342 (+- 0.009741535960090298)\n",
      "X^2 for MWPM and NN: 0.6428571428571429\n",
      "X^2 for PLUT and NN: 9.481481481481481\n",
      "> AUC for class X34: 0.9424640589657761 (+- 0.013027191443122837)\n",
      "X^2 for MWPM and NN: 1.44\n",
      "X^2 for PLUT and NN: 4.653846153846154\n",
      "> AUC for class X40: 0.9373772498416955 (+- 0.03467636604357259)\n",
      "X^2 for MWPM and NN: 0.6428571428571429\n",
      "X^2 for PLUT and NN: 11.25\n",
      "> AUC for class X41: 0.9887903247963392 (+- 0.004041436614799852)\n",
      "X^2 for MWPM and NN: 9.6\n",
      "X^2 for PLUT and NN: 7.681818181818182\n",
      "> AUC for class X42: 0.9916853415220153 (+- 0.004948344852723938)\n",
      "X^2 for MWPM and NN: 8.642857142857142\n",
      "X^2 for PLUT and NN: 4.761904761904762\n",
      "> AUC for class X43: 0.9902869358201839 (+- 0.006150580314239845)\n",
      "X^2 for MWPM and NN: 3.272727272727273\n",
      "X^2 for PLUT and NN: 2.56\n",
      "> AUC for class X44: 0.985131355165267 (+- 0.004801099632271905)\n",
      "X^2 for MWPM and NN: 9.142857142857142\n",
      "X^2 for PLUT and NN: 2.0833333333333335\n",
      "> AUC for class Z00: 0.9361007962888381 (+- 0.023602932682121217)\n",
      "X^2 for MWPM and NN: 0.0975609756097561\n",
      "X^2 for PLUT and NN: 0.26666666666666666\n",
      "> AUC for class Z01: 0.9359199845921271 (+- 0.0129956839970422)\n",
      "X^2 for MWPM and NN: 0.45\n",
      "X^2 for PLUT and NN: 2.227272727272727\n",
      "> AUC for class Z02: 0.9444618913341751 (+- 0.010360792023125696)\n",
      "X^2 for MWPM and NN: 0.5952380952380952\n",
      "X^2 for PLUT and NN: 11.25\n",
      "> AUC for class Z03: 0.9388396341522982 (+- 0.011870874837920972)\n",
      "X^2 for MWPM and NN: 0.05\n",
      "X^2 for PLUT and NN: 7.5625\n",
      "> AUC for class Z04: 0.9746566650493007 (+- 0.011626938016545902)\n",
      "X^2 for MWPM and NN: 7.6923076923076925\n",
      "X^2 for PLUT and NN: 3.1153846153846154\n",
      "> AUC for class Z10: 0.9945616923340314 (+- 0.004093461138216496)\n",
      "X^2 for MWPM and NN: 4.0\n",
      "X^2 for PLUT and NN: 15.428571428571429\n",
      "> AUC for class Z11: 0.9840190292327524 (+- 0.005161485759939512)\n",
      "X^2 for MWPM and NN: 0.45\n",
      "X^2 for PLUT and NN: 3.7037037037037037\n",
      "> AUC for class Z12: 0.9822444706001561 (+- 0.007857370929924749)\n",
      "X^2 for MWPM and NN: 3.0476190476190474\n",
      "X^2 for PLUT and NN: 0.9411764705882353\n",
      "> AUC for class Z13: 0.9766858673710063 (+- 0.010657165454006889)\n",
      "X^2 for MWPM and NN: 5.882352941176471\n",
      "X^2 for PLUT and NN: 7.5625\n",
      "> AUC for class Z14: 0.983342538780224 (+- 0.012663313292360647)\n",
      "X^2 for MWPM and NN: 2.2857142857142856\n",
      "X^2 for PLUT and NN: 6.666666666666667\n",
      "> AUC for class Z20: 0.9899992061118631 (+- 0.005272660775795528)\n",
      "X^2 for MWPM and NN: 4.9\n",
      "X^2 for PLUT and NN: 14.0625\n",
      "> AUC for class Z21: 0.9849173279295993 (+- 0.007262724647592609)\n",
      "X^2 for MWPM and NN: 3.764705882352941\n",
      "X^2 for PLUT and NN: 12.071428571428571\n",
      "> AUC for class Z22: 0.9812188292261494 (+- 0.011465263229964344)\n",
      "X^2 for MWPM and NN: 6.722222222222222\n",
      "X^2 for PLUT and NN: 6.75\n",
      "> AUC for class Z23: 0.9840841739226628 (+- 0.006506067190878126)\n",
      "X^2 for MWPM and NN: 4.0\n",
      "X^2 for PLUT and NN: 8.470588235294118\n",
      "> AUC for class Z24: 0.9838289201719679 (+- 0.019686197142055488)\n",
      "X^2 for MWPM and NN: 8.1\n",
      "X^2 for PLUT and NN: 13.136363636363637\n",
      "> AUC for class Z30: 0.987490539005478 (+- 0.0053383361861524025)\n",
      "X^2 for MWPM and NN: 5.785714285714286\n",
      "X^2 for PLUT and NN: 2.56\n",
      "> AUC for class Z31: 0.9815155860388574 (+- 0.01167548041061115)\n",
      "X^2 for MWPM and NN: 2.4\n",
      "X^2 for PLUT and NN: 6.722222222222222\n",
      "> AUC for class Z32: 0.9822158253105588 (+- 0.006878751261494427)\n",
      "X^2 for MWPM and NN: 3.3684210526315788\n",
      "X^2 for PLUT and NN: 6.857142857142857\n",
      "> AUC for class Z33: 0.9799265595176806 (+- 0.00551762806673634)\n",
      "X^2 for MWPM and NN: 2.4\n",
      "X^2 for PLUT and NN: 8.470588235294118\n",
      "> AUC for class Z34: 0.9849801693058545 (+- 0.006679552498667166)\n",
      "X^2 for MWPM and NN: 9.142857142857142\n",
      "X^2 for PLUT and NN: 15.041666666666666\n",
      "> AUC for class Z40: 0.9697524775466905 (+- 0.01451246267868368)\n",
      "X^2 for MWPM and NN: 7.5625\n",
      "X^2 for PLUT and NN: 5.0625\n",
      "> AUC for class Z41: 0.9397562059388831 (+- 0.019053995619324474)\n",
      "X^2 for MWPM and NN: 0.25\n",
      "X^2 for PLUT and NN: 8.521739130434783\n",
      "> AUC for class Z42: 0.9430860470612942 (+- 0.008952774522706645)\n",
      "X^2 for MWPM and NN: 0.6956521739130435\n",
      "X^2 for PLUT and NN: 1.1363636363636365\n",
      "> AUC for class Z43: 0.9379445967300688 (+- 0.013834698595025978)\n",
      "X^2 for MWPM and NN: 0.5161290322580645\n",
      "X^2 for PLUT and NN: 4.266666666666667\n",
      "> AUC for class Z44: 0.9385358106303443 (+- 0.0409003402712064)\n",
      "X^2 for MWPM and NN: 0.08333333333333333\n",
      "X^2 for PLUT and NN: 5.0625\n",
      "###################################################################################\n",
      "TOTAL F1 NN: [0.6776315789473685, 0.6575139146567719, 0.6886721680420105, 0.691696750902527, 0.669187145557656]\n",
      "TOTAL F1 PLUT: [0.2894472361809045, 0.283, 0.27332324760463944, 0.27613806903451726, 0.26449643947100715]\n",
      "TOTAL F1 MWPM: [0.7686411149825785, 0.7655172413793103, 0.7755798090040927, 0.7890835579514826, 0.769445413324032]\n",
      "TOTAL ACC NN: [0.9688954718578069, 0.9673909203320962, 0.9706765589118537, 0.9698286521815921, 0.9690867337926158]\n",
      "TOTAL ACC PLUT: [0.9501340104387033, 0.9493375728669801, 0.9490902667373221, 0.948878290054757, 0.9489136195018513]\n",
      "TOTAL ACC MWPM: [0.9765834391310457, 0.9759759759759734, 0.9767532238120455, 0.9778837661190583, 0.9766472354707629]\n",
      "TOTAL TIME NN: [0.0805409, 0.0995499, 0.1454229, 0.1044327, 0.1180936]\n",
      "TOTAL TIME PLUT: [0.0195177, 0.0146425, 0.0185448, 0.0204952, 0.0126876]\n",
      "TOTAL TIME MWPM: [28.315339699999985, 30.161186100000005, 33.296251499999975, 30.822274099999987, 27.525426200000005]\n",
      "------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "inputs = x_d5.copy()\n",
    "inputs = np.array(inputs)\n",
    "inputs = np.stack(inputs)\n",
    "targets = np.array(y_d5)\n",
    "#targets = np.stack(targets)\n",
    "targets = np.array([np.array(xi) for xi in targets])\n",
    "\n",
    "\n",
    "n_classes = 51\n",
    "\n",
    "acc_per_fold = []\n",
    "f1_per_fold = []\n",
    "time_per_fold = []\n",
    "\n",
    "acc_per_fold_plut = []\n",
    "f1_per_fold_plut = []\n",
    "time_per_fold_plut = []\n",
    "\n",
    "acc_per_fold_mwpm = []\n",
    "f1_per_fold_mwpm = []\n",
    "time_per_fold_mwpm = []\n",
    "\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "aucs_classes = {}\n",
    "for i in mlb.classes_:\n",
    "    aucs_classes[i] = []\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "for i, (train, test) in enumerate(kfold.split(inputs, targets)):\n",
    "    i_train = inputs[train].copy()\n",
    "    i_test = inputs[test].copy()\n",
    "    x_test_d5 = i_test[:,:2]\n",
    "    inputs_train = i_train[:,3:]\n",
    "    inputs_test = i_test[:,3:]\n",
    "    \n",
    "    ####################################################################################################\n",
    "    #test MWPM decoder for this fold\n",
    "    #labels = targets[train], features = inputs[train]\n",
    "    #x_test_d5 = translate_to_graph(testData_d5_MWPM, targets[test], mlb)\n",
    "    decoding_d5, time_mwpm = do_new_decoding(x_test_d5, 5, 0)\n",
    "    decoding_d5['combine'] = decoding_d5[[0, 1]].values.tolist()\n",
    "    decoding_d5['combine'].apply(lambda x: x[0].extend(x[1]))\n",
    "    decoding_d5 = np.array(decoding_d5[0])\n",
    "                                              \n",
    "    time_per_fold_mwpm.append(time_mwpm)\n",
    "                                              \n",
    "    pred_mwpm = mlb.transform(decoding_d5)\n",
    "    if fold_no < 5:\n",
    "        acc = partial_accuracy(targets[test], pred_mwpm)\n",
    "    else:\n",
    "        acc, contingency_mwpm = partial_accuracy_and_contingency(targets[test], pred_mwpm, mlb)\n",
    "        \n",
    "    acc_per_fold_mwpm.append(acc)\n",
    "    f1_per_fold_mwpm.append(f1_score(targets[test], pred_mwpm, average='micro'))\n",
    "    \n",
    "    #####################################################################################################\n",
    "    #test the plut decoder for this fold\n",
    "    \n",
    "    lookup_d5 = lookup_decoder(5)\n",
    "    \n",
    "    lookup_d5 = train_plut(lookup_d5, inputs_train, targets[train])\n",
    "    \n",
    "    start = time.time_ns()\n",
    "    pred_plut_d5 = test_plut(lookup_d5, inputs_test)\n",
    "    end = time.time_ns() \n",
    "    time_per_fold_plut.append((end - start)/ (10 ** 9))\n",
    "    \n",
    "    if fold_no < 5:\n",
    "        acc = partial_accuracy(targets[test], pred_plut_d5)\n",
    "    else:\n",
    "        acc, contingency_plut = partial_accuracy_and_contingency(targets[test], pred_plut_d5, mlb)\n",
    "        \n",
    "    acc_per_fold_plut.append(acc)\n",
    "    f1_per_fold_plut.append(f1_score(targets[test], pred_plut_d5, average='micro'))\n",
    "\n",
    "    #####################################################################################################\n",
    "    #Test the NN decoder for this fold\n",
    "    \n",
    "    model_d5 = compile_FFNN_model_DepthFive(5)\n",
    "    # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    # Fit data to model\n",
    "    \n",
    "    history = model_d5.fit(\n",
    "    inputs_train,\n",
    "    targets[train],\n",
    "    validation_split=.25,\n",
    "    epochs = 800\n",
    ")\n",
    "   # Generate generalization metrics\n",
    "    scores = model_d5.evaluate(inputs_test, targets[test], verbose=0)\n",
    "    \n",
    "    start = time.time_ns()\n",
    "    predictions_d5 = model_d5.predict(inputs_test)\n",
    "    end = time.time_ns() \n",
    "    time_per_fold.append((end - start)/ (10 ** 9))\n",
    "    \n",
    "    #threshold based on previous tests with train_test_split in hyperparameter tuning\n",
    "    pred=predictions_d5.copy() #change here\n",
    "    pred[pred>=.4]=1 \n",
    "    pred[pred<.4]=0\n",
    "    \n",
    "    if fold_no < 5:\n",
    "        acc = scores[1]\n",
    "    else:\n",
    "        acc, contingency_nn = partial_accuracy_and_contingency(targets[test], pred, mlb)\n",
    "\n",
    "    acc_per_fold.append(acc)\n",
    "    f1_per_fold.append(f1_score(targets[test], pred, average='micro'))\n",
    "\n",
    "    \n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(targets[test].ravel(), predictions_d5.ravel())\n",
    "    tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "    tprs[-1][0] = 0.0\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "    plt.plot(fpr, tpr, lw=1, alpha=0.3, label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
    "    \n",
    "    #get the AUCs of each class, used to get average AUC of each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(targets[test][:, i], predictions_d5[:, i]) \n",
    "        aucs_classes[mlb.classes_[i]].append(auc(fpr[i], tpr[i]))\n",
    "        \n",
    "        \n",
    "#########################################################################################################\n",
    "#compute McNemar's statistic on results of last fold\n",
    "\n",
    "mcnemar_results_mwpm = {}\n",
    "mcnemar_results_plut = {}\n",
    "\n",
    "for class_ in mlb.classes_:\n",
    "    \n",
    "    #compute the x^2 for NN and MWPM\n",
    "    mcnemar_results_mwpm[class_]=contingency_table_and_t(contingency_nn[class_], contingency_mwpm[class_])[1]    \n",
    "    #comput the x^2 for NN and PLUT\n",
    "    mcnemar_results_plut[class_] = contingency_table_and_t(contingency_nn[class_], contingency_plut[class_])[1]\n",
    "        \n",
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds of MWPM:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold_mwpm)} (+- {np.std(acc_per_fold_mwpm)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold_mwpm)}(+- {np.std(f1_per_fold_mwpm)})')\n",
    "print(f'> Time: {np.mean(time_per_fold_mwpm)} (+- {np.std(time_per_fold_mwpm)})')\n",
    "print(\"##############################################################################\")\n",
    "print('Average scores for all folds of PLUT:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold_plut)} (+- {np.std(acc_per_fold_plut)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold_plut)}(+- {np.std(f1_per_fold_plut)})')\n",
    "print(f'> Time: {np.mean(time_per_fold_plut)} (+- {np.std(time_per_fold_plut)})')\n",
    "print(\"##############################################################################\")\n",
    "print('Average scores for all folds of NN:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold)}(+- {np.std(f1_per_fold)})')\n",
    "print(f'> Time: {np.mean(time_per_fold)} (+- {np.std(time_per_fold)})')\n",
    "print(\"##############################################################################\")\n",
    "for key in aucs_classes:\n",
    "    cleanedList = [x for x in aucs_classes[key] if str(x) != 'nan']\n",
    "    print(f'> AUC for class {key}: {np.mean(cleanedList)} (+- {np.std(cleanedList)})') #this has to be a for loop\n",
    "    print(\"X^2 for MWPM and NN: \" + str(mcnemar_results_mwpm[key]))  \n",
    "    print(\"X^2 for PLUT and NN: \" + str(mcnemar_results_plut[key]))\n",
    "    \n",
    "print(\"###################################################################################\")\n",
    "print(\"TOTAL F1 NN: \" + str(f1_per_fold))\n",
    "print(\"TOTAL F1 PLUT: \" + str(f1_per_fold_plut))\n",
    "print(\"TOTAL F1 MWPM: \" + str(f1_per_fold_mwpm))\n",
    "print(\"TOTAL ACC NN: \" + str(acc_per_fold))\n",
    "print(\"TOTAL ACC PLUT: \" + str(acc_per_fold_plut))\n",
    "print(\"TOTAL ACC MWPM: \" + str(acc_per_fold_mwpm))\n",
    "print(\"TOTAL TIME NN: \" + str(time_per_fold))\n",
    "print(\"TOTAL TIME PLUT: \" + str(time_per_fold_plut))\n",
    "print(\"TOTAL TIME MWPM: \" + str(time_per_fold_mwpm))\n",
    "print('------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "         label='Chance', alpha=.8)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "         label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "         lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                 label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "plt.xlim([-0.01, 1.01])\n",
    "plt.ylim([-0.01, 1.01])\n",
    "plt.xlabel('False Positive Rate',fontsize=18)\n",
    "plt.ylabel('True Positive Rate',fontsize=18)\n",
    "plt.title('Cross-Validation ROC of Depth 5 NN',fontsize=18)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Train on 254089 samples, validate on 84697 samples\n",
      "Epoch 1/150\n",
      "254089/254089 [==============================] - 42s 164us/step - loss: 0.1630 - accuracy: 0.9604 - val_loss: 0.1871 - val_accuracy: 0.9523\n",
      "Epoch 2/150\n",
      "254089/254089 [==============================] - 40s 159us/step - loss: 0.1417 - accuracy: 0.9624 - val_loss: 0.1512 - val_accuracy: 0.9529\n",
      "Epoch 3/150\n",
      "254089/254089 [==============================] - 48s 189us/step - loss: 0.1117 - accuracy: 0.9643 - val_loss: 0.1195 - val_accuracy: 0.9587\n",
      "Epoch 4/150\n",
      "254089/254089 [==============================] - 56s 222us/step - loss: 0.0910 - accuracy: 0.9689 - val_loss: 0.1023 - val_accuracy: 0.9644\n",
      "Epoch 5/150\n",
      "254089/254089 [==============================] - 50s 195us/step - loss: 0.0787 - accuracy: 0.9727 - val_loss: 0.0936 - val_accuracy: 0.9677\n",
      "Epoch 6/150\n",
      "254089/254089 [==============================] - 46s 180us/step - loss: 0.0717 - accuracy: 0.9750 - val_loss: 0.0878 - val_accuracy: 0.9698\n",
      "Epoch 7/150\n",
      "254089/254089 [==============================] - 48s 190us/step - loss: 0.0668 - accuracy: 0.9766 - val_loss: 0.0832 - val_accuracy: 0.9716\n",
      "Epoch 8/150\n",
      "254089/254089 [==============================] - 47s 186us/step - loss: 0.0627 - accuracy: 0.9780 - val_loss: 0.0793 - val_accuracy: 0.9730\n",
      "Epoch 9/150\n",
      "254089/254089 [==============================] - 47s 184us/step - loss: 0.0596 - accuracy: 0.9790 - val_loss: 0.0761 - val_accuracy: 0.9741\n",
      "Epoch 10/150\n",
      "254089/254089 [==============================] - 47s 183us/step - loss: 0.0570 - accuracy: 0.9799 - val_loss: 0.0738 - val_accuracy: 0.9750\n",
      "Epoch 11/150\n",
      "254089/254089 [==============================] - 41s 160us/step - loss: 0.0548 - accuracy: 0.9807 - val_loss: 0.0710 - val_accuracy: 0.9758\n",
      "Epoch 12/150\n",
      "254089/254089 [==============================] - 43s 168us/step - loss: 0.0530 - accuracy: 0.9813 - val_loss: 0.0691 - val_accuracy: 0.9767\n",
      "Epoch 13/150\n",
      "254089/254089 [==============================] - 40s 158us/step - loss: 0.0515 - accuracy: 0.9818 - val_loss: 0.0666 - val_accuracy: 0.9773\n",
      "Epoch 14/150\n",
      "254089/254089 [==============================] - 53s 207us/step - loss: 0.0503 - accuracy: 0.9822 - val_loss: 0.0653 - val_accuracy: 0.9779\n",
      "Epoch 15/150\n",
      "254089/254089 [==============================] - 43s 169us/step - loss: 0.0492 - accuracy: 0.9825 - val_loss: 0.0643 - val_accuracy: 0.9781\n",
      "Epoch 16/150\n",
      "254089/254089 [==============================] - 44s 173us/step - loss: 0.0483 - accuracy: 0.9829 - val_loss: 0.0633 - val_accuracy: 0.9786\n",
      "Epoch 17/150\n",
      "254089/254089 [==============================] - 44s 172us/step - loss: 0.0475 - accuracy: 0.9831 - val_loss: 0.0620 - val_accuracy: 0.9790\n",
      "Epoch 18/150\n",
      "254089/254089 [==============================] - 55s 217us/step - loss: 0.0468 - accuracy: 0.9833 - val_loss: 0.0615 - val_accuracy: 0.9792\n",
      "Epoch 19/150\n",
      "254089/254089 [==============================] - 50s 198us/step - loss: 0.0461 - accuracy: 0.9835 - val_loss: 0.0605 - val_accuracy: 0.9795\n",
      "Epoch 20/150\n",
      "254089/254089 [==============================] - 45s 179us/step - loss: 0.0455 - accuracy: 0.9837 - val_loss: 0.0597 - val_accuracy: 0.9798\n",
      "Epoch 21/150\n",
      "254089/254089 [==============================] - 46s 182us/step - loss: 0.0450 - accuracy: 0.9838 - val_loss: 0.0586 - val_accuracy: 0.9801\n",
      "Epoch 22/150\n",
      "254089/254089 [==============================] - 47s 187us/step - loss: 0.0445 - accuracy: 0.9840 - val_loss: 0.0586 - val_accuracy: 0.9801\n",
      "Epoch 23/150\n",
      "254089/254089 [==============================] - 47s 184us/step - loss: 0.0440 - accuracy: 0.9841 - val_loss: 0.0575 - val_accuracy: 0.9804\n",
      "Epoch 24/150\n",
      "254089/254089 [==============================] - 44s 175us/step - loss: 0.0436 - accuracy: 0.9842 - val_loss: 0.0568 - val_accuracy: 0.9807\n",
      "Epoch 25/150\n",
      "254089/254089 [==============================] - 45s 179us/step - loss: 0.0431 - accuracy: 0.9843 - val_loss: 0.0569 - val_accuracy: 0.9807\n",
      "Epoch 26/150\n",
      "254089/254089 [==============================] - 45s 177us/step - loss: 0.0428 - accuracy: 0.9844 - val_loss: 0.0561 - val_accuracy: 0.9809\n",
      "Epoch 27/150\n",
      "254089/254089 [==============================] - 45s 177us/step - loss: 0.0424 - accuracy: 0.9845 - val_loss: 0.0554 - val_accuracy: 0.9811\n",
      "Epoch 28/150\n",
      "254089/254089 [==============================] - 45s 176us/step - loss: 0.0420 - accuracy: 0.9846 - val_loss: 0.0561 - val_accuracy: 0.9809\n",
      "Epoch 29/150\n",
      "254089/254089 [==============================] - 45s 177us/step - loss: 0.0417 - accuracy: 0.9847 - val_loss: 0.0548 - val_accuracy: 0.9814\n",
      "Epoch 30/150\n",
      "254089/254089 [==============================] - 45s 178us/step - loss: 0.0414 - accuracy: 0.9847 - val_loss: 0.0548 - val_accuracy: 0.9814\n",
      "Epoch 31/150\n",
      "254089/254089 [==============================] - 45s 177us/step - loss: 0.0411 - accuracy: 0.9848 - val_loss: 0.0539 - val_accuracy: 0.9816\n",
      "Epoch 32/150\n",
      "254089/254089 [==============================] - 45s 178us/step - loss: 0.0408 - accuracy: 0.9849 - val_loss: 0.0540 - val_accuracy: 0.9817\n",
      "Epoch 33/150\n",
      "254089/254089 [==============================] - 45s 177us/step - loss: 0.0405 - accuracy: 0.9850 - val_loss: 0.0533 - val_accuracy: 0.9817\n",
      "Epoch 34/150\n",
      "254089/254089 [==============================] - 46s 183us/step - loss: 0.0402 - accuracy: 0.9850 - val_loss: 0.0532 - val_accuracy: 0.9818\n",
      "Epoch 35/150\n",
      "254089/254089 [==============================] - 44s 174us/step - loss: 0.0400 - accuracy: 0.9851 - val_loss: 0.0527 - val_accuracy: 0.9820\n",
      "Epoch 36/150\n",
      "254089/254089 [==============================] - 47s 184us/step - loss: 0.0397 - accuracy: 0.9851 - val_loss: 0.0525 - val_accuracy: 0.9821\n",
      "Epoch 37/150\n",
      "254089/254089 [==============================] - 44s 173us/step - loss: 0.0395 - accuracy: 0.9852 - val_loss: 0.0524 - val_accuracy: 0.9821\n",
      "Epoch 38/150\n",
      "254089/254089 [==============================] - 44s 173us/step - loss: 0.0393 - accuracy: 0.9853 - val_loss: 0.0518 - val_accuracy: 0.9823\n",
      "Epoch 39/150\n",
      "254089/254089 [==============================] - 44s 174us/step - loss: 0.0390 - accuracy: 0.9853 - val_loss: 0.0517 - val_accuracy: 0.9823\n",
      "Epoch 40/150\n",
      "254089/254089 [==============================] - 44s 173us/step - loss: 0.0388 - accuracy: 0.9854 - val_loss: 0.0511 - val_accuracy: 0.9825\n",
      "Epoch 41/150\n",
      "254089/254089 [==============================] - 44s 174us/step - loss: 0.0386 - accuracy: 0.9854 - val_loss: 0.0506 - val_accuracy: 0.9826\n",
      "Epoch 42/150\n",
      "254089/254089 [==============================] - 50s 197us/step - loss: 0.0384 - accuracy: 0.9855 - val_loss: 0.0505 - val_accuracy: 0.9826\n",
      "Epoch 43/150\n",
      "254089/254089 [==============================] - 44s 174us/step - loss: 0.0382 - accuracy: 0.9855 - val_loss: 0.0503 - val_accuracy: 0.9826\n",
      "Epoch 44/150\n",
      "254089/254089 [==============================] - 44s 173us/step - loss: 0.0380 - accuracy: 0.9856 - val_loss: 0.0501 - val_accuracy: 0.9829\n",
      "Epoch 45/150\n",
      "254089/254089 [==============================] - 45s 176us/step - loss: 0.0378 - accuracy: 0.9856 - val_loss: 0.0502 - val_accuracy: 0.9828\n",
      "Epoch 46/150\n",
      "254089/254089 [==============================] - 45s 177us/step - loss: 0.0377 - accuracy: 0.9856 - val_loss: 0.0502 - val_accuracy: 0.9828\n",
      "Epoch 47/150\n",
      "254089/254089 [==============================] - 45s 177us/step - loss: 0.0375 - accuracy: 0.9857 - val_loss: 0.0498 - val_accuracy: 0.9829\n",
      "Epoch 48/150\n",
      "254089/254089 [==============================] - 45s 176us/step - loss: 0.0373 - accuracy: 0.9857 - val_loss: 0.0498 - val_accuracy: 0.9829\n",
      "Epoch 49/150\n",
      "254089/254089 [==============================] - 45s 178us/step - loss: 0.0371 - accuracy: 0.9858 - val_loss: 0.0491 - val_accuracy: 0.9831\n",
      "Epoch 50/150\n",
      "254089/254089 [==============================] - 45s 176us/step - loss: 0.0370 - accuracy: 0.9858 - val_loss: 0.0493 - val_accuracy: 0.9831\n",
      "Epoch 51/150\n",
      "254089/254089 [==============================] - 44s 173us/step - loss: 0.0368 - accuracy: 0.9859 - val_loss: 0.0485 - val_accuracy: 0.9833\n",
      "Epoch 52/150\n",
      "254089/254089 [==============================] - 44s 175us/step - loss: 0.0367 - accuracy: 0.9859 - val_loss: 0.0489 - val_accuracy: 0.9832\n",
      "Epoch 53/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254089/254089 [==============================] - 44s 175us/step - loss: 0.0365 - accuracy: 0.9859 - val_loss: 0.0486 - val_accuracy: 0.9833\n",
      "Epoch 54/150\n",
      "254089/254089 [==============================] - 44s 172us/step - loss: 0.0364 - accuracy: 0.9860 - val_loss: 0.0489 - val_accuracy: 0.9832\n",
      "Epoch 55/150\n",
      "254089/254089 [==============================] - 44s 173us/step - loss: 0.0362 - accuracy: 0.9860 - val_loss: 0.0486 - val_accuracy: 0.9833\n",
      "Epoch 56/150\n",
      "254089/254089 [==============================] - 46s 182us/step - loss: 0.0361 - accuracy: 0.9860 - val_loss: 0.0483 - val_accuracy: 0.9835\n",
      "Epoch 57/150\n",
      "254089/254089 [==============================] - 47s 184us/step - loss: 0.0359 - accuracy: 0.9861 - val_loss: 0.0482 - val_accuracy: 0.9835\n",
      "Epoch 58/150\n",
      "254089/254089 [==============================] - 49s 192us/step - loss: 0.0358 - accuracy: 0.9861 - val_loss: 0.0480 - val_accuracy: 0.9835\n",
      "Epoch 59/150\n",
      "254089/254089 [==============================] - 42s 163us/step - loss: 0.0357 - accuracy: 0.9862 - val_loss: 0.0479 - val_accuracy: 0.9835\n",
      "Epoch 60/150\n",
      "254089/254089 [==============================] - 44s 172us/step - loss: 0.0356 - accuracy: 0.9862 - val_loss: 0.0485 - val_accuracy: 0.9834\n",
      "Epoch 61/150\n",
      "254089/254089 [==============================] - 42s 165us/step - loss: 0.0354 - accuracy: 0.9862 - val_loss: 0.0477 - val_accuracy: 0.9836\n",
      "Epoch 62/150\n",
      "254089/254089 [==============================] - 40s 158us/step - loss: 0.0353 - accuracy: 0.9863 - val_loss: 0.0479 - val_accuracy: 0.9836\n",
      "Epoch 63/150\n",
      "254089/254089 [==============================] - 43s 167us/step - loss: 0.0352 - accuracy: 0.9863 - val_loss: 0.0472 - val_accuracy: 0.9837\n",
      "Epoch 64/150\n",
      "254089/254089 [==============================] - 40s 159us/step - loss: 0.0351 - accuracy: 0.9863 - val_loss: 0.0473 - val_accuracy: 0.9837\n",
      "Epoch 65/150\n",
      "254089/254089 [==============================] - 42s 167us/step - loss: 0.0350 - accuracy: 0.9864 - val_loss: 0.0472 - val_accuracy: 0.9838\n",
      "Epoch 66/150\n",
      "254089/254089 [==============================] - 45s 176us/step - loss: 0.0348 - accuracy: 0.9864 - val_loss: 0.0469 - val_accuracy: 0.9839\n",
      "Epoch 67/150\n",
      "254089/254089 [==============================] - 44s 173us/step - loss: 0.0347 - accuracy: 0.9864 - val_loss: 0.0467 - val_accuracy: 0.9839\n",
      "Epoch 68/150\n",
      "254089/254089 [==============================] - 43s 170us/step - loss: 0.0346 - accuracy: 0.9864 - val_loss: 0.0465 - val_accuracy: 0.9840\n",
      "Epoch 69/150\n",
      "254089/254089 [==============================] - 43s 171us/step - loss: 0.0345 - accuracy: 0.9865 - val_loss: 0.0467 - val_accuracy: 0.9840\n",
      "Epoch 70/150\n",
      "254089/254089 [==============================] - 44s 172us/step - loss: 0.0344 - accuracy: 0.9865 - val_loss: 0.0470 - val_accuracy: 0.9839\n",
      "Epoch 71/150\n",
      "254089/254089 [==============================] - 43s 170us/step - loss: 0.0343 - accuracy: 0.9865 - val_loss: 0.0464 - val_accuracy: 0.9841\n",
      "Epoch 72/150\n",
      "254089/254089 [==============================] - 44s 173us/step - loss: 0.0342 - accuracy: 0.9866 - val_loss: 0.0464 - val_accuracy: 0.9841\n",
      "Epoch 73/150\n",
      "254089/254089 [==============================] - 45s 175us/step - loss: 0.0341 - accuracy: 0.9866 - val_loss: 0.0462 - val_accuracy: 0.9842\n",
      "Epoch 74/150\n",
      "254089/254089 [==============================] - 44s 172us/step - loss: 0.0340 - accuracy: 0.9866 - val_loss: 0.0463 - val_accuracy: 0.9841\n",
      "Epoch 75/150\n",
      "254089/254089 [==============================] - 45s 176us/step - loss: 0.0339 - accuracy: 0.9866 - val_loss: 0.0458 - val_accuracy: 0.9843\n",
      "Epoch 76/150\n",
      "254089/254089 [==============================] - 44s 172us/step - loss: 0.0339 - accuracy: 0.9867 - val_loss: 0.0461 - val_accuracy: 0.9842\n",
      "Epoch 77/150\n",
      "254089/254089 [==============================] - 46s 181us/step - loss: 0.0338 - accuracy: 0.9867 - val_loss: 0.0455 - val_accuracy: 0.9843\n",
      "Epoch 78/150\n",
      "254089/254089 [==============================] - 43s 171us/step - loss: 0.0337 - accuracy: 0.9867 - val_loss: 0.0456 - val_accuracy: 0.9843\n",
      "Epoch 79/150\n",
      "254089/254089 [==============================] - 43s 170us/step - loss: 0.0336 - accuracy: 0.9867 - val_loss: 0.0457 - val_accuracy: 0.9842\n",
      "Epoch 80/150\n",
      "254089/254089 [==============================] - 43s 170us/step - loss: 0.0335 - accuracy: 0.9868 - val_loss: 0.0453 - val_accuracy: 0.9844\n",
      "Epoch 81/150\n",
      "254089/254089 [==============================] - 44s 172us/step - loss: 0.0334 - accuracy: 0.9868 - val_loss: 0.0458 - val_accuracy: 0.9842\n",
      "Epoch 82/150\n",
      "254089/254089 [==============================] - 43s 171us/step - loss: 0.0333 - accuracy: 0.9868 - val_loss: 0.0456 - val_accuracy: 0.9843\n",
      "Epoch 83/150\n",
      "254089/254089 [==============================] - 43s 170us/step - loss: 0.0333 - accuracy: 0.9868 - val_loss: 0.0455 - val_accuracy: 0.9844\n",
      "Epoch 84/150\n",
      "254089/254089 [==============================] - 44s 173us/step - loss: 0.0332 - accuracy: 0.9869 - val_loss: 0.0458 - val_accuracy: 0.9844\n",
      "Epoch 85/150\n",
      "254089/254089 [==============================] - 44s 171us/step - loss: 0.0331 - accuracy: 0.9869 - val_loss: 0.0451 - val_accuracy: 0.9845\n",
      "Epoch 86/150\n",
      "254089/254089 [==============================] - 44s 174us/step - loss: 0.0330 - accuracy: 0.9869 - val_loss: 0.0456 - val_accuracy: 0.9844\n",
      "Epoch 87/150\n",
      "254089/254089 [==============================] - 43s 170us/step - loss: 0.0330 - accuracy: 0.9869 - val_loss: 0.0453 - val_accuracy: 0.9844\n",
      "Epoch 88/150\n",
      "254089/254089 [==============================] - 44s 174us/step - loss: 0.0329 - accuracy: 0.9870 - val_loss: 0.0448 - val_accuracy: 0.9846\n",
      "Epoch 89/150\n",
      "254089/254089 [==============================] - 44s 175us/step - loss: 0.0328 - accuracy: 0.9870 - val_loss: 0.0451 - val_accuracy: 0.9845\n",
      "Epoch 90/150\n",
      "254089/254089 [==============================] - 45s 177us/step - loss: 0.0327 - accuracy: 0.9870 - val_loss: 0.0453 - val_accuracy: 0.9845\n",
      "Epoch 91/150\n",
      "254089/254089 [==============================] - 45s 175us/step - loss: 0.0327 - accuracy: 0.9870 - val_loss: 0.0448 - val_accuracy: 0.9846\n",
      "Epoch 92/150\n",
      "254089/254089 [==============================] - 43s 171us/step - loss: 0.0326 - accuracy: 0.9870 - val_loss: 0.0449 - val_accuracy: 0.9845\n",
      "Epoch 93/150\n",
      "254089/254089 [==============================] - 46s 183us/step - loss: 0.0325 - accuracy: 0.9871 - val_loss: 0.0447 - val_accuracy: 0.9847\n",
      "Epoch 94/150\n",
      "254089/254089 [==============================] - 45s 177us/step - loss: 0.0325 - accuracy: 0.9871 - val_loss: 0.0445 - val_accuracy: 0.9846\n",
      "Epoch 95/150\n",
      "254089/254089 [==============================] - 46s 180us/step - loss: 0.0324 - accuracy: 0.9871 - val_loss: 0.0447 - val_accuracy: 0.9847\n",
      "Epoch 96/150\n",
      "254089/254089 [==============================] - 44s 174us/step - loss: 0.0323 - accuracy: 0.9872 - val_loss: 0.0448 - val_accuracy: 0.9846\n",
      "Epoch 97/150\n",
      "254089/254089 [==============================] - 46s 182us/step - loss: 0.0323 - accuracy: 0.9872 - val_loss: 0.0443 - val_accuracy: 0.9848\n",
      "Epoch 98/150\n",
      "254089/254089 [==============================] - 44s 172us/step - loss: 0.0322 - accuracy: 0.9872 - val_loss: 0.0444 - val_accuracy: 0.9848\n",
      "Epoch 99/150\n",
      "254089/254089 [==============================] - 47s 187us/step - loss: 0.0321 - accuracy: 0.9872 - val_loss: 0.0443 - val_accuracy: 0.9848\n",
      "Epoch 100/150\n",
      "254089/254089 [==============================] - 46s 181us/step - loss: 0.0321 - accuracy: 0.9872 - val_loss: 0.0447 - val_accuracy: 0.9847\n",
      "Epoch 101/150\n",
      "254089/254089 [==============================] - 43s 171us/step - loss: 0.0320 - accuracy: 0.9872 - val_loss: 0.0444 - val_accuracy: 0.9848\n",
      "Epoch 102/150\n",
      "254089/254089 [==============================] - 44s 172us/step - loss: 0.0319 - accuracy: 0.9873 - val_loss: 0.0439 - val_accuracy: 0.9849\n",
      "Epoch 103/150\n",
      "254089/254089 [==============================] - 46s 179us/step - loss: 0.0319 - accuracy: 0.9873 - val_loss: 0.0445 - val_accuracy: 0.9847\n",
      "Epoch 104/150\n",
      "254089/254089 [==============================] - 46s 181us/step - loss: 0.0318 - accuracy: 0.9873 - val_loss: 0.0446 - val_accuracy: 0.9847\n",
      "Epoch 105/150\n",
      "254089/254089 [==============================] - 43s 168us/step - loss: 0.0318 - accuracy: 0.9873 - val_loss: 0.0444 - val_accuracy: 0.9848\n",
      "Epoch 106/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254089/254089 [==============================] - 41s 160us/step - loss: 0.0317 - accuracy: 0.9873 - val_loss: 0.0437 - val_accuracy: 0.9849\n",
      "Epoch 107/150\n",
      "254089/254089 [==============================] - 44s 174us/step - loss: 0.0316 - accuracy: 0.9873 - val_loss: 0.0443 - val_accuracy: 0.9848\n",
      "Epoch 108/150\n",
      "254089/254089 [==============================] - 46s 183us/step - loss: 0.0316 - accuracy: 0.9874 - val_loss: 0.0440 - val_accuracy: 0.9849\n",
      "Epoch 109/150\n",
      "254089/254089 [==============================] - 39s 155us/step - loss: 0.0315 - accuracy: 0.9874 - val_loss: 0.0440 - val_accuracy: 0.9849\n",
      "Epoch 110/150\n",
      "254089/254089 [==============================] - 39s 155us/step - loss: 0.0315 - accuracy: 0.9874 - val_loss: 0.0436 - val_accuracy: 0.9850\n",
      "Epoch 111/150\n",
      "254089/254089 [==============================] - 39s 155us/step - loss: 0.0314 - accuracy: 0.9874 - val_loss: 0.0434 - val_accuracy: 0.9850\n",
      "Epoch 112/150\n",
      "254089/254089 [==============================] - 40s 157us/step - loss: 0.0314 - accuracy: 0.9874 - val_loss: 0.0435 - val_accuracy: 0.9850\n",
      "Epoch 113/150\n",
      "254089/254089 [==============================] - 39s 155us/step - loss: 0.0313 - accuracy: 0.9874 - val_loss: 0.0441 - val_accuracy: 0.9849\n",
      "Epoch 114/150\n",
      "254089/254089 [==============================] - 40s 159us/step - loss: 0.0313 - accuracy: 0.9875 - val_loss: 0.0441 - val_accuracy: 0.9849\n",
      "Epoch 115/150\n",
      "254089/254089 [==============================] - 40s 156us/step - loss: 0.0312 - accuracy: 0.9875 - val_loss: 0.0438 - val_accuracy: 0.9849\n",
      "Epoch 116/150\n",
      "254089/254089 [==============================] - 58s 227us/step - loss: 0.0312 - accuracy: 0.9875 - val_loss: 0.0436 - val_accuracy: 0.9850\n",
      "Epoch 117/150\n",
      "254089/254089 [==============================] - 99s 388us/step - loss: 0.0311 - accuracy: 0.9875 - val_loss: 0.0431 - val_accuracy: 0.9851\n",
      "Epoch 118/150\n",
      "254089/254089 [==============================] - 95s 374us/step - loss: 0.0311 - accuracy: 0.9875 - val_loss: 0.0440 - val_accuracy: 0.9848\n",
      "Epoch 119/150\n",
      "254089/254089 [==============================] - 99s 391us/step - loss: 0.0310 - accuracy: 0.9875 - val_loss: 0.0438 - val_accuracy: 0.9850\n",
      "Epoch 120/150\n",
      "254089/254089 [==============================] - 95s 376us/step - loss: 0.0310 - accuracy: 0.9875 - val_loss: 0.0433 - val_accuracy: 0.9851\n",
      "Epoch 121/150\n",
      "254089/254089 [==============================] - 96s 380us/step - loss: 0.0309 - accuracy: 0.9876 - val_loss: 0.0436 - val_accuracy: 0.9851\n",
      "Epoch 122/150\n",
      "254089/254089 [==============================] - 106s 416us/step - loss: 0.0309 - accuracy: 0.9876 - val_loss: 0.0435 - val_accuracy: 0.9851\n",
      "Epoch 123/150\n",
      "254089/254089 [==============================] - 102s 401us/step - loss: 0.0308 - accuracy: 0.9876 - val_loss: 0.0431 - val_accuracy: 0.9851\n",
      "Epoch 124/150\n",
      "254089/254089 [==============================] - 96s 377us/step - loss: 0.0308 - accuracy: 0.9876 - val_loss: 0.0433 - val_accuracy: 0.9852\n",
      "Epoch 125/150\n",
      "254089/254089 [==============================] - 94s 372us/step - loss: 0.0307 - accuracy: 0.9876 - val_loss: 0.0440 - val_accuracy: 0.9850\n",
      "Epoch 126/150\n",
      "254089/254089 [==============================] - 94s 371us/step - loss: 0.0307 - accuracy: 0.9876 - val_loss: 0.0432 - val_accuracy: 0.9852\n",
      "Epoch 127/150\n",
      "254089/254089 [==============================] - 98s 384us/step - loss: 0.0306 - accuracy: 0.9877 - val_loss: 0.0430 - val_accuracy: 0.9852\n",
      "Epoch 128/150\n",
      "254089/254089 [==============================] - 74s 290us/step - loss: 0.0306 - accuracy: 0.9877 - val_loss: 0.0427 - val_accuracy: 0.9852\n",
      "Epoch 129/150\n",
      "254089/254089 [==============================] - 40s 159us/step - loss: 0.0305 - accuracy: 0.9877 - val_loss: 0.0431 - val_accuracy: 0.9852\n",
      "Epoch 130/150\n",
      "254089/254089 [==============================] - 40s 156us/step - loss: 0.0305 - accuracy: 0.9877 - val_loss: 0.0432 - val_accuracy: 0.9851\n",
      "Epoch 131/150\n",
      "254089/254089 [==============================] - 41s 161us/step - loss: 0.0304 - accuracy: 0.9877 - val_loss: 0.0435 - val_accuracy: 0.9852\n",
      "Epoch 132/150\n",
      "254089/254089 [==============================] - 41s 160us/step - loss: 0.0304 - accuracy: 0.9877 - val_loss: 0.0420 - val_accuracy: 0.9855\n",
      "Epoch 133/150\n",
      "254089/254089 [==============================] - 39s 154us/step - loss: 0.0304 - accuracy: 0.9877 - val_loss: 0.0423 - val_accuracy: 0.9854\n",
      "Epoch 134/150\n",
      "254089/254089 [==============================] - 40s 156us/step - loss: 0.0303 - accuracy: 0.9878 - val_loss: 0.0434 - val_accuracy: 0.9852\n",
      "Epoch 135/150\n",
      "254089/254089 [==============================] - 40s 156us/step - loss: 0.0303 - accuracy: 0.9878 - val_loss: 0.0424 - val_accuracy: 0.9854\n",
      "Epoch 136/150\n",
      "254089/254089 [==============================] - 39s 155us/step - loss: 0.0302 - accuracy: 0.9878 - val_loss: 0.0426 - val_accuracy: 0.9853\n",
      "Epoch 137/150\n",
      "254089/254089 [==============================] - 39s 155us/step - loss: 0.0302 - accuracy: 0.9878 - val_loss: 0.0426 - val_accuracy: 0.9854\n",
      "Epoch 138/150\n",
      "254089/254089 [==============================] - 39s 155us/step - loss: 0.0302 - accuracy: 0.9878 - val_loss: 0.0431 - val_accuracy: 0.9853\n",
      "Epoch 139/150\n",
      "254089/254089 [==============================] - 39s 155us/step - loss: 0.0301 - accuracy: 0.9878 - val_loss: 0.0425 - val_accuracy: 0.9854\n",
      "Epoch 140/150\n",
      "254089/254089 [==============================] - 39s 155us/step - loss: 0.0301 - accuracy: 0.9878 - val_loss: 0.0424 - val_accuracy: 0.9854\n",
      "Epoch 141/150\n",
      "254089/254089 [==============================] - 40s 156us/step - loss: 0.0300 - accuracy: 0.9878 - val_loss: 0.0424 - val_accuracy: 0.9853\n",
      "Epoch 142/150\n",
      "254089/254089 [==============================] - 40s 157us/step - loss: 0.0300 - accuracy: 0.9878 - val_loss: 0.0422 - val_accuracy: 0.9854\n",
      "Epoch 143/150\n",
      "254089/254089 [==============================] - 39s 155us/step - loss: 0.0300 - accuracy: 0.9879 - val_loss: 0.0420 - val_accuracy: 0.9855\n",
      "Epoch 144/150\n",
      "254089/254089 [==============================] - 40s 157us/step - loss: 0.0299 - accuracy: 0.9879 - val_loss: 0.0426 - val_accuracy: 0.9854\n",
      "Epoch 145/150\n",
      "254089/254089 [==============================] - 39s 155us/step - loss: 0.0299 - accuracy: 0.9879 - val_loss: 0.0423 - val_accuracy: 0.9854\n",
      "Epoch 146/150\n",
      "254089/254089 [==============================] - 40s 158us/step - loss: 0.0298 - accuracy: 0.9879 - val_loss: 0.0424 - val_accuracy: 0.9854\n",
      "Epoch 147/150\n",
      "254089/254089 [==============================] - 40s 159us/step - loss: 0.0298 - accuracy: 0.9879 - val_loss: 0.0424 - val_accuracy: 0.9854\n",
      "Epoch 148/150\n",
      "254089/254089 [==============================] - 39s 155us/step - loss: 0.0298 - accuracy: 0.9879 - val_loss: 0.0421 - val_accuracy: 0.9855\n",
      "Epoch 149/150\n",
      "254089/254089 [==============================] - 40s 156us/step - loss: 0.0297 - accuracy: 0.9879 - val_loss: 0.0421 - val_accuracy: 0.9856\n",
      "Epoch 150/150\n",
      "254089/254089 [==============================] - 39s 154us/step - loss: 0.0297 - accuracy: 0.9879 - val_loss: 0.0419 - val_accuracy: 0.9855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clair\\Anaconda3\\envs\\CNNs\\lib\\site-packages\\ipykernel_launcher.py:136: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "C:\\Users\\clair\\Anaconda3\\envs\\CNNs\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:808: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Train on 254090 samples, validate on 84697 samples\n",
      "Epoch 1/150\n",
      "254090/254090 [==============================] - 40s 159us/step - loss: 0.1632 - accuracy: 0.9607 - val_loss: 0.1882 - val_accuracy: 0.9523\n",
      "Epoch 2/150\n",
      "254090/254090 [==============================] - 40s 158us/step - loss: 0.1426 - accuracy: 0.9624 - val_loss: 0.1496 - val_accuracy: 0.9528\n",
      "Epoch 3/150\n",
      "254090/254090 [==============================] - 42s 165us/step - loss: 0.1105 - accuracy: 0.9645 - val_loss: 0.1188 - val_accuracy: 0.9585\n",
      "Epoch 4/150\n",
      "254090/254090 [==============================] - 46s 179us/step - loss: 0.0921 - accuracy: 0.9683 - val_loss: 0.1044 - val_accuracy: 0.9634\n",
      "Epoch 5/150\n",
      "254090/254090 [==============================] - 40s 158us/step - loss: 0.0805 - accuracy: 0.9718 - val_loss: 0.0942 - val_accuracy: 0.9668\n",
      "Epoch 6/150\n",
      "254090/254090 [==============================] - 41s 160us/step - loss: 0.0729 - accuracy: 0.9744 - val_loss: 0.0881 - val_accuracy: 0.9690\n",
      "Epoch 7/150\n",
      "254090/254090 [==============================] - 40s 158us/step - loss: 0.0677 - accuracy: 0.9762 - val_loss: 0.0837 - val_accuracy: 0.9706\n",
      "Epoch 8/150\n",
      "254090/254090 [==============================] - 40s 159us/step - loss: 0.0638 - accuracy: 0.9775 - val_loss: 0.0799 - val_accuracy: 0.9721\n",
      "Epoch 9/150\n",
      "254090/254090 [==============================] - 42s 167us/step - loss: 0.0607 - accuracy: 0.9785 - val_loss: 0.0767 - val_accuracy: 0.9733\n",
      "Epoch 10/150\n",
      "254090/254090 [==============================] - 41s 161us/step - loss: 0.0582 - accuracy: 0.9794 - val_loss: 0.0742 - val_accuracy: 0.9743\n",
      "Epoch 11/150\n",
      "254090/254090 [==============================] - 41s 162us/step - loss: 0.0559 - accuracy: 0.9802 - val_loss: 0.0726 - val_accuracy: 0.9750\n",
      "Epoch 12/150\n",
      "254090/254090 [==============================] - 40s 158us/step - loss: 0.0541 - accuracy: 0.9808 - val_loss: 0.0704 - val_accuracy: 0.9758\n",
      "Epoch 13/150\n",
      "254090/254090 [==============================] - 42s 166us/step - loss: 0.0525 - accuracy: 0.9814 - val_loss: 0.0683 - val_accuracy: 0.9765\n",
      "Epoch 14/150\n",
      "254090/254090 [==============================] - 40s 159us/step - loss: 0.0512 - accuracy: 0.9818 - val_loss: 0.0668 - val_accuracy: 0.9771\n",
      "Epoch 15/150\n",
      "254090/254090 [==============================] - 41s 159us/step - loss: 0.0500 - accuracy: 0.9822 - val_loss: 0.0653 - val_accuracy: 0.9775\n",
      "Epoch 16/150\n",
      "254090/254090 [==============================] - 40s 158us/step - loss: 0.0490 - accuracy: 0.9826 - val_loss: 0.0640 - val_accuracy: 0.9779\n",
      "Epoch 17/150\n",
      "254090/254090 [==============================] - 41s 160us/step - loss: 0.0481 - accuracy: 0.9828 - val_loss: 0.0624 - val_accuracy: 0.9784\n",
      "Epoch 18/150\n",
      "254090/254090 [==============================] - 40s 157us/step - loss: 0.0474 - accuracy: 0.9831 - val_loss: 0.0623 - val_accuracy: 0.9786\n",
      "Epoch 19/150\n",
      "254090/254090 [==============================] - 40s 158us/step - loss: 0.0467 - accuracy: 0.9833 - val_loss: 0.0615 - val_accuracy: 0.9789\n",
      "Epoch 20/150\n",
      "254090/254090 [==============================] - 40s 158us/step - loss: 0.0461 - accuracy: 0.9834 - val_loss: 0.0605 - val_accuracy: 0.9793\n",
      "Epoch 21/150\n",
      "254090/254090 [==============================] - 41s 161us/step - loss: 0.0455 - accuracy: 0.9836 - val_loss: 0.0594 - val_accuracy: 0.9796\n",
      "Epoch 22/150\n",
      "254090/254090 [==============================] - 40s 157us/step - loss: 0.0450 - accuracy: 0.9837 - val_loss: 0.0593 - val_accuracy: 0.9797\n",
      "Epoch 23/150\n",
      "254090/254090 [==============================] - 40s 158us/step - loss: 0.0445 - accuracy: 0.9839 - val_loss: 0.0585 - val_accuracy: 0.9800\n",
      "Epoch 24/150\n",
      "254090/254090 [==============================] - 41s 161us/step - loss: 0.0440 - accuracy: 0.9840 - val_loss: 0.0590 - val_accuracy: 0.9797\n",
      "Epoch 25/150\n",
      "254090/254090 [==============================] - 40s 158us/step - loss: 0.0436 - accuracy: 0.9841 - val_loss: 0.0568 - val_accuracy: 0.9804\n",
      "Epoch 26/150\n",
      "254090/254090 [==============================] - 41s 162us/step - loss: 0.0432 - accuracy: 0.9842 - val_loss: 0.0569 - val_accuracy: 0.9804\n",
      "Epoch 27/150\n",
      "254090/254090 [==============================] - 41s 162us/step - loss: 0.0428 - accuracy: 0.9843 - val_loss: 0.0570 - val_accuracy: 0.9804\n",
      "Epoch 28/150\n",
      "254090/254090 [==============================] - 46s 182us/step - loss: 0.0424 - accuracy: 0.9844 - val_loss: 0.0556 - val_accuracy: 0.9808\n",
      "Epoch 29/150\n",
      "254090/254090 [==============================] - 43s 171us/step - loss: 0.0421 - accuracy: 0.9845 - val_loss: 0.0561 - val_accuracy: 0.9808\n",
      "Epoch 30/150\n",
      "254090/254090 [==============================] - 43s 169us/step - loss: 0.0417 - accuracy: 0.9846 - val_loss: 0.0559 - val_accuracy: 0.9809\n",
      "Epoch 31/150\n",
      "254090/254090 [==============================] - 44s 174us/step - loss: 0.0414 - accuracy: 0.9847 - val_loss: 0.0545 - val_accuracy: 0.9813\n",
      "Epoch 32/150\n",
      "254090/254090 [==============================] - 44s 172us/step - loss: 0.0411 - accuracy: 0.9847 - val_loss: 0.0548 - val_accuracy: 0.9811\n",
      "Epoch 33/150\n",
      "254090/254090 [==============================] - 47s 186us/step - loss: 0.0408 - accuracy: 0.9848 - val_loss: 0.0541 - val_accuracy: 0.9813\n",
      "Epoch 34/150\n",
      "254090/254090 [==============================] - 50s 197us/step - loss: 0.0405 - accuracy: 0.9849 - val_loss: 0.0531 - val_accuracy: 0.9816\n",
      "Epoch 35/150\n",
      "254090/254090 [==============================] - 57s 224us/step - loss: 0.0403 - accuracy: 0.9850 - val_loss: 0.0531 - val_accuracy: 0.9817\n",
      "Epoch 36/150\n",
      "254090/254090 [==============================] - 43s 168us/step - loss: 0.0400 - accuracy: 0.9850 - val_loss: 0.0537 - val_accuracy: 0.9816\n",
      "Epoch 37/150\n",
      "254090/254090 [==============================] - 47s 184us/step - loss: 0.0397 - accuracy: 0.9851 - val_loss: 0.0535 - val_accuracy: 0.9817\n",
      "Epoch 38/150\n",
      "254090/254090 [==============================] - 45s 176us/step - loss: 0.0395 - accuracy: 0.9851 - val_loss: 0.0525 - val_accuracy: 0.9819\n",
      "Epoch 39/150\n",
      "254090/254090 [==============================] - 41s 161us/step - loss: 0.0393 - accuracy: 0.9852 - val_loss: 0.0525 - val_accuracy: 0.9819\n",
      "Epoch 40/150\n",
      "254090/254090 [==============================] - 41s 162us/step - loss: 0.0390 - accuracy: 0.9852 - val_loss: 0.0527 - val_accuracy: 0.9820\n",
      "Epoch 41/150\n",
      "254090/254090 [==============================] - 42s 167us/step - loss: 0.0388 - accuracy: 0.9853 - val_loss: 0.0518 - val_accuracy: 0.9822\n",
      "Epoch 42/150\n",
      "254090/254090 [==============================] - 41s 163us/step - loss: 0.0386 - accuracy: 0.9854 - val_loss: 0.0525 - val_accuracy: 0.9820\n",
      "Epoch 43/150\n",
      "254090/254090 [==============================] - 41s 160us/step - loss: 0.0384 - accuracy: 0.9854 - val_loss: 0.0504 - val_accuracy: 0.9825\n",
      "Epoch 44/150\n",
      "254090/254090 [==============================] - 43s 168us/step - loss: 0.0382 - accuracy: 0.9855 - val_loss: 0.0511 - val_accuracy: 0.9823\n",
      "Epoch 45/150\n",
      "254090/254090 [==============================] - 41s 161us/step - loss: 0.0380 - accuracy: 0.9855 - val_loss: 0.0519 - val_accuracy: 0.9822\n",
      "Epoch 46/150\n",
      "254090/254090 [==============================] - 40s 159us/step - loss: 0.0378 - accuracy: 0.9855 - val_loss: 0.0504 - val_accuracy: 0.9826\n",
      "Epoch 47/150\n",
      "254090/254090 [==============================] - 41s 163us/step - loss: 0.0376 - accuracy: 0.9856 - val_loss: 0.0503 - val_accuracy: 0.9827\n",
      "Epoch 48/150\n",
      "254090/254090 [==============================] - 42s 165us/step - loss: 0.0375 - accuracy: 0.9856 - val_loss: 0.0507 - val_accuracy: 0.9826\n",
      "Epoch 49/150\n",
      "254090/254090 [==============================] - 41s 162us/step - loss: 0.0373 - accuracy: 0.9857 - val_loss: 0.0497 - val_accuracy: 0.9828\n",
      "Epoch 50/150\n",
      "254090/254090 [==============================] - 40s 159us/step - loss: 0.0371 - accuracy: 0.9857 - val_loss: 0.0498 - val_accuracy: 0.9828\n",
      "Epoch 51/150\n",
      "254090/254090 [==============================] - 41s 159us/step - loss: 0.0370 - accuracy: 0.9858 - val_loss: 0.0496 - val_accuracy: 0.9828\n",
      "Epoch 52/150\n",
      "254090/254090 [==============================] - 40s 159us/step - loss: 0.0368 - accuracy: 0.9858 - val_loss: 0.0488 - val_accuracy: 0.9830\n",
      "Epoch 53/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254090/254090 [==============================] - 40s 159us/step - loss: 0.0367 - accuracy: 0.9858 - val_loss: 0.0494 - val_accuracy: 0.9829\n",
      "Epoch 54/150\n",
      "254090/254090 [==============================] - 41s 161us/step - loss: 0.0366 - accuracy: 0.9859 - val_loss: 0.0500 - val_accuracy: 0.9829\n",
      "Epoch 55/150\n",
      "254090/254090 [==============================] - 41s 160us/step - loss: 0.0364 - accuracy: 0.9859 - val_loss: 0.0491 - val_accuracy: 0.9830\n",
      "Epoch 56/150\n",
      "254090/254090 [==============================] - 43s 168us/step - loss: 0.0363 - accuracy: 0.9860 - val_loss: 0.0493 - val_accuracy: 0.9831\n",
      "Epoch 57/150\n",
      "254090/254090 [==============================] - 42s 165us/step - loss: 0.0362 - accuracy: 0.9860 - val_loss: 0.0486 - val_accuracy: 0.9832\n",
      "Epoch 58/150\n",
      "254090/254090 [==============================] - 40s 159us/step - loss: 0.0360 - accuracy: 0.9860 - val_loss: 0.0485 - val_accuracy: 0.9832\n",
      "Epoch 59/150\n",
      "254090/254090 [==============================] - 41s 160us/step - loss: 0.0359 - accuracy: 0.9861 - val_loss: 0.0486 - val_accuracy: 0.9832\n",
      "Epoch 60/150\n",
      "254090/254090 [==============================] - 40s 158us/step - loss: 0.0358 - accuracy: 0.9861 - val_loss: 0.0483 - val_accuracy: 0.9833\n",
      "Epoch 61/150\n",
      "254090/254090 [==============================] - 40s 159us/step - loss: 0.0357 - accuracy: 0.9861 - val_loss: 0.0480 - val_accuracy: 0.9834\n",
      "Epoch 62/150\n",
      "254090/254090 [==============================] - 40s 158us/step - loss: 0.0355 - accuracy: 0.9861 - val_loss: 0.0486 - val_accuracy: 0.9833\n",
      "Epoch 63/150\n",
      "254090/254090 [==============================] - 40s 158us/step - loss: 0.0354 - accuracy: 0.9862 - val_loss: 0.0483 - val_accuracy: 0.9834\n",
      "Epoch 64/150\n",
      "254090/254090 [==============================] - 40s 158us/step - loss: 0.0353 - accuracy: 0.9862 - val_loss: 0.0480 - val_accuracy: 0.9835\n",
      "Epoch 65/150\n",
      "254090/254090 [==============================] - 41s 159us/step - loss: 0.0352 - accuracy: 0.9863 - val_loss: 0.0480 - val_accuracy: 0.9834\n",
      "Epoch 66/150\n",
      "254090/254090 [==============================] - 40s 158us/step - loss: 0.0351 - accuracy: 0.9863 - val_loss: 0.0477 - val_accuracy: 0.9835\n",
      "Epoch 67/150\n",
      "254090/254090 [==============================] - 40s 159us/step - loss: 0.0350 - accuracy: 0.9863 - val_loss: 0.0481 - val_accuracy: 0.9834\n",
      "Epoch 68/150\n",
      "254090/254090 [==============================] - 41s 159us/step - loss: 0.0349 - accuracy: 0.9863 - val_loss: 0.0473 - val_accuracy: 0.9838\n",
      "Epoch 69/150\n",
      "254090/254090 [==============================] - 43s 169us/step - loss: 0.0348 - accuracy: 0.9864 - val_loss: 0.0478 - val_accuracy: 0.9835\n",
      "Epoch 70/150\n",
      "254090/254090 [==============================] - 47s 184us/step - loss: 0.0347 - accuracy: 0.9864 - val_loss: 0.0472 - val_accuracy: 0.9838\n",
      "Epoch 71/150\n",
      "254090/254090 [==============================] - 42s 163us/step - loss: 0.0346 - accuracy: 0.9864 - val_loss: 0.0473 - val_accuracy: 0.9837\n",
      "Epoch 72/150\n",
      "254090/254090 [==============================] - 42s 167us/step - loss: 0.0345 - accuracy: 0.9864 - val_loss: 0.0469 - val_accuracy: 0.9838\n",
      "Epoch 73/150\n",
      "254090/254090 [==============================] - 41s 162us/step - loss: 0.0344 - accuracy: 0.9865 - val_loss: 0.0469 - val_accuracy: 0.9838\n",
      "Epoch 74/150\n",
      "254090/254090 [==============================] - 40s 159us/step - loss: 0.0343 - accuracy: 0.9865 - val_loss: 0.0470 - val_accuracy: 0.9837\n",
      "Epoch 75/150\n",
      "254090/254090 [==============================] - 40s 158us/step - loss: 0.0342 - accuracy: 0.9865 - val_loss: 0.0473 - val_accuracy: 0.9837\n",
      "Epoch 76/150\n",
      "254090/254090 [==============================] - 40s 158us/step - loss: 0.0342 - accuracy: 0.9866 - val_loss: 0.0466 - val_accuracy: 0.9838\n",
      "Epoch 77/150\n",
      "254090/254090 [==============================] - 40s 159us/step - loss: 0.0341 - accuracy: 0.9866 - val_loss: 0.0464 - val_accuracy: 0.9840\n",
      "Epoch 78/150\n",
      "254090/254090 [==============================] - 40s 158us/step - loss: 0.0340 - accuracy: 0.9866 - val_loss: 0.0472 - val_accuracy: 0.9838\n",
      "Epoch 79/150\n",
      "254090/254090 [==============================] - 40s 159us/step - loss: 0.0339 - accuracy: 0.9866 - val_loss: 0.0466 - val_accuracy: 0.9839\n",
      "Epoch 80/150\n",
      "254090/254090 [==============================] - 40s 158us/step - loss: 0.0338 - accuracy: 0.9867 - val_loss: 0.0464 - val_accuracy: 0.9840\n",
      "Epoch 81/150\n",
      "254090/254090 [==============================] - 40s 159us/step - loss: 0.0337 - accuracy: 0.9867 - val_loss: 0.0465 - val_accuracy: 0.9840\n",
      "Epoch 82/150\n",
      "254090/254090 [==============================] - 40s 158us/step - loss: 0.0337 - accuracy: 0.9867 - val_loss: 0.0466 - val_accuracy: 0.9840\n",
      "Epoch 83/150\n",
      "254090/254090 [==============================] - 40s 159us/step - loss: 0.0336 - accuracy: 0.9867 - val_loss: 0.0460 - val_accuracy: 0.9842\n",
      "Epoch 84/150\n",
      "254090/254090 [==============================] - 41s 162us/step - loss: 0.0335 - accuracy: 0.9868 - val_loss: 0.0468 - val_accuracy: 0.9841\n",
      "Epoch 85/150\n",
      "254090/254090 [==============================] - 42s 166us/step - loss: 0.0334 - accuracy: 0.9868 - val_loss: 0.0459 - val_accuracy: 0.9842\n",
      "Epoch 86/150\n",
      "254090/254090 [==============================] - 41s 162us/step - loss: 0.0334 - accuracy: 0.9868 - val_loss: 0.0459 - val_accuracy: 0.9841\n",
      "Epoch 87/150\n",
      "254090/254090 [==============================] - 40s 158us/step - loss: 0.0333 - accuracy: 0.9868 - val_loss: 0.0465 - val_accuracy: 0.9841\n",
      "Epoch 88/150\n",
      "254090/254090 [==============================] - 41s 160us/step - loss: 0.0332 - accuracy: 0.9869 - val_loss: 0.0464 - val_accuracy: 0.9842\n",
      "Epoch 89/150\n",
      "254090/254090 [==============================] - 40s 158us/step - loss: 0.0331 - accuracy: 0.9869 - val_loss: 0.0459 - val_accuracy: 0.9842\n",
      "Epoch 90/150\n",
      "254090/254090 [==============================] - 41s 160us/step - loss: 0.0331 - accuracy: 0.9869 - val_loss: 0.0459 - val_accuracy: 0.9842\n",
      "Epoch 91/150\n",
      "254090/254090 [==============================] - 40s 158us/step - loss: 0.0330 - accuracy: 0.9869 - val_loss: 0.0457 - val_accuracy: 0.9843\n",
      "Epoch 92/150\n",
      "254090/254090 [==============================] - 40s 158us/step - loss: 0.0329 - accuracy: 0.9869 - val_loss: 0.0459 - val_accuracy: 0.9842\n",
      "Epoch 93/150\n",
      "254090/254090 [==============================] - 40s 157us/step - loss: 0.0329 - accuracy: 0.9870 - val_loss: 0.0454 - val_accuracy: 0.9843\n",
      "Epoch 94/150\n",
      "254090/254090 [==============================] - 40s 159us/step - loss: 0.0328 - accuracy: 0.9870 - val_loss: 0.0454 - val_accuracy: 0.9844\n",
      "Epoch 95/150\n",
      "254090/254090 [==============================] - 40s 158us/step - loss: 0.0327 - accuracy: 0.9870 - val_loss: 0.0458 - val_accuracy: 0.9843\n",
      "Epoch 96/150\n",
      "254090/254090 [==============================] - 40s 159us/step - loss: 0.0327 - accuracy: 0.9870 - val_loss: 0.0446 - val_accuracy: 0.9846\n",
      "Epoch 97/150\n",
      "254090/254090 [==============================] - 40s 159us/step - loss: 0.0326 - accuracy: 0.9870 - val_loss: 0.0453 - val_accuracy: 0.9845\n",
      "Epoch 98/150\n",
      "254090/254090 [==============================] - 40s 158us/step - loss: 0.0325 - accuracy: 0.9871 - val_loss: 0.0447 - val_accuracy: 0.9846\n",
      "Epoch 99/150\n",
      "254090/254090 [==============================] - 40s 157us/step - loss: 0.0325 - accuracy: 0.9871 - val_loss: 0.0455 - val_accuracy: 0.9845\n",
      "Epoch 100/150\n",
      "254090/254090 [==============================] - 43s 168us/step - loss: 0.0324 - accuracy: 0.9871 - val_loss: 0.0451 - val_accuracy: 0.9845\n",
      "Epoch 101/150\n",
      "254090/254090 [==============================] - 41s 162us/step - loss: 0.0323 - accuracy: 0.9871 - val_loss: 0.0453 - val_accuracy: 0.9845\n",
      "Epoch 102/150\n",
      "254090/254090 [==============================] - 40s 159us/step - loss: 0.0323 - accuracy: 0.9871 - val_loss: 0.0451 - val_accuracy: 0.9845\n",
      "Epoch 103/150\n",
      "254090/254090 [==============================] - 43s 168us/step - loss: 0.0322 - accuracy: 0.9872 - val_loss: 0.0449 - val_accuracy: 0.9844\n",
      "Epoch 104/150\n",
      "254090/254090 [==============================] - 40s 159us/step - loss: 0.0322 - accuracy: 0.9872 - val_loss: 0.0450 - val_accuracy: 0.9846\n",
      "Epoch 105/150\n",
      "254090/254090 [==============================] - 40s 157us/step - loss: 0.0321 - accuracy: 0.9872 - val_loss: 0.0446 - val_accuracy: 0.9847\n",
      "Epoch 106/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254090/254090 [==============================] - 40s 156us/step - loss: 0.0320 - accuracy: 0.9872 - val_loss: 0.0450 - val_accuracy: 0.9846\n",
      "Epoch 107/150\n",
      "254090/254090 [==============================] - 40s 157us/step - loss: 0.0320 - accuracy: 0.9872 - val_loss: 0.0447 - val_accuracy: 0.9847\n",
      "Epoch 108/150\n",
      "254090/254090 [==============================] - 40s 156us/step - loss: 0.0319 - accuracy: 0.9872 - val_loss: 0.0446 - val_accuracy: 0.9847\n",
      "Epoch 109/150\n",
      "254090/254090 [==============================] - 40s 157us/step - loss: 0.0319 - accuracy: 0.9872 - val_loss: 0.0448 - val_accuracy: 0.9847\n",
      "Epoch 110/150\n",
      "254090/254090 [==============================] - 40s 157us/step - loss: 0.0318 - accuracy: 0.9873 - val_loss: 0.0448 - val_accuracy: 0.9846\n",
      "Epoch 111/150\n",
      "254090/254090 [==============================] - 40s 157us/step - loss: 0.0318 - accuracy: 0.9873 - val_loss: 0.0446 - val_accuracy: 0.9846\n",
      "Epoch 112/150\n",
      "254090/254090 [==============================] - 40s 158us/step - loss: 0.0317 - accuracy: 0.9873 - val_loss: 0.0443 - val_accuracy: 0.9848\n",
      "Epoch 113/150\n",
      "254090/254090 [==============================] - 40s 157us/step - loss: 0.0317 - accuracy: 0.9873 - val_loss: 0.0448 - val_accuracy: 0.9846\n",
      "Epoch 114/150\n",
      "254090/254090 [==============================] - 40s 157us/step - loss: 0.0316 - accuracy: 0.9873 - val_loss: 0.0445 - val_accuracy: 0.9846\n",
      "Epoch 115/150\n",
      "254090/254090 [==============================] - 42s 165us/step - loss: 0.0316 - accuracy: 0.9874 - val_loss: 0.0445 - val_accuracy: 0.9848\n",
      "Epoch 116/150\n",
      "254090/254090 [==============================] - 41s 160us/step - loss: 0.0315 - accuracy: 0.9874 - val_loss: 0.0444 - val_accuracy: 0.9848\n",
      "Epoch 117/150\n",
      "254090/254090 [==============================] - 40s 157us/step - loss: 0.0315 - accuracy: 0.9874 - val_loss: 0.0439 - val_accuracy: 0.9848\n",
      "Epoch 118/150\n",
      "254090/254090 [==============================] - 40s 157us/step - loss: 0.0314 - accuracy: 0.9874 - val_loss: 0.0439 - val_accuracy: 0.9848\n",
      "Epoch 119/150\n",
      "254090/254090 [==============================] - 40s 159us/step - loss: 0.0314 - accuracy: 0.9874 - val_loss: 0.0446 - val_accuracy: 0.9847\n",
      "Epoch 120/150\n",
      "254090/254090 [==============================] - 40s 157us/step - loss: 0.0313 - accuracy: 0.9874 - val_loss: 0.0446 - val_accuracy: 0.9848\n",
      "Epoch 121/150\n",
      "254090/254090 [==============================] - 40s 158us/step - loss: 0.0313 - accuracy: 0.9874 - val_loss: 0.0436 - val_accuracy: 0.9850\n",
      "Epoch 122/150\n",
      "254090/254090 [==============================] - 40s 157us/step - loss: 0.0312 - accuracy: 0.9875 - val_loss: 0.0439 - val_accuracy: 0.9849\n",
      "Epoch 123/150\n",
      "254090/254090 [==============================] - 40s 159us/step - loss: 0.0312 - accuracy: 0.9875 - val_loss: 0.0434 - val_accuracy: 0.9850\n",
      "Epoch 124/150\n",
      "254090/254090 [==============================] - 40s 159us/step - loss: 0.0311 - accuracy: 0.9875 - val_loss: 0.0441 - val_accuracy: 0.9849\n",
      "Epoch 125/150\n",
      "254090/254090 [==============================] - 41s 160us/step - loss: 0.0311 - accuracy: 0.9875 - val_loss: 0.0442 - val_accuracy: 0.9849\n",
      "Epoch 126/150\n",
      "254090/254090 [==============================] - 40s 159us/step - loss: 0.0310 - accuracy: 0.9875 - val_loss: 0.0434 - val_accuracy: 0.9851\n",
      "Epoch 127/150\n",
      "254090/254090 [==============================] - 40s 158us/step - loss: 0.0310 - accuracy: 0.9875 - val_loss: 0.0436 - val_accuracy: 0.9850\n",
      "Epoch 128/150\n",
      "254090/254090 [==============================] - 40s 159us/step - loss: 0.0309 - accuracy: 0.9875 - val_loss: 0.0441 - val_accuracy: 0.9850\n",
      "Epoch 129/150\n",
      "254090/254090 [==============================] - 40s 159us/step - loss: 0.0309 - accuracy: 0.9876 - val_loss: 0.0443 - val_accuracy: 0.9849\n",
      "Epoch 130/150\n",
      "254090/254090 [==============================] - 42s 167us/step - loss: 0.0308 - accuracy: 0.9876 - val_loss: 0.0438 - val_accuracy: 0.9849\n",
      "Epoch 131/150\n",
      "254090/254090 [==============================] - 41s 162us/step - loss: 0.0308 - accuracy: 0.9876 - val_loss: 0.0437 - val_accuracy: 0.9850\n",
      "Epoch 132/150\n",
      "254090/254090 [==============================] - 40s 159us/step - loss: 0.0307 - accuracy: 0.9876 - val_loss: 0.0435 - val_accuracy: 0.9851\n",
      "Epoch 133/150\n",
      "254090/254090 [==============================] - 41s 162us/step - loss: 0.0307 - accuracy: 0.9876 - val_loss: 0.0439 - val_accuracy: 0.9851\n",
      "Epoch 134/150\n",
      "254090/254090 [==============================] - 41s 161us/step - loss: 0.0307 - accuracy: 0.9876 - val_loss: 0.0441 - val_accuracy: 0.9849\n",
      "Epoch 135/150\n",
      "254090/254090 [==============================] - 41s 163us/step - loss: 0.0306 - accuracy: 0.9877 - val_loss: 0.0429 - val_accuracy: 0.9852\n",
      "Epoch 136/150\n",
      "254090/254090 [==============================] - 45s 175us/step - loss: 0.0306 - accuracy: 0.9877 - val_loss: 0.0433 - val_accuracy: 0.9851\n",
      "Epoch 137/150\n",
      "254090/254090 [==============================] - 42s 167us/step - loss: 0.0305 - accuracy: 0.9877 - val_loss: 0.0442 - val_accuracy: 0.9850\n",
      "Epoch 138/150\n",
      "254090/254090 [==============================] - 45s 178us/step - loss: 0.0305 - accuracy: 0.9877 - val_loss: 0.0430 - val_accuracy: 0.9851\n",
      "Epoch 139/150\n",
      "254090/254090 [==============================] - 45s 177us/step - loss: 0.0305 - accuracy: 0.9877 - val_loss: 0.0439 - val_accuracy: 0.9851\n",
      "Epoch 140/150\n",
      "254090/254090 [==============================] - 47s 185us/step - loss: 0.0304 - accuracy: 0.9877 - val_loss: 0.0431 - val_accuracy: 0.9852\n",
      "Epoch 141/150\n",
      "254090/254090 [==============================] - 52s 203us/step - loss: 0.0304 - accuracy: 0.9877 - val_loss: 0.0443 - val_accuracy: 0.9850\n",
      "Epoch 142/150\n",
      "254090/254090 [==============================] - 41s 163us/step - loss: 0.0303 - accuracy: 0.9877 - val_loss: 0.0437 - val_accuracy: 0.9851\n",
      "Epoch 143/150\n",
      "254090/254090 [==============================] - 48s 188us/step - loss: 0.0303 - accuracy: 0.9877 - val_loss: 0.0433 - val_accuracy: 0.9852\n",
      "Epoch 144/150\n",
      "254090/254090 [==============================] - 42s 165us/step - loss: 0.0303 - accuracy: 0.9878 - val_loss: 0.0433 - val_accuracy: 0.9852\n",
      "Epoch 145/150\n",
      "254090/254090 [==============================] - 42s 164us/step - loss: 0.0302 - accuracy: 0.9878 - val_loss: 0.0434 - val_accuracy: 0.9852\n",
      "Epoch 146/150\n",
      "254090/254090 [==============================] - 40s 159us/step - loss: 0.0302 - accuracy: 0.9878 - val_loss: 0.0429 - val_accuracy: 0.9853\n",
      "Epoch 147/150\n",
      "254090/254090 [==============================] - 41s 162us/step - loss: 0.0301 - accuracy: 0.9878 - val_loss: 0.0431 - val_accuracy: 0.9852\n",
      "Epoch 148/150\n",
      "254090/254090 [==============================] - 42s 164us/step - loss: 0.0301 - accuracy: 0.9878 - val_loss: 0.0427 - val_accuracy: 0.9854\n",
      "Epoch 149/150\n",
      "254090/254090 [==============================] - 41s 163us/step - loss: 0.0301 - accuracy: 0.9878 - val_loss: 0.0436 - val_accuracy: 0.9851\n",
      "Epoch 150/150\n",
      "254090/254090 [==============================] - 41s 162us/step - loss: 0.0300 - accuracy: 0.9878 - val_loss: 0.0435 - val_accuracy: 0.9851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clair\\Anaconda3\\envs\\CNNs\\lib\\site-packages\\ipykernel_launcher.py:136: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "C:\\Users\\clair\\Anaconda3\\envs\\CNNs\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:808: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Train on 254090 samples, validate on 84697 samples\n",
      "Epoch 1/150\n",
      "254090/254090 [==============================] - 39s 152us/step - loss: 0.1634 - accuracy: 0.9600 - val_loss: 0.1869 - val_accuracy: 0.9522\n",
      "Epoch 2/150\n",
      "254090/254090 [==============================] - 37s 147us/step - loss: 0.1408 - accuracy: 0.9625 - val_loss: 0.1512 - val_accuracy: 0.9528\n",
      "Epoch 3/150\n",
      "254090/254090 [==============================] - 38s 151us/step - loss: 0.1115 - accuracy: 0.9644 - val_loss: 0.1201 - val_accuracy: 0.9585\n",
      "Epoch 4/150\n",
      "254090/254090 [==============================] - 40s 156us/step - loss: 0.0908 - accuracy: 0.9688 - val_loss: 0.1040 - val_accuracy: 0.9634\n",
      "Epoch 5/150\n",
      "254090/254090 [==============================] - 40s 158us/step - loss: 0.0797 - accuracy: 0.9721 - val_loss: 0.0949 - val_accuracy: 0.9666\n",
      "Epoch 6/150\n",
      "254090/254090 [==============================] - 40s 156us/step - loss: 0.0724 - accuracy: 0.9745 - val_loss: 0.0888 - val_accuracy: 0.9689\n",
      "Epoch 7/150\n",
      "254090/254090 [==============================] - 43s 168us/step - loss: 0.0671 - accuracy: 0.9763 - val_loss: 0.0840 - val_accuracy: 0.9707\n",
      "Epoch 8/150\n",
      "254090/254090 [==============================] - 42s 165us/step - loss: 0.0630 - accuracy: 0.9777 - val_loss: 0.0801 - val_accuracy: 0.9722\n",
      "Epoch 9/150\n",
      "254090/254090 [==============================] - 40s 157us/step - loss: 0.0597 - accuracy: 0.9788 - val_loss: 0.0771 - val_accuracy: 0.9733\n",
      "Epoch 10/150\n",
      "254090/254090 [==============================] - 41s 162us/step - loss: 0.0571 - accuracy: 0.9797 - val_loss: 0.0742 - val_accuracy: 0.9744\n",
      "Epoch 11/150\n",
      "254090/254090 [==============================] - 40s 157us/step - loss: 0.0549 - accuracy: 0.9805 - val_loss: 0.0721 - val_accuracy: 0.9752\n",
      "Epoch 12/150\n",
      "254090/254090 [==============================] - 40s 158us/step - loss: 0.0532 - accuracy: 0.9811 - val_loss: 0.0698 - val_accuracy: 0.9760\n",
      "Epoch 13/150\n",
      "254090/254090 [==============================] - 42s 166us/step - loss: 0.0516 - accuracy: 0.9816 - val_loss: 0.0681 - val_accuracy: 0.9767\n",
      "Epoch 14/150\n",
      "254090/254090 [==============================] - 41s 160us/step - loss: 0.0504 - accuracy: 0.9820 - val_loss: 0.0664 - val_accuracy: 0.9773\n",
      "Epoch 15/150\n",
      "254090/254090 [==============================] - 40s 158us/step - loss: 0.0493 - accuracy: 0.9824 - val_loss: 0.0652 - val_accuracy: 0.9777\n",
      "Epoch 16/150\n",
      "254090/254090 [==============================] - 40s 158us/step - loss: 0.0483 - accuracy: 0.9827 - val_loss: 0.0638 - val_accuracy: 0.9781\n",
      "Epoch 17/150\n",
      "254090/254090 [==============================] - 40s 158us/step - loss: 0.0475 - accuracy: 0.9830 - val_loss: 0.0623 - val_accuracy: 0.9786\n",
      "Epoch 18/150\n",
      "254090/254090 [==============================] - 41s 160us/step - loss: 0.0468 - accuracy: 0.9832 - val_loss: 0.0609 - val_accuracy: 0.9790\n",
      "Epoch 19/150\n",
      "254090/254090 [==============================] - 41s 160us/step - loss: 0.0461 - accuracy: 0.9834 - val_loss: 0.0604 - val_accuracy: 0.9793\n",
      "Epoch 20/150\n",
      "254090/254090 [==============================] - 41s 159us/step - loss: 0.0455 - accuracy: 0.9836 - val_loss: 0.0605 - val_accuracy: 0.9794\n",
      "Epoch 21/150\n",
      "254090/254090 [==============================] - 40s 158us/step - loss: 0.0449 - accuracy: 0.9837 - val_loss: 0.0594 - val_accuracy: 0.9797\n",
      "Epoch 22/150\n",
      "254090/254090 [==============================] - 41s 161us/step - loss: 0.0444 - accuracy: 0.9839 - val_loss: 0.0596 - val_accuracy: 0.9797\n",
      "Epoch 23/150\n",
      "254090/254090 [==============================] - 42s 166us/step - loss: 0.0440 - accuracy: 0.9840 - val_loss: 0.0579 - val_accuracy: 0.9801\n",
      "Epoch 24/150\n",
      "254090/254090 [==============================] - 41s 159us/step - loss: 0.0435 - accuracy: 0.9841 - val_loss: 0.0576 - val_accuracy: 0.9803\n",
      "Epoch 25/150\n",
      "254090/254090 [==============================] - 41s 161us/step - loss: 0.0431 - accuracy: 0.9842 - val_loss: 0.0569 - val_accuracy: 0.9806\n",
      "Epoch 26/150\n",
      "254090/254090 [==============================] - 40s 159us/step - loss: 0.0427 - accuracy: 0.9843 - val_loss: 0.0564 - val_accuracy: 0.9807\n",
      "Epoch 27/150\n",
      "254090/254090 [==============================] - 41s 163us/step - loss: 0.0423 - accuracy: 0.9844 - val_loss: 0.0566 - val_accuracy: 0.9807\n",
      "Epoch 28/150\n",
      "254090/254090 [==============================] - 42s 164us/step - loss: 0.0420 - accuracy: 0.9845 - val_loss: 0.0560 - val_accuracy: 0.9809\n",
      "Epoch 29/150\n",
      "254090/254090 [==============================] - 40s 159us/step - loss: 0.0417 - accuracy: 0.9846 - val_loss: 0.0546 - val_accuracy: 0.9812\n",
      "Epoch 30/150\n",
      "254090/254090 [==============================] - 40s 159us/step - loss: 0.0413 - accuracy: 0.9847 - val_loss: 0.0548 - val_accuracy: 0.9812\n",
      "Epoch 31/150\n",
      "254090/254090 [==============================] - 40s 159us/step - loss: 0.0410 - accuracy: 0.9847 - val_loss: 0.0547 - val_accuracy: 0.9813\n",
      "Epoch 32/150\n",
      "254090/254090 [==============================] - 41s 161us/step - loss: 0.0407 - accuracy: 0.9848 - val_loss: 0.0548 - val_accuracy: 0.9813\n",
      "Epoch 33/150\n",
      "254090/254090 [==============================] - 40s 159us/step - loss: 0.0405 - accuracy: 0.9849 - val_loss: 0.0540 - val_accuracy: 0.9815\n",
      "Epoch 34/150\n",
      "254090/254090 [==============================] - 40s 158us/step - loss: 0.0402 - accuracy: 0.9849 - val_loss: 0.0534 - val_accuracy: 0.9817\n",
      "Epoch 35/150\n",
      "254090/254090 [==============================] - 40s 159us/step - loss: 0.0399 - accuracy: 0.9850 - val_loss: 0.0531 - val_accuracy: 0.9817\n",
      "Epoch 36/150\n",
      "254090/254090 [==============================] - 41s 163us/step - loss: 0.0397 - accuracy: 0.9851 - val_loss: 0.0529 - val_accuracy: 0.9819\n",
      "Epoch 37/150\n",
      "254090/254090 [==============================] - 40s 159us/step - loss: 0.0394 - accuracy: 0.9851 - val_loss: 0.0522 - val_accuracy: 0.9820\n",
      "Epoch 38/150\n",
      "254090/254090 [==============================] - 41s 160us/step - loss: 0.0392 - accuracy: 0.9852 - val_loss: 0.0528 - val_accuracy: 0.9819\n",
      "Epoch 39/150\n",
      "254090/254090 [==============================] - 41s 163us/step - loss: 0.0390 - accuracy: 0.9853 - val_loss: 0.0524 - val_accuracy: 0.9820\n",
      "Epoch 40/150\n",
      "254090/254090 [==============================] - 42s 165us/step - loss: 0.0388 - accuracy: 0.9853 - val_loss: 0.0515 - val_accuracy: 0.9823\n",
      "Epoch 41/150\n",
      "254090/254090 [==============================] - 40s 159us/step - loss: 0.0386 - accuracy: 0.9854 - val_loss: 0.0520 - val_accuracy: 0.9822\n",
      "Epoch 42/150\n",
      "254090/254090 [==============================] - 43s 167us/step - loss: 0.0384 - accuracy: 0.9854 - val_loss: 0.0519 - val_accuracy: 0.9821\n",
      "Epoch 43/150\n",
      "254090/254090 [==============================] - 40s 158us/step - loss: 0.0382 - accuracy: 0.9855 - val_loss: 0.0520 - val_accuracy: 0.9821\n",
      "Epoch 44/150\n",
      "254090/254090 [==============================] - 40s 159us/step - loss: 0.0380 - accuracy: 0.9855 - val_loss: 0.0508 - val_accuracy: 0.9825\n",
      "Epoch 45/150\n",
      "254090/254090 [==============================] - 41s 160us/step - loss: 0.0378 - accuracy: 0.9856 - val_loss: 0.0511 - val_accuracy: 0.9824\n",
      "Epoch 46/150\n",
      "254090/254090 [==============================] - 40s 159us/step - loss: 0.0376 - accuracy: 0.9856 - val_loss: 0.0511 - val_accuracy: 0.9825\n",
      "Epoch 47/150\n",
      "254090/254090 [==============================] - 40s 158us/step - loss: 0.0374 - accuracy: 0.9857 - val_loss: 0.0505 - val_accuracy: 0.9827\n",
      "Epoch 48/150\n",
      "254090/254090 [==============================] - 40s 159us/step - loss: 0.0373 - accuracy: 0.9857 - val_loss: 0.0505 - val_accuracy: 0.9827\n",
      "Epoch 49/150\n",
      "254090/254090 [==============================] - 40s 158us/step - loss: 0.0371 - accuracy: 0.9858 - val_loss: 0.0501 - val_accuracy: 0.9827\n",
      "Epoch 50/150\n",
      "254090/254090 [==============================] - 41s 159us/step - loss: 0.0370 - accuracy: 0.9858 - val_loss: 0.0494 - val_accuracy: 0.9830\n",
      "Epoch 51/150\n",
      "254090/254090 [==============================] - 40s 159us/step - loss: 0.0368 - accuracy: 0.9858 - val_loss: 0.0496 - val_accuracy: 0.9830\n",
      "Epoch 52/150\n",
      "254090/254090 [==============================] - 41s 160us/step - loss: 0.0367 - accuracy: 0.9859 - val_loss: 0.0494 - val_accuracy: 0.9830\n",
      "Epoch 53/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254090/254090 [==============================] - 40s 159us/step - loss: 0.0365 - accuracy: 0.9859 - val_loss: 0.0492 - val_accuracy: 0.9830\n",
      "Epoch 54/150\n",
      "254090/254090 [==============================] - 41s 163us/step - loss: 0.0364 - accuracy: 0.9860 - val_loss: 0.0493 - val_accuracy: 0.9831\n",
      "Epoch 55/150\n",
      "254090/254090 [==============================] - 40s 159us/step - loss: 0.0363 - accuracy: 0.9860 - val_loss: 0.0489 - val_accuracy: 0.9831\n",
      "Epoch 56/150\n",
      "254090/254090 [==============================] - 40s 157us/step - loss: 0.0361 - accuracy: 0.9860 - val_loss: 0.0494 - val_accuracy: 0.9831\n",
      "Epoch 57/150\n",
      "254090/254090 [==============================] - 42s 166us/step - loss: 0.0360 - accuracy: 0.9861 - val_loss: 0.0491 - val_accuracy: 0.9832\n",
      "Epoch 58/150\n",
      "254090/254090 [==============================] - 40s 159us/step - loss: 0.0359 - accuracy: 0.9861 - val_loss: 0.0481 - val_accuracy: 0.9833\n",
      "Epoch 59/150\n",
      "254090/254090 [==============================] - 41s 159us/step - loss: 0.0358 - accuracy: 0.9861 - val_loss: 0.0483 - val_accuracy: 0.9833\n",
      "Epoch 60/150\n",
      "254090/254090 [==============================] - 43s 168us/step - loss: 0.0356 - accuracy: 0.9862 - val_loss: 0.0481 - val_accuracy: 0.9833\n",
      "Epoch 61/150\n",
      "254090/254090 [==============================] - 41s 160us/step - loss: 0.0355 - accuracy: 0.9862 - val_loss: 0.0482 - val_accuracy: 0.9834\n",
      "Epoch 62/150\n",
      "254090/254090 [==============================] - 40s 158us/step - loss: 0.0354 - accuracy: 0.9862 - val_loss: 0.0485 - val_accuracy: 0.9833\n",
      "Epoch 63/150\n",
      "254090/254090 [==============================] - 41s 160us/step - loss: 0.0353 - accuracy: 0.9863 - val_loss: 0.0480 - val_accuracy: 0.9835\n",
      "Epoch 64/150\n",
      "254090/254090 [==============================] - 40s 158us/step - loss: 0.0352 - accuracy: 0.9863 - val_loss: 0.0477 - val_accuracy: 0.9836\n",
      "Epoch 65/150\n",
      "254090/254090 [==============================] - 40s 158us/step - loss: 0.0351 - accuracy: 0.9863 - val_loss: 0.0476 - val_accuracy: 0.9836\n",
      "Epoch 66/150\n",
      "254090/254090 [==============================] - 41s 160us/step - loss: 0.0350 - accuracy: 0.9864 - val_loss: 0.0478 - val_accuracy: 0.9836\n",
      "Epoch 67/150\n",
      "254090/254090 [==============================] - 41s 162us/step - loss: 0.0349 - accuracy: 0.9864 - val_loss: 0.0474 - val_accuracy: 0.9837\n",
      "Epoch 68/150\n",
      "254090/254090 [==============================] - 40s 159us/step - loss: 0.0348 - accuracy: 0.9864 - val_loss: 0.0476 - val_accuracy: 0.9836\n",
      "Epoch 69/150\n",
      "254090/254090 [==============================] - 42s 164us/step - loss: 0.0347 - accuracy: 0.9864 - val_loss: 0.0473 - val_accuracy: 0.9836\n",
      "Epoch 70/150\n",
      "254090/254090 [==============================] - 41s 160us/step - loss: 0.0346 - accuracy: 0.9864 - val_loss: 0.0470 - val_accuracy: 0.9838\n",
      "Epoch 71/150\n",
      "254090/254090 [==============================] - 41s 161us/step - loss: 0.0345 - accuracy: 0.9865 - val_loss: 0.0469 - val_accuracy: 0.9838\n",
      "Epoch 72/150\n",
      "254090/254090 [==============================] - 43s 169us/step - loss: 0.0344 - accuracy: 0.9865 - val_loss: 0.0470 - val_accuracy: 0.9838\n",
      "Epoch 73/150\n",
      "254090/254090 [==============================] - 40s 158us/step - loss: 0.0343 - accuracy: 0.9865 - val_loss: 0.0468 - val_accuracy: 0.9838\n",
      "Epoch 74/150\n",
      "254090/254090 [==============================] - 40s 159us/step - loss: 0.0343 - accuracy: 0.9866 - val_loss: 0.0471 - val_accuracy: 0.9838\n",
      "Epoch 75/150\n",
      "254090/254090 [==============================] - 40s 159us/step - loss: 0.0342 - accuracy: 0.9866 - val_loss: 0.0470 - val_accuracy: 0.9838\n",
      "Epoch 76/150\n",
      "254090/254090 [==============================] - 41s 161us/step - loss: 0.0341 - accuracy: 0.9866 - val_loss: 0.0472 - val_accuracy: 0.9839\n",
      "Epoch 77/150\n",
      "254090/254090 [==============================] - 40s 159us/step - loss: 0.0340 - accuracy: 0.9866 - val_loss: 0.0471 - val_accuracy: 0.9838\n",
      "Epoch 78/150\n",
      "254090/254090 [==============================] - 40s 158us/step - loss: 0.0339 - accuracy: 0.9867 - val_loss: 0.0473 - val_accuracy: 0.9839\n",
      "Epoch 79/150\n",
      "254090/254090 [==============================] - 40s 159us/step - loss: 0.0339 - accuracy: 0.9867 - val_loss: 0.0463 - val_accuracy: 0.9840\n",
      "Epoch 80/150\n",
      "254090/254090 [==============================] - 40s 159us/step - loss: 0.0338 - accuracy: 0.9867 - val_loss: 0.0459 - val_accuracy: 0.9841\n",
      "Epoch 81/150\n",
      "254090/254090 [==============================] - 41s 161us/step - loss: 0.0337 - accuracy: 0.9867 - val_loss: 0.0460 - val_accuracy: 0.9841\n",
      "Epoch 82/150\n",
      "254090/254090 [==============================] - 40s 158us/step - loss: 0.0336 - accuracy: 0.9868 - val_loss: 0.0465 - val_accuracy: 0.9840\n",
      "Epoch 83/150\n",
      "254090/254090 [==============================] - 41s 161us/step - loss: 0.0335 - accuracy: 0.9868 - val_loss: 0.0471 - val_accuracy: 0.9838\n",
      "Epoch 84/150\n",
      "254090/254090 [==============================] - 41s 162us/step - loss: 0.0335 - accuracy: 0.9868 - val_loss: 0.0463 - val_accuracy: 0.9841\n",
      "Epoch 85/150\n",
      "254090/254090 [==============================] - 40s 159us/step - loss: 0.0334 - accuracy: 0.9868 - val_loss: 0.0459 - val_accuracy: 0.9842\n",
      "Epoch 86/150\n",
      "254090/254090 [==============================] - 42s 164us/step - loss: 0.0333 - accuracy: 0.9869 - val_loss: 0.0458 - val_accuracy: 0.9841\n",
      "Epoch 87/150\n",
      "254090/254090 [==============================] - 41s 160us/step - loss: 0.0333 - accuracy: 0.9869 - val_loss: 0.0462 - val_accuracy: 0.9841\n",
      "Epoch 88/150\n",
      "254090/254090 [==============================] - 40s 159us/step - loss: 0.0332 - accuracy: 0.9869 - val_loss: 0.0463 - val_accuracy: 0.9842\n",
      "Epoch 89/150\n",
      "254090/254090 [==============================] - 41s 161us/step - loss: 0.0331 - accuracy: 0.9869 - val_loss: 0.0461 - val_accuracy: 0.9841\n",
      "Epoch 90/150\n",
      "254090/254090 [==============================] - 41s 163us/step - loss: 0.0331 - accuracy: 0.9869 - val_loss: 0.0459 - val_accuracy: 0.9843\n",
      "Epoch 91/150\n",
      "254090/254090 [==============================] - 40s 158us/step - loss: 0.0330 - accuracy: 0.9869 - val_loss: 0.0458 - val_accuracy: 0.9843\n",
      "Epoch 92/150\n",
      "254090/254090 [==============================] - 40s 158us/step - loss: 0.0329 - accuracy: 0.9870 - val_loss: 0.0457 - val_accuracy: 0.9843\n",
      "Epoch 93/150\n",
      "254090/254090 [==============================] - 40s 157us/step - loss: 0.0329 - accuracy: 0.9870 - val_loss: 0.0463 - val_accuracy: 0.9843\n",
      "Epoch 94/150\n",
      "254090/254090 [==============================] - 41s 162us/step - loss: 0.0328 - accuracy: 0.9870 - val_loss: 0.0459 - val_accuracy: 0.9842\n",
      "Epoch 95/150\n",
      "254090/254090 [==============================] - 40s 158us/step - loss: 0.0327 - accuracy: 0.9870 - val_loss: 0.0456 - val_accuracy: 0.9843\n",
      "Epoch 96/150\n",
      "254090/254090 [==============================] - 40s 158us/step - loss: 0.0327 - accuracy: 0.9870 - val_loss: 0.0455 - val_accuracy: 0.9844\n",
      "Epoch 97/150\n",
      "254090/254090 [==============================] - 40s 159us/step - loss: 0.0326 - accuracy: 0.9870 - val_loss: 0.0458 - val_accuracy: 0.9844\n",
      "Epoch 98/150\n",
      "254090/254090 [==============================] - 41s 161us/step - loss: 0.0326 - accuracy: 0.9871 - val_loss: 0.0457 - val_accuracy: 0.9843\n",
      "Epoch 99/150\n",
      "254090/254090 [==============================] - 41s 162us/step - loss: 0.0325 - accuracy: 0.9871 - val_loss: 0.0456 - val_accuracy: 0.9844\n",
      "Epoch 100/150\n",
      "254090/254090 [==============================] - 40s 157us/step - loss: 0.0324 - accuracy: 0.9871 - val_loss: 0.0452 - val_accuracy: 0.9845\n",
      "Epoch 101/150\n",
      "254090/254090 [==============================] - 42s 166us/step - loss: 0.0324 - accuracy: 0.9871 - val_loss: 0.0453 - val_accuracy: 0.9845\n",
      "Epoch 102/150\n",
      "254090/254090 [==============================] - 40s 158us/step - loss: 0.0323 - accuracy: 0.9871 - val_loss: 0.0452 - val_accuracy: 0.9845\n",
      "Epoch 103/150\n",
      "254090/254090 [==============================] - 42s 163us/step - loss: 0.0323 - accuracy: 0.9872 - val_loss: 0.0454 - val_accuracy: 0.9845\n",
      "Epoch 104/150\n",
      "254090/254090 [==============================] - 41s 162us/step - loss: 0.0322 - accuracy: 0.9872 - val_loss: 0.0450 - val_accuracy: 0.9846\n",
      "Epoch 105/150\n",
      "254090/254090 [==============================] - 40s 159us/step - loss: 0.0321 - accuracy: 0.9872 - val_loss: 0.0451 - val_accuracy: 0.9846\n",
      "Epoch 106/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254090/254090 [==============================] - 40s 159us/step - loss: 0.0321 - accuracy: 0.9872 - val_loss: 0.0452 - val_accuracy: 0.9846\n",
      "Epoch 107/150\n",
      "254090/254090 [==============================] - 41s 161us/step - loss: 0.0320 - accuracy: 0.9872 - val_loss: 0.0450 - val_accuracy: 0.9847\n",
      "Epoch 108/150\n",
      "254090/254090 [==============================] - 40s 158us/step - loss: 0.0320 - accuracy: 0.9873 - val_loss: 0.0444 - val_accuracy: 0.9847\n",
      "Epoch 109/150\n",
      "254090/254090 [==============================] - 40s 157us/step - loss: 0.0319 - accuracy: 0.9873 - val_loss: 0.0452 - val_accuracy: 0.9846\n",
      "Epoch 110/150\n",
      "254090/254090 [==============================] - 40s 158us/step - loss: 0.0319 - accuracy: 0.9873 - val_loss: 0.0442 - val_accuracy: 0.9848\n",
      "Epoch 111/150\n",
      "254090/254090 [==============================] - 40s 159us/step - loss: 0.0318 - accuracy: 0.9873 - val_loss: 0.0447 - val_accuracy: 0.9847\n",
      "Epoch 112/150\n",
      "254090/254090 [==============================] - 42s 164us/step - loss: 0.0318 - accuracy: 0.9873 - val_loss: 0.0447 - val_accuracy: 0.9847\n",
      "Epoch 113/150\n",
      "254090/254090 [==============================] - 41s 162us/step - loss: 0.0317 - accuracy: 0.9873 - val_loss: 0.0449 - val_accuracy: 0.9847\n",
      "Epoch 114/150\n",
      "254090/254090 [==============================] - 40s 158us/step - loss: 0.0317 - accuracy: 0.9873 - val_loss: 0.0443 - val_accuracy: 0.9848\n",
      "Epoch 115/150\n",
      "254090/254090 [==============================] - 40s 157us/step - loss: 0.0316 - accuracy: 0.9873 - val_loss: 0.0452 - val_accuracy: 0.9847\n",
      "Epoch 116/150\n",
      "254090/254090 [==============================] - 43s 169us/step - loss: 0.0316 - accuracy: 0.9874 - val_loss: 0.0450 - val_accuracy: 0.9847\n",
      "Epoch 117/150\n",
      "254090/254090 [==============================] - 40s 157us/step - loss: 0.0315 - accuracy: 0.9874 - val_loss: 0.0442 - val_accuracy: 0.9848\n",
      "Epoch 118/150\n",
      "254090/254090 [==============================] - 40s 157us/step - loss: 0.0315 - accuracy: 0.9874 - val_loss: 0.0443 - val_accuracy: 0.9849\n",
      "Epoch 119/150\n",
      "254090/254090 [==============================] - 41s 161us/step - loss: 0.0314 - accuracy: 0.9874 - val_loss: 0.0443 - val_accuracy: 0.9848\n",
      "Epoch 120/150\n",
      "254090/254090 [==============================] - 40s 158us/step - loss: 0.0314 - accuracy: 0.9874 - val_loss: 0.0448 - val_accuracy: 0.9847\n",
      "Epoch 121/150\n",
      "254090/254090 [==============================] - 41s 161us/step - loss: 0.0313 - accuracy: 0.9874 - val_loss: 0.0440 - val_accuracy: 0.9849\n",
      "Epoch 122/150\n",
      "254090/254090 [==============================] - 40s 157us/step - loss: 0.0313 - accuracy: 0.9875 - val_loss: 0.0439 - val_accuracy: 0.9848\n",
      "Epoch 123/150\n",
      "254090/254090 [==============================] - 40s 158us/step - loss: 0.0312 - accuracy: 0.9875 - val_loss: 0.0446 - val_accuracy: 0.9848\n",
      "Epoch 124/150\n",
      "254090/254090 [==============================] - 40s 158us/step - loss: 0.0312 - accuracy: 0.9875 - val_loss: 0.0441 - val_accuracy: 0.9849\n",
      "Epoch 125/150\n",
      "254090/254090 [==============================] - 42s 165us/step - loss: 0.0311 - accuracy: 0.9875 - val_loss: 0.0438 - val_accuracy: 0.9850\n",
      "Epoch 126/150\n",
      "254090/254090 [==============================] - 40s 159us/step - loss: 0.0311 - accuracy: 0.9875 - val_loss: 0.0445 - val_accuracy: 0.9849\n",
      "Epoch 127/150\n",
      "254090/254090 [==============================] - 40s 158us/step - loss: 0.0310 - accuracy: 0.9875 - val_loss: 0.0443 - val_accuracy: 0.9849\n",
      "Epoch 128/150\n",
      "254090/254090 [==============================] - 41s 163us/step - loss: 0.0310 - accuracy: 0.9875 - val_loss: 0.0438 - val_accuracy: 0.9850\n",
      "Epoch 129/150\n",
      "254090/254090 [==============================] - 40s 158us/step - loss: 0.0310 - accuracy: 0.9876 - val_loss: 0.0442 - val_accuracy: 0.9849\n",
      "Epoch 130/150\n",
      "254090/254090 [==============================] - 41s 161us/step - loss: 0.0309 - accuracy: 0.9876 - val_loss: 0.0436 - val_accuracy: 0.9850\n",
      "Epoch 131/150\n",
      "254090/254090 [==============================] - 41s 163us/step - loss: 0.0309 - accuracy: 0.9876 - val_loss: 0.0433 - val_accuracy: 0.9850\n",
      "Epoch 132/150\n",
      "254090/254090 [==============================] - 40s 157us/step - loss: 0.0308 - accuracy: 0.9876 - val_loss: 0.0431 - val_accuracy: 0.9851\n",
      "Epoch 133/150\n",
      "254090/254090 [==============================] - 40s 158us/step - loss: 0.0308 - accuracy: 0.9876 - val_loss: 0.0439 - val_accuracy: 0.9851\n",
      "Epoch 134/150\n",
      "254090/254090 [==============================] - 41s 160us/step - loss: 0.0307 - accuracy: 0.9876 - val_loss: 0.0442 - val_accuracy: 0.9850\n",
      "Epoch 135/150\n",
      "254090/254090 [==============================] - 40s 157us/step - loss: 0.0307 - accuracy: 0.9876 - val_loss: 0.0442 - val_accuracy: 0.9850\n",
      "Epoch 136/150\n",
      "254090/254090 [==============================] - 40s 158us/step - loss: 0.0307 - accuracy: 0.9876 - val_loss: 0.0437 - val_accuracy: 0.9851\n",
      "Epoch 137/150\n",
      "254090/254090 [==============================] - 40s 156us/step - loss: 0.0306 - accuracy: 0.9877 - val_loss: 0.0433 - val_accuracy: 0.9851\n",
      "Epoch 138/150\n",
      "254090/254090 [==============================] - 40s 157us/step - loss: 0.0306 - accuracy: 0.9877 - val_loss: 0.0436 - val_accuracy: 0.9851\n",
      "Epoch 139/150\n",
      "254090/254090 [==============================] - 41s 159us/step - loss: 0.0305 - accuracy: 0.9877 - val_loss: 0.0437 - val_accuracy: 0.9851\n",
      "Epoch 140/150\n",
      "254090/254090 [==============================] - 40s 158us/step - loss: 0.0305 - accuracy: 0.9877 - val_loss: 0.0438 - val_accuracy: 0.9850\n",
      "Epoch 141/150\n",
      "254090/254090 [==============================] - 40s 158us/step - loss: 0.0305 - accuracy: 0.9877 - val_loss: 0.0438 - val_accuracy: 0.9850\n",
      "Epoch 142/150\n",
      "254090/254090 [==============================] - 40s 158us/step - loss: 0.0304 - accuracy: 0.9877 - val_loss: 0.0436 - val_accuracy: 0.9851\n",
      "Epoch 143/150\n",
      "254090/254090 [==============================] - 42s 164us/step - loss: 0.0304 - accuracy: 0.9877 - val_loss: 0.0435 - val_accuracy: 0.9851\n",
      "Epoch 144/150\n",
      "254090/254090 [==============================] - 40s 158us/step - loss: 0.0303 - accuracy: 0.9877 - val_loss: 0.0435 - val_accuracy: 0.9852\n",
      "Epoch 145/150\n",
      "254090/254090 [==============================] - 41s 160us/step - loss: 0.0303 - accuracy: 0.9878 - val_loss: 0.0432 - val_accuracy: 0.9852\n",
      "Epoch 146/150\n",
      "254090/254090 [==============================] - 41s 161us/step - loss: 0.0303 - accuracy: 0.9878 - val_loss: 0.0430 - val_accuracy: 0.9852\n",
      "Epoch 147/150\n",
      "254090/254090 [==============================] - 40s 158us/step - loss: 0.0302 - accuracy: 0.9878 - val_loss: 0.0432 - val_accuracy: 0.9852\n",
      "Epoch 148/150\n",
      "254090/254090 [==============================] - 40s 159us/step - loss: 0.0302 - accuracy: 0.9878 - val_loss: 0.0431 - val_accuracy: 0.9852\n",
      "Epoch 149/150\n",
      "254090/254090 [==============================] - 40s 157us/step - loss: 0.0301 - accuracy: 0.9878 - val_loss: 0.0431 - val_accuracy: 0.9852\n",
      "Epoch 150/150\n",
      "254090/254090 [==============================] - 41s 161us/step - loss: 0.0301 - accuracy: 0.9878 - val_loss: 0.0426 - val_accuracy: 0.9854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clair\\Anaconda3\\envs\\CNNs\\lib\\site-packages\\ipykernel_launcher.py:136: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20000, 0]\n",
      "[0, 0]\n",
      "both b and c are zero\n",
      "[20000, 0]\n",
      "[0, 0]\n",
      "both b and c are zero\n",
      "[18662, 534]\n",
      "[708, 96]\n",
      "[18864, 332]\n",
      "[60, 744]\n",
      "[19264, 324]\n",
      "[231, 181]\n",
      "[19051, 537]\n",
      "[126, 286]\n",
      "[19684, 193]\n",
      "[59, 64]\n",
      "[19291, 586]\n",
      "[43, 80]\n",
      "[19764, 115]\n",
      "[55, 66]\n",
      "[19244, 635]\n",
      "[48, 73]\n",
      "[19650, 235]\n",
      "[76, 39]\n",
      "[19284, 601]\n",
      "[27, 88]\n",
      "[19524, 329]\n",
      "[79, 68]\n",
      "[19210, 643]\n",
      "[67, 80]\n",
      "[18187, 1247]\n",
      "[287, 279]\n",
      "[18894, 540]\n",
      "[181, 385]\n",
      "[18506, 950]\n",
      "[328, 216]\n",
      "[18972, 484]\n",
      "[100, 444]\n",
      "[19473, 296]\n",
      "[121, 110]\n",
      "[19137, 632]\n",
      "[64, 167]\n",
      "[19623, 201]\n",
      "[91, 85]\n",
      "[19239, 585]\n",
      "[72, 104]\n",
      "[19654, 204]\n",
      "[64, 78]\n",
      "[19238, 620]\n",
      "[71, 71]\n",
      "[19660, 204]\n",
      "[59, 77]\n",
      "[19188, 676]\n",
      "[67, 69]\n",
      "[19363, 461]\n",
      "[79, 97]\n",
      "[19172, 652]\n",
      "[56, 120]\n",
      "[18891, 487]\n",
      "[247, 375]\n",
      "[18850, 528]\n",
      "[203, 419]\n",
      "[18988, 428]\n",
      "[250, 334]\n",
      "[18941, 475]\n",
      "[198, 386]\n",
      "[19343, 496]\n",
      "[95, 66]\n",
      "[19225, 614]\n",
      "[58, 103]\n",
      "[19664, 197]\n",
      "[67, 72]\n",
      "[19296, 565]\n",
      "[56, 83]\n",
      "[19618, 237]\n",
      "[79, 66]\n",
      "[19236, 619]\n",
      "[46, 99]\n",
      "[19484, 369]\n",
      "[72, 75]\n",
      "[19239, 614]\n",
      "[73, 74]\n",
      "[19478, 331]\n",
      "[109, 82]\n",
      "[19183, 626]\n",
      "[88, 103]\n",
      "[18531, 871]\n",
      "[235, 363]\n",
      "[18883, 519]\n",
      "[240, 358]\n",
      "[18383, 1030]\n",
      "[316, 271]\n",
      "[18932, 481]\n",
      "[130, 457]\n",
      "[19420, 408]\n",
      "[99, 73]\n",
      "[19199, 629]\n",
      "[59, 113]\n",
      "[19619, 237]\n",
      "[66, 78]\n",
      "[19230, 626]\n",
      "[51, 93]\n",
      "[19671, 200]\n",
      "[58, 71]\n",
      "[19265, 606]\n",
      "[42, 87]\n",
      "[19595, 246]\n",
      "[87, 72]\n",
      "[19215, 626]\n",
      "[80, 79]\n",
      "[19361, 479]\n",
      "[79, 81]\n",
      "[19186, 654]\n",
      "[55, 105]\n",
      "[19002, 445]\n",
      "[182, 371]\n",
      "[18972, 475]\n",
      "[148, 405]\n",
      "[19084, 381]\n",
      "[84, 451]\n",
      "[19018, 447]\n",
      "[86, 449]\n",
      "[19342, 501]\n",
      "[75, 82]\n",
      "[19204, 639]\n",
      "[77, 80]\n",
      "[19657, 201]\n",
      "[72, 70]\n",
      "[19215, 643]\n",
      "[67, 75]\n",
      "[19615, 251]\n",
      "[65, 69]\n",
      "[19278, 588]\n",
      "[37, 97]\n",
      "[19562, 288]\n",
      "[85, 65]\n",
      "[19189, 661]\n",
      "[61, 89]\n",
      "[19489, 350]\n",
      "[73, 88]\n",
      "[19229, 610]\n",
      "[64, 97]\n",
      "[18508, 929]\n",
      "[202, 361]\n",
      "[18884, 553]\n",
      "[204, 359]\n",
      "[18574, 845]\n",
      "[273, 308]\n",
      "[18925, 494]\n",
      "[172, 409]\n",
      "[19473, 340]\n",
      "[105, 82]\n",
      "[19188, 625]\n",
      "[82, 105]\n",
      "[19581, 268]\n",
      "[66, 85]\n",
      "[19232, 617]\n",
      "[52, 99]\n",
      "[19639, 209]\n",
      "[63, 89]\n",
      "[19222, 626]\n",
      "[53, 99]\n",
      "[19649, 214]\n",
      "[63, 74]\n",
      "[19242, 621]\n",
      "[52, 85]\n",
      "[19443, 353]\n",
      "[81, 123]\n",
      "[19145, 651]\n",
      "[95, 109]\n",
      "[18987, 432]\n",
      "[205, 376]\n",
      "[18960, 459]\n",
      "[156, 425]\n",
      "[19047, 403]\n",
      "[156, 394]\n",
      "[18999, 451]\n",
      "[125, 425]\n",
      "[19714, 170]\n",
      "[39, 77]\n",
      "[19235, 649]\n",
      "[38, 78]\n",
      "[19824, 68]\n",
      "[51, 57]\n",
      "[19251, 641]\n",
      "[60, 48]\n",
      "[19795, 108]\n",
      "[34, 63]\n",
      "[19237, 666]\n",
      "[54, 43]\n",
      "[19764, 121]\n",
      "[39, 76]\n",
      "[19238, 647]\n",
      "[61, 54]\n",
      "[19703, 141]\n",
      "[61, 95]\n",
      "[19202, 642]\n",
      "[52, 104]\n",
      "[19542, 183]\n",
      "[91, 184]\n",
      "[19154, 571]\n",
      "[111, 164]\n",
      "[18192, 1146]\n",
      "[301, 361]\n",
      "[19000, 338]\n",
      "[228, 434]\n",
      "[19010, 407]\n",
      "[152, 431]\n",
      "[18903, 514]\n",
      "[130, 453]\n",
      "[18469, 997]\n",
      "[311, 223]\n",
      "[19045, 421]\n",
      "[108, 426]\n",
      "[19093, 396]\n",
      "[96, 415]\n",
      "[19022, 467]\n",
      "[81, 430]\n",
      "[18445, 997]\n",
      "[337, 221]\n",
      "[18995, 447]\n",
      "[98, 460]\n",
      "[19109, 354]\n",
      "[102, 435]\n",
      "[19037, 426]\n",
      "[85, 452]\n",
      "[19511, 175]\n",
      "[159, 155]\n",
      "[19160, 526]\n",
      "[88, 226]\n",
      "[19536, 309]\n",
      "[98, 57]\n",
      "[19282, 563]\n",
      "[41, 114]\n",
      "[19318, 465]\n",
      "[114, 103]\n",
      "[19203, 580]\n",
      "[71, 146]\n",
      "[19460, 333]\n",
      "[106, 101]\n",
      "[19187, 606]\n",
      "[61, 146]\n",
      "[19337, 440]\n",
      "[118, 105]\n",
      "[19180, 597]\n",
      "[83, 140]\n",
      "[19397, 390]\n",
      "[114, 99]\n",
      "[19164, 623]\n",
      "[84, 129]\n",
      "[19409, 403]\n",
      "[106, 82]\n",
      "[19182, 630]\n",
      "[75, 113]\n",
      "[19709, 129]\n",
      "[84, 78]\n",
      "[19188, 650]\n",
      "[59, 103]\n",
      "[19742, 150]\n",
      "[54, 54]\n",
      "[19297, 595]\n",
      "[48, 60]\n",
      "[19645, 196]\n",
      "[86, 73]\n",
      "[19205, 636]\n",
      "[80, 79]\n",
      "[19527, 305]\n",
      "[85, 83]\n",
      "[19284, 548]\n",
      "[51, 117]\n",
      "[19503, 326]\n",
      "[102, 69]\n",
      "[19224, 605]\n",
      "[48, 123]\n",
      "[19564, 260]\n",
      "[102, 74]\n",
      "[19242, 582]\n",
      "[57, 119]\n",
      "[19656, 172]\n",
      "[85, 87]\n",
      "[19217, 611]\n",
      "[70, 102]\n",
      "[19720, 157]\n",
      "[67, 56]\n",
      "[19294, 583]\n",
      "[40, 83]\n",
      "[19677, 178]\n",
      "[85, 60]\n",
      "[19245, 610]\n",
      "[34, 111]\n",
      "[19644, 210]\n",
      "[72, 74]\n",
      "[19250, 604]\n",
      "[38, 108]\n",
      "[19597, 247]\n",
      "[78, 78]\n",
      "[19259, 585]\n",
      "[32, 124]\n",
      "[19622, 203]\n",
      "[89, 86]\n",
      "[19238, 587]\n",
      "[65, 110]\n",
      "[19642, 198]\n",
      "[87, 73]\n",
      "[19268, 572]\n",
      "[57, 103]\n",
      "[19637, 205]\n",
      "[75, 83]\n",
      "[19233, 609]\n",
      "[40, 118]\n",
      "[19765, 101]\n",
      "[59, 75]\n",
      "[19198, 668]\n",
      "[58, 76]\n",
      "[19676, 187]\n",
      "[74, 63]\n",
      "[19244, 619]\n",
      "[33, 104]\n",
      "[19593, 242]\n",
      "[82, 83]\n",
      "[19239, 596]\n",
      "[51, 114]\n",
      "[19573, 278]\n",
      "[85, 64]\n",
      "[19222, 629]\n",
      "[49, 100]\n",
      "[19511, 307]\n",
      "[106, 76]\n",
      "[19239, 579]\n",
      "[38, 144]\n",
      "[19569, 249]\n",
      "[89, 93]\n",
      "[19204, 614]\n",
      "[84, 98]\n",
      "[19574, 244]\n",
      "[85, 97]\n",
      "[19215, 603]\n",
      "[69, 113]\n",
      "[19755, 101]\n",
      "[77, 67]\n",
      "[19219, 637]\n",
      "[44, 100]\n",
      "[19423, 417]\n",
      "[88, 72]\n",
      "[19245, 595]\n",
      "[55, 105]\n",
      "[19572, 232]\n",
      "[85, 111]\n",
      "[19228, 576]\n",
      "[44, 152]\n",
      "[19342, 453]\n",
      "[113, 92]\n",
      "[19192, 603]\n",
      "[69, 136]\n",
      "[19493, 302]\n",
      "[82, 123]\n",
      "[19238, 557]\n",
      "[56, 149]\n",
      "[19334, 452]\n",
      "[96, 118]\n",
      "[19162, 624]\n",
      "[86, 128]\n",
      "[19459, 332]\n",
      "[93, 116]\n",
      "[19221, 570]\n",
      "[48, 161]\n",
      "[19671, 187]\n",
      "[66, 76]\n",
      "[19237, 621]\n",
      "[41, 101]\n",
      "[19267, 469]\n",
      "[126, 138]\n",
      "[19186, 550]\n",
      "[114, 150]\n",
      "[18438, 1040]\n",
      "[317, 205]\n",
      "[18999, 479]\n",
      "[83, 439]\n",
      "[19072, 385]\n",
      "[129, 414]\n",
      "[19013, 444]\n",
      "[101, 442]\n",
      "[18468, 1015]\n",
      "[287, 230]\n",
      "[18977, 506]\n",
      "[115, 402]\n",
      "[18909, 514]\n",
      "[269, 308]\n",
      "[18907, 516]\n",
      "[218, 359]\n",
      "[18407, 1002]\n",
      "[346, 245]\n",
      "[18967, 442]\n",
      "[121, 470]\n",
      "[19031, 410]\n",
      "[168, 391]\n",
      "[19008, 433]\n",
      "[118, 441]\n",
      "------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds of MWPM:\n",
      "> Accuracy: 0.9740728956229128 (+- 6.41637243913647e-05)\n",
      "> F1: 0.6989635170840348(+- 0.0007045742334666265)\n",
      "> Time: 3310.980690433331 (+- 57.87214651447051)\n",
      "Average scores for all folds of PLUT:\n",
      "> Accuracy: 0.962125135865279 (+- 1.7512619412737018e-05)\n",
      "> F1: 0.21745138362860852(+- 0.00042871448977584154)\n",
      "> Time: 5.394669633333334 (+- 0.1790883232414726)\n",
      "Average scores for all folds of NN:\n",
      "> Accuracy: 0.9865066146375333 (+- 8.370010686060943e-05)\n",
      "> F1: 0.8214243120606451(+- 0.0007256249779055657)\n",
      "> Time: 5.932276533333333 (+- 0.1120422246275134)\n",
      "> AUC for class : 0.9973080192689147 (+- 0.0)\n",
      "X^2 for MWPM and NN: 0\n",
      "X^2 for PLUT and NN: 0\n",
      "> AUC for class X00: 0.8754114828928028 (+- 0.002339450944796075)\n",
      "X^2 for MWPM and NN: 24.65780998389694\n",
      "X^2 for PLUT and NN: 187.34948979591837\n",
      "> AUC for class X01: 0.9856740220472912 (+- 0.00030800780190851703)\n",
      "X^2 for MWPM and NN: 15.250450450450451\n",
      "X^2 for PLUT and NN: 253.5444947209653\n",
      "> AUC for class X02: 0.9982601846486924 (+- 0.00028340631288781043)\n",
      "X^2 for MWPM and NN: 70.19444444444444\n",
      "X^2 for PLUT and NN: 467.033386327504\n",
      "> AUC for class X03: 0.99878941812558 (+- 8.690489580794966e-05)\n",
      "X^2 for MWPM and NN: 20.476470588235294\n",
      "X^2 for PLUT and NN: 502.77598828696927\n",
      "> AUC for class X04: 0.9985478315945507 (+- 0.00010970061316727045)\n",
      "X^2 for MWPM and NN: 80.27009646302251\n",
      "X^2 for PLUT and NN: 522.8168789808917\n",
      "> AUC for class X05: 0.9980617517622066 (+- 0.0002391094328452319)\n",
      "X^2 for MWPM and NN: 151.96323529411765\n",
      "X^2 for PLUT and NN: 465.66901408450707\n",
      "> AUC for class X06: 0.9805773589431248 (+- 0.00031712217326180587)\n",
      "X^2 for MWPM and NN: 599.5312907431552\n",
      "X^2 for PLUT and NN: 177.75866851595006\n",
      "> AUC for class X10: 0.9793000448601389 (+- 0.00048781353437478647)\n",
      "X^2 for MWPM and NN: 301.7535211267606\n",
      "X^2 for PLUT and NN: 251.17979452054794\n",
      "> AUC for class X11: 0.9957562490841211 (+- 0.0001773305277382383)\n",
      "X^2 for MWPM and NN: 72.60431654676259\n",
      "X^2 for PLUT and NN: 461.9094827586207\n",
      "> AUC for class X12: 0.997354597305907 (+- 0.00011721274528248819)\n",
      "X^2 for MWPM and NN: 40.68835616438356\n",
      "X^2 for PLUT and NN: 399.0015220700152\n",
      "> AUC for class X13: 0.9978736305158508 (+- 0.00013859552027428224)\n",
      "X^2 for MWPM and NN: 72.09328358208955\n",
      "X^2 for PLUT and NN: 434.5933429811867\n",
      "> AUC for class X14: 0.9978118773722775 (+- 7.601463528315523e-05)\n",
      "X^2 for MWPM and NN: 78.84410646387833\n",
      "X^2 for PLUT and NN: 497.52893674293404\n",
      "> AUC for class X15: 0.9969539077736043 (+- 0.0001255606603380254)\n",
      "X^2 for MWPM and NN: 268.81666666666666\n",
      "X^2 for PLUT and NN: 500.0353107344633\n",
      "> AUC for class X16: 0.9800625243151241 (+- 0.00041438154024774077)\n",
      "X^2 for MWPM and NN: 77.82152588555859\n",
      "X^2 for PLUT and NN: 143.6060191518468\n",
      "> AUC for class X20: 0.9796543072166383 (+- 0.0015430082035079221)\n",
      "X^2 for MWPM and NN: 46.20796460176991\n",
      "X^2 for PLUT and NN: 113.18870728083209\n",
      "> AUC for class X21: 0.9967765434510332 (+- 0.0001173171894463195)\n",
      "X^2 for MWPM and NN: 270.72758037225043\n",
      "X^2 for PLUT and NN: 458.3705357142857\n",
      "> AUC for class X22: 0.9975836526499996 (+- 4.432957497497511e-05)\n",
      "X^2 for MWPM and NN: 63.03409090909091\n",
      "X^2 for PLUT and NN: 415.5619967793881\n",
      "> AUC for class X23: 0.997919141592201 (+- 0.00021951377361635157)\n",
      "X^2 for MWPM and NN: 78.00316455696202\n",
      "X^2 for PLUT and NN: 492.006015037594\n",
      "> AUC for class X24: 0.997711548250968 (+- 0.00014204166591415325)\n",
      "X^2 for MWPM and NN: 198.67573696145124\n",
      "X^2 for PLUT and NN: 424.4541484716157\n",
      "> AUC for class X25: 0.9968483602722179 (+- 8.401924017984733e-05)\n",
      "X^2 for MWPM and NN: 111.00227272727273\n",
      "X^2 for PLUT and NN: 403.8781512605042\n",
      "> AUC for class X26: 0.9806110481770899 (+- 0.0004373246104477904)\n",
      "X^2 for MWPM and NN: 364.57956600361666\n",
      "X^2 for PLUT and NN: 101.82345191040844\n",
      "> AUC for class X30: 0.9805628432126084 (+- 9.127821250139486e-05)\n",
      "X^2 for MWPM and NN: 377.6887072808321\n",
      "X^2 for PLUT and NN: 200.49099836333878\n",
      "> AUC for class X31: 0.9967689309510899 (+- 0.00018210948883781657)\n",
      "X^2 for MWPM and NN: 187.10848126232742\n",
      "X^2 for PLUT and NN: 470.5828488372093\n",
      "> AUC for class X32: 0.9978073240387713 (+- 9.179955396368122e-05)\n",
      "X^2 for MWPM and NN: 95.37953795379538\n",
      "X^2 for PLUT and NN: 486.67060561299854\n",
      "> AUC for class X33: 0.9978090681002026 (+- 0.00017336336578088868)\n",
      "X^2 for MWPM and NN: 77.05813953488372\n",
      "X^2 for PLUT and NN: 489.1496913580247\n",
      "> AUC for class X34: 0.9976955029860934 (+- 5.266078982125676e-05)\n",
      "X^2 for MWPM and NN: 74.96696696696696\n",
      "X^2 for PLUT and NN: 420.71529745042494\n",
      "> AUC for class X35: 0.9967194207910556 (+- 7.724236748386963e-05)\n",
      "X^2 for MWPM and NN: 285.30645161290323\n",
      "X^2 for PLUT and NN: 504.37799717912554\n",
      "> AUC for class X36: 0.9807336086346535 (+- 0.0006503167915049386)\n",
      "X^2 for MWPM and NN: 109.48006379585327\n",
      "X^2 for PLUT and NN: 170.58747993579453\n",
      "> AUC for class X40: 0.980274393433573 (+- 0.0001335779852820403)\n",
      "X^2 for MWPM and NN: 188.42150537634407\n",
      "X^2 for PLUT and NN: 243.15196998123827\n",
      "> AUC for class X41: 0.9970527335506977 (+- 1.6758404779014412e-05)\n",
      "X^2 for MWPM and NN: 313.58506944444446\n",
      "X^2 for PLUT and NN: 439.554469273743\n",
      "> AUC for class X42: 0.9976911789437058 (+- 8.902069856922156e-05)\n",
      "X^2 for MWPM and NN: 60.014652014652015\n",
      "X^2 for PLUT and NN: 465.66901408450707\n",
      "> AUC for class X43: 0.997657727173639 (+- 0.00013026034434011126)\n",
      "X^2 for MWPM and NN: 108.30696202531645\n",
      "X^2 for PLUT and NN: 484.0\n",
      "> AUC for class X44: 0.9975341418887022 (+- 0.00010920055461340571)\n",
      "X^2 for MWPM and NN: 109.3941018766756\n",
      "X^2 for PLUT and NN: 496.95429362880884\n",
      "> AUC for class X45: 0.9969152727251195 (+- 0.00015067310670158237)\n",
      "X^2 for MWPM and NN: 180.08510638297872\n",
      "X^2 for PLUT and NN: 440.6899109792285\n",
      "> AUC for class X46: 0.9801372835137773 (+- 0.001260998961947179)\n",
      "X^2 for MWPM and NN: 466.026525198939\n",
      "X^2 for PLUT and NN: 159.9788639365918\n",
      "> AUC for class X50: 0.9803096838478762 (+- 0.0011841516839524637)\n",
      "X^2 for MWPM and NN: 291.628801431127\n",
      "X^2 for PLUT and NN: 154.71621621621622\n",
      "> AUC for class X51: 0.9969206594228041 (+- 0.00010398271135162506)\n",
      "X^2 for MWPM and NN: 123.04719101123595\n",
      "X^2 for PLUT and NN: 415.5077793493635\n",
      "> AUC for class X52: 0.9975224457954582 (+- 0.0001263402564001928)\n",
      "X^2 for MWPM and NN: 120.96107784431138\n",
      "X^2 for PLUT and NN: 475.47982062780267\n",
      "> AUC for class X53: 0.997981120105022 (+- 0.00014095542723804457)\n",
      "X^2 for MWPM and NN: 77.29779411764706\n",
      "X^2 for PLUT and NN: 481.86156111929307\n",
      "> AUC for class X54: 0.9976412303290743 (+- 7.836950948851268e-05)\n",
      "X^2 for MWPM and NN: 81.2274368231047\n",
      "X^2 for PLUT and NN: 479.3818722139673\n",
      "> AUC for class X55: 0.9971784848775732 (+- 0.00011001406629150249)\n",
      "X^2 for MWPM and NN: 169.2188940092166\n",
      "X^2 for PLUT and NN: 412.90214477211794\n",
      "> AUC for class X56: 0.980416337765388 (+- 0.0004136730570079822)\n",
      "X^2 for MWPM and NN: 80.18210361067504\n",
      "X^2 for PLUT and NN: 148.29918699186993\n",
      "> AUC for class X60: 0.9802618250378808 (+- 0.00038465325108545174)\n",
      "X^2 for MWPM and NN: 108.25760286225403\n",
      "X^2 for PLUT and NN: 183.37673611111111\n",
      "> AUC for class X61: 0.9981798960970306 (+- 0.00017220256394014625)\n",
      "X^2 for MWPM and NN: 80.86124401913875\n",
      "X^2 for PLUT and NN: 541.6302765647744\n",
      "> AUC for class X62: 0.9987597510159233 (+- 6.418255727701328e-05)\n",
      "X^2 for MWPM and NN: 2.1512605042016806\n",
      "X^2 for PLUT and NN: 479.88587731811697\n",
      "> AUC for class X63: 0.9987338544064635 (+- 7.847686828874334e-05)\n",
      "X^2 for MWPM and NN: 37.528169014084504\n",
      "X^2 for PLUT and NN: 518.5013888888889\n",
      "> AUC for class X64: 0.9985616374830485 (+- 9.395887758661698e-05)\n",
      "X^2 for MWPM and NN: 41.00625\n",
      "X^2 for PLUT and NN: 483.3686440677966\n",
      "> AUC for class X65: 0.9976165062310255 (+- 0.00010768916013223665)\n",
      "X^2 for MWPM and NN: 30.896039603960396\n",
      "X^2 for PLUT and NN: 499.88616714697406\n",
      "> AUC for class X66: 0.9931550914999511 (+- 0.0002756702601545991)\n",
      "X^2 for MWPM and NN: 30.222627737226276\n",
      "X^2 for PLUT and NN: 308.91642228739005\n",
      "> AUC for class Z00: 0.9737049269788537 (+- 0.0002974777427702566)\n",
      "X^2 for MWPM and NN: 492.28472702142363\n",
      "X^2 for PLUT and NN: 20.991166077738516\n",
      "> AUC for class Z01: 0.9746470098961106 (+- 0.0008503855291518433)\n",
      "X^2 for MWPM and NN: 115.41323792486583\n",
      "X^2 for PLUT and NN: 227.777950310559\n",
      "> AUC for class Z02: 0.9767790802037525 (+- 0.00047532664002989484)\n",
      "X^2 for MWPM and NN: 358.7347094801223\n",
      "X^2 for PLUT and NN: 184.01512287334594\n",
      "> AUC for class Z03: 0.9774785804745171 (+- 0.00043086455750648687)\n",
      "X^2 for MWPM and NN: 181.70934959349594\n",
      "X^2 for PLUT and NN: 270.4835766423358\n",
      "> AUC for class Z04: 0.9770327465058145 (+- 0.0007637189005145421)\n",
      "X^2 for MWPM and NN: 325.547976011994\n",
      "X^2 for PLUT and NN: 222.2091743119266\n",
      "> AUC for class Z05: 0.9776267609607444 (+- 0.0005216970625019921)\n",
      "X^2 for MWPM and NN: 138.16008771929825\n",
      "X^2 for PLUT and NN: 226.22309197651663\n",
      "> AUC for class Z06: 0.9919965374652114 (+- 0.00037206985985865546)\n",
      "X^2 for MWPM and NN: 0.6736526946107785\n",
      "X^2 for PLUT and NN: 311.0244299674267\n",
      "> AUC for class Z10: 0.9975401103371414 (+- 0.00015942730843055393)\n",
      "X^2 for MWPM and NN: 108.35380835380835\n",
      "X^2 for PLUT and NN: 449.40562913907286\n",
      "> AUC for class Z11: 0.9952449419127095 (+- 0.00042524239718930195)\n",
      "X^2 for MWPM and NN: 211.57167530224524\n",
      "X^2 for PLUT and NN: 396.4116743471582\n",
      "> AUC for class Z12: 0.9957303143939974 (+- 0.00020453320068757294)\n",
      "X^2 for MWPM and NN: 116.34624145785877\n",
      "X^2 for PLUT and NN: 443.68215892053973\n",
      "> AUC for class Z13: 0.9959218319326304 (+- 0.0003971990896498716)\n",
      "X^2 for MWPM and NN: 184.66129032258064\n",
      "X^2 for PLUT and NN: 387.01323529411764\n",
      "> AUC for class Z14: 0.99599557858691 (+- 0.00010060537559325051)\n",
      "X^2 for MWPM and NN: 150.04960317460316\n",
      "X^2 for PLUT and NN: 409.3974540311174\n",
      "> AUC for class Z15: 0.9960738290048026 (+- 0.0002840509961858471)\n",
      "X^2 for MWPM and NN: 172.13359528487229\n",
      "X^2 for PLUT and NN: 435.3418439716312\n",
      "> AUC for class Z16: 0.9971594689806543 (+- 0.0001991112018200803)\n",
      "X^2 for MWPM and NN: 9.089201877934272\n",
      "X^2 for PLUT and NN: 490.9732016925247\n",
      "> AUC for class Z20: 0.9981568373388411 (+- 0.00017907090587737705)\n",
      "X^2 for MWPM and NN: 44.240196078431374\n",
      "X^2 for PLUT and NN: 463.63297045101086\n",
      "> AUC for class Z21: 0.9968176244657849 (+- 0.00021898579368164852)\n",
      "X^2 for MWPM and NN: 42.13120567375886\n",
      "X^2 for PLUT and NN: 430.20251396648047\n",
      "> AUC for class Z22: 0.9964592884114394 (+- 0.0003656272067924927)\n",
      "X^2 for MWPM and NN: 122.97692307692307\n",
      "X^2 for PLUT and NN: 410.7111853088481\n",
      "> AUC for class Z23: 0.9966664675848814 (+- 0.00025209105360076567)\n",
      "X^2 for MWPM and NN: 116.1892523364486\n",
      "X^2 for PLUT and NN: 473.40888208269524\n",
      "> AUC for class Z24: 0.9963317585419732 (+- 0.0001061419312527859)\n",
      "X^2 for MWPM and NN: 68.09116022099448\n",
      "X^2 for PLUT and NN: 429.6964006259781\n",
      "> AUC for class Z25: 0.99668919123842 (+- 0.00019259924116076668)\n",
      "X^2 for MWPM and NN: 28.77821011673152\n",
      "X^2 for PLUT and NN: 428.19383259911893\n",
      "> AUC for class Z26: 0.9981275890138811 (+- 0.00013311155779415163)\n",
      "X^2 for MWPM and NN: 35.361607142857146\n",
      "X^2 for PLUT and NN: 471.53130016051364\n",
      "> AUC for class Z30: 0.9981758383447019 (+- 0.0002518767457829254)\n",
      "X^2 for MWPM and NN: 32.18250950570342\n",
      "X^2 for PLUT and NN: 513.3928571428571\n",
      "> AUC for class Z31: 0.9971302789442076 (+- 0.00016641292138464835)\n",
      "X^2 for MWPM and NN: 66.55673758865248\n",
      "X^2 for PLUT and NN: 497.23520249221184\n",
      "> AUC for class Z32: 0.9964458847010961 (+- 0.0002475958761515828)\n",
      "X^2 for MWPM and NN: 86.84307692307692\n",
      "X^2 for PLUT and NN: 493.8476499189627\n",
      "> AUC for class Z33: 0.9964162712465741 (+- 0.0002930934384901463)\n",
      "X^2 for MWPM and NN: 43.72945205479452\n",
      "X^2 for PLUT and NN: 416.32055214723925\n",
      "> AUC for class Z34: 0.9961923403733524 (+- 0.0003410464187767008)\n",
      "X^2 for MWPM and NN: 42.45614035087719\n",
      "X^2 for PLUT and NN: 420.0254372019078\n",
      "> AUC for class Z35: 0.9970596107058395 (+- 0.00026258073344668836)\n",
      "X^2 for MWPM and NN: 59.43214285714286\n",
      "X^2 for PLUT and NN: 497.1093990755008\n",
      "> AUC for class Z36: 0.9982221336888654 (+- 0.00015513408767855023)\n",
      "X^2 for MWPM and NN: 10.50625\n",
      "X^2 for PLUT and NN: 510.85537190082647\n",
      "> AUC for class Z40: 0.9981343205102883 (+- 0.00022305686235154593)\n",
      "X^2 for MWPM and NN: 48.06130268199234\n",
      "X^2 for PLUT and NN: 524.8849693251534\n",
      "> AUC for class Z41: 0.99679717143658 (+- 0.00042220582005678603)\n",
      "X^2 for MWPM and NN: 78.02777777777777\n",
      "X^2 for PLUT and NN: 457.39721792890265\n",
      "> AUC for class Z42: 0.9963800557382944 (+- 5.243277194789775e-05)\n",
      "X^2 for MWPM and NN: 101.55371900826447\n",
      "X^2 for PLUT and NN: 494.45575221238937\n",
      "> AUC for class Z43: 0.9965286703804646 (+- 5.1230296617296774e-05)\n",
      "X^2 for MWPM and NN: 96.85230024213075\n",
      "X^2 for PLUT and NN: 472.60940032414914\n",
      "> AUC for class Z44: 0.9962487782411508 (+- 0.0001906764592184454)\n",
      "X^2 for MWPM and NN: 74.79585798816568\n",
      "X^2 for PLUT and NN: 400.9183381088825\n",
      "> AUC for class Z45: 0.9967134190540552 (+- 0.0002639650037015462)\n",
      "X^2 for MWPM and NN: 75.87841945288754\n",
      "X^2 for PLUT and NN: 422.7514880952381\n",
      "> AUC for class Z46: 0.9980069571186413 (+- 7.956844896414846e-05)\n",
      "X^2 for MWPM and NN: 2.9719101123595504\n",
      "X^2 for PLUT and NN: 514.6314243759177\n",
      "> AUC for class Z50: 0.997102952286519 (+- 0.0003435945437056253)\n",
      "X^2 for MWPM and NN: 213.03762376237623\n",
      "X^2 for PLUT and NN: 446.9553846153846\n",
      "> AUC for class Z51: 0.9959051857642022 (+- 0.0004746373447849767)\n",
      "X^2 for MWPM and NN: 67.2429022082019\n",
      "X^2 for PLUT and NN: 454.7758064516129\n",
      "> AUC for class Z52: 0.9957844630428508 (+- 0.0001950212572795427)\n",
      "X^2 for MWPM and NN: 203.04063604240284\n",
      "X^2 for PLUT and NN: 422.7514880952381\n",
      "> AUC for class Z53: 0.9957060187582384 (+- 0.00029051428037600276)\n",
      "X^2 for MWPM and NN: 124.8984375\n",
      "X^2 for PLUT and NN: 407.8303425774878\n",
      "> AUC for class Z54: 0.9954345305436881 (+- 0.0002594826621572507)\n",
      "X^2 for MWPM and NN: 229.97262773722628\n",
      "X^2 for PLUT and NN: 406.15352112676055\n",
      "> AUC for class Z55: 0.9954715527822465 (+- 0.0005030152977551174)\n",
      "X^2 for MWPM and NN: 133.28\n",
      "X^2 for PLUT and NN: 439.22491909385116\n",
      "> AUC for class Z56: 0.9977613259309485 (+- 3.52938821794975e-05)\n",
      "X^2 for MWPM and NN: 56.91699604743083\n",
      "X^2 for PLUT and NN: 506.40634441087616\n",
      "> AUC for class Z60: 0.9921892377526959 (+- 0.0009063308245144203)\n",
      "X^2 for MWPM and NN: 196.5781512605042\n",
      "X^2 for PLUT and NN: 284.9774096385542\n",
      "> AUC for class Z61: 0.9765189473591654 (+- 0.0004960206653482908)\n",
      "X^2 for MWPM and NN: 384.14443625644805\n",
      "X^2 for PLUT and NN: 277.62455516014234\n",
      "> AUC for class Z62: 0.9766488459945867 (+- 0.0011627513088625805)\n",
      "X^2 for MWPM and NN: 126.50778210116732\n",
      "X^2 for PLUT and NN: 214.61284403669725\n",
      "> AUC for class Z63: 0.9771883311782347 (+- 0.00034796110601017024)\n",
      "X^2 for MWPM and NN: 405.9362519201229\n",
      "X^2 for PLUT and NN: 244.92753623188406\n",
      "> AUC for class Z64: 0.9777157330566261 (+- 0.0005383076363242412)\n",
      "X^2 for MWPM and NN: 76.03575989782887\n",
      "X^2 for PLUT and NN: 120.1757493188011\n",
      "> AUC for class Z65: 0.9772084041496104 (+- 0.0004338425724580213)\n",
      "X^2 for MWPM and NN: 318.26780415430267\n",
      "X^2 for PLUT and NN: 181.88277087033748\n",
      "> AUC for class Z66: 0.9785580276765041 (+- 0.0006827469313093384)\n",
      "X^2 for MWPM and NN: 100.48615916955018\n",
      "X^2 for PLUT and NN: 178.94010889292196\n",
      "###################################################################################\n",
      "TOTAL F1 NN: [0.8216187116337902, 0.8204544989477296, 0.8221997256004157]\n",
      "TOTAL F1 PLUT: [0.21766525375186158, 0.2178357619582105, 0.21685313517575353]\n",
      "TOTAL F1 MWPM: [0.6999577702702703, 0.6984095334162189, 0.6985232475656156]\n",
      "TOTAL ACC NN: [0.9866034388542175, 0.986399233341217, 0.9865171717171654]\n",
      "TOTAL ACC PLUT: [0.9621289938698115, 0.9621443935241187, 0.962102020201907]\n",
      "TOTAL ACC MWPM: [0.9741636363636537, 0.9740272727272915, 0.9740277777777931]\n",
      "TOTAL TIME NN: [5.7770827, 5.9821905, 6.0375564]\n",
      "TOTAL TIME PLUT: [5.4490014, 5.1532726, 5.5817349]\n",
      "TOTAL TIME MWPM: [3258.326625199987, 3391.5704181000146, 3283.0450279999905]\n",
      "------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "kfold = KFold(n_splits=3, shuffle=True)\n",
    "inputs = x_d7.copy()\n",
    "inputs = np.array(inputs)\n",
    "inputs = np.stack(inputs)\n",
    "targets = np.array(y_d7)\n",
    "targets = np.array([np.array(xi) for xi in targets])\n",
    "\n",
    "n_classes = 99\n",
    "\n",
    "acc_per_fold = []\n",
    "f1_per_fold = []\n",
    "time_per_fold = []\n",
    "\n",
    "acc_per_fold_plut = []\n",
    "f1_per_fold_plut = []\n",
    "time_per_fold_plut = []\n",
    "\n",
    "acc_per_fold_mwpm = []\n",
    "f1_per_fold_mwpm = []\n",
    "time_per_fold_mwpm = []\n",
    "\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "aucs_classes = {}\n",
    "for i in mlb_d7.classes_:\n",
    "    aucs_classes[i] = []\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "for i, (train, test) in enumerate(kfold.split(inputs, targets)):\n",
    "    i_train = inputs[train].copy()\n",
    "    i_test = inputs[test].copy()\n",
    "    t_test = targets[test].copy()\n",
    "    x_test_d7 = i_test[:,:2]\n",
    "    inputs_train = i_train[:,3:]\n",
    "    inputs_test = i_test[:,3:]\n",
    "    indices = np.random.choice(inputs[test].shape[0], 20000, replace=False)\n",
    "    x_test_d7 = x_test_d7[indices]\n",
    "    inputs_test_2 = inputs_test[indices]\n",
    "    targets_test_2 = targets[test][indices]\n",
    "    ####################################################################################################\n",
    "    #test MWPM decoder for this fold\n",
    "    #labels = targets[train], features = inputs[train]\n",
    "   # x_test_d7 = translate_to_graph(testData_d7_MWPM, targs, mlb_d7)\n",
    "    decoding_d7, time_mwpm = do_new_decoding(x_test_d7, 7, 0)\n",
    "    decoding_d7['combine'] = decoding_d7[[0, 1]].values.tolist()\n",
    "    decoding_d7['combine'].apply(lambda x: x[0].extend(x[1]))\n",
    "    decoding_d7 = np.array(decoding_d7[0])\n",
    "                                              \n",
    "    time_per_fold_mwpm.append(time_mwpm)\n",
    "                                              \n",
    "    pred_mwpm = mlb_d7.transform(decoding_d7)\n",
    "\n",
    "    if fold_no < 3:\n",
    "        acc = partial_accuracy(targets_test_2, pred_mwpm)\n",
    "    else:\n",
    "        acc, contingency_mwpm = partial_accuracy_and_contingency(targets_test_2, pred_mwpm, mlb_d7)\n",
    "        \n",
    "    acc_per_fold_mwpm.append(acc)\n",
    "    f1_per_fold_mwpm.append(f1_score(targets_test_2, pred_mwpm, average='micro'))\n",
    "\n",
    "    #####################################################################################################\n",
    "    #test the plut decoder for this fold\n",
    "    \n",
    "    lookup_d7 = lookup_decoder(7)\n",
    "    \n",
    "    lookup_d7 = train_plut(lookup_d7, inputs_train, targets[train])\n",
    "    \n",
    "    start = time.time_ns()\n",
    "    pred_plut_d7 = test_plut(lookup_d7, inputs_test)\n",
    "    end = time.time_ns() \n",
    "    time_per_fold_plut.append((end - start)/ (10 ** 9))\n",
    "    \n",
    "    if fold_no < 3:\n",
    "        acc = partial_accuracy(targets[test], pred_plut_d7)\n",
    "        f1 = f1_score(targets[test], pred_plut_d7, average='micro')\n",
    "    else:\n",
    "        pred_plut_d7 = test_plut(lookup_d7, inputs_test_2)\n",
    "        f1 = f1_score(targets_test_2, pred_plut_d7, average='micro')\n",
    "        acc, contingency_plut = partial_accuracy_and_contingency(targets_test_2, pred_plut_d7, mlb_d7)\n",
    "        \n",
    "    acc_per_fold_plut.append(acc)\n",
    "    f1_per_fold_plut.append(f1)\n",
    "    \n",
    "    #####################################################################################################\n",
    "    #Test the NN decoder for this fold\n",
    "    \n",
    "    model_d7 = compile_FFNN_model_DepthSeven(7)\n",
    "\n",
    "    # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    # Fit data to model\n",
    "    history = model_d7.fit(\n",
    "        x=inputs_train ,\n",
    "        y=targets[train],\n",
    "        validation_split=.25,\n",
    "        epochs= 150)\n",
    "    \n",
    "   # Generate generalization metrics\n",
    "    scores = model_d7.evaluate(inputs_test, targets[test], verbose=0)\n",
    "    \n",
    "    start = time.time_ns()\n",
    "    predictions_d7 = model_d7.predict(inputs_test)\n",
    "    end = time.time_ns() \n",
    "    time_per_fold.append((end - start)/ (10 ** 9))\n",
    "    \n",
    "    #threshold based on previous tests with train_test_split in hyperparameter tuning\n",
    "    pred=predictions_d7.copy() #change here\n",
    "    pred[pred>=.5]=1 \n",
    "    pred[pred<.5]=0\n",
    "     \n",
    "    if fold_no < 3:\n",
    "        acc = scores[1]\n",
    "        f1 = f1_score(targets[test], pred, average='micro')\n",
    "    else:\n",
    "        pred = model_d7.predict(inputs_test_2)\n",
    "        pred[pred>=.5]=1 \n",
    "        pred[pred<.5]=0\n",
    "        acc, contingency_nn = partial_accuracy_and_contingency(targets_test_2, pred, mlb_d7)\n",
    "        f1 = f1_score(targets_test_2, pred, average='micro')\n",
    " \n",
    "    acc_per_fold.append(acc)\n",
    "    f1_per_fold.append(f1)\n",
    "\n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(targets[test].ravel(), predictions_d7.ravel())\n",
    "    tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "    tprs[-1][0] = 0.0\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "    plt.plot(fpr, tpr, lw=1, alpha=0.3, label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
    "    \n",
    "    #get the AUCs of each class, used to get average AUC of each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(targets[test][:, i], predictions_d7[:, i]) \n",
    "        aucs_classes[mlb_d7.classes_[i]].append(auc(fpr[i], tpr[i]))\n",
    "\n",
    "#########################################################################################################\n",
    "#compute McNemar's statistic on results of last fold\n",
    "\n",
    "mcnemar_results_mwpm = {}\n",
    "mcnemar_results_plut = {}\n",
    "\n",
    "for class_ in mlb_d7.classes_:\n",
    "    \n",
    "    #compute the x^2 for NN and MWPM\n",
    "    mcnemar_results_mwpm[class_]=contingency_table_and_t(contingency_nn[class_], contingency_mwpm[class_])[1]    \n",
    "    #comput the x^2 for NN and PLUT\n",
    "    mcnemar_results_plut[class_] = contingency_table_and_t(contingency_nn[class_], contingency_plut[class_])[1]\n",
    "        \n",
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds of MWPM:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold_mwpm)} (+- {np.std(acc_per_fold_mwpm)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold_mwpm)}(+- {np.std(f1_per_fold_mwpm)})')\n",
    "print(f'> Time: {np.mean(time_per_fold_mwpm)} (+- {np.std(time_per_fold_mwpm)})')\n",
    "print('Average scores for all folds of PLUT:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold_plut)} (+- {np.std(acc_per_fold_plut)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold_plut)}(+- {np.std(f1_per_fold_plut)})')\n",
    "print(f'> Time: {np.mean(time_per_fold_plut)} (+- {np.std(time_per_fold_plut)})')\n",
    "print('Average scores for all folds of NN:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold)}(+- {np.std(f1_per_fold)})')\n",
    "print(f'> Time: {np.mean(time_per_fold)} (+- {np.std(time_per_fold)})')\n",
    "for key in aucs_classes:\n",
    "    cleanedList = [x for x in aucs_classes[key] if str(x) != 'nan']\n",
    "    print(f'> AUC for class {key}: {np.mean(cleanedList)} (+- {np.std(cleanedList)})') #this has to be a for loop\n",
    "    print(\"X^2 for MWPM and NN: \" + str(mcnemar_results_mwpm[key]))  \n",
    "    print(\"X^2 for PLUT and NN: \" + str(mcnemar_results_plut[key]))\n",
    "print(\"###################################################################################\")\n",
    "print(\"TOTAL F1 NN: \" + str(f1_per_fold))\n",
    "print(\"TOTAL F1 PLUT: \" + str(f1_per_fold_plut))\n",
    "print(\"TOTAL F1 MWPM: \" + str(f1_per_fold_mwpm))\n",
    "print(\"TOTAL ACC NN: \" + str(acc_per_fold))\n",
    "print(\"TOTAL ACC PLUT: \" + str(acc_per_fold_plut))\n",
    "print(\"TOTAL ACC MWPM: \" + str(acc_per_fold_mwpm))\n",
    "print(\"TOTAL TIME NN: \" + str(time_per_fold))\n",
    "print(\"TOTAL TIME PLUT: \" + str(time_per_fold_plut))\n",
    "print(\"TOTAL TIME MWPM: \" + str(time_per_fold_mwpm))\n",
    "print('------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "         label='Chance', alpha=.8)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "         label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "         lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                 label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "plt.xlim([-0.01, 1.01])\n",
    "plt.ylim([-0.01, 1.01])\n",
    "plt.xlabel('False Positive Rate',fontsize=18)\n",
    "plt.ylabel('True Positive Rate',fontsize=18)\n",
    "plt.title('Cross-Validation ROC of Depth 7 NN',fontsize=18)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL F1 NN: [0.7466026170895966, 0.7447298695420905, 0.7449999416902822]\n",
      "TOTAL F1 PLUT: [0.07436437291975014, 0.07319961646740877, 0.07392483964771193]\n",
      "TOTAL F1 MWPM: [0.5570304203885315, 0.5576631378129446, 0.5564358397963814]\n",
      "TOTAL ACC NN: [0.9817760586738586, 0.9817712903022766, 0.9817626262626084]\n",
      "TOTAL ACC PLUT: [0.960282959944449, 0.960287018882497, 0.9603099766722879]\n",
      "TOTAL ACC MWPM: [0.9570212121211669, 0.9571409090908701, 0.9570479797979491]\n",
      "TOTAL TIME NN: [13.6240815, 12.273374, 11.9565955]\n",
      "TOTAL TIME PLUT: [27.279464, 16.1340635, 5.2510398]\n",
      "TOTAL TIME MWPM: [4881.778657700013, 9217.311648799974, 3500.318302099989]\n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"TOTAL F1 NN: \" + str(f1_per_fold))\n",
    "print(\"TOTAL F1 PLUT: \" + str(f1_per_fold_plut))\n",
    "print(\"TOTAL F1 MWPM: \" + str(f1_per_fold_mwpm))\n",
    "print(\"TOTAL ACC NN: \" + str(acc_per_fold))\n",
    "print(\"TOTAL ACC PLUT: \" + str(acc_per_fold_plut))\n",
    "print(\"TOTAL ACC MWPM: \" + str(acc_per_fold_mwpm))\n",
    "print(\"TOTAL TIME NN: \" + str(time_per_fold))\n",
    "print(\"TOTAL TIME PLUT: \" + str(time_per_fold_plut))\n",
    "print(\"TOTAL TIME MWPM: \" + str(time_per_fold_mwpm))\n",
    "print('-----------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test MWPM decoder for this fold\n",
    "#labels = targets[train], features = inputs[train]\n",
    "# x_test_d7 = translate_to_graph(testData_d7_MWPM, targs, mlb_d7)\n",
    "\"\"\"\n",
    "decoding_d7, time_mwpm = do_new_decoding(x_test_d7, 7, 0)\n",
    "decoding_d7['combine'] = decoding_d7[[0, 1]].values.tolist()\n",
    "decoding_d7['combine'].apply(lambda x: x[0].extend(x[1]))\n",
    "decoding_d7 = np.array(decoding_d7[0])\n",
    "\n",
    "time_per_fold_mwpm.append(time_mwpm)\n",
    "\n",
    "pred_mwpm = mlb_d7.transform(decoding_d7)\n",
    "\n",
    "if fold_no < 3:\n",
    "    acc = partial_accuracy(targets_test_2, pred_mwpm)\n",
    "else:\n",
    "    acc, contingency_mwpm = partial_accuracy_and_contingency(targets_test_2, pred_mwpm, mlb_d7)\"\"\"\n",
    "\n",
    "#acc_per_fold_mwpm.append(acc)\n",
    "#f1_per_fold_mwpm.append(f1_score(targets_test_2, pred_mwpm, average='micro'))\n",
    "\n",
    "#####################################################################################################\n",
    "#test the plut decoder for this fold\n",
    "\n",
    "#lookup_d7 = lookup_decoder(7)\n",
    "\n",
    "#lookup_d7 = train_plut(lookup_d7, inputs_train, targets[train])\n",
    "\n",
    "#start = time.time_ns()\n",
    "#pred_plut_d7 = test_plut(lookup_d7, inputs_test)\n",
    "#end = time.time_ns() \n",
    "#time_per_fold_plut.append((end - start)/ (10 ** 9))\n",
    "\n",
    "pred_plut_d7 = test_plut(lookup_d7, inputs_test_2)\n",
    "#f1 = f1_score(targets_test_2, pred_plut_d7, average='micro')\n",
    "acc, contingency_plut = partial_accuracy_and_contingency(targets_test_2, pred_plut_d7, mlb_d7)\n",
    "\n",
    "#acc_per_fold_plut.append(acc)\n",
    "#f1_per_fold_plut.append(f1)\n",
    "\n",
    "#####################################################################################################\n",
    "#Test the NN decoder for this fold\n",
    "\"\"\"\n",
    "model_d7 = compile_FFNN_model_DepthSeven(7)\n",
    "\n",
    "# Generate a print\n",
    "print('------------------------------------------------------------------------')\n",
    "print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "# Fit data to model\n",
    "history = model_d7.fit(\n",
    "    x=inputs_train ,\n",
    "    y=targets[train],\n",
    "    validation_split=.25,\n",
    "    epochs= 150)\"\"\"\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(targets[test].ravel(), predictions_d7.ravel())\n",
    "tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "tprs[-1][0] = 0.0\n",
    "roc_auc = auc(fpr, tpr)\n",
    "aucs.append(roc_auc)\n",
    "plt.plot(fpr, tpr, lw=1, alpha=0.3, label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
    "\n",
    "#get the AUCs of each class, used to get average AUC of each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(targets[test][:, i], predictions_d7[:, i]) \n",
    "    aucs_classes[mlb_d7.classes_[i]].append(auc(fpr[i], tpr[i]))\n",
    "\n",
    "#########################################################################################################\n",
    "#compute McNemar's statistic on results of last fold\n",
    "\n",
    "mcnemar_results_mwpm = {}\n",
    "mcnemar_results_plut = {}\n",
    "\n",
    "for class_ in mlb_d7.classes_:\n",
    "    \n",
    "    #compute the x^2 for NN and MWPM\n",
    "    mcnemar_results_mwpm[class_]=contingency_table_and_t(contingency_nn[class_], contingency_mwpm[class_])[1]    \n",
    "    #comput the x^2 for NN and PLUT\n",
    "    mcnemar_results_plut[class_] = contingency_table_and_t(contingency_nn[class_], contingency_plut[class_])[1]\n",
    "        \n",
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds of MWPM:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold_mwpm)} (+- {np.std(acc_per_fold_mwpm)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold_mwpm)}(+- {np.std(f1_per_fold_mwpm)})')\n",
    "print(f'> Time: {np.mean(time_per_fold_mwpm)} (+- {np.std(time_per_fold_mwpm)})')\n",
    "print('Average scores for all folds of PLUT:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold_plut)} (+- {np.std(acc_per_fold_plut)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold_plut)}(+- {np.std(f1_per_fold_plut)})')\n",
    "print(f'> Time: {np.mean(time_per_fold_plut)} (+- {np.std(time_per_fold_plut)})')\n",
    "print('Average scores for all folds of NN:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold)}(+- {np.std(f1_per_fold)})')\n",
    "print(f'> Time: {np.mean(time_per_fold)} (+- {np.std(time_per_fold)})')\n",
    "for key in aucs_classes:\n",
    "    cleanedList = [x for x in aucs_classes[key] if str(x) != 'nan']\n",
    "    print(f'> AUC for class {key}: {np.mean(cleanedList)} (+- {np.std(cleanedList)})') #this has to be a for loop\n",
    "    print(\"X^2 for MWPM and NN: \" + str(mcnemar_results_mwpm[key]))  \n",
    "    print(\"X^2 for PLUT and NN: \" + str(mcnemar_results_plut[key]))\n",
    "print(\"###################################################################################\")\n",
    "print(\"TOTAL F1 NN: \" + str(f1_per_fold))\n",
    "print(\"TOTAL F1 PLUT: \" + str(f1_per_fold_plut))\n",
    "print(\"TOTAL F1 MWPM: \" + str(f1_per_fold_mwpm))\n",
    "print(\"TOTAL ACC NN: \" + str(acc_per_fold))\n",
    "print(\"TOTAL ACC PLUT: \" + str(acc_per_fold_plut))\n",
    "print(\"TOTAL ACC MWPM: \" + str(acc_per_fold_mwpm))\n",
    "print(\"TOTAL TIME NN: \" + str(time_per_fold))\n",
    "print(\"TOTAL TIME PLUT: \" + str(time_per_fold_plut))\n",
    "print(\"TOTAL TIME MWPM: \" + str(time_per_fold_mwpm))\n",
    "print('------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "         label='Chance', alpha=.8)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "         label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "         lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                 label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "plt.xlim([-0.01, 1.01])\n",
    "plt.ylim([-0.01, 1.01])\n",
    "plt.xlabel('False Positive Rate',fontsize=18)\n",
    "plt.ylabel('True Positive Rate',fontsize=18)\n",
    "plt.title('Cross-Validation ROC of Depth 7 NN',fontsize=18)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clair\\Anaconda3\\envs\\CNNs\\lib\\site-packages\\ipykernel_launcher.py:74: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "C:\\Users\\clair\\Anaconda3\\envs\\CNNs\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:808: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20000, 0]\n",
      "[0, 0]\n",
      "both b and c are zero\n",
      "[20000, 0]\n",
      "[0, 0]\n",
      "both b and c are zero\n",
      "[18054, 1145]\n",
      "[621, 180]\n",
      "[19047, 152]\n",
      "[21, 780]\n",
      "[18626, 796]\n",
      "[335, 243]\n",
      "[19002, 420]\n",
      "[148, 430]\n",
      "[19287, 478]\n",
      "[115, 120]\n",
      "[19151, 614]\n",
      "[90, 145]\n",
      "[19477, 295]\n",
      "[89, 139]\n",
      "[19136, 636]\n",
      "[59, 169]\n",
      "[19293, 487]\n",
      "[99, 121]\n",
      "[19111, 669]\n",
      "[85, 135]\n",
      "[19112, 663]\n",
      "[87, 138]\n",
      "[19133, 642]\n",
      "[99, 126]\n",
      "[17479, 1895]\n",
      "[365, 261]\n",
      "[19026, 348]\n",
      "[115, 511]\n",
      "[17995, 1396]\n",
      "[309, 300]\n",
      "[19020, 371]\n",
      "[105, 504]\n",
      "[19027, 641]\n",
      "[145, 187]\n",
      "[19091, 577]\n",
      "[106, 226]\n",
      "[19194, 507]\n",
      "[138, 161]\n",
      "[19105, 596]\n",
      "[119, 180]\n",
      "[19288, 449]\n",
      "[97, 166]\n",
      "[19101, 636]\n",
      "[140, 123]\n",
      "[19404, 347]\n",
      "[94, 155]\n",
      "[19098, 653]\n",
      "[123, 126]\n",
      "[18949, 761]\n",
      "[133, 157]\n",
      "[19085, 625]\n",
      "[136, 154]\n",
      "[19090, 284]\n",
      "[76, 550]\n",
      "[19053, 321]\n",
      "[82, 544]\n",
      "[19066, 339]\n",
      "[87, 508]\n",
      "[19007, 398]\n",
      "[91, 504]\n",
      "[18783, 938]\n",
      "[119, 160]\n",
      "[19113, 608]\n",
      "[95, 184]\n",
      "[19198, 514]\n",
      "[116, 172]\n",
      "[19068, 644]\n",
      "[150, 138]\n",
      "[19176, 549]\n",
      "[109, 166]\n",
      "[19082, 643]\n",
      "[87, 188]\n",
      "[19093, 654]\n",
      "[130, 123]\n",
      "[19165, 582]\n",
      "[80, 173]\n",
      "[19201, 515]\n",
      "[115, 169]\n",
      "[19101, 615]\n",
      "[105, 179]\n",
      "[18018, 1404]\n",
      "[303, 275]\n",
      "[19056, 366]\n",
      "[86, 492]\n",
      "[18054, 1371]\n",
      "[268, 307]\n",
      "[19018, 407]\n",
      "[146, 429]\n",
      "[18955, 754]\n",
      "[134, 157]\n",
      "[19076, 633]\n",
      "[143, 148]\n",
      "[19138, 616]\n",
      "[102, 144]\n",
      "[19126, 628]\n",
      "[99, 147]\n",
      "[19235, 467]\n",
      "[103, 195]\n",
      "[19081, 621]\n",
      "[101, 197]\n",
      "[19199, 505]\n",
      "[109, 187]\n",
      "[19062, 642]\n",
      "[110, 186]\n",
      "[18978, 758]\n",
      "[105, 159]\n",
      "[19109, 627]\n",
      "[87, 177]\n",
      "[19059, 368]\n",
      "[126, 447]\n",
      "[19026, 401]\n",
      "[126, 447]\n",
      "[19082, 366]\n",
      "[121, 431]\n",
      "[19065, 383]\n",
      "[112, 440]\n",
      "[18751, 977]\n",
      "[135, 137]\n",
      "[19176, 552]\n",
      "[113, 159]\n",
      "[19273, 442]\n",
      "[100, 185]\n",
      "[19050, 665]\n",
      "[115, 170]\n",
      "[19152, 552]\n",
      "[123, 173]\n",
      "[19096, 608]\n",
      "[86, 210]\n",
      "[19089, 640]\n",
      "[107, 164]\n",
      "[19114, 615]\n",
      "[113, 158]\n",
      "[19167, 533]\n",
      "[109, 191]\n",
      "[19090, 610]\n",
      "[126, 174]\n",
      "[17906, 1526]\n",
      "[278, 290]\n",
      "[19072, 360]\n",
      "[128, 440]\n",
      "[18017, 1365]\n",
      "[287, 331]\n",
      "[18973, 409]\n",
      "[206, 412]\n",
      "[19089, 614]\n",
      "[114, 183]\n",
      "[19076, 627]\n",
      "[99, 198]\n",
      "[19137, 570]\n",
      "[127, 166]\n",
      "[19024, 683]\n",
      "[183, 110]\n",
      "[19294, 448]\n",
      "[85, 173]\n",
      "[19149, 593]\n",
      "[89, 169]\n",
      "[19278, 447]\n",
      "[120, 155]\n",
      "[19106, 619]\n",
      "[87, 188]\n",
      "[18967, 748]\n",
      "[106, 179]\n",
      "[19117, 598]\n",
      "[72, 213]\n",
      "[19088, 337]\n",
      "[124, 451]\n",
      "[19077, 348]\n",
      "[127, 448]\n",
      "[19138, 255]\n",
      "[65, 542]\n",
      "[19101, 292]\n",
      "[69, 538]\n",
      "[19286, 480]\n",
      "[100, 134]\n",
      "[19163, 603]\n",
      "[78, 156]\n",
      "[19637, 133]\n",
      "[75, 155]\n",
      "[19094, 676]\n",
      "[82, 148]\n",
      "[19461, 318]\n",
      "[89, 132]\n",
      "[19142, 637]\n",
      "[83, 138]\n",
      "[19471, 297]\n",
      "[80, 152]\n",
      "[19141, 627]\n",
      "[59, 173]\n",
      "[19463, 281]\n",
      "[114, 142]\n",
      "[19099, 645]\n",
      "[113, 143]\n",
      "[18995, 479]\n",
      "[254, 272]\n",
      "[19025, 449]\n",
      "[140, 386]\n",
      "[17303, 1894]\n",
      "[474, 329]\n",
      "[19039, 158]\n",
      "[143, 660]\n",
      "[19076, 272]\n",
      "[54, 598]\n",
      "[19021, 327]\n",
      "[68, 584]\n",
      "[18011, 1418]\n",
      "[292, 279]\n",
      "[19121, 308]\n",
      "[55, 516]\n",
      "[19110, 296]\n",
      "[57, 537]\n",
      "[19082, 324]\n",
      "[66, 528]\n",
      "[17807, 1599]\n",
      "[317, 277]\n",
      "[19058, 348]\n",
      "[79, 515]\n",
      "[19161, 263]\n",
      "[24, 552]\n",
      "[19108, 316]\n",
      "[36, 540]\n",
      "[19065, 429]\n",
      "[230, 276]\n",
      "[19045, 449]\n",
      "[167, 339]\n",
      "[18994, 715]\n",
      "[153, 138]\n",
      "[19081, 628]\n",
      "[108, 183]\n",
      "[18817, 874]\n",
      "[148, 161]\n",
      "[19099, 592]\n",
      "[104, 205]\n",
      "[19184, 537]\n",
      "[108, 171]\n",
      "[19175, 546]\n",
      "[87, 192]\n",
      "[18903, 805]\n",
      "[129, 163]\n",
      "[19111, 597]\n",
      "[106, 186]\n",
      "[18916, 761]\n",
      "[151, 172]\n",
      "[19089, 588]\n",
      "[134, 189]\n",
      "[18781, 919]\n",
      "[130, 170]\n",
      "[19107, 593]\n",
      "[110, 190]\n",
      "[19461, 267]\n",
      "[102, 170]\n",
      "[19142, 586]\n",
      "[85, 187]\n",
      "[19380, 349]\n",
      "[121, 150]\n",
      "[19066, 663]\n",
      "[124, 147]\n",
      "[19278, 441]\n",
      "[101, 180]\n",
      "[19095, 624]\n",
      "[104, 177]\n",
      "[19027, 706]\n",
      "[128, 139]\n",
      "[19099, 634]\n",
      "[140, 127]\n",
      "[18998, 721]\n",
      "[136, 145]\n",
      "[19082, 637]\n",
      "[65, 216]\n",
      "[19091, 591]\n",
      "[154, 164]\n",
      "[19147, 535]\n",
      "[68, 250]\n",
      "[19329, 399]\n",
      "[126, 146]\n",
      "[19175, 553]\n",
      "[77, 195]\n",
      "[19331, 420]\n",
      "[122, 127]\n",
      "[19179, 572]\n",
      "[66, 183]\n",
      "[19303, 475]\n",
      "[112, 110]\n",
      "[19159, 619]\n",
      "[74, 148]\n",
      "[19231, 497]\n",
      "[99, 173]\n",
      "[19147, 581]\n",
      "[57, 215]\n",
      "[19288, 409]\n",
      "[112, 191]\n",
      "[19088, 609]\n",
      "[119, 184]\n",
      "[19287, 441]\n",
      "[96, 176]\n",
      "[19104, 624]\n",
      "[85, 187]\n",
      "[19201, 498]\n",
      "[136, 165]\n",
      "[19080, 619]\n",
      "[93, 208]\n",
      "[19208, 510]\n",
      "[127, 155]\n",
      "[19100, 618]\n",
      "[88, 194]\n",
      "[19532, 212]\n",
      "[104, 152]\n",
      "[19125, 619]\n",
      "[61, 195]\n",
      "[19367, 411]\n",
      "[102, 120]\n",
      "[19162, 616]\n",
      "[59, 163]\n",
      "[19243, 459]\n",
      "[132, 166]\n",
      "[19167, 535]\n",
      "[93, 205]\n",
      "[19116, 608]\n",
      "[127, 149]\n",
      "[19103, 621]\n",
      "[93, 183]\n",
      "[19073, 666]\n",
      "[119, 142]\n",
      "[19146, 593]\n",
      "[74, 187]\n",
      "[19229, 503]\n",
      "[98, 170]\n",
      "[19114, 618]\n",
      "[88, 180]\n",
      "[19257, 470]\n",
      "[120, 153]\n",
      "[19138, 589]\n",
      "[83, 190]\n",
      "[19483, 259]\n",
      "[110, 148]\n",
      "[19167, 575]\n",
      "[86, 172]\n",
      "[18980, 795]\n",
      "[129, 96]\n",
      "[19148, 627]\n",
      "[87, 138]\n",
      "[19295, 414]\n",
      "[89, 202]\n",
      "[19131, 578]\n",
      "[81, 210]\n",
      "[18940, 769]\n",
      "[128, 163]\n",
      "[19132, 577]\n",
      "[102, 189]\n",
      "[19168, 545]\n",
      "[114, 173]\n",
      "[19174, 539]\n",
      "[119, 168]\n",
      "[18904, 783]\n",
      "[131, 182]\n",
      "[19072, 615]\n",
      "[123, 190]\n",
      "[19154, 519]\n",
      "[121, 206]\n",
      "[19096, 577]\n",
      "[98, 229]\n",
      "[19286, 443]\n",
      "[96, 175]\n",
      "[19139, 590]\n",
      "[119, 152]\n",
      "[18527, 999]\n",
      "[267, 207]\n",
      "[19109, 417]\n",
      "[123, 351]\n",
      "[18052, 1404]\n",
      "[283, 261]\n",
      "[19116, 340]\n",
      "[64, 480]\n",
      "[19135, 314]\n",
      "[37, 514]\n",
      "[19119, 330]\n",
      "[43, 508]\n",
      "[18009, 1437]\n",
      "[286, 268]\n",
      "[19133, 313]\n",
      "[57, 497]\n",
      "[19163, 260]\n",
      "[26, 551]\n",
      "[19127, 296]\n",
      "[35, 542]\n",
      "[17896, 1492]\n",
      "[335, 277]\n",
      "[19096, 292]\n",
      "[90, 522]\n",
      "[19132, 267]\n",
      "[69, 532]\n",
      "[19092, 307]\n",
      "[75, 526]\n",
      "------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds of MWPM:\n",
      "> Accuracy: 0.9570700336699954 (+- 5.129406134054286e-05)\n",
      "> F1: 0.5570431326659525(+- 0.0005011229435730007)\n",
      "> Time: 5708.04626292499 (+- 2127.064047847968)\n",
      "Average scores for all folds of PLUT:\n",
      "> Accuracy: 0.9602933184997448 (+- 1.1895090832780539e-05)\n",
      "> F1: 0.07382960967829029(+- 0.000480254070692271)\n",
      "> Time: 16.221522433333334 (+- 8.993279156590631)\n",
      "Average scores for all folds of NN:\n",
      "> Accuracy: 0.9817699917462478 (+- 5.5601021464884325e-06)\n",
      "> F1: 0.7454087193138745(+- 0.0007184395330878576)\n",
      "> Time: 12.618017 (+- 0.7230543954345114)\n",
      "> AUC for class : 0.9833169021151996 (+- 0.0)\n",
      "X^2 for MWPM and NN: 0\n",
      "X^2 for PLUT and NN: 0\n",
      "> AUC for class X00: 0.8214824191454625 (+- 0.0025965188951278424)\n",
      "X^2 for MWPM and NN: 154.88618346545866\n",
      "X^2 for PLUT and NN: 97.6878612716763\n",
      "> AUC for class X01: 0.9659140492205993 (+- 0.0013856368529444657)\n",
      "X^2 for MWPM and NN: 187.09106984969054\n",
      "X^2 for PLUT and NN: 129.29753521126761\n",
      "> AUC for class X02: 0.9941481185545963 (+- 0.00018517768896231514)\n",
      "X^2 for MWPM and NN: 220.9848229342327\n",
      "X^2 for PLUT and NN: 388.5355113636364\n",
      "> AUC for class X03: 0.9948369987791448 (+- 0.00016248593694092892)\n",
      "X^2 for MWPM and NN: 109.44010416666667\n",
      "X^2 for PLUT and NN: 477.37553956834535\n",
      "> AUC for class X04: 0.9950563100344874 (+- 0.00021123204941837263)\n",
      "X^2 for MWPM and NN: 255.57849829351537\n",
      "X^2 for PLUT and NN: 450.7811671087533\n",
      "> AUC for class X05: 0.9937200300930737 (+- 0.0005736280249339026)\n",
      "X^2 for MWPM and NN: 440.8333333333333\n",
      "X^2 for PLUT and NN: 396.442645074224\n",
      "> AUC for class X06: 0.9643083059751663 (+- 0.0007666845909616321)\n",
      "X^2 for MWPM and NN: 1034.4429203539823\n",
      "X^2 for PLUT and NN: 116.25053995680345\n",
      "> AUC for class X10: 0.9638944665277996 (+- 0.0004798839319670582)\n",
      "X^2 for MWPM and NN: 691.7278592375367\n",
      "X^2 for PLUT and NN: 147.53151260504202\n",
      "> AUC for class X11: 0.9893589221952863 (+- 0.00015059111072014743)\n",
      "X^2 for MWPM and NN: 311.73664122137404\n",
      "X^2 for PLUT and NN: 323.4260614934114\n",
      "> AUC for class X12: 0.9913337108417408 (+- 0.0003338436690197651)\n",
      "X^2 for MWPM and NN: 209.9596899224806\n",
      "X^2 for PLUT and NN: 316.8895104895105\n",
      "> AUC for class X13: 0.992870749227103 (+- 0.0004650173267408303)\n",
      "X^2 for MWPM and NN: 225.64285714285714\n",
      "X^2 for PLUT and NN: 315.75386597938143\n",
      "> AUC for class X14: 0.9924938456512202 (+- 0.00044251441583538563)\n",
      "X^2 for MWPM and NN: 144.0\n",
      "X^2 for PLUT and NN: 360.61984536082474\n",
      "> AUC for class X15: 0.9915292718041612 (+- 0.000519549173662067)\n",
      "X^2 for MWPM and NN: 439.741610738255\n",
      "X^2 for PLUT and NN: 312.93561103810777\n",
      "> AUC for class X16: 0.9639277073858591 (+- 0.0010890794911275143)\n",
      "X^2 for MWPM and NN: 119.025\n",
      "X^2 for PLUT and NN: 140.5558312655087\n",
      "> AUC for class X20: 0.9645450711977941 (+- 0.001298706383829854)\n",
      "X^2 for MWPM and NN: 147.88967136150234\n",
      "X^2 for PLUT and NN: 191.48466257668713\n",
      "> AUC for class X21: 0.9911660070676287 (+- 0.000498133071855436)\n",
      "X^2 for MWPM and NN: 633.0406811731315\n",
      "X^2 for PLUT and NN: 372.8933143669986\n",
      "> AUC for class X22: 0.9919101005304631 (+- 7.08320407296451e-05)\n",
      "X^2 for MWPM and NN: 250.17301587301588\n",
      "X^2 for PLUT and NN: 306.10705289672546\n",
      "> AUC for class X23: 0.9924075854809054 (+- 0.0003694914622968242)\n",
      "X^2 for MWPM and NN: 292.8890577507599\n",
      "X^2 for PLUT and NN: 421.95205479452056\n",
      "> AUC for class X24: 0.9917942665664415 (+- 0.0005538642562364193)\n",
      "X^2 for MWPM and NN: 348.8890306122449\n",
      "X^2 for PLUT and NN: 379.15558912386706\n",
      "> AUC for class X25: 0.9912222878230179 (+- 0.0002069275692700141)\n",
      "X^2 for MWPM and NN: 252.7\n",
      "X^2 for PLUT and NN: 359.83472222222224\n",
      "> AUC for class X26: 0.9644859507253847 (+- 0.0014554333050149255)\n",
      "X^2 for MWPM and NN: 708.8459285295841\n",
      "X^2 for PLUT and NN: 172.2146017699115\n",
      "> AUC for class X30: 0.9642839176375231 (+- 0.001296156299046868)\n",
      "X^2 for MWPM and NN: 740.9420378279439\n",
      "X^2 for PLUT and NN: 122.24231464737794\n",
      "> AUC for class X31: 0.991024583720112 (+- 0.00011754990612471821)\n",
      "X^2 for MWPM and NN: 431.4876126126126\n",
      "X^2 for PLUT and NN: 308.14561855670104\n",
      "> AUC for class X32: 0.9920813941482659 (+- 0.0003993664306928)\n",
      "X^2 for MWPM and NN: 366.5306406685237\n",
      "X^2 for PLUT and NN: 383.47180192572216\n",
      "> AUC for class X33: 0.9926187925020055 (+- 0.00014827110898040324)\n",
      "X^2 for MWPM and NN: 231.17368421052632\n",
      "X^2 for PLUT and NN: 373.0761772853186\n",
      "> AUC for class X34: 0.9922919215270747 (+- 0.0004187504634700006)\n",
      "X^2 for MWPM and NN: 254.11237785016286\n",
      "X^2 for PLUT and NN: 374.94813829787233\n",
      "> AUC for class X35: 0.9911107604076475 (+- 5.7442476497794174e-05)\n",
      "X^2 for MWPM and NN: 492.58864426419467\n",
      "X^2 for PLUT and NN: 406.8921568627451\n",
      "> AUC for class X36: 0.9648943008127626 (+- 0.0007933734673146436)\n",
      "X^2 for MWPM and NN: 117.57287449392713\n",
      "X^2 for PLUT and NN: 142.45920303605314\n",
      "> AUC for class X40: 0.9644342142983016 (+- 0.0003615609895865136)\n",
      "X^2 for MWPM and NN: 122.25051334702259\n",
      "X^2 for PLUT and NN: 147.27272727272728\n",
      "> AUC for class X41: 0.9910831063493444 (+- 0.00012056652331653613)\n",
      "X^2 for MWPM and NN: 636.0440647482014\n",
      "X^2 for PLUT and NN: 288.4872180451128\n",
      "> AUC for class X42: 0.9926111666446996 (+- 0.00022953180136432139)\n",
      "X^2 for MWPM and NN: 214.54059040590406\n",
      "X^2 for PLUT and NN: 386.41153846153844\n",
      "> AUC for class X43: 0.9922875857964295 (+- 0.00017198395271088976)\n",
      "X^2 for MWPM and NN: 271.3837037037037\n",
      "X^2 for PLUT and NN: 391.1253602305475\n",
      "> AUC for class X44: 0.9921383209433783 (+- 0.00020143016542978364)\n",
      "X^2 for MWPM and NN: 378.88085676037485\n",
      "X^2 for PLUT and NN: 344.7815934065934\n",
      "> AUC for class X45: 0.9910246183197766 (+- 0.0004849259165691765)\n",
      "X^2 for MWPM and NN: 278.7056074766355\n",
      "X^2 for PLUT and NN: 316.96875\n",
      "> AUC for class X46: 0.9641168101332597 (+- 0.0007289162318493675)\n",
      "X^2 for MWPM and NN: 861.9783813747229\n",
      "X^2 for PLUT and NN: 109.34631147540983\n",
      "> AUC for class X50: 0.9637312314765388 (+- 0.00044174594401776035)\n",
      "X^2 for MWPM and NN: 702.1361985472155\n",
      "X^2 for PLUT and NN: 66.3479674796748\n",
      "> AUC for class X51: 0.9913077295803272 (+- 0.00037321008188926356)\n",
      "X^2 for MWPM and NN: 342.03434065934067\n",
      "X^2 for PLUT and NN: 382.54683195592287\n",
      "> AUC for class X52: 0.9925652901435948 (+- 0.0002513101341999641)\n",
      "X^2 for MWPM and NN: 280.2926829268293\n",
      "X^2 for PLUT and NN: 287.5300230946882\n",
      "> AUC for class X53: 0.9929022147422848 (+- 0.00044414417042450534)\n",
      "X^2 for MWPM and NN: 245.86116322701687\n",
      "X^2 for PLUT and NN: 370.9809384164223\n",
      "> AUC for class X54: 0.9923540377658493 (+- 0.00016355575164228069)\n",
      "X^2 for MWPM and NN: 187.43562610229276\n",
      "X^2 for PLUT and NN: 399.3781869688385\n",
      "> AUC for class X55: 0.9915173078629916 (+- 0.0003779368707950591)\n",
      "X^2 for MWPM and NN: 481.12529274004686\n",
      "X^2 for PLUT and NN: 411.3805970149254\n",
      "> AUC for class X56: 0.9649818907199452 (+- 0.0002708534335919717)\n",
      "X^2 for MWPM and NN: 97.49240780911063\n",
      "X^2 for PLUT and NN: 101.89473684210526\n",
      "> AUC for class X60: 0.9633828630913024 (+- 0.0003393581193685094)\n",
      "X^2 for MWPM and NN: 111.628125\n",
      "X^2 for PLUT and NN: 136.5207756232687\n",
      "> AUC for class X61: 0.9941973983247008 (+- 0.00019569542226404162)\n",
      "X^2 for MWPM and NN: 247.65689655172415\n",
      "X^2 for PLUT and NN: 403.19530102790014\n",
      "> AUC for class X62: 0.9948532187163599 (+- 0.000103727036994222)\n",
      "X^2 for MWPM and NN: 15.620192307692308\n",
      "X^2 for PLUT and NN: 463.9168865435356\n",
      "> AUC for class X63: 0.9951388459250474 (+- 0.00024226241520952104)\n",
      "X^2 for MWPM and NN: 127.72481572481573\n",
      "X^2 for PLUT and NN: 424.7347222222222\n",
      "> AUC for class X64: 0.9945299182888703 (+- 0.0004554492761440269)\n",
      "X^2 for MWPM and NN: 123.75596816976127\n",
      "X^2 for PLUT and NN: 468.64285714285717\n",
      "> AUC for class X65: 0.9930255945711417 (+- 3.475856758863264e-05)\n",
      "X^2 for MWPM and NN: 69.76202531645569\n",
      "X^2 for PLUT and NN: 371.9802110817942\n",
      "> AUC for class X66: 0.9795916915648133 (+- 0.0005834965535613788)\n",
      "X^2 for MWPM and NN: 68.45293315143248\n",
      "X^2 for PLUT and NN: 161.05942275042446\n",
      "> AUC for class Z00: 0.9525997267249513 (+- 0.000903718971728793)\n",
      "X^2 for MWPM and NN: 850.3213682432432\n",
      "X^2 for PLUT and NN: 0.6511627906976745\n",
      "> AUC for class Z01: 0.956403202844173 (+- 0.0005298346532048797)\n",
      "X^2 for MWPM and NN: 144.44478527607362\n",
      "X^2 for PLUT and NN: 168.51645569620254\n",
      "> AUC for class Z02: 0.9620305594641221 (+- 0.0014570896471886752)\n",
      "X^2 for MWPM and NN: 740.1315789473684\n",
      "X^2 for PLUT and NN: 174.94214876033058\n",
      "> AUC for class Z03: 0.9635397548661386 (+- 0.0012350624435508825)\n",
      "X^2 for MWPM and NN: 160.46458923512748\n",
      "X^2 for PLUT and NN: 169.35641025641024\n",
      "> AUC for class Z04: 0.9619354708543594 (+- 0.0004948386532119759)\n",
      "X^2 for MWPM and NN: 856.4514613778706\n",
      "X^2 for PLUT and NN: 168.20608899297423\n",
      "> AUC for class Z05: 0.9621777388185793 (+- 0.0008868640261445937)\n",
      "X^2 for MWPM and NN: 197.3658536585366\n",
      "X^2 for PLUT and NN: 221.13920454545453\n",
      "> AUC for class Z06: 0.9786469150540269 (+- 0.0002576214459584225)\n",
      "X^2 for MWPM and NN: 59.490136570561454\n",
      "X^2 for PLUT and NN: 128.18344155844156\n",
      "> AUC for class Z10: 0.9928751423816233 (+- 0.0001440901638348285)\n",
      "X^2 for MWPM and NN: 362.58179723502303\n",
      "X^2 for PLUT and NN: 365.9796195652174\n",
      "> AUC for class Z11: 0.9888575792003811 (+- 0.0004764121043467036)\n",
      "X^2 for MWPM and NN: 514.3101761252447\n",
      "X^2 for PLUT and NN: 340.76005747126436\n",
      "> AUC for class Z12: 0.9895734260006195 (+- 0.00042675160571411776)\n",
      "X^2 for MWPM and NN: 284.0062015503876\n",
      "X^2 for PLUT and NN: 331.38072669826227\n",
      "> AUC for class Z13: 0.9894767908168498 (+- 0.00028942438407495775)\n",
      "X^2 for MWPM and NN: 487.82119914346896\n",
      "X^2 for PLUT and NN: 341.53627311522047\n",
      "> AUC for class Z14: 0.9904000169593574 (+- 0.00028628047399610254)\n",
      "X^2 for MWPM and NN: 406.66776315789474\n",
      "X^2 for PLUT and NN: 284.2229916897507\n",
      "> AUC for class Z15: 0.9903049887661759 (+- 0.0004323795689815059)\n",
      "X^2 for MWPM and NN: 591.9389895138227\n",
      "X^2 for PLUT and NN: 330.475106685633\n",
      "> AUC for class Z16: 0.9920589977489737 (+- 0.0003041163455265835)\n",
      "X^2 for MWPM and NN: 72.88888888888889\n",
      "X^2 for PLUT and NN: 372.5782414307005\n",
      "> AUC for class Z20: 0.9941852880641522 (+- 0.00023656706676816776)\n",
      "X^2 for MWPM and NN: 109.63617021276596\n",
      "X^2 for PLUT and NN: 367.7814485387548\n",
      "> AUC for class Z21: 0.9908421238490683 (+- 0.000503875017711224)\n",
      "X^2 for MWPM and NN: 212.03136531365314\n",
      "X^2 for PLUT and NN: 370.0013736263736\n",
      "> AUC for class Z22: 0.9910383153979971 (+- 0.00023166122082677476)\n",
      "X^2 for MWPM and NN: 399.1954436450839\n",
      "X^2 for PLUT and NN: 314.01679586563307\n",
      "> AUC for class Z23: 0.991459254885439 (+- 9.209537945980369e-05)\n",
      "X^2 for MWPM and NN: 397.9649941656943\n",
      "X^2 for PLUT and NN: 464.44586894586894\n",
      "> AUC for class Z24: 0.990827911332598 (+- 0.0003819356729801369)\n",
      "X^2 for MWPM and NN: 255.16241610738254\n",
      "X^2 for PLUT and NN: 360.12603648424545\n",
      "> AUC for class Z25: 0.9913976282848337 (+- 0.00011421206981863637)\n",
      "X^2 for MWPM and NN: 140.92190476190476\n",
      "X^2 for PLUT and NN: 358.13492063492066\n",
      "> AUC for class Z26: 0.9945302620164747 (+- 0.0004047449679020229)\n",
      "X^2 for MWPM and NN: 162.74723247232473\n",
      "X^2 for PLUT and NN: 399.7257053291536\n",
      "> AUC for class Z30: 0.9939488716844647 (+- 0.00029041340961524515)\n",
      "X^2 for MWPM and NN: 223.24361158432708\n",
      "X^2 for PLUT and NN: 427.03607503607503\n",
      "> AUC for class Z31: 0.9913240224507143 (+- 0.00022417852749659968)\n",
      "X^2 for MWPM and NN: 264.44463087248323\n",
      "X^2 for PLUT and NN: 428.72884012539186\n",
      "> AUC for class Z32: 0.990387614987274 (+- 0.0003458087386458517)\n",
      "X^2 for MWPM and NN: 168.16890595009596\n",
      "X^2 for PLUT and NN: 328.4629120879121\n",
      "> AUC for class Z33: 0.9912942526752065 (+- 0.0002397136462101745)\n",
      "X^2 for MWPM and NN: 220.36499068901304\n",
      "X^2 for PLUT and NN: 408.2425952045134\n",
      "> AUC for class Z34: 0.9904966246503596 (+- 0.00030556131396860484)\n",
      "X^2 for MWPM and NN: 205.55362776025237\n",
      "X^2 for PLUT and NN: 387.1137640449438\n",
      "> AUC for class Z35: 0.991373401155569 (+- 0.0003576302524544807)\n",
      "X^2 for MWPM and NN: 229.08006279434852\n",
      "X^2 for PLUT and NN: 396.3753541076487\n",
      "> AUC for class Z36: 0.9943409833859709 (+- 0.00015665576020928654)\n",
      "X^2 for MWPM and NN: 36.23101265822785\n",
      "X^2 for PLUT and NN: 456.2485294117647\n",
      "> AUC for class Z40: 0.9940398301263239 (+- 0.0004966705021906446)\n",
      "X^2 for MWPM and NN: 184.92007797270955\n",
      "X^2 for PLUT and NN: 457.97925925925927\n",
      "> AUC for class Z41: 0.9916740134567952 (+- 0.00046429972591283065)\n",
      "X^2 for MWPM and NN: 179.82402707275804\n",
      "X^2 for PLUT and NN: 309.6831210191083\n",
      "> AUC for class Z42: 0.9907306656804362 (+- 0.0002591884919940091)\n",
      "X^2 for MWPM and NN: 313.46938775510205\n",
      "X^2 for PLUT and NN: 388.9761904761905\n",
      "> AUC for class Z43: 0.9905733192182476 (+- 0.00010025085602284877)\n",
      "X^2 for MWPM and NN: 379.7656050955414\n",
      "X^2 for PLUT and NN: 402.2848575712144\n",
      "> AUC for class Z44: 0.9910515260340833 (+- 0.0003781693750810755)\n",
      "X^2 for MWPM and NN: 271.5740432612313\n",
      "X^2 for PLUT and NN: 396.3753541076487\n",
      "> AUC for class Z45: 0.991547320065576 (+- 0.0002589608433178719)\n",
      "X^2 for MWPM and NN: 206.44237288135594\n",
      "X^2 for PLUT and NN: 379.5014880952381\n",
      "> AUC for class Z46: 0.9943715269781732 (+- 0.0004337105057427746)\n",
      "X^2 for MWPM and NN: 59.360433604336045\n",
      "X^2 for PLUT and NN: 360.2783661119516\n",
      "> AUC for class Z50: 0.9926937080869497 (+- 0.00018113683624329586)\n",
      "X^2 for MWPM and NN: 478.5984848484849\n",
      "X^2 for PLUT and NN: 406.8921568627451\n",
      "> AUC for class Z51: 0.990678025536443 (+- 0.0004979642221350934)\n",
      "X^2 for MWPM and NN: 208.69980119284295\n",
      "X^2 for PLUT and NN: 373.3171471927162\n",
      "> AUC for class Z52: 0.9897448878944639 (+- 0.00014976993629415403)\n",
      "X^2 for MWPM and NN: 456.63322185061315\n",
      "X^2 for PLUT and NN: 330.8924889543446\n",
      "> AUC for class Z53: 0.9901327559016765 (+- 0.00010894475268285256)\n",
      "X^2 for MWPM and NN: 280.5766312594841\n",
      "X^2 for PLUT and NN: 266.8100303951368\n",
      "> AUC for class Z54: 0.9897299766603224 (+- 0.00013647425169653762)\n",
      "X^2 for MWPM and NN: 463.67724288840265\n",
      "X^2 for PLUT and NN: 326.6680216802168\n",
      "> AUC for class Z55: 0.9896182650747791 (+- 0.0007809797041923034)\n",
      "X^2 for MWPM and NN: 246.2640625\n",
      "X^2 for PLUT and NN: 338.4948148148148\n",
      "> AUC for class Z56: 0.9934724287486076 (+- 0.00018431747489311935)\n",
      "X^2 for MWPM and NN: 222.10760667903526\n",
      "X^2 for PLUT and NN: 311.5655853314527\n",
      "> AUC for class Z60: 0.9792915582055787 (+- 0.0006432404233993493)\n",
      "X^2 for MWPM and NN: 422.0860979462875\n",
      "X^2 for PLUT and NN: 158.97962962962964\n",
      "> AUC for class Z61: 0.9610838010132154 (+- 0.0005107878129249448)\n",
      "X^2 for MWPM and NN: 743.5684647302904\n",
      "X^2 for PLUT and NN: 187.19059405940595\n",
      "> AUC for class Z62: 0.9629733508930871 (+- 0.0015571763237176124)\n",
      "X^2 for MWPM and NN: 217.02564102564102\n",
      "X^2 for PLUT and NN: 219.2922252010724\n",
      "> AUC for class Z63: 0.9620784499590056 (+- 0.0007602525456524797)\n",
      "X^2 for MWPM and NN: 767.5565873476495\n",
      "X^2 for PLUT and NN: 175.74324324324326\n",
      "> AUC for class Z64: 0.9626508879631023 (+- 0.00018957995743445154)\n",
      "X^2 for MWPM and NN: 189.82167832167832\n",
      "X^2 for PLUT and NN: 204.2296072507553\n",
      "> AUC for class Z65: 0.9617796131345026 (+- 0.00013811026312843358)\n",
      "X^2 for MWPM and NN: 731.4373289545704\n",
      "X^2 for PLUT and NN: 105.76178010471205\n",
      "> AUC for class Z66: 0.9616068316649268 (+- 0.000671276853393005)\n",
      "X^2 for MWPM and NN: 115.50297619047619\n",
      "X^2 for PLUT and NN: 139.68848167539267\n",
      "###################################################################################\n",
      "TOTAL F1 NN: [0.7466026170895966, 0.7447298695420905, 0.7449999416902822, 0.7453024489335289]\n",
      "TOTAL F1 PLUT: [0.07436437291975014, 0.07319961646740877, 0.07392483964771193]\n",
      "TOTAL F1 MWPM: [0.5570304203885315, 0.5576631378129446, 0.5564358397963814]\n",
      "TOTAL ACC NN: [0.9817760586738586, 0.9817712903022766, 0.9817626262626084]\n",
      "TOTAL ACC PLUT: [0.960282959944449, 0.960287018882497, 0.9603099766722879]\n",
      "TOTAL ACC MWPM: [0.9570212121211669, 0.9571409090908701, 0.9570479797979491]\n",
      "TOTAL TIME NN: [13.6240815, 12.273374, 11.9565955]\n",
      "TOTAL TIME PLUT: [27.279464, 16.1340635, 5.2510398]\n",
      "TOTAL TIME MWPM: [4881.778657700013, 9217.311648799974, 3500.318302099989, 5232.7764430999805]\n",
      "------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#test MWPM decoder for this fold\n",
    "#labels = targets[train], features = inputs[train]\n",
    "# x_test_d7 = translate_to_graph(testData_d7_MWPM, targs, mlb_d7)\n",
    "\"\"\"\n",
    "decoding_d7, time_mwpm = do_new_decoding(x_test_d7, 7, 0)\n",
    "decoding_d7['combine'] = decoding_d7[[0, 1]].values.tolist()\n",
    "decoding_d7['combine'].apply(lambda x: x[0].extend(x[1]))\n",
    "decoding_d7 = np.array(decoding_d7[0])\n",
    "\n",
    "time_per_fold_mwpm.append(time_mwpm)\n",
    "\n",
    "pred_mwpm = mlb_d7.transform(decoding_d7)\n",
    "\n",
    "if fold_no < 3:\n",
    "    acc = partial_accuracy(targets_test_2, pred_mwpm)\n",
    "else:\n",
    "    acc, contingency_mwpm = partial_accuracy_and_contingency(targets_test_2, pred_mwpm, mlb_d7)\"\"\"\n",
    "\n",
    "#acc_per_fold_mwpm.append(acc)\n",
    "#f1_per_fold_mwpm.append(f1_score(targets_test_2, pred_mwpm, average='micro'))\n",
    "\n",
    "#####################################################################################################\n",
    "#test the plut decoder for this fold\n",
    "\n",
    "#lookup_d7 = lookup_decoder(7)\n",
    "\n",
    "#lookup_d7 = train_plut(lookup_d7, inputs_train, targets[train])\n",
    "\n",
    "#start = time.time_ns()\n",
    "#pred_plut_d7 = test_plut(lookup_d7, inputs_test)\n",
    "#end = time.time_ns() \n",
    "#time_per_fold_plut.append((end - start)/ (10 ** 9))\n",
    "\n",
    "if fold_no < 3:\n",
    "    acc = partial_accuracy(targets[test], pred_plut_d7)\n",
    "    f1 = f1_score(targets[test], pred_plut_d7, average='micro')\n",
    "else:\n",
    "    pred_plut_d7 = test_plut(lookup_d7, inputs_test_2)\n",
    "    #f1 = f1_score(targets_test_2, pred_plut_d7, average='micro')\n",
    "    acc, contingency_plut = partial_accuracy_and_contingency(targets_test_2, pred_plut_d7, mlb_d7)\n",
    "\n",
    "#acc_per_fold_plut.append(acc)\n",
    "#f1_per_fold_plut.append(f1)\n",
    "\n",
    "#####################################################################################################\n",
    "#Test the NN decoder for this fold\n",
    "\"\"\"\n",
    "model_d7 = compile_FFNN_model_DepthSeven(7)\n",
    "\n",
    "# Generate a print\n",
    "print('------------------------------------------------------------------------')\n",
    "print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "# Fit data to model\n",
    "history = model_d7.fit(\n",
    "    x=inputs_train ,\n",
    "    y=targets[train],\n",
    "    validation_split=.25,\n",
    "    epochs= 150)\"\"\"\n",
    "\n",
    "pred = model_d7.predict(inputs_test_2)\n",
    "pred[pred>=.5]=1 \n",
    "pred[pred<.5]=0\n",
    "acc, contingency_nn = partial_accuracy_and_contingency(targets_test_2, pred, mlb_d7)\n",
    "f1 = f1_score(targets_test_2, pred, average='micro')\n",
    "\n",
    "#acc_per_fold.append(acc)\n",
    "f1_per_fold.append(f1)\n",
    "\n",
    "# Increase fold number\n",
    "fold_no = fold_no + 1\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(targets[test].ravel(), predictions_d7.ravel())\n",
    "tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "tprs[-1][0] = 0.0\n",
    "roc_auc = auc(fpr, tpr)\n",
    "aucs.append(roc_auc)\n",
    "plt.plot(fpr, tpr, lw=1, alpha=0.3, label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
    "\n",
    "#get the AUCs of each class, used to get average AUC of each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(targets[test][:, i], predictions_d7[:, i]) \n",
    "    aucs_classes[mlb_d7.classes_[i]].append(auc(fpr[i], tpr[i]))\n",
    "\n",
    "#########################################################################################################\n",
    "#compute McNemar's statistic on results of last fold\n",
    "\n",
    "mcnemar_results_mwpm = {}\n",
    "mcnemar_results_plut = {}\n",
    "\n",
    "for class_ in mlb_d7.classes_:\n",
    "    \n",
    "    #compute the x^2 for NN and MWPM\n",
    "    mcnemar_results_mwpm[class_]=contingency_table_and_t(contingency_nn[class_], contingency_mwpm[class_])[1]    \n",
    "    #comput the x^2 for NN and PLUT\n",
    "    mcnemar_results_plut[class_] = contingency_table_and_t(contingency_nn[class_], contingency_plut[class_])[1]\n",
    "        \n",
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds of MWPM:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold_mwpm)} (+- {np.std(acc_per_fold_mwpm)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold_mwpm)}(+- {np.std(f1_per_fold_mwpm)})')\n",
    "print(f'> Time: {np.mean(time_per_fold_mwpm)} (+- {np.std(time_per_fold_mwpm)})')\n",
    "print('Average scores for all folds of PLUT:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold_plut)} (+- {np.std(acc_per_fold_plut)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold_plut)}(+- {np.std(f1_per_fold_plut)})')\n",
    "print(f'> Time: {np.mean(time_per_fold_plut)} (+- {np.std(time_per_fold_plut)})')\n",
    "print('Average scores for all folds of NN:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold)}(+- {np.std(f1_per_fold)})')\n",
    "print(f'> Time: {np.mean(time_per_fold)} (+- {np.std(time_per_fold)})')\n",
    "for key in aucs_classes:\n",
    "    cleanedList = [x for x in aucs_classes[key] if str(x) != 'nan']\n",
    "    print(f'> AUC for class {key}: {np.mean(cleanedList)} (+- {np.std(cleanedList)})') #this has to be a for loop\n",
    "    print(\"X^2 for MWPM and NN: \" + str(mcnemar_results_mwpm[key]))  \n",
    "    print(\"X^2 for PLUT and NN: \" + str(mcnemar_results_plut[key]))\n",
    "print(\"###################################################################################\")\n",
    "print(\"TOTAL F1 NN: \" + str(f1_per_fold))\n",
    "print(\"TOTAL F1 PLUT: \" + str(f1_per_fold_plut))\n",
    "print(\"TOTAL F1 MWPM: \" + str(f1_per_fold_mwpm))\n",
    "print(\"TOTAL ACC NN: \" + str(acc_per_fold))\n",
    "print(\"TOTAL ACC PLUT: \" + str(acc_per_fold_plut))\n",
    "print(\"TOTAL ACC MWPM: \" + str(acc_per_fold_mwpm))\n",
    "print(\"TOTAL TIME NN: \" + str(time_per_fold))\n",
    "print(\"TOTAL TIME PLUT: \" + str(time_per_fold_plut))\n",
    "print(\"TOTAL TIME MWPM: \" + str(time_per_fold_mwpm))\n",
    "print('------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "         label='Chance', alpha=.8)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "         label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "         lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                 label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "plt.xlim([-0.01, 1.01])\n",
    "plt.ylim([-0.01, 1.01])\n",
    "plt.xlabel('False Positive Rate',fontsize=18)\n",
    "plt.ylabel('True Positive Rate',fontsize=18)\n",
    "plt.title('Cross-Validation ROC of Depth 7 NN',fontsize=18)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "model_d5 = compile_FFNN_model_DepthFive(5)\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "print(\"Fit model on training data\")\n",
    "history = model_d5.fit(\n",
    "    x=x_train_d5.values,\n",
    "    y=Y_train_d5,\n",
    "    validation_split=.25,\n",
    "    epochs = 500\n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Make a quick epoch vs. loss plot\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.grid()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Depth 5 Model Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (MSE)')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Make a quick epoch vs. accuracy plot\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.grid()\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Depth 5 Model Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# predict\n",
    "from sklearn.metrics import precision_score,accuracy_score, f1_score, recall_score, hamming_loss\n",
    "\n",
    "start = time.time()\n",
    "predictions_d5 = model_d5.predict(x_test_d5.values)\n",
    "end = time.time()\n",
    "print(\"Time: \" + str(end - start))\n",
    "\n",
    "pred=predictions_d5.copy()\n",
    "\n",
    "thresholds=[0.1, 0.2, 0.3, .4, .5, .6, .7, .8, .9]\n",
    "for val in thresholds:\n",
    "    pred=predictions_d5.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "    \n",
    "  \n",
    "    precision = precision_score(Y_test_d5, pred, average='micro')\n",
    "    recall = recall_score(Y_test_d5, pred, average='micro')\n",
    "    f1 = f1_score(Y_test_d5, pred, average='micro')\n",
    "   \n",
    "    print(\"Micro-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "    print(\"Accuracy = \",accuracy_score(Y_test_d5, pred))\n",
    "    print(\"Accuracy = \",partial_accuracy(Y_test_d5, pred))\n",
    "    print(\"\\n\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "pred=predictions_d5.copy()\n",
    "from sklearn.metrics import multilabel_confusion_matrix, classification_report\n",
    "#look at confusion matrix to see what got misclassified    \n",
    "pred[pred>=.5]=1\n",
    "pred[pred<.5]=0\n",
    "multilabel_confusion_matrix(Y_test_d5, pred)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#look at classifcation report to see what got mislabeled\n",
    "print(classification_report(Y_test_d5, pred))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit model on training data\n",
      "Train on 304908 samples, validate on 101636 samples\n",
      "Epoch 1/150\n",
      "304908/304908 [==============================] - 61s 200us/step - loss: 0.1685 - accuracy: 0.9585 - val_loss: 0.1573 - val_accuracy: 0.9599\n",
      "Epoch 2/150\n",
      "304908/304908 [==============================] - 62s 202us/step - loss: 0.1335 - accuracy: 0.9607 - val_loss: 0.1117 - val_accuracy: 0.9631\n",
      "Epoch 3/150\n",
      "304908/304908 [==============================] - 64s 210us/step - loss: 0.0985 - accuracy: 0.9665 - val_loss: 0.0879 - val_accuracy: 0.9699\n",
      "Epoch 4/150\n",
      "304908/304908 [==============================] - 68s 224us/step - loss: 0.0809 - accuracy: 0.9721 - val_loss: 0.0749 - val_accuracy: 0.9742\n",
      "Epoch 5/150\n",
      "304908/304908 [==============================] - 73s 238us/step - loss: 0.0704 - accuracy: 0.9757 - val_loss: 0.0666 - val_accuracy: 0.9771\n",
      "Epoch 6/150\n",
      "304908/304908 [==============================] - 58s 190us/step - loss: 0.0630 - accuracy: 0.9783 - val_loss: 0.0602 - val_accuracy: 0.9793\n",
      "Epoch 7/150\n",
      "304908/304908 [==============================] - 57s 189us/step - loss: 0.0575 - accuracy: 0.9803 - val_loss: 0.0552 - val_accuracy: 0.9810\n",
      "Epoch 8/150\n",
      "304908/304908 [==============================] - 60s 197us/step - loss: 0.0532 - accuracy: 0.9817 - val_loss: 0.0519 - val_accuracy: 0.9821\n",
      "Epoch 9/150\n",
      "304908/304908 [==============================] - 60s 198us/step - loss: 0.0499 - accuracy: 0.9828 - val_loss: 0.0490 - val_accuracy: 0.9831\n",
      "Epoch 10/150\n",
      "304908/304908 [==============================] - 61s 199us/step - loss: 0.0473 - accuracy: 0.9837 - val_loss: 0.0463 - val_accuracy: 0.9840\n",
      "Epoch 11/150\n",
      "304908/304908 [==============================] - 59s 195us/step - loss: 0.0452 - accuracy: 0.9843 - val_loss: 0.0449 - val_accuracy: 0.9844\n",
      "Epoch 12/150\n",
      "304908/304908 [==============================] - 58s 190us/step - loss: 0.0434 - accuracy: 0.9849 - val_loss: 0.0430 - val_accuracy: 0.9850\n",
      "Epoch 13/150\n",
      "304908/304908 [==============================] - 63s 207us/step - loss: 0.0419 - accuracy: 0.9853 - val_loss: 0.0414 - val_accuracy: 0.9855\n",
      "Epoch 14/150\n",
      "304908/304908 [==============================] - 56s 184us/step - loss: 0.0406 - accuracy: 0.9857 - val_loss: 0.0403 - val_accuracy: 0.9858\n",
      "Epoch 15/150\n",
      "304908/304908 [==============================] - 66s 215us/step - loss: 0.0395 - accuracy: 0.9860 - val_loss: 0.0392 - val_accuracy: 0.9861\n",
      "Epoch 16/150\n",
      "304908/304908 [==============================] - 57s 186us/step - loss: 0.0385 - accuracy: 0.9863 - val_loss: 0.0384 - val_accuracy: 0.9863\n",
      "Epoch 17/150\n",
      "304908/304908 [==============================] - 60s 196us/step - loss: 0.0376 - accuracy: 0.9865 - val_loss: 0.0374 - val_accuracy: 0.9866\n",
      "Epoch 18/150\n",
      "304908/304908 [==============================] - 62s 205us/step - loss: 0.0367 - accuracy: 0.9867 - val_loss: 0.0367 - val_accuracy: 0.9867\n",
      "Epoch 19/150\n",
      "304908/304908 [==============================] - 60s 196us/step - loss: 0.0360 - accuracy: 0.9869 - val_loss: 0.0362 - val_accuracy: 0.9868\n",
      "Epoch 20/150\n",
      "304908/304908 [==============================] - 62s 205us/step - loss: 0.0353 - accuracy: 0.9871 - val_loss: 0.0358 - val_accuracy: 0.9870\n",
      "Epoch 21/150\n",
      "304908/304908 [==============================] - 59s 194us/step - loss: 0.0346 - accuracy: 0.9873 - val_loss: 0.0348 - val_accuracy: 0.9872\n",
      "Epoch 22/150\n",
      "304908/304908 [==============================] - 56s 185us/step - loss: 0.0340 - accuracy: 0.9874 - val_loss: 0.0342 - val_accuracy: 0.9873\n",
      "Epoch 23/150\n",
      "304908/304908 [==============================] - 56s 184us/step - loss: 0.0334 - accuracy: 0.9876 - val_loss: 0.0334 - val_accuracy: 0.9876\n",
      "Epoch 24/150\n",
      "304908/304908 [==============================] - 60s 197us/step - loss: 0.0329 - accuracy: 0.9877 - val_loss: 0.0330 - val_accuracy: 0.9877\n",
      "Epoch 25/150\n",
      "304908/304908 [==============================] - 60s 197us/step - loss: 0.0323 - accuracy: 0.9878 - val_loss: 0.0325 - val_accuracy: 0.9878\n",
      "Epoch 26/150\n",
      "304908/304908 [==============================] - 57s 186us/step - loss: 0.0318 - accuracy: 0.9880 - val_loss: 0.0320 - val_accuracy: 0.9879\n",
      "Epoch 27/150\n",
      "304908/304908 [==============================] - 62s 203us/step - loss: 0.0314 - accuracy: 0.9881 - val_loss: 0.0313 - val_accuracy: 0.9881\n",
      "Epoch 28/150\n",
      "304908/304908 [==============================] - 58s 191us/step - loss: 0.0309 - accuracy: 0.9882 - val_loss: 0.0310 - val_accuracy: 0.9881\n",
      "Epoch 29/150\n",
      "304908/304908 [==============================] - 55s 181us/step - loss: 0.0305 - accuracy: 0.9883 - val_loss: 0.0307 - val_accuracy: 0.9882\n",
      "Epoch 30/150\n",
      "304908/304908 [==============================] - 55s 181us/step - loss: 0.0300 - accuracy: 0.9884 - val_loss: 0.0305 - val_accuracy: 0.9882\n",
      "Epoch 31/150\n",
      "304908/304908 [==============================] - 56s 184us/step - loss: 0.0296 - accuracy: 0.9885 - val_loss: 0.0302 - val_accuracy: 0.9883\n",
      "Epoch 32/150\n",
      "304908/304908 [==============================] - 56s 185us/step - loss: 0.0292 - accuracy: 0.9886 - val_loss: 0.0297 - val_accuracy: 0.9885\n",
      "Epoch 33/150\n",
      "304908/304908 [==============================] - 69s 227us/step - loss: 0.0289 - accuracy: 0.9887 - val_loss: 0.0298 - val_accuracy: 0.9884\n",
      "Epoch 34/150\n",
      "304908/304908 [==============================] - 67s 218us/step - loss: 0.0285 - accuracy: 0.9888 - val_loss: 0.0292 - val_accuracy: 0.9886\n",
      "Epoch 35/150\n",
      "304908/304908 [==============================] - 61s 200us/step - loss: 0.0282 - accuracy: 0.9889 - val_loss: 0.0286 - val_accuracy: 0.9887\n",
      "Epoch 36/150\n",
      "304908/304908 [==============================] - 58s 190us/step - loss: 0.0279 - accuracy: 0.9890 - val_loss: 0.0286 - val_accuracy: 0.9887\n",
      "Epoch 37/150\n",
      "304908/304908 [==============================] - 58s 190us/step - loss: 0.0276 - accuracy: 0.9891 - val_loss: 0.0281 - val_accuracy: 0.9888\n",
      "Epoch 38/150\n",
      "304908/304908 [==============================] - 61s 199us/step - loss: 0.0273 - accuracy: 0.9892 - val_loss: 0.0278 - val_accuracy: 0.9889\n",
      "Epoch 39/150\n",
      "304908/304908 [==============================] - 58s 191us/step - loss: 0.0270 - accuracy: 0.9892 - val_loss: 0.0274 - val_accuracy: 0.9891\n",
      "Epoch 40/150\n",
      "304908/304908 [==============================] - 63s 207us/step - loss: 0.0267 - accuracy: 0.9893 - val_loss: 0.0273 - val_accuracy: 0.9890\n",
      "Epoch 41/150\n",
      "304908/304908 [==============================] - 58s 190us/step - loss: 0.0264 - accuracy: 0.9894 - val_loss: 0.0271 - val_accuracy: 0.9891\n",
      "Epoch 42/150\n",
      "304908/304908 [==============================] - 58s 189us/step - loss: 0.0262 - accuracy: 0.9895 - val_loss: 0.0267 - val_accuracy: 0.9893\n",
      "Epoch 43/150\n",
      "304908/304908 [==============================] - 58s 191us/step - loss: 0.0259 - accuracy: 0.9895 - val_loss: 0.0265 - val_accuracy: 0.9892\n",
      "Epoch 44/150\n",
      "304908/304908 [==============================] - 58s 190us/step - loss: 0.0257 - accuracy: 0.9896 - val_loss: 0.0262 - val_accuracy: 0.9893\n",
      "Epoch 45/150\n",
      "304908/304908 [==============================] - 60s 196us/step - loss: 0.0255 - accuracy: 0.9897 - val_loss: 0.0263 - val_accuracy: 0.9893\n",
      "Epoch 46/150\n",
      "304908/304908 [==============================] - 58s 190us/step - loss: 0.0252 - accuracy: 0.9897 - val_loss: 0.0259 - val_accuracy: 0.9894\n",
      "Epoch 47/150\n",
      "304908/304908 [==============================] - 58s 189us/step - loss: 0.0250 - accuracy: 0.9898 - val_loss: 0.0257 - val_accuracy: 0.9895\n",
      "Epoch 48/150\n",
      "304908/304908 [==============================] - 61s 199us/step - loss: 0.0248 - accuracy: 0.9898 - val_loss: 0.0257 - val_accuracy: 0.9894\n",
      "Epoch 49/150\n",
      "304908/304908 [==============================] - 58s 190us/step - loss: 0.0246 - accuracy: 0.9899 - val_loss: 0.0253 - val_accuracy: 0.9895\n",
      "Epoch 50/150\n",
      "304908/304908 [==============================] - 58s 190us/step - loss: 0.0244 - accuracy: 0.9899 - val_loss: 0.0252 - val_accuracy: 0.9896\n",
      "Epoch 51/150\n",
      "304908/304908 [==============================] - 58s 189us/step - loss: 0.0242 - accuracy: 0.9900 - val_loss: 0.0251 - val_accuracy: 0.9896\n",
      "Epoch 52/150\n",
      "304908/304908 [==============================] - 58s 191us/step - loss: 0.0241 - accuracy: 0.9900 - val_loss: 0.0250 - val_accuracy: 0.9896\n",
      "Epoch 53/150\n",
      "304908/304908 [==============================] - 58s 189us/step - loss: 0.0239 - accuracy: 0.9901 - val_loss: 0.0245 - val_accuracy: 0.9898\n",
      "Epoch 54/150\n",
      "304908/304908 [==============================] - 58s 189us/step - loss: 0.0237 - accuracy: 0.9901 - val_loss: 0.0244 - val_accuracy: 0.9898\n",
      "Epoch 55/150\n",
      "304908/304908 [==============================] - 58s 191us/step - loss: 0.0235 - accuracy: 0.9902 - val_loss: 0.0241 - val_accuracy: 0.9898\n",
      "Epoch 56/150\n",
      "304908/304908 [==============================] - 59s 193us/step - loss: 0.0234 - accuracy: 0.9902 - val_loss: 0.0241 - val_accuracy: 0.9898\n",
      "Epoch 57/150\n",
      "304908/304908 [==============================] - 58s 190us/step - loss: 0.0232 - accuracy: 0.9902 - val_loss: 0.0240 - val_accuracy: 0.9899\n",
      "Epoch 58/150\n",
      "304908/304908 [==============================] - 59s 193us/step - loss: 0.0231 - accuracy: 0.9903 - val_loss: 0.0240 - val_accuracy: 0.9899\n",
      "Epoch 59/150\n",
      "304908/304908 [==============================] - 58s 191us/step - loss: 0.0229 - accuracy: 0.9903 - val_loss: 0.0238 - val_accuracy: 0.9899\n",
      "Epoch 60/150\n",
      "304908/304908 [==============================] - 58s 189us/step - loss: 0.0228 - accuracy: 0.9904 - val_loss: 0.0236 - val_accuracy: 0.9899\n",
      "Epoch 61/150\n",
      "304908/304908 [==============================] - 59s 192us/step - loss: 0.0226 - accuracy: 0.9904 - val_loss: 0.0238 - val_accuracy: 0.9899\n",
      "Epoch 62/150\n",
      "304908/304908 [==============================] - 58s 190us/step - loss: 0.0225 - accuracy: 0.9904 - val_loss: 0.0235 - val_accuracy: 0.9900\n",
      "Epoch 63/150\n",
      "304908/304908 [==============================] - 58s 189us/step - loss: 0.0224 - accuracy: 0.9905 - val_loss: 0.0245 - val_accuracy: 0.9896\n",
      "Epoch 64/150\n",
      "304908/304908 [==============================] - 57s 188us/step - loss: 0.0222 - accuracy: 0.9905 - val_loss: 0.0232 - val_accuracy: 0.9901\n",
      "Epoch 65/150\n",
      "304908/304908 [==============================] - 58s 189us/step - loss: 0.0221 - accuracy: 0.9905 - val_loss: 0.0230 - val_accuracy: 0.9901\n",
      "Epoch 66/150\n",
      "304908/304908 [==============================] - 59s 193us/step - loss: 0.0220 - accuracy: 0.9906 - val_loss: 0.0229 - val_accuracy: 0.9902\n",
      "Epoch 67/150\n",
      "304908/304908 [==============================] - 57s 188us/step - loss: 0.0219 - accuracy: 0.9906 - val_loss: 0.0228 - val_accuracy: 0.9902\n",
      "Epoch 68/150\n",
      "304908/304908 [==============================] - 58s 189us/step - loss: 0.0217 - accuracy: 0.9906 - val_loss: 0.0228 - val_accuracy: 0.9902\n",
      "Epoch 69/150\n",
      "304908/304908 [==============================] - 60s 196us/step - loss: 0.0216 - accuracy: 0.9907 - val_loss: 0.0227 - val_accuracy: 0.9903\n",
      "Epoch 70/150\n",
      "304908/304908 [==============================] - 57s 187us/step - loss: 0.0215 - accuracy: 0.9907 - val_loss: 0.0225 - val_accuracy: 0.9903\n",
      "Epoch 71/150\n",
      "304908/304908 [==============================] - 58s 191us/step - loss: 0.0214 - accuracy: 0.9907 - val_loss: 0.0224 - val_accuracy: 0.9903\n",
      "Epoch 72/150\n",
      "304908/304908 [==============================] - 57s 188us/step - loss: 0.0213 - accuracy: 0.9908 - val_loss: 0.0223 - val_accuracy: 0.9904\n",
      "Epoch 73/150\n",
      "304908/304908 [==============================] - 58s 189us/step - loss: 0.0212 - accuracy: 0.9908 - val_loss: 0.0224 - val_accuracy: 0.9903\n",
      "Epoch 74/150\n",
      "304908/304908 [==============================] - 58s 189us/step - loss: 0.0211 - accuracy: 0.9908 - val_loss: 0.0221 - val_accuracy: 0.9904\n",
      "Epoch 75/150\n",
      "304908/304908 [==============================] - 57s 188us/step - loss: 0.0210 - accuracy: 0.9908 - val_loss: 0.0220 - val_accuracy: 0.9904\n",
      "Epoch 76/150\n",
      "304908/304908 [==============================] - 59s 194us/step - loss: 0.0209 - accuracy: 0.9909 - val_loss: 0.0219 - val_accuracy: 0.9904\n",
      "Epoch 77/150\n",
      "304908/304908 [==============================] - 58s 189us/step - loss: 0.0208 - accuracy: 0.9909 - val_loss: 0.0216 - val_accuracy: 0.9905\n",
      "Epoch 78/150\n",
      "304908/304908 [==============================] - 58s 189us/step - loss: 0.0207 - accuracy: 0.9909 - val_loss: 0.0221 - val_accuracy: 0.9903\n",
      "Epoch 79/150\n",
      "304908/304908 [==============================] - 61s 199us/step - loss: 0.0206 - accuracy: 0.9909 - val_loss: 0.0218 - val_accuracy: 0.9904\n",
      "Epoch 80/150\n",
      "304908/304908 [==============================] - 57s 188us/step - loss: 0.0205 - accuracy: 0.9910 - val_loss: 0.0215 - val_accuracy: 0.9905\n",
      "Epoch 81/150\n",
      "304908/304908 [==============================] - 58s 191us/step - loss: 0.0204 - accuracy: 0.9910 - val_loss: 0.0216 - val_accuracy: 0.9905\n",
      "Epoch 82/150\n",
      "304908/304908 [==============================] - 58s 190us/step - loss: 0.0203 - accuracy: 0.9910 - val_loss: 0.0214 - val_accuracy: 0.9905\n",
      "Epoch 83/150\n",
      "304908/304908 [==============================] - 57s 187us/step - loss: 0.0202 - accuracy: 0.9910 - val_loss: 0.0218 - val_accuracy: 0.9903\n",
      "Epoch 84/150\n",
      "304908/304908 [==============================] - 58s 189us/step - loss: 0.0201 - accuracy: 0.9911 - val_loss: 0.0211 - val_accuracy: 0.9906\n",
      "Epoch 85/150\n",
      "304908/304908 [==============================] - 57s 188us/step - loss: 0.0201 - accuracy: 0.9911 - val_loss: 0.0212 - val_accuracy: 0.9906\n",
      "Epoch 86/150\n",
      "304908/304908 [==============================] - 58s 190us/step - loss: 0.0200 - accuracy: 0.9911 - val_loss: 0.0211 - val_accuracy: 0.9906\n",
      "Epoch 87/150\n",
      "304908/304908 [==============================] - 60s 195us/step - loss: 0.0199 - accuracy: 0.9911 - val_loss: 0.0210 - val_accuracy: 0.9907\n",
      "Epoch 88/150\n",
      "304908/304908 [==============================] - 58s 189us/step - loss: 0.0198 - accuracy: 0.9912 - val_loss: 0.0208 - val_accuracy: 0.9906\n",
      "Epoch 89/150\n",
      "304908/304908 [==============================] - 59s 193us/step - loss: 0.0197 - accuracy: 0.9912 - val_loss: 0.0210 - val_accuracy: 0.9907\n",
      "Epoch 90/150\n",
      "304908/304908 [==============================] - 58s 192us/step - loss: 0.0197 - accuracy: 0.9912 - val_loss: 0.0207 - val_accuracy: 0.9907\n",
      "Epoch 91/150\n",
      "304908/304908 [==============================] - 57s 187us/step - loss: 0.0196 - accuracy: 0.9912 - val_loss: 0.0207 - val_accuracy: 0.9907\n",
      "Epoch 92/150\n",
      "304908/304908 [==============================] - 59s 195us/step - loss: 0.0195 - accuracy: 0.9912 - val_loss: 0.0208 - val_accuracy: 0.9907\n",
      "Epoch 93/150\n",
      "304908/304908 [==============================] - 58s 189us/step - loss: 0.0194 - accuracy: 0.9913 - val_loss: 0.0207 - val_accuracy: 0.9906\n",
      "Epoch 94/150\n",
      "304908/304908 [==============================] - 58s 189us/step - loss: 0.0194 - accuracy: 0.9913 - val_loss: 0.0208 - val_accuracy: 0.9907\n",
      "Epoch 95/150\n",
      "304908/304908 [==============================] - 58s 189us/step - loss: 0.0193 - accuracy: 0.9913 - val_loss: 0.0206 - val_accuracy: 0.9908\n",
      "Epoch 96/150\n",
      "304908/304908 [==============================] - 57s 188us/step - loss: 0.0192 - accuracy: 0.9913 - val_loss: 0.0206 - val_accuracy: 0.9906\n",
      "Epoch 97/150\n",
      "304908/304908 [==============================] - 59s 193us/step - loss: 0.0191 - accuracy: 0.9913 - val_loss: 0.0204 - val_accuracy: 0.9908\n",
      "Epoch 98/150\n",
      "304908/304908 [==============================] - 58s 189us/step - loss: 0.0191 - accuracy: 0.9914 - val_loss: 0.0205 - val_accuracy: 0.9907\n",
      "Epoch 99/150\n",
      "304908/304908 [==============================] - 57s 188us/step - loss: 0.0190 - accuracy: 0.9914 - val_loss: 0.0204 - val_accuracy: 0.9908\n",
      "Epoch 100/150\n",
      "304908/304908 [==============================] - 60s 196us/step - loss: 0.0189 - accuracy: 0.9914 - val_loss: 0.0200 - val_accuracy: 0.9908\n",
      "Epoch 101/150\n",
      "304908/304908 [==============================] - 58s 189us/step - loss: 0.0189 - accuracy: 0.9914 - val_loss: 0.0200 - val_accuracy: 0.9908\n",
      "Epoch 102/150\n",
      "304908/304908 [==============================] - 59s 193us/step - loss: 0.0188 - accuracy: 0.9914 - val_loss: 0.0200 - val_accuracy: 0.9908\n",
      "Epoch 103/150\n",
      "304908/304908 [==============================] - 57s 188us/step - loss: 0.0187 - accuracy: 0.9914 - val_loss: 0.0202 - val_accuracy: 0.9908\n",
      "Epoch 104/150\n",
      "304908/304908 [==============================] - 57s 188us/step - loss: 0.0187 - accuracy: 0.9915 - val_loss: 0.0199 - val_accuracy: 0.9908\n",
      "Epoch 105/150\n",
      "304908/304908 [==============================] - 58s 190us/step - loss: 0.0186 - accuracy: 0.9915 - val_loss: 0.0200 - val_accuracy: 0.9909\n",
      "Epoch 106/150\n",
      "304908/304908 [==============================] - 58s 189us/step - loss: 0.0186 - accuracy: 0.9915 - val_loss: 0.0200 - val_accuracy: 0.9908\n",
      "Epoch 107/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304908/304908 [==============================] - 59s 194us/step - loss: 0.0185 - accuracy: 0.9915 - val_loss: 0.0202 - val_accuracy: 0.9908\n",
      "Epoch 108/150\n",
      "304908/304908 [==============================] - 58s 189us/step - loss: 0.0184 - accuracy: 0.9915 - val_loss: 0.0204 - val_accuracy: 0.9908\n",
      "Epoch 109/150\n",
      "304908/304908 [==============================] - 58s 189us/step - loss: 0.0184 - accuracy: 0.9915 - val_loss: 0.0197 - val_accuracy: 0.9909\n",
      "Epoch 110/150\n",
      "304908/304908 [==============================] - 60s 198us/step - loss: 0.0183 - accuracy: 0.9916 - val_loss: 0.0196 - val_accuracy: 0.9909\n",
      "Epoch 111/150\n",
      "304908/304908 [==============================] - 58s 190us/step - loss: 0.0183 - accuracy: 0.9915 - val_loss: 0.0197 - val_accuracy: 0.9909\n",
      "Epoch 112/150\n",
      "304908/304908 [==============================] - 58s 189us/step - loss: 0.0182 - accuracy: 0.9916 - val_loss: 0.0197 - val_accuracy: 0.9909\n",
      "Epoch 113/150\n",
      "304908/304908 [==============================] - 57s 188us/step - loss: 0.0182 - accuracy: 0.9916 - val_loss: 0.0194 - val_accuracy: 0.9910\n",
      "Epoch 114/150\n",
      "304908/304908 [==============================] - 57s 189us/step - loss: 0.0181 - accuracy: 0.9916 - val_loss: 0.0195 - val_accuracy: 0.9909\n",
      "Epoch 115/150\n",
      "304908/304908 [==============================] - 58s 189us/step - loss: 0.0181 - accuracy: 0.9916 - val_loss: 0.0195 - val_accuracy: 0.9909\n",
      "Epoch 116/150\n",
      "304908/304908 [==============================] - 58s 190us/step - loss: 0.0180 - accuracy: 0.9916 - val_loss: 0.0193 - val_accuracy: 0.9910\n",
      "Epoch 117/150\n",
      "304908/304908 [==============================] - 58s 189us/step - loss: 0.0179 - accuracy: 0.9917 - val_loss: 0.0196 - val_accuracy: 0.9909\n",
      "Epoch 118/150\n",
      "304908/304908 [==============================] - 59s 195us/step - loss: 0.0179 - accuracy: 0.9917 - val_loss: 0.0195 - val_accuracy: 0.9910\n",
      "Epoch 119/150\n",
      "304908/304908 [==============================] - 58s 190us/step - loss: 0.0179 - accuracy: 0.9917 - val_loss: 0.0193 - val_accuracy: 0.9910\n",
      "Epoch 120/150\n",
      "304908/304908 [==============================] - 59s 193us/step - loss: 0.0178 - accuracy: 0.9917 - val_loss: 0.0197 - val_accuracy: 0.9908\n",
      "Epoch 121/150\n",
      "304908/304908 [==============================] - 58s 190us/step - loss: 0.0178 - accuracy: 0.9917 - val_loss: 0.0192 - val_accuracy: 0.9910\n",
      "Epoch 122/150\n",
      "304908/304908 [==============================] - 57s 188us/step - loss: 0.0177 - accuracy: 0.9917 - val_loss: 0.0192 - val_accuracy: 0.9910\n",
      "Epoch 123/150\n",
      "304908/304908 [==============================] - 58s 192us/step - loss: 0.0177 - accuracy: 0.9917 - val_loss: 0.0192 - val_accuracy: 0.9910\n",
      "Epoch 124/150\n",
      "304908/304908 [==============================] - 57s 188us/step - loss: 0.0176 - accuracy: 0.9917 - val_loss: 0.0191 - val_accuracy: 0.9911\n",
      "Epoch 125/150\n",
      "304908/304908 [==============================] - 58s 190us/step - loss: 0.0176 - accuracy: 0.9918 - val_loss: 0.0191 - val_accuracy: 0.9910\n",
      "Epoch 126/150\n",
      "304908/304908 [==============================] - 57s 188us/step - loss: 0.0175 - accuracy: 0.9918 - val_loss: 0.0189 - val_accuracy: 0.9911\n",
      "Epoch 127/150\n",
      "304908/304908 [==============================] - 58s 189us/step - loss: 0.0175 - accuracy: 0.9918 - val_loss: 0.0190 - val_accuracy: 0.9910\n",
      "Epoch 128/150\n",
      "304908/304908 [==============================] - 59s 193us/step - loss: 0.0174 - accuracy: 0.9918 - val_loss: 0.0189 - val_accuracy: 0.9911\n",
      "Epoch 129/150\n",
      "304908/304908 [==============================] - 57s 188us/step - loss: 0.0174 - accuracy: 0.9918 - val_loss: 0.0189 - val_accuracy: 0.9911\n",
      "Epoch 130/150\n",
      "304908/304908 [==============================] - 58s 189us/step - loss: 0.0174 - accuracy: 0.9918 - val_loss: 0.0188 - val_accuracy: 0.9911\n",
      "Epoch 131/150\n",
      "304908/304908 [==============================] - 60s 196us/step - loss: 0.0173 - accuracy: 0.9918 - val_loss: 0.0192 - val_accuracy: 0.9909\n",
      "Epoch 132/150\n",
      "304908/304908 [==============================] - 57s 189us/step - loss: 0.0173 - accuracy: 0.9918 - val_loss: 0.0188 - val_accuracy: 0.9910\n",
      "Epoch 133/150\n",
      "304908/304908 [==============================] - 59s 193us/step - loss: 0.0172 - accuracy: 0.9918 - val_loss: 0.0186 - val_accuracy: 0.9911\n",
      "Epoch 134/150\n",
      "304908/304908 [==============================] - 57s 187us/step - loss: 0.0172 - accuracy: 0.9919 - val_loss: 0.0188 - val_accuracy: 0.9911\n",
      "Epoch 135/150\n",
      "304908/304908 [==============================] - 59s 193us/step - loss: 0.0171 - accuracy: 0.9919 - val_loss: 0.0187 - val_accuracy: 0.9911\n",
      "Epoch 136/150\n",
      "304908/304908 [==============================] - 58s 190us/step - loss: 0.0171 - accuracy: 0.9919 - val_loss: 0.0187 - val_accuracy: 0.9911\n",
      "Epoch 137/150\n",
      "304908/304908 [==============================] - 57s 188us/step - loss: 0.0171 - accuracy: 0.9919 - val_loss: 0.0187 - val_accuracy: 0.9911\n",
      "Epoch 138/150\n",
      "304908/304908 [==============================] - 59s 193us/step - loss: 0.0170 - accuracy: 0.9919 - val_loss: 0.0185 - val_accuracy: 0.9911\n",
      "Epoch 139/150\n",
      "304908/304908 [==============================] - 57s 188us/step - loss: 0.0170 - accuracy: 0.9919 - val_loss: 0.0185 - val_accuracy: 0.9911\n",
      "Epoch 140/150\n",
      "304908/304908 [==============================] - 58s 190us/step - loss: 0.0169 - accuracy: 0.9919 - val_loss: 0.0185 - val_accuracy: 0.9911\n",
      "Epoch 141/150\n",
      "304908/304908 [==============================] - 60s 197us/step - loss: 0.0169 - accuracy: 0.9919 - val_loss: 0.0186 - val_accuracy: 0.9911\n",
      "Epoch 142/150\n",
      "304908/304908 [==============================] - 57s 188us/step - loss: 0.0169 - accuracy: 0.9919 - val_loss: 0.0184 - val_accuracy: 0.9912\n",
      "Epoch 143/150\n",
      "304908/304908 [==============================] - 58s 192us/step - loss: 0.0168 - accuracy: 0.9920 - val_loss: 0.0183 - val_accuracy: 0.9912\n",
      "Epoch 144/150\n",
      "304908/304908 [==============================] - 58s 189us/step - loss: 0.0168 - accuracy: 0.9920 - val_loss: 0.0189 - val_accuracy: 0.9910\n",
      "Epoch 145/150\n",
      "304908/304908 [==============================] - 57s 188us/step - loss: 0.0168 - accuracy: 0.9920 - val_loss: 0.0184 - val_accuracy: 0.9911\n",
      "Epoch 146/150\n",
      "304908/304908 [==============================] - 58s 190us/step - loss: 0.0167 - accuracy: 0.9920 - val_loss: 0.0185 - val_accuracy: 0.9912\n",
      "Epoch 147/150\n",
      "304908/304908 [==============================] - 57s 188us/step - loss: 0.0167 - accuracy: 0.9920 - val_loss: 0.0185 - val_accuracy: 0.9912\n",
      "Epoch 148/150\n",
      "304908/304908 [==============================] - 58s 190us/step - loss: 0.0166 - accuracy: 0.9920 - val_loss: 0.0183 - val_accuracy: 0.9912\n",
      "Epoch 149/150\n",
      "304908/304908 [==============================] - 59s 194us/step - loss: 0.0166 - accuracy: 0.9920 - val_loss: 0.0183 - val_accuracy: 0.9912\n",
      "Epoch 150/150\n",
      "304908/304908 [==============================] - 58s 190us/step - loss: 0.0166 - accuracy: 0.9920 - val_loss: 0.0184 - val_accuracy: 0.9912\n"
     ]
    }
   ],
   "source": [
    "model_d7 = compile_FFNN_model_DepthSeven(7)\n",
    "print(\"Fit model on training data\")\n",
    "history = model_d7.fit(\n",
    "    x=x_train_d7.values,\n",
    "    y=Y_train_d7,\n",
    "    validation_split=.25,\n",
    "    epochs= 150\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss (MSE)')"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmcAAAJcCAYAAAC8DwN/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nOzdeZhcdZ3v8fe3tu50ZyUJDWRjJ4ZFwQgoLnEHRXHBBVzQOyNyR8R9m3Guy51xmVFRvCiD4oIbKm6oKCrSArIjiLJJDFtIIIQQks7S6+/+UdWxuxPSXdV1UtXJ+/U8eZ6qOudUffvn4/iZ7+/3OydSSkiSJKk55BpdgCRJkv7BcCZJktREDGeSJElNxHAmSZLURAxnkiRJTcRwJkmS1EQMZ5ImlIi4JyKe1+g6xioi9o6IFBGFMZz7poi4ckfUJal5Gc4k1awSlDZFxPqIWBsRV0XEaRFRl//bEhHfiIj/GMf1/xoRXUP+bYqIgYiY9Tjn3xMRPSOPR8TNlYC1d621jFc1IU/SxGY4kzReL0kpTQEWAJ8CPgCc19iSylJKn0gpTR78B3wa6Ewprd7OZXcDJw2+iYhDgUkZlypJWxjOJNVFSumxlNJFwGuAUyLiEICIaImIz0TEfRHxUEScExGTKseWRMTySodrdaVz9brKsVOB1wHvr3S9fj7k554UEbdExGMR8f2IaB2tvogI4A3AN0c59VvAG4e8PwU4f8R3TYuI8yPi4Yi4NyI+PNgtjIh85e9dHRHLgBdv49rzImJlRDwQEf8REfnR6h/lb9srIi6KiDURsTQi3jLk2JERcUNErKuM/+cqn7dGxLcj4pFK1/P6iOgYTx2S6sNwJqmuUkrXAcuBZ1Q++jRwIPAkYH9gDvB/hlyyBzCr8vkpwLkRcVBK6VzgO8B/VTpfLxlyzauBY4F9gMOAN42htGcAHcCPRjnvGmBqRDyhEppeA3x7xDlfBKYB+wLPohzm3lw59hbgeOBwYDFw4ohrvwn0UR6Lw4EXAP88hvq353uUx3yvyu99IiKeWzn2BeALKaWpwH7ADyqfn1L5G+YBM4HTgE3jrENSHRjOJGVhBbBbpVv1FuBdKaU1KaX1wCeA1444/99TSt0ppT8Av6QcvrbnrJTSipTSGuDnlIPfaE4BLkwpdY3h3MHu2fOBO4AHBg8MCWwfSimtTyndA3yWcleOSu2fTyndX6nvk0Ou7QCOA96ZUtqQUloFnMnW4zFmETEPeDrwgZTS5pTSzcBXh9TTC+wfEbNSSl0ppWuGfD4T2D+l1J9SujGltK7WOiTVjwtLJWVhDrAGmA20ATeWcxoAAQydxns0pbRhyPt7KXeAtufBIa83jnZ+ZRr1VcAJo1Ze9i3gcsqdufNHHJsFlCp1DrqX8t9MpZb7RxwbtAAoAiuHjEduxPnV2gsYDL5Df3Nx5fU/AR8H7oiIu4GPpZR+QflvnAdcEBHTKXcH/y2l1DuOWiTVgZ0zSXUVEU+hHFSuBFZTnio7OKU0vfJvWmVx/qAZEdE+5P18yp03gFSnsl5BOSx2juXklNK9lDcGvAj48YjDqyl3nRYM+Ww+/+iuraQceoYeG3Q/0A3MGjIeU1NKB4/x79iWwS7llG3Vk1K6K6V0ErA75SnmCyOiPaXUm1L6WEppEfA0ylOxb0RSwxnOJNVFREyNiOOBC4Bvp5T+klIaAL4CnBkRu1fOmxMRLxxx+cciohQRz6AcEn5Y+fwhyuu6xusU4PyUUjVh75+A54zo6pFS6qe8bus/I2JKRCwA3s0/1qX9ADgjIuZGxAzgg0OuXQn8BvhsZbxyEbFfRDyrirpaKov5WysbIR4ArgI+WfnssErt3wGIiNdHxOzKfxZrK9/RHxHPjohDK9O06ygHzv4q6pCUEcOZpPH6eUSsp9wV+jfgc/xjcTyUb62xFLgmItYBvwMOGnL8QeBRyh2g7wCnpZTuqBw7D1hU2U3401qKi4g5wHPYenpyu1JKf08p3fA4h98ObACWUe4Qfhf4WuXYV4BLgD8Df2LrztsbKU+L3kb5774Q2LOK0roodyMH/z2H8q0/9qY8hj8BPpJS+m3l/GOBWyOii/LmgNemlDZT3ohxIeVgdjvwB7be+CCpAaK6/0dSkuonIpZQ7rLNbXQtktQs7JxJkiQ1EcOZJElSE3FaU5IkqYnYOZMkSWoiO9VNaGfNmpX23nvvTH9jw4YNtLe3j37iLsLxGM7xGM7xGM7xGM7xGM7x2NrOPiY33njj6pTS7JGf71ThbO+99+aGGx5v53t9dHZ2smTJkkx/YyJxPIZzPIZzPIZzPIZzPIZzPLa2s49JRNy7rc+d1pQkSWoihjNJkqQmYjiTJElqIoYzSZKkJmI4kyRJaiKGM0mSpCZiOJMkSWoihjNJkqQmYjiTJElqIoYzSZKkJmI4kyRJaiKGM0mSpCZiOJMkSWoihjNJkqQmYjiTJElqIoYzSZKkJmI4kyRJaiKGM0mSpCZiOJMkSWoihjNJkqQmYjiTJElqIoYzSZKkJmI4q8Kp59/At27rbnQZkiRpJ1ZodAETycrHNpPrSY0uQ5Ik7cTsnFWhpZCjd8BwJkmSsmM4q0JLMUffQKOrkCRJOzPDWRVK+Ry9/Y2uQpIk7cwMZ1VoKeSd1pQkSZkynFWhpZij12lNSZKUIcNZFUp5w5kkScqW4awK5c6Z05qSJCk7hrMqtBTy7taUJEmZMpxVoVRwt6YkScqW4awKLYUcfQkGnNqUJEkZMZxVoaWQB6Cn37lNSZKUDcNZFUqF8nB1u/BMkiRlxHBWhZYt4cyFZ5IkKRuGsyoMhrMeO2eSJCkjhrMqOK0pSZKyZjirwuCGgG4fEyBJkjJiOKtCS7EyreluTUmSlBHDWRVa8pVpTe9EK0mSMmI4q8Jg58w1Z5IkKSuGsyqU8pWb0BrOJElSRgxnVbBzJkmSsmY4q8KW+5z1u+ZMkiRlw3BWhS33OfNWGpIkKSOGsypsuc+Z05qSJCkjmYaziDg2Iu6MiKUR8cFtHF8YEVdHRHdEvHfEsekRcWFE3BERt0fEU7OsdSx8fJMkScpaIasvjog8cDbwfGA5cH1EXJRSum3IaWuAM4CXbeMrvgD8OqV0YkSUgLasah2rkg8+lyRJGcuyc3YksDSltCyl1ANcAJww9ISU0qqU0vVA79DPI2Iq8EzgvMp5PSmltRnWOiaFXBA4rSlJkrKTWecMmAPcP+T9cuCoMV67L/Aw8PWIeCJwI/COlNKGkSdGxKnAqQAdHR10dnaOp+ZRFXOJpXffS2fnykx/Z6Lo6urKfMwnEsdjOMdjOMdjOMdjOMdja7vqmGQZzmIbn6UxXlsAjgDenlK6NiK+AHwQ+PetvjClc4FzARYvXpyWLFlSW7VjVLz0l3TsOYclSw7O9Hcmis7OTrIe84nE8RjO8RjO8RjO8RjO8djarjomWU5rLgfmDXk/F1hRxbXLU0rXVt5fSDmsNVwxF645kyRJmckynF0PHBAR+1QW9L8WuGgsF6aUHgTuj4iDKh89F7htO5fsMMWca84kSVJ2MpvWTCn1RcTpwCVAHvhaSunWiDitcvyciNgDuAGYCgxExDuBRSmldcDbge9Ugt0y4M1Z1VqNguFMkiRlKMs1Z6SULgYuHvHZOUNeP0h5unNb194MLM6yvloUc+ETAiRJUmZ8QkCVijno6TecSZKkbBjOqvHnC3hGup7uXjcESJKkbBjOqnH1/+PY/j+45kySJGXGcFaNfIkSfT5bU5IkZcZwVo18iWL0eZ8zSZKUGcNZNfJFivQ5rSlJkjJjOKtGrhzOnNaUJElZMZxVI1+ycyZJkjJlOKtGvkiBftecSZKkzBjOqpEvUUjlac2UUqOrkSRJOyHDWTXyJQr0MZCgb8BwJkmS6s9wVo18kQJ9gA8/lyRJ2TCcVSNfIp/K4cwdm5IkKQuGs2pU1pwBbgqQJEmZMJxVI18gj50zSZKUHcNZNbZMaybXnEmSpEwYzqqRLxEk8gzQ3Ws4kyRJ9Wc4q0a+CECBfnr6XXMmSZLqz3BWjXwJgBJ9ds4kSVImDGfVqIQzn68pSZKyYjirRmVa03AmSZKyYjirxmDnLPq8z5kkScqE4awauXLnrGTnTJIkZcRwVo0h05rehFaSJGXBcFYNNwRIkqSMGc6qsSWc9bvmTJIkZcJwVg2nNSVJUsYMZ9WodM7a8gNOa0qSpEwYzqqxJZz1+4QASZKUCcNZNSrTmpPyAz5bU5IkZcJwVo3BcJazcyZJkrJhOKtGZVpzkmvOJElSRgxn1RjSOXO3piRJyoLhrBqVzllLbsD7nEmSpEwYzqpRCWetOZ8QIEmSsmE4q0ZlWrPVaU1JkpQRw1k1Bqc1o9/OmSRJyoThrBq5f3TOXHMmSZKyYDirRi5HIkcpnNaUJEnZMJxVaSBXoCXcECBJkrJhOKtSiiIlXHMmSZKyYTir0kCuQNFpTUmSlBHDWZVSFCjR54YASZKUCcNZlQZyBYr00dufGBhIjS5HkiTtZAxnVUpRoEAfAD39Tm1KkqT6MpxVabBzBtDdaziTJEn1ZTirUooh4azfdWeSJKm+DGdVGsgVyKdewM6ZJEmqP8NZlYauOfNeZ5Ikqd4MZ1Uqd84qGwIMZ5Ikqc4MZ1VK8Y9w5r3OJElSvRnOqjRszZmdM0mSVGeGsyqlKJAbKIczpzUlSVK9Gc6qNJArkh+wcyZJkrJhOKtSigKxJZy55kySJNWX4axKKfJOa0qSpMxkGs4i4tiIuDMilkbEB7dxfGFEXB0R3RHx3m0cz0fETRHxiyzrrMZAbmjnzHAmSZLqK7NwFhF54GzgOGARcFJELBpx2hrgDOAzj/M17wBuz6rGWqQo/iOc9TqtKUmS6ivLztmRwNKU0rKUUg9wAXDC0BNSSqtSStcDvSMvjoi5wIuBr2ZYY9UGcgXor0xr9ts5kyRJ9VXI8LvnAPcPeb8cOKqK6z8PvB+Ysr2TIuJU4FSAjo4OOjs7q6uySnv1DlQ6Z4k77vo7nQP3j3rNzqyrqyvzMZ9IHI/hHI/hHI/hHI/hHI+t7apjkmU4i218lsZ0YcTxwKqU0o0RsWR756aUzgXOBVi8eHFasmS7p4/bsnt/CEBrboC95i5gyZKDMv29ZtfZ2UnWYz6ROB7DOR7DOR7DOR7DOR5b21XHJMtpzeXAvCHv5wIrxnjtMcBLI+IeytOhz4mIb9e3vNqkKOfZ9vyA05qSJKnusgxn1wMHRMQ+EVECXgtcNJYLU0ofSinNTSntXbnu9yml12dX6tgN5CrhrNDvhgBJklR3mU1rppT6IuJ04BIgD3wtpXRrRJxWOX5OROwB3ABMBQYi4p3AopTSuqzqGq8tnbPCgLfSkCRJdZflmjNSShcDF4/47Jwhrx+kPN25ve/oBDozKK8mg52ztvyAN6GVJEl15xMCqmTnTJIkZclwVqV/dM6Sz9aUJEl1Zzir0mDnbFKu386ZJEmqO8NZlQZyRQAm5Q1nkiSp/gxnVRrsnLkhQJIkZcFwVqXBNWetTmtKkqQMGM6qlCIPDK45c0OAJEmqL8NZlQanNVtzTmtKkqT6M5xVaXBDQIvTmpIkKQOGsypt6ZyFz9aUJEn1Zzir0uCGgJZcPz39ds4kSVJ9Gc6qNNg5a4k+uvsGSCk1uCJJkrQzMZxVaUvnLPpJCXr7DWeSJKl+DGdVGuyclaK83sypTUmSVE+GsyoNds5K0QfgpgBJklRXhrMqbemcUQln3k5DkiTVkeGsSoNPCChSmdY0nEmSpDoynFUrAvIlimHnTJIk1Z/hrBb5EoU0GM5ccyZJkurHcFaLfJFiZc2Z05qSJKmeDGe1yJcouCFAkiRlwHBWC6c1JUlSRgxntcgVyKdewGlNSZJUX4azWuRLW8KZ05qSJKmeDGe1yJfID05r9hrOJElS/RjOapEvkhsMZz5bU5Ik1ZHhrBb5EvmByrSmz9aUJEl1ZDirRb5IbsA1Z5Ikqf4MZ7XIl4gBd2tKkqT6M5zVIl8i+nsoFXJ2ziRJUl0ZzmqRL0B/Ly2FnJ0zSZJUV4azWuRL0N9DSyHnEwIkSVJdGc5qkS9VOmd5pzUlSVJdGc5qkS/CgNOakiSp/gxntahMa5ac1pQkSXVmOKvFlmlNd2tKkqT6MpzVIl+sbAjIO60pSZLqynBWi2HTmoYzSZJUP4azWuSKkAZozSfXnEmSpLoynNUiXwSgvTDgtKYkSaorw1kt8iUAJuX6ndaUJEl1ZTirxWA4yye6ew1nkiSpfgxntahMa7YV+unpN5xJkqT6MZzVotI5a40BunvdECBJkurHcFaLLdOarjmTJEn1ZTirRWVaszX66RtI9A+kBhckSZJ2FoazWgyGs3x5StPbaUiSpHoxnNViy5qzcjjzRrSSJKleDGe1qHTOWnJ9gJ0zSZJUP4azWlQ6Zy1RDmVuCpAkSfViOKvFlnDmtKYkSaovw1ktKtOaxShPa9o5kyRJ9WI4q8VWnTPDmSRJqg/DWS0q4axEpXPm8zUlSVKdGM5qkSsA/5jW9PmakiSpXgxntah0zopbOmduCJAkSfWRaTiLiGMj4s6IWBoRH9zG8YURcXVEdEfEe4d8Pi8iLouI2yPi1oh4R5Z1Vm3EtKadM0mSVC+FrL44IvLA2cDzgeXA9RFxUUrptiGnrQHOAF424vI+4D0ppT9FxBTgxoj47YhrG6eyW7OQKhsCXHMmSZLqJMvO2ZHA0pTSspRSD3ABcMLQE1JKq1JK1wO9Iz5fmVL6U+X1euB2YE6GtVan0jkreCsNSZJUZ5l1ziiHqfuHvF8OHFXtl0TE3sDhwLWPc/xU4FSAjo4OOjs7q/2JqnR1ddF55R9ZAqy4dxlwMLfecSedm5Zl+rvNqqurK/Mxn0gcj+Ecj+Ecj+Ecj+Ecj63tqmOSZTiLbXyWqvqCiMnAj4B3ppTWbeuclNK5wLkAixcvTkuWLKmyzOp0dnayZMkSuDzPvvP2hLtg/t77suRZ+2X6u81qy3gIcDxGcjyGczyGczyGczy2tquOSZbTmsuBeUPezwVWjPXiiChSDmbfSSn9uM61jV++RD6VZ2Od1pQkSfWSZTi7HjggIvaJiBLwWuCisVwYEQGcB9yeUvpchjXWLl8iN9BHIRf0GM4kSVKdZDatmVLqi4jTgUuAPPC1lNKtEXFa5fg5EbEHcAMwFRiIiHcCi4DDgDcAf4mImytf+a8ppYuzqrdq+QL091Aq5HzwuSRJqpss15xRCVMXj/jsnCGvH6Q83TnSlWx7zVrzyJegv4eWQs5pTUmSVDc+IaBW+SL099JSyDutKUmS6sZwVqt8CQZ6K9OahjNJklQfhrNaDZvWdM2ZJEmqD8NZrQanNYs5pzUlSVLdGM5qVemclfJOa0qSpPoxnNUqX9qyIcAHn0uSpHoxnNUqN+Q+Z/2GM0mSVB+Gs1oN3RDQ64YASZJUH4azWg1Oaxa9z5kkSaofw1mtKrs13RAgSZLqyXBWq8FpzaLhTJIk1Y/hrFZbdmt6E1pJklQ/hrNa5Ytbdmu65kySJNWL4axWW3Zr5unuGyCl1OiKJEnSTsBwVqvBxzcVykPY473OJElSHRjOalWZ1twSzpzalCRJdWA4q1W+BAO9tOQDwB2bkiSpLgxntcoXAWjNl9ea2TmTJEn1YDirVb4EQGuufBsNO2eSJKkeDGe12iqcea8zSZI0foazWlWmNSfl+gCnNSVJUn0YzmpVbAOgJfUATmtKkqT6MJzVqhLOJsVmALp7DWeSJGn8DGe1KrUD0DqwCYCeftecSZKk8TOc1aoSzkrJzpkkSaofw1mtBtecDVTCmWvOJElSHRjOajXYOauEM3drSpKkejCc1arSOSv0bwS8z5kkSaoPw1mtKp2zYn95Q4DTmpIkqR4MZ7WqhLOC4UySJNWR4axW+SLkiuT7DGeSJKl+DGfjUWojejfQUsi55kySJNWF4Ww8SpOhZyOlQs7dmpIkqS4MZ+NRbIPeDbQU8k5rSpKkujCcjUepDXo2lqc1fUKAJEmqA8PZeBTboae85qyn33AmSZLGz3A2HqV26N1AqZCju9cNAZIkafwMZ+MxOK1ZdM2ZJEmqD8PZeBTboXcjLXl3a0qSpPownI1Hqa285qzofc4kSVJ9GM7Go9i2ZUOA05qSJKkeDGfjUZoM/d205JPTmpIkqS4MZ+NRagNgSq7XzpkkSaqLQqMLmNCKg+Gsm56+1OBiJEnSzsDO2XiU2gFoj243BEiSpLownI1HJZxNznU7rSlJkurCcDYelWnNNnrcECBJkurCcDYelc5ZW3TTN5Do8/makiRpnAxn47Glc7YJwIefS5KkcTOcjUelc9aaugGc2pQkSeNmOBuPwXBGOZy5KUCSJI2X4Ww8KtOarWkzAN29hjNJkjQ+hrPxqHTOWtLgmjPvdSZJksbHcDYeuTwUWikNlMPZZjtnkiRpnAxn41VsozRQmdZ0zZkkSRonw9l4ldop9pfDmbs1JUnSeGUaziLi2Ii4MyKWRsQHt3F8YURcHRHdEfHeaq5tGsU2ipVpTZ+vKUmSxiuzcBYReeBs4DhgEXBSRCwacdoa4AzgMzVc2xxK7RT6NwJOa0qSpPGrKpxFRHslOI3FkcDSlNKylFIPcAFwwtATUkqrUkrXA73VXts0Su0U+iq7NQ1nkiRpnArbOxgROeC1wOuApwDdQEtEPAxcDJybUrrrcS6fA9w/5P1y4Kgx1jXmayPiVOBUgI6ODjo7O8f4E7Xp6uoa9huHrttEbvMaAP7811uZ8ujfMv39ZjNyPHZ1jsdwjsdwjsdwjsdwjsfWdtUx2W44Ay4Dfgd8CPhrSmkAICJ2A54NfCoifpJS+vY2ro1tfJbGWNeYr00pnQucC7B48eK0ZMmSMf5EbTo7Oxn2Gw9/g74VawHYZ/8DWXLUgkx/v9lsNR67OMdjOMdjOMdjOMdjOMdja7vqmIwWzp6XUho55UhKaQ3wI+BHEVF8nGuXA/OGvJ8LrBhjXeO5dscqtZPr3QA4rSlJksZvtDVnzxh8ERH7DD0QEa8A2FZ4q7geOCAi9omIEuXp0YvGWNd4rt2xiu1EnxsCJElSfYwWzobuovzRiGMf3t6FKaU+4HTgEuB24AcppVsj4rSIOA0gIvaIiOXAu4EPR8TyiJj6eNeO+a/akUpt0FMOZ5t7vZWGJEkan9GmNeNxXm/r/VZSShdT3jgw9LNzhrx+kPKU5ZiubUrFdmKgl6nFxIbuvkZXI0mSJrjROmfpcV5v6/2uqfLw89ktfazfbDiTJEnjM1rnbN+IuIhyl2zwNZX3+zz+ZbuQUhsAswxnkiSpDkYLZ0Nv/PqZEcdGvt81Fcuds1kt/azb/Hh7IyRJksZmu+EspfSHoe8rt804BHggpbQqy8ImjErnbLdSH8vtnEmSpHHa7pqziDgnIg6uvJ4G/Bk4H7gpIk7aAfU1v2I5nM0o9LLezpkkSRqnUe9zNuQWFm8G/pZSOhR4MvD+TCubKEqTAZhR6HHNmSRJGrfRwlnPkNfPB34KW26BIdgyrTnNzpkkSaqD0cLZ2og4PiIOB44Bfg0QEQVgUtbFTQiVac2puR429w74CCdJkjQuo+3WfCtwFrAH8M4hHbPnAr/MsrAJo3Kfs8m5cpNx/eZeZk5uaWRFkiRpAhttt+bfgGO38fkllB+tpEo4a4/NAKzf3Gc4kyRJNdtuOIuIs7Z3PKV0Rn3LmYAK5dndtugGcFOAJEkal9GmNU8D/gr8AFjBGJ6nucvJ5aDYxiQGw5mbAiRJUu1GC2d7Aq8CXgP0Ad8HfpRSejTrwiaUYhutqTytuc7OmSRJGoft7tZMKT2SUjonpfRs4E3AdODWiHjDjihuwii10zKwCcBHOEmSpHEZrXMGQEQcAZxE+V5nvwJuzLKoCafUTmngHxsCJEmSajXahoCPAccDtwMXAB9KKZk+Riq2Uegvd85ccyZJksZjtM7ZvwPLgCdW/n0iIqC8MSCllA7LtrwJotRG9G6kvZS3cyZJksZltHC2zw6pYqIrTYaN9zOltWjnTJIkjcto4ey+lFLa3gkREaOds9MrtkHvBqZOKtg5kyRJ4zLaszUvi4i3R8T8oR9GRCkinhMR3wROya68CaLUBj0bmdJadLemJEkal9E6Z8cC/wv4XkTsA6wFWoE88BvgzJTSzdmWOAEU26F3I1NaC6zZ0NPoaiRJ0gQ22rM1NwNfAr4UEUVgFrAppbR2RxQ3YZTaoWcDU1oK3PvIxkZXI0mSJrAx3ecMIKXUC6zMsJaJq9QGqZ8ZLQNuCJAkSeMy2pozjUWxHYDdiv0+vkmSJI2L4aweSm0AzCj20NM3wObe/gYXJEmSJqoxhbOIaI+IXOX1gRHx0soaNEF5zRkwvVCe0vR2GpIkqVZj7ZxdDrRGxBzgUuDNwDeyKmrCqUxrTsuXd2q67kySJNVqrOEsUkobgVcAX0wpvRxYlF1ZE0xlWnPKlnBm50ySJNVmzOEsIp4KvA74ZeWzMe/03OlVOmdTcoYzSZI0PmMNZ+8EPgT8JKV0a0TsC1yWXVkTTKVz1h7dgNOakiSpdmPqfqWU/gD8AaCyMWB1SumMLAubUCobAtpiM4CPcJIkSTUb627N70bE1IhoB24D7oyI92Vb2gRSmdZsTYOdM6c1JUlSbcY6rbkopbQOeBlwMTAfeENmVU00lWnNloHNROCNaCVJUs3GGs6KlfuavQz4WeVRTim7siaYQivkiuR61jG5peCaM0mSVLOxhrP/Ae4B2oHLI2IBsC6roiacCGibCRtWM7W16LSmJEmq2Vg3BJwFnDXko3sj4tnZlDRBtc+CjWuY0lpg3SY7Z5IkqTZj3RAwLSI+FxE3VP59lnIXTYPadoONq5nSWrBzJkmSajbWac2vAeuBV1f+rQO+nlVRE1LbLNiwmimtRdZ32zmTJEm1Getd/vdLKb1yyPuPRcTNWRQ0YbXPgo2PMGX3An9/2M6ZJEmqzVg7Z5si4umDbyLiGGBTNiVNUG0zYfNapreE05qSJKlmY+2cnQacHxHTKu8fBU7JpqQJqm0mALvnN7BuUy8pJSKiwUVJkqSJZkyds5TSn1NKTwQOAw5LKR0OPCfTytl7miwAACAASURBVCaa9lkAzMp10TeQ2Nw70OCCJEnSRDTWaU0AUkrrKk8KAHh3BvVMXJXO2W5RHh5vRCtJkmpRVTgbwTm7odrKnbPpPAb4CCdJklSb8YQzH980VGVac2paD9g5kyRJtdnuhoCIWM+2Q1gAkzKpaKKaNAOAKX1rAdyxKUmSarLdcJZSmrKjCpnw8kVonU5b36MArLNzJkmSajCeaU2N1D6Llp5yOLNzJkmSamE4q6e2mRS7B8OZnTNJklQ9w1k9tc0iv+kRcmHnTJIk1cZwVk/tM4mNj5Qffm44kyRJNTCc1VPbzPLDz1vyrNvktKYkSaqe4aye2mbBQB97tPR4E1pJklQTw1k9VW5Eu1dpgxsCJElSTQxn9VR5hNMehQ2uOZMkSTUxnNVT224AdOS7WN9t50ySJFUv03AWEcdGxJ0RsTQiPriN4xERZ1WO3xIRRww59q6IuDUi/hoR34uI1ixrrYvKtObs/HrWdPU0uBhJkjQRZRbOIiIPnA0cBywCToqIRSNOOw44oPLvVODLlWvnAGcAi1NKhwB54LVZ1Vo3lWnN3fMb2NDTT1e3U5uSJKk6WXbOjgSWppSWpZR6gAuAE0accwJwfiq7BpgeEXtWjhWASRFRANqAFRnWWh+lNihMYmasA2DVus0NLkiSJE00233w+TjNAe4f8n45cNQYzpmTUrohIj4D3AdsAn6TUvrNtn4kIk6l3HWjo6ODzs7O+lT/OLq6urb7G0fnJ9P/yD0A/OaKa1m4Wz7TehpttPHY1TgewzkewzkewzkewzkeW9tVxyTLcBbb+CyN5ZyImEG5q7YPsBb4YUS8PqX07a1OTulc4FyAxYsXpyVLloyr6NF0dnay3d+4cw7zijl4EPbcdyFLnjQn03oabdTx2MU4HsM5HsM5HsM5HsM5HlvbVccky2nN5cC8Ie/nsvXU5OOd8zzg7pTSwymlXuDHwNMyrLV+2mbS2lt++Pmqdd0NLkaSJE00WYaz64EDImKfiChRXtB/0YhzLgLeWNm1eTTwWEppJeXpzKMjoi0iAngucHuGtdZP+yxymx5hUjHPqvWuOZMkSdXJbFozpdQXEacDl1Debfm1lNKtEXFa5fg5wMXAi4ClwEbgzZVj10bEhcCfgD7gJipTl02vbRaxcQ27T23hITtnkiSpSlmuOSOldDHlADb0s3OGvE7A2x7n2o8AH8myvky07QY9XcydkbNzJkmSquYTAuqtciPavds2ueZMkiRVzXBWb5Ub0S5o3cRD3udMkiRVyXBWb20zAdir5FMCJElS9Qxn9VaZ1uzIdwE+JUCSJFXHcFZvlc7ZrFgPwKr1rjuTJEljZzirt9bpEHmmUX6+puvOJElSNQxn9ZbLQdtuTO5fC/iUAEmSVB3DWRbaZlHcvIbWovc6kyRJ1TGcZaFtJrFxDR1TW31KgCRJqorhLAvtM2HjajqmtNo5kyRJVTGcZaF9d1j/ELOntrjmTJIkVcVwloXp86H7MeZP6nW3piRJqorhLAvT5wOwb2mNTwmQJElVMZxloRLO5sXDgE8JkCRJY2c4y8L0BQB09D8E+JQASZI0doazLLTtBsV2ZvSuBHxKgCRJGjvDWRYiYMYC2jetAHxKgCRJGjvDWVamz6ew7n6fEiBJkqpiOMvK9PnE2vvomNLiUwIkSdKYGc6yMn0+dK9jn8m9ds4kSdKYGc6yUrmdxkGta11zJkmSxsxwlpXK7TT2LTzibk1JkjRmhrOsDLkRrU8JkCRJY2U4y8qkGVCaQsdA5Ua0ds8kSdIYGM6yEgHT5zOj50HApwRIkqSxMZxlacYCJm8u34h2+aObGlyMJEmaCAxnWZo+n+L6+8nn4L5HNjS6GkmSNAEYzrI0fT7R08UTpvVx9yMbG12NJEmaAAxnWars2Dx86jrutXMmSZLGwHCWpUo4WzRpLXev3kBKqcEFSZKkZmc4y9LgjWiLj7B+cx+PbuxtcEGSJKnZGc6yNGk6tExjr7QKgHuc2pQkSaMwnGVt+nx26y3f68x1Z5IkaTSGs6xNn8+kjQ+QC7hntTs2JUnS9hnOsjZ9Prm197PXtFanNSVJ0qgMZ1mbsQB6N3DojD7u8V5nkiRpFIazrFVup3Ho5MdccyZJkkZlOMta5XYaC4sPs3ZjL2s39jS4IEmS1MwMZ1mbuT9Ejr15AMCpTUmStF2Gs6wVW2HG3szefDfg7TQkSdL2Gc52hNkLaV/3d8LbaUiSpFEYznaE2QeRW/N35k0tejsNSZK0XYazHWH2Qhjo48hpaw1nkiRpuwxnO8LsgwB40qSHuNcNAZIkaTsMZzvCrAMBOCj3AGs29PDYpt4GFyRJkpqV4WxHKLXD9PnM7bsPcMemJEl6fIazHWX2QmZsXAZ4rzNJkvT4DGc7yuyDaFm7jDz93LvazpkkSdo2w9mOMnsh0d/NEVMe426nNSVJ0uMwnO0osxcC8NSpD7N0VVeDi5EkSc3KcLajVHZsHt76EHc8uJ6+/oEGFyRJkpqR4WxHaZ0KU+ewXzxAT98Af3/YqU1JkrQ1w9mONPsgdu++F4DbVj7W4GIkSVIzMpztSLMX0rJ2Ka0FuPWBdY2uRpIkNSHD2Y40+yCidyPPmL2Z21YaziRJ0tYMZztSZcfmMdMe4dYV60gpNbggSZLUbDINZxFxbETcGRFLI+KD2zgeEXFW5fgtEXHEkGPTI+LCiLgjIm6PiKdmWesOUdmxeWjLSh7b1MuKxzY3uCBJktRsMgtnEZEHzgaOAxYBJ0XEohGnHQccUPl3KvDlIce+APw6pbQQeCJwe1a17jBtu8HkDvYeKD9j89YH3BQgSZKGy7JzdiSwNKW0LKXUA1wAnDDinBOA81PZNcD0iNgzIqYCzwTOA0gp9aSU1mZY646z+xOYse5vROC6M0mStJVCht89B7h/yPvlwFFjOGcO0Ac8DHw9Ip4I3Ai8I6W01c3BIuJUyl03Ojo66OzsrFf929TV1TWu39inbxbzH7qcBZN6+cMty3hSYUX9imuA8Y7HzsbxGM7xGM7xGM7xGM7x2NquOiZZhrPYxmcjV8A/3jkF4Ajg7SmlayPiC8AHgX/f6uSUzgXOBVi8eHFasmTJeGoeVWdnJ+P6jY4uuO9CXjK3ix+vmjq+72oC4x6PnYzjMZzjMZzjMZzjMZzjsbVddUyynNZcDswb8n4uMLJN9HjnLAeWp5SurXx+IeWwNvHNKf8ZR5fu5oG1m1i7safBBUmSpGaSZTi7HjggIvaJiBLwWuCiEedcBLyxsmvzaOCxlNLKlNKDwP0RcVDlvOcCt2VY644zdS+Ysif79/0NcN2ZJEkaLrNpzZRSX0ScDlwC5IGvpZRujYjTKsfPAS4GXgQsBTYCbx7yFW8HvlMJdstGHJvY9jqCWav+CsBtK9bxtP1mNbggSZLULLJcc0ZK6WLKAWzoZ+cMeZ2Atz3OtTcDi7Osr2HmHEH+zl+y35Q+blth50ySJP2DTwhohMq6s+N2W8mthjNJkjSE4awR9jocgKNK97D04S429/Y3uCBJktQsDGeNMGkG7LYfB/bfRf9A4i8+KUCSJFUYzhplzpOZva68KeDaZY80uBhJktQsDGeNMucIcl0PcszuPVyzbE2jq5EkSU3CcNYoc54MwEtmreTGex+lt3+gwQVJkqRmYDhrlD0OhVyBxcV72NTbzy3LXXcmSZIMZ41TnAS7L2L+5tsBuMZ1Z5IkCcNZY805gtJDf+ag3du49m7XnUmSJMNZY81ZDJsf4yV7PsaN96xx3ZkkSTKcNdQBzweC5+duYENPP3/1fmeSJO3yDGeNNGUPmPsU9n2kE8CpTUmSZDhruIUvprjqLzx15gZvRitJkgxnDfeElwBw8rS/csM9j9LnujNJknZphrNGm7kfzF7I0T1Xs767j9tWrmt0RZIkqYEMZ81g4fHMeuQGprPe+51JkrSLM5w1g4UvJtIAr5txO5fevqrR1UiSpAYynDWDvQ6HqXN4+aQ/cf09a3ikq7vRFUmSpAYxnDWDCFj4YvZ97DpKqZtL77B7JknSrspw1iwWHk+ufzMvn3IHv7n1oUZXI0mSGsRw1iwWPA0m7cbJk2/kirseZmNPX6MrkiRJDWA4axb5Ihz8chatu5JC3wYu/9vDja5IkiQ1gOGsmRz2GvL9m3n5pJu4xKlNSZJ2SYazZjLvSJi+gDe0Xcultz9Er08LkCRpl2M4ayYRcNirOXDDjbRsfpjrfBC6JEm7HMNZszn01QQDvKJ0DZfc+mCjq5EkSTuY4azZzD4Q9nwSJ7eWw1n/QGp0RZIkaQcynDWjw17Dgp67mLx+GZ13ekNaSZJ2JYazZnTIK0mR4+RJ1/C96+5rdDWSJGkHMpw1oykdxL5LOLF4FZfd8SAr1m5qdEWSJGkHMZw1qyeezLTulRwdt/H96+9vdDWSJGkHMZw1qye8BFqnc/r0q/jBDffT5z3PJEnaJRjOmlWxFQ57NUdt/iObHnuYzjt9nJMkSbsCw1kzO+KN5AZ6eX3btXzXjQGSJO0SDGfNbI9DYc8ncUrr5XTe+RAPuDFAkqSdnuGs2R3xRmZvXMphubs574q7G12NJEnKmOGs2R16IhQm8YHdr+M7197LqvWbG12RJEnKkOGs2bVOg4NfxlFdv6fQv5Gv2j2TJGmnZjibCI44hVxvF5+dcznfuvpeHunqbnRFkiQpI4aziWD+0XDYa3jhI+fzxP6/8tUr7Z5JkrSzMpxNBBHw4s8SM/bhnLYvc9FVt/Dohp5GVyVJkjJgOJsoWqbAq77B1IF1fDx9ifOu+HujK5IkSRkwnE0kex5G7oX/yXPzN9F71dne90ySpJ2Q4WyiOfItbNrn+ZwRP+CLP7ui0dVIkqQ6M5xNNBFMOv7TtOb6edJdZ/PHpasbXZEkSaojw9lENHM/0pGn8urCH/jmT35Bb/9AoyuSJEl1YjiboApL3k9faRqnrDuXb/7RW2tIkrSzMJxNVJNmUHzOhzgmfys3Xfp9Vj7m5gBJknYGhrMJLJ7yT/RO35f38C0+8IMbGRhIjS5JkiSNk+FsIssXKb7oU+wbKzjknm/y9avuaXRFkiRpnAxnE92BLyQtehnvKv6EH/z699z54PpGVyRJksbBcLYTiOP+i3xLG58ofJV3XfAnuvv6G12SJEmqkeFsZzClg9wL/5MncxtPfPhnfOpXdzS6IkmSVCPD2c7i8NfDPs/kIy0X8Ms/3sQPb7i/0RVJkqQaGM52FhFw/OdpyQ3w3Sln8YmfXMeN9z7a6KokSVKVDGc7k5n7ESd+jf36/843Wj7DO751FQ8+trnRVUmSpCpkGs4i4tiIuDMilkbEB7dxPCLirMrxWyLiiBHH8xFxU0T8Iss6dyoLX0S84lwOS3fw6d5P8S/fvIoN3X2NrkqSJI1RZuEsIvLA2cBxwCLgpIhYNOK044ADKv9OBb484vg7gNuzqnGndcgriRPO5pi4hbc9/HHe9q1r3MEpSdIEkWXn7EhgaUppWUqpB7gAOGHEOScA56eya4DpEbEnQETMBV4MfDXDGndeTzoZXvw5npu/iRPv/Tjv/f6f6PcJApIkNb1IKZv/wY6IE4FjU0r/XHn/BuColNLpQ875BfCplNKVlfeXAh9IKd0QERcCnwSmAO9NKR3/OL9zKuWuGx0dHU++4IILMvl7BnV1dTF58uRMf6Oe5t7/M/b/+9f4Uf/T+WXH6bzh4FYiom7fP9HGI2uOx3COx3COx3COx3COx9Z29jF59rOffWNKafHIzwsZ/ua2EsDIJLjNcyLieGBVSunGiFiyvR9JKZ0LnAuwePHitGTJdk8ft87OTrL+jfpaApfvxSt//x90P1jiygX/wYePX1S3gDbxxiNbjsdwjsdwjsdwjsdwjsfWdtUxyXJaczkwb8j7ucCKMZ5zDPDSiLiH8nTocyLi29mVupN75vtIT38PJxd+z+HXvYv/++PrfEi6JElNKstwdj1wQETsExEl4LXARSPOuQh4Y2XX5tHAYymllSmlD6WU5qaU9q5c9/uU0uszrHWnF8/9d9LzPsaL8tdz0p/fzH9/9xeuQZMkqQllFs5SSn3A6cAllHdc/iCldGtEnBYRp1VOuxhYBiwFvgL8S1b17PIiiKe/k3jDT5hT2sC/3PUWzv3KF9nc6y5OSZKaSZZrzkgpXUw5gA397JwhrxPwtlG+oxPozKC8XVLst4S206/k4fNezakr/g9f+uJaXvfWD7Jbe6nRpUmSJHxCwK5p+jxmn/5b1ux+FG9f91nOP+vD3LN6Q6OrkiRJGM52XS2TmX3qz1g777m8s/t/+NnZ7+OqpQ83uipJknZ5hrNdWbGV6W/6Pl0HvJR3pO+w/vyT+M7vriere99JkqTRGc52dfkik0/6Bpuf/RGek7uZY694OV//yufZ1ONGAUmSGsFwJsjlaX3Wu8n/7yvomzKP/7Xio9zx6WfxwFUXQH9vo6uTJGmXYjjTFrmOJ9DxrstZdsSH2KN/JXN+81Y2/tcTSJd9AtaNvH+wJEnKguFMw+WL7PvSD1J89184c/b/5dqNe5H+8F+kMw+B778elnWCa9IkScqM4UzbNGtqG+/4329n2Qu/yfP7zuSb6cV0//0KOP8E+PkZMOCaNEmSsmA40+PK5YJ/evo+/M8Zr+Jnu5/GYes+z8XTT4I/nQ8/fBP0dTe6REmSdjqGM41q/90nc+FpT+Pdxx3GO1efwKfSKXD7RQx859Xk+zY1ujxJknYqmT6+STuPfC5467P244UH78FHfz6Td981if+++1wOWbEU9i7B/s9tdImSJO0U7JypKnvPaufrb3oKLzj5Xbyn+GEe29QL334FPee/Clbf1ejyJEma8AxnqlpEcOwhe/CJ972Dz+11Jp/qO5meZVfQf/bR9F/1JXdzSpI0DoYz1aytVOAVC9s58Yz/4r0dX+fSvieS/82HWHXea0ib1ja6PEmSJiTDmcZt/92n8OXTjmXg1d/my6U3sdv9v+XBzzyVpddebBdNkqQqGc5UFxHBsYfuxT9/4Ex+d9R55Po3s/+vTmL5p57M6iu+Cr3u6pQkaSwMZ6qrYj7HsS96BW3vuZnf7PevbNjcy6xL38OGTy+k67efBKc7JUnaLsOZMjFlyjRe8IYPMP3d13HuPl/gup69mfzHT9H930+g65cfhnUrG12iJElNyXCmTHVMm8Spp7yJfd9xMWfudx6/6z2Mtuv+H/2fO5hN330j3HeN69IkSRrCcKYdYsHMdt71hhM57F0/4bMLv8c3+19I752/ha+9kO6vvABWL210iZIkNQXDmXaoebu18b6TjuMF7z6PMw/9KR/tfzObHriVnrOfyvKL/9sHqkuSdnmGMzXE3BltfOTEo3jrez/J+Yd/nz+mQ5l73X9w5yefxl9/dib9j9zd6BIlSWoIn62phtpz2iTOeNkz6Dr2Ei7/+TkcdNvn6bjpo3DTR1k3aS6tBz2H0gHPhX2eCW27NbpcSZIyZzhTU5jcWuSZr3o7/f1v44prr+ZvV/+c+Wuv46k3/ZDSzeeTCNjrScS+z4b9ng3zjoJCS6PLliSp7gxnair5fI5nPO0YnvG0Y7hl+Vo+cc0y7rnlChb338ILVt3GopVnkbvyc1CYBHMXw4Knwfyjy2Gt1N7o8iVJGjfDmZrWYXOnc9iJR7D++EP52c0reP+193Hvyod4ZvFOTp65jCevv4NJl/83kQagdTo8873wlLdAsbXRpUuSVDPDmZrelNYirz96Aa87aj5/eeAxvnfdgbz15hVs7Onn8I48b9v/EZ615kcUf/NhuOYcOOYdkPrh0Xvg0XthzyfCMWfYWZMkTQiGM00YEVHups2dzr+9eBEX3byC7113H//8x35KhbfwtgUv4k0bv860X72vfEFpMkzdC/72K/jTN+F5H4VDXw05NylLkpqX4UwT0uSWAicfNZ+Tj5rPX5Y/xo/+tJzz/1zgzA0f4smTHuIpiw7k+U85mCMWzCDuvxZ+9QH4yVvhuq/AsZ+CeU9p9J8gSdI2Gc404R06dxqHzp3Gv734CVx512p+fNMDfP3PD3LOjVezYGYbL33iXrzoJT9j4UO/IC79OJz3PDjsNeVO2tS9Gl2+JEnDGM600yjmczx74e48e+HurN/cy6//+iA/vfkBzr5sKV/8/VL2nT2Xlx18ASf3XMjMW75K3HYRTJtbXp+WBmD6fHjq6XDACyCi0X+OJGkXZTjTTmlKa5FXLZ7HqxbP4+H13Vxy64P88paVfP7ylXwuHcPTZj6R90/+Nfu09zC1rZWIHNx3NXz31bD7weVNBYtOcOenJGmHM5xppzd7SguvP3oBrz96wbCg9oq7T2QgwR5TW3nuE3bn+S/8KE/b/AdKV58FPzkVLn4fLHpJeQp0wdPdSCBJ2iEMZ9qlDA1qq7u6ueyOVVx6+yp+ctMDfOfa+5hU3I1n7H8WJ+97N0eu/x1tt/4Ubvo2TNkLDj0RDns1dBzitKckKTOGM+2yZk1u2TL12d3XzzXL1vC72x7i0tsf4k23TwZexiGzX8Wb5tzBkp7LmHnNl4irzoLpC2DKnuVnfU7aDToWlZ9QsMdhjf6TJEk7AcOZBLQU8jzrwNk868DZfPyEg7nzofVc8bfVXH7Xw/zbXQfQ3bcfHfnX8tbZt/Cs4p109G+gfe19xAN/gpu/Xf6SfAuHTX0CdLwXDjwO8v7XS5JUPf/XQxohIli4x1QW7jGVtzxzXzb39nPd3Wu44q6H+cFde/Lx+58GwKzJJZ6+/yyeM2eAp5b+zqxHb6Ltph/A919fngY94o2w4Kkw+wkweXenQiVJY2I4k0bRWszzzANn88wDZwPw0LrNXHlXuat2xV2r+enNPUA7e0x9Hvu2PZ23Hryap6z+MW1/+NQ/vmTSjPJjpOYdXX5Q++6LYPNjsGEVbFwDex0O0+c15g+UJDUVw5lUpY6prbzyyXN55ZPnklJi6aourrl7Ddcue4Qr7nyQU66aBZzKAe1v5Pg9H+Wpkx/iwFjOtDW3EH/4NJC28a0B+z4LDn8DLHwxFCft4L9KktQsDGfSOEQEB3RM4YCOKbzh6AVcdtllLDjkKVxbCWsX3L2GM5fuCTyJqa0v46g5RY6ddj+HtD7MXnvOYcrMPaFlCtz1u/LatR/9E+T/f3t3Hh1ndeZ5/PvUvmqzrF3eDbbZjcOSkOAs0DRJIOeEpMkkaSbLySQTOsvpaSDpc2amz/SZJt09IWGyDR2YhIQJ3Q3pxDAJSwhmGfbFrDYgvMmWbW2WSrVvd/64b0lVUgkEWFJZ9XzOeU+9dd+3qt66QZWf773vvX7o3gy9Z9kWNYB0DDITsPxEWPtBndZDKaWWMA1nSh1DIsKa5RHWLI/wqbNWYIyhfzTFY3tGeHb/GDv6x/ir3a0UTSsAvS1FTusRTu/9BKdf+gVOzb+Ib/e90P84PPojKOZmfkjriXDuf4RTL9dJcpVSagnScKbUPBIRViwLsWJZiE9usWPKktk8Lx6MsaP/KM/1j/Ps/jHufP4QAB6XsKHzIk7ruZzNpwR5V3iQ7tZG3MFG8Iag7w/wyP+EO74Od30boh12So/QMjutR6hlaoqP0DK7H2mHZev0hgSllDpOaDhTaoGFfB7OWt3CWatbJssGJ9I81z/Ojv6j7OgfY9uOAW55PA9A0BtjQ+cYmzobOKnrHE66+AI2Zp7D99rvIDkMyRGIDcDhFyE1CrnkzA/tPB3O+yZs/Ci43Av1VZVSSr0NGs6UqgFt0QAXbApwwaZ2AIpFw+7hODv6x3l5IMZLA+Nse26AWx7fD4DbJaxd/mEb2FY1sqmrgU2dDTSHfZBL2TtAU6M2uA2/Bo/9GP71CmhZCyd9DBq67ES6TSvsWqILPYZtz0N2sfk15y/s5yql1HFAw5lSNcjlEta1RVnXFoUzbZkxhgNHU7w0UApsMR7fM8pvdgxMvq6rMcCmrkY2dkZZ19bC2uUrWHv6ewlu+TzsvAMeuR4evs4Go5JwG6y/EE64EJpXgycAHh/4orZb9Fh3hz77S9j2F+DywOfvgu4zj+37K6XUcU7DmVLHCRGhtyVEb0uIi07unCwfiWfYeWjChrZDNrT9cdcRimUzdnQ3BVnX1sXajh+w/pQgG6IZVvtjNCV2w2v3wK47plY6KOf223Ft0Q4wBvJpu0XaYdV5dut5F3j8c/sSj/0Y7roG1myFkd3wz38O/+EBCLe+o7pRSqmlRMOZUse5ZRE/5633c976qYCTyRfYN5KkbzBO32Cc14fs4xN7RknlCpPnNYdaWLv8i5yw7krO9e9lVTBJZ8RFi9/gykzAxIAdzxY/AuK2Nxl4fHB0L2y/FjC2BSzSAQ2dNrTlUvY1EwOcl8vA7jPsmLdiHp78J9h4CXz8pzC4E268EG77PHzm17rclVJKOfTXUKklyO9xc0J7lBPaoxXlxaJhYDzF60OJiuB2984R/k/CD/id17tYs3wF69rOZu3yMOvWR1i7PMLq1jABr3NDQeoo7HsEDj4NsUM2yI302Ql0l62F1e/l8MGD9JhhePpnkE/B6Z+Gj15vg1jX6fCR78Jvvwr3/Q186L/qzQpKKYWGM6Xqissl9DSH6GkOcb6zHFXJ0USW14emWtn6BuM81z/Gnc8PYJwuUpdAb0uI1a1hVraE6G3ZyMrOLaw8OcSKltBUcHP0bd9Oz9atUMhDYsh2j5aPYTvjM3DgSTsW7skb7aS73ZttuIu02zVJI+0QXj73rlOllDrOaThTSgHQHPaxJdzCllUtFeXpXIE9w1MtbX1DcfaNJHh631Em0vmKc9sb/KxoCbGiJczKZSESR/I07D/KypYQLdEOpNrNBRf/I6w8Dw48YVvhHv8JFLIzzws02bDm8jCVFj12Il5vEPwNNtytONcGPF0CSyl1nNJwppR6QwGvm42dDWzsbKgoN8YwlsyxbzTJvpEEHyko4gAAF9lJREFU+0eS7B9Nsm80yf/rG+b2Z9IA/K/nHwEg4vfQ0xx0thDdTWX7ay+l+ZTLbHgr5OwYt/gRiA86j0P2MTEIxYLT+iZ2HFsu5Yxz2wW77rQX5/LaqUICDTa0hVqgZbWdSqR5pV10fvwAjPXbFrnV77Ohzh9ZyKpVSqmqNJwppd4WEaE57KM57OP03qYZx9O5Arff/QDta05i32iS/tEkB44mOXA0xWO7R4lnKlvdQj43Pc1BJ7SF6Gluoru5k+7lQTrXBVke9eN2vcm0HslR6H8C+h+z4+AyE5CJ2bneXrsXCpnK831RW/bI9bYVrvN0O/dbqUu1aQW0rLHdrIHG6p/3zM02OK58j717NTizLpRS6q3QcKaUmhcBr5vuiIutzsS65YwxxFJ5DozZsHbgaIqDR1OT4e2Z/WOMpyrXFXW7hLaon47GAB0NAToaA3Q2BuhoDNrHhgBtDY34T7wITrxo5gUVCxA7CEf32QDV2Gsfs0kb5vY8CP1PwqHnbNjKxitfH+mwi9GvOAfaT4Kdd8KOW+yKDJ4APPYjEJcNeCf8CZx4MXScMjXGrpDDna+yesPQK/D0z+15519tW/uUUnVNw5lSasGJCI0hL42hRk7qqtIiBUykcxwcs6HtcCzN4fE0h8bt46tHJnjw1SES2cKM17VGfE6AC9LR6KezMUhHQynItdDR3UXIV/bT5wvB2g/YrVwmDmP7YfR1GHkdjrxkQ9zObfa42wenfBLO+Qq0ngAHn7IBr+8+O83I9r+zATDc6kxHMsh7MfDyGjvxbttG6Psj7HvYdsOagu2WvewmnZhXqTqn4UwpVZOiAS8bOrxs6Ji9JWkinasIbYdjpX3bCvfUvlHGkrkZr2sIeGxom2xx89PWEKA96jw2+GmNhPC2b4L2TZUvjh2yrWtdZ0C0rFVw5bvttvUaO1bu1bvh1bvspL0dp0C0i937+1njH4e9D8ML/wrNq+BDf2OnGBnpg9u/aOd+e99V9r3HD9gNsSGvdPdqYy809dpxdTr9iFJLjoYzpdRxKxrwEg14WT9tPrdyqWyBI6XQFktNBTkn1O08FGM4nqlYUQFsL+OysI+2qA1rbdEAyyI+WiN+WqOn0nrETWtygtaIn6agF1f5eLhIG2z+rN3K7N++nTVbt9oniREINk+taxpZDl952C5ttf2/OxfhsgEMsTdDTL+L1eWBhm47Nq5phb1DdeKw3dJjtgVu/YW2VTDUAsUi5BLOWLyJqTF5/ka73mqkzX5mYsjeLJEYtDdRLFurIVCpBTSv4UxELgK+D7iBnxpjrp12XJzjFwNJ4N8bY54RkV7gZqADKAI3GGO+P5/XqpRamoI+N6taw6xqDc96TqFoGIlnOBLLMDiR5kgsw5FYmsGJDIOxNEcm0rw0EGMkkaUwPcVhx8O1hJ3gFpl6XBbxV5QdTRfJFYp43S4IL6tysc3wyV/Y1RP8ERvM3F57zBgbpCaOwPh+2+U61g/j/Xb/9fvt+Ldo59SSW31/gOf/2QYuX9S+npnXP0nc9vPy6cpyX8S2/gVb7J2umXEb9Lo329bCFefY1zorQ5DP2pa9phUQ7dLVH5R6i+btL0ZE3MAPgQuAA8CTIrLNGPNy2Wl/Cqx3trOBHzuPeeAvnaAWBZ4WkXunvVYppY4Jt0toawjQ1hAAqo+BA7vCwngqx3A8w3A86zxmGCnbH4pn2TOcYDieIZ0rzniPb27/PY1Brw1vYT9NIS8tzl2vzSEvzSEfLeFlNIV8tOSzNIcMDQGnZS7QaLflJ8ztixULMLAD+u6F1Bj4o1NbaZoRX9gGrpizVFchY7tNG3vt5L8jr9n3OLQDxvbZz2/osdOY7LwDnv3FG1+Dy2tb7k75BGy42H7eG11v7CCM7rGthF2bq4fY6RIjMPyqvdbhV2G4zz66vXDh38L6C+ZWX0rViPn858xZQJ8xZjeAiNwKXAqUB6xLgZuNMQZ4TESaRKTTGHMIOARgjJkQkZ1A97TXKqXUgnK5pqYPWT/zJtQKxhgS2QIjpdA2keWRZ15gWdeqySA3msiybyTJs/1jjCWz5ArVW7VcAs0h31SQCzlb2EdL2GuDXMhHc7gU7nxOoHNDz5l2e7t63wWn/7vqx4pFGNpppy9xuW0rWUOXvVmi1KI3tAte3gav3Q3eECw/0YYwU+RdE+Pwgt8+L+btXbLTu25b1truWW/AnlfIQTYBqVE7lUn8iO3CLXH7Ydk66DjZtkDechmcfBlcdK3tOlbqOCDGvEET9zt5Y5HLgIuMMV90nn8WONsYc2XZOXcC1xpjHnae3wdcbYx5quycVcCDwMnGmFiVz/kS8CWA9vb2M2+99dZ5+T4l8XicSEQnqizR+qik9VFJ66PSG9WHMYZ0ASayhnjOEM8aZx/iTln5sXjOnjtLnkOAiBciPiHiFSI+ITq5DxFv2XNnP+QFV7VVHN4pU6RxfCdtgw8RSA9ixIURF7mCwePxYcSNERdZXzOpYAepYCdGXDTEXqEh9grRidcRU8CIByNuCm4/OW+UvCdK1tdIKthFMtRDMtRNOrDcdrECUsyxct9trNh/GwW3n0R4NXlPkII7hJgC3twYvuwY3lycgjtAwR0k7wmSDHUTa9jIeOMGUsEuXMUsnnwCTz6JmAK2a9iQ94TJ+FtttzGAKRCdeJ2msRdwFfNkfc1kfU1k/K0kwiswLs/kdS0fepSugbsRk6O/92MMt55DPJHUv5dplvpvyPvf//6njTFbppfPZ8tZtb/w6T8jb3iOiESA24FvVAtmAMaYG4AbALZs2WK2lgbbzpPt27cz359xPNH6qKT1UUnro9Kxro9S69zRRJbRRJajSWdL5DiatGVjydzksYGYPZYtzOxuBdtC1xj0Ol2sdmsMemkMemkIeqb2A147FUppP+gl4HVVX55r0geAr85rfVR3AQx+E9eDf0/TxGE77i496NxM0QbhNXa+u1x68gaJpiNP0HXoXvtycdtpTmbjDUHregi32SXI0uPVz/MEoONU2yX92r22xa95FYibxpe+A20nsbPlQjaedrFdtcITqHz0hhZufdl0rGbm26vX35D5DGcHgN6y5z3AwFzPEREvNpjdYoz59Txep1JKHZdEhIjfQ8TvobclNKfXGGNIZgtlYS43Ge7GkllGy8oOHE3y8kCOWDo/Y0WH6XxuFw1BDw3BytBWCnVTz6eODyaLjCWzRAPeN1/94Z1o22Dnj5urYtGOWet/zE5a7I/asXb+Bntzg7gAgaQz1m1ol53yZMNHYe37YfX59vyEs+zY6G4YeNauHfvyNug9G876Eqz7EGDgxdvhgb9n467rYNd1s1+Xv9F2zYaX2+7f9LjdfGG7/FhpXdlswt6xGz8Cpmhv6PBHwBOEYs52DRfz9gaUhi57E0liCF7+rd2OvAitJ8KmS2DTpdB+8tRkyrUoNWbvVF6o8LoA5jOcPQmsF5HVwEHgcmD6wIVtwJXOeLSzgXFjzCHnLs4bgZ3GmO/O4zUqpVRdERHCfg/htxDoAPKFIrF0nlgqx3jZFkuX7adyxFJ5xlO25W7vSMKWpfNV73IFuOpB20IV9dtgZ8Obh4aAl4hznZGADaBhn5tIwEvE77blzlY6J+zzHJuQ53LZQNe24Z29T2O33bo3wymXzX7eqZ+Ekz/OM3fcwOZN6+3dsoUM5DN2P5+xkyKXwl5yxLakNa20LVyJYXjtHnjuV+/sehF75+37roL9j8JD/wMe/Acb3nrPgt5z7HJmI312PN/wK8519ELjCnstqTE7BjA9bq+7kLVhMNruBMhzoHl1Zdgzxhl3mLPftzTNSy6FPz1sw3JpyplyiRF44Fp48kY7/99534DNf26D2nFu3sKZMSYvIlcCd2On0rjJGPOSiHzZOf4T4HfYaTT6sFNpfM55+XuAzwIviMgOp+zbxpjfzdf1KqWUmp3H7aIlbG82eKtK3a9TAc6GuSd2vEDnynWTz2NpG+5iqRz7R5PEM3kSGdtqN9vNEtMFve6pMOd3VwS4sN9DtOq+m2jA2fd5Jve97iqBYL643MQaN8L6rW/v9cbYNWQPP2+7aSPOdCoutxN24pBP2Zs1XF7brZsatXfHxgZs9+mGD9vXlCSGYdf/hb0PQf/jtlWtJNxmb+7IJeG1P0D88NQxv3NXscdvP8/tsa2Qz9zsHG8ApKwVb+ZE0SXnAjx1JbSstsGw9Jgag4e/Z5dZO+PT9g7d319lA+WmS+3x+GH7HYpl3dL+6NS8gI09U3cs+8KQS9ngmxyxwXLr1W/vf4tjYF4nn3HC1O+mlf2kbN8wfRCCLX+Y6uPRlFJKHWfKu1+7m6ZaNXxDu9h63uo5vUcmXyCRKZDI5JlI50lk88Sd7tZSgKvct+fG03kGxtIV52fy1cfcTefzuKqEOdtqF3Va6ib3qwU+v3fyfL/nzcbkvUMidjxbtWlWgs1v7z3DrXDmFXYDuzrG0b3OGLvWynNzaRvUAo3VJywuFm337/5Hbauby20Dottrw6LbCYwevw1M/ih4Arz61P2c0Oqx06uM9NnxeoWMfc/1F8IF/22qhXPPQ/DAd+DZX9qu30i7DXKuUtQxtkVv4Fk7DcwbhEJ8ETj/qkXrztWZAZVSStU8v8eN3+N+Wy130+UKxckQl8gUiGdyFWFu9sCXZzieZe/IVKtessr6rtV43TKjZW564Bs5nOUVeX0y8AW9bkI+D0Gfm5CzBb1u5/kx6sJ9Kxo67Vb1CwbsNhuXyy6FNn05tDcxcCjACeU3BBSLdqLjbHJmEF39XrvNRbFgW9WycWdL2NbD0DK7+cKLOs5Ow5lSSqm64nW7aAr5aAq986BXKBoS2fy0YDct8JWHvNI52TzjySwHjyad8+3xba/vmvNn+zwuJ8C5pwKc10PA5yY0rdyGOk+VMhv0Kp+7CXjclUuS1QqXy3ZHvuP3cTtr477JhIWLRMOZUkop9Ta5XUJDwN59+gaLS8zJH++/n7Pf/d7JMJfMFEhm86RyBVLZAslsgWSuQCqbJ5UtkszlJ8tT2QKpnD1/PJnlkPN88nhubi185UphrRQAQz43Aacs4HET8LoIeN1lm30eLNv3e0rnT51bOu53Hn3uee7yPQ5pOFNKKaVqgKvsTtpj3Z5TLBrS+cqwlszaMJee3C8PeTYEziyz07CkcwXSuSKpXIF0rkAmV5x1/rw3/95MBTyPi0BZ+EvFU/xi75M26JXCoMdN0OdyzqkeEoNl+5MB0Xl/z0Le6PE2aThTSimlljiXS5zuSw9zWK30bSkUjRPaCqTzRdJOy10mb4NcOldwwlxx6rzy5/kCqWyRdL5AximPFeFwLF15nvP+s03P8mY8LiHodU+23FVr7Yv43Xzv8jOOcQ29hWtctE9WSiml1JLhdk21/B0rdoWA6oP8c4XKlrvprXmTITFrg9/M40Uyk4HRPo9n8gxNZOZnGbO3QMOZUkoppY47XrcLr9tlx/stMbXf8aqUUkopVUc0nCmllFJK1RANZ0oppZRSNUTDmVJKKaVUDdFwppRSSilVQzScKaWUUkrVEA1nSimllFI1RMOZUkoppVQN0XCmlFJKKVVDNJwppZRSStUQDWdKKaWUUjVEw5lSSimlVA3RcKaUUkopVUM0nCmllFJK1RANZ0oppZRSNUTDmVJKKaVUDdFwppRSSilVQzScKaWUUkrVEA1nSimllFI1RMOZUkoppVQN0XCmlFJKKVVDNJwppZRSStUQDWdKKaWUUjVEjDGLfQ3HjIgMAfvm+WNageF5/ozjidZHJa2PSloflbQ+Kml9VNL6mGmp18lKY8zy6YVLKpwtBBF5yhizZbGvo1ZofVTS+qik9VFJ66OS1kclrY+Z6rVOtFtTKaWUUqqGaDhTSimllKohGs7euhsW+wJqjNZHJa2PSloflbQ+Kml9VNL6mKku60THnCmllFJK1RBtOVNKKaWUqiEazpRSSimlaoiGszkSkYtE5BUR6RORaxb7ehaaiPSKyP0islNEXhKRrzvlLSJyr4i85jw2L/a1LiQRcYvIsyJyp/O83uujSURuE5Fdzn8r59ZznYjIN52/lxdF5FciEqin+hCRm0RkUEReLCub9fuLyLec39hXRORPFueq588s9fEPzt/L8yLybyLSVHas7uqj7Nh/EhEjIq1lZUu6PsppOJsDEXEDPwT+FNgEfEpENi3uVS24PPCXxpiNwDnAV506uAa4zxizHrjPeV5Pvg7sLHte7/XxfeAuY8wG4DRs3dRlnYhIN/A1YIsx5mTADVxOfdXHz4CLppVV/f7O78nlwEnOa37k/PYuJT9jZn3cC5xsjDkVeBX4FtR1fSAivcAFwP6ysnqoj0kazubmLKDPGLPbGJMFbgUuXeRrWlDGmEPGmGec/Qns/+l2Y+vh585pPwc+tjhXuPBEpAf4MPDTsuJ6ro8G4H3AjQDGmKwxZow6rhPAAwRFxAOEgAHqqD6MMQ8Co9OKZ/v+lwK3GmMyxpg9QB/2t3fJqFYfxph7jDF55+ljQI+zX5f14bgOuAoov2NxyddHOQ1nc9MN9Jc9P+CU1SURWQWcATwOtBtjDoENcEDb4l3Zgvse9gekWFZWz/WxBhgC/rfT1ftTEQlTp3VijDkI/CP2X/+HgHFjzD3UaX2Ume376+8sfB74vbNfl/UhIpcAB40xz007VFf1oeFsbqRKWV3OQSIiEeB24BvGmNhiX89iEZGPAIPGmKcX+1pqiAfYDPzYGHMGkGBpd9m9IWcs1aXAaqALCIvIZxb3qmpaXf/OishfY4eP3FIqqnLakq4PEQkBfw3852qHq5Qt2frQcDY3B4Desuc92O6JuiIiXmwwu8UY82un+IiIdDrHO4HBxbq+BfYe4BIR2Yvt5v6AiPyS+q0PsH8nB4wxjzvPb8OGtXqtkw8Be4wxQ8aYHPBr4N3Ub32UzPb96/Z3VkSuAD4CfNpMTT5aj/WxFvuPmeec39Ye4BkR6aDO6kPD2dw8CawXkdUi4sMOSty2yNe0oEREsGOJdhpjvlt2aBtwhbN/BfDbhb62xWCM+ZYxpscYswr738MfjTGfoU7rA8AYcxjoF5ETnaIPAi9Tv3WyHzhHRELO388HsWM167U+Smb7/tuAy0XELyKrgfXAE4twfQtKRC4CrgYuMcYkyw7VXX0YY14wxrQZY1Y5v60HgM3Ob0td1YdnsS/geGCMyYvIlcDd2DuubjLGvLTIl7XQ3gN8FnhBRHY4Zd8GrgX+RUS+gP0/o08s0vXVinqvj78AbnH+EbMb+Bz2H4F1VyfGmMdF5DbgGWx31bPYpWgi1El9iMivgK1Aq4gcAP4Ls/yNGGNeEpF/wQb6PPBVY0xhUS58nsxSH98C/MC9NsPzmDHmy/VaH8aYG6udWw/1UU6Xb1JKKaWUqiHaramUUkopVUM0nCmllFJK1RANZ0oppZRSNUTDmVJKKaVUDdFwppRSSilVQzScKaWWNBEpiMiOsu2YrVogIqtE5MVj9X5KKQU6z5lSaulLGWNOX+yLUEqpudKWM6VUXRKRvSLyHRF5wtnWOeUrReQ+EXneeVzhlLeLyL+JyHPO9m7nrdwi8k8i8pKI3CMiQef8r4nIy8773LpIX1MpdRzScKaUWuqC07o1/6zsWMwYcxbwA+B7TtkPgJuNMadiF6G+3im/HnjAGHMads3Q0ioh64EfGmNOAsaAjzvl1wBnOO/z5fn6ckqppUdXCFBKLWkiEjfGRKqU7wU+YIzZLSJe4LAxZpmIDAOdxpicU37IGNMqIkNAjzEmU/Yeq4B7jTHrnedXA15jzN+KyF1AHPgN8BtjTHyev6pSaonQljOlVD0zs+zPdk41mbL9AlNjeT8M/BA4E3haRHSMr1JqTjScKaXq2Z+VPT7q7D8CXO7sfxp42Nm/D/gKgIi4RaRhtjcVERfQa4y5H7gKaMIueK6UUm9K/yWnlFrqgiKyo+z5XcaY0nQafhF5HPsP1U85ZV8DbhKRvwKGgM855V8HbhCRL2BbyL4CHJrlM93AL0WkERDgOmPM2DH7RkqpJU3HnCml6pIz5myLMWZ4sa9FKaXKabemUkoppVQN0ZYzpZRSSqkaoi1nSimllFI1RMOZUkoppVQN0XCmlFJKKVVDNJwppZRSStUQDWdKKaWUUjXk/wMtfkVWP9NOyQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make a quick epoch vs. loss plot\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.grid()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Depth 7 Model Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (MSE)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Accuracy')"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make a quick epoch vs. accuracy plot\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.grid()\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Depth 7 Model Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 4.547666311264038\n",
      "Micro-average quality numbers\n",
      "Precision: 0.7567, Recall: 0.9810, F1-measure: 0.8544\n",
      "Accuracy =  0.36361131882403874\n",
      "Accuracy =  0.9866029137051278\n",
      "\n",
      "\n",
      "Micro-average quality numbers\n",
      "Precision: 0.8088, Recall: 0.9604, F1-measure: 0.8781\n",
      "Accuracy =  0.43610531701365657\n",
      "Accuracy =  0.9893198783056936\n",
      "\n",
      "\n",
      "Micro-average quality numbers\n",
      "Precision: 0.8427, Recall: 0.9340, F1-measure: 0.8860\n",
      "Accuracy =  0.4861171238537526\n",
      "Accuracy =  0.9903745431803124\n",
      "\n",
      "\n",
      "Micro-average quality numbers\n",
      "Precision: 0.8738, Recall: 0.9030, F1-measure: 0.8882\n",
      "Accuracy =  0.5202782478649297\n",
      "Accuracy =  0.9908926328892962\n",
      "\n",
      "\n",
      "Micro-average quality numbers\n",
      "Precision: 0.9049, Recall: 0.8706, F1-measure: 0.8874\n",
      "Accuracy =  0.527687040025188\n",
      "Accuracy =  0.9911501372890424\n",
      "\n",
      "\n",
      "Micro-average quality numbers\n",
      "Precision: 0.9356, Recall: 0.8387, F1-measure: 0.8845\n",
      "Accuracy =  0.5064839230194026\n",
      "Accuracy =  0.9912244766525676\n",
      "\n",
      "\n",
      "Micro-average quality numbers\n",
      "Precision: 0.9620, Recall: 0.8108, F1-measure: 0.8799\n",
      "Accuracy =  0.4698532016214727\n",
      "Accuracy =  0.9911376148827753\n",
      "\n",
      "\n",
      "Micro-average quality numbers\n",
      "Precision: 0.9807, Recall: 0.7842, F1-measure: 0.8715\n",
      "Accuracy =  0.42834231965051756\n",
      "Accuracy =  0.9907367985013541\n",
      "\n",
      "\n",
      "Micro-average quality numbers\n",
      "Precision: 0.9918, Recall: 0.7462, F1-measure: 0.8516\n",
      "Accuracy =  0.3733322838364359\n",
      "Accuracy =  0.9895851346711012\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"\"\"\n",
    "# predict\n",
    "from sklearn.metrics import precision_score,accuracy_score, f1_score, recall_score, hamming_loss\n",
    "\n",
    "start = time.time()\n",
    "predictions_d7 = model_d7.predict(x_test_d7.values)\n",
    "end = time.time()\n",
    "print(\"Time: \" + str(end - start))\n",
    "\n",
    "pred=predictions_d7.copy()\n",
    "\n",
    "\n",
    "thresholds=[0.1, 0.2, 0.3, .4, .5, .6, .7, .8, .9]\n",
    "for val in thresholds:\n",
    "    pred=predictions_d7.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "    \n",
    "  \n",
    "    precision = precision_score(Y_test_d7, pred, average='micro')\n",
    "    recall = recall_score(Y_test_d7, pred, average='micro')\n",
    "    f1 = f1_score(Y_test_d7, pred, average='micro')\n",
    "   \n",
    "    print(\"Micro-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "    print(\"Accuracy = \",accuracy_score(Y_test_d7, pred))\n",
    "    print(\"Accuracy = \",partial_accuracy(Y_test_d7, pred))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "predictions_d7 = model_d7.predict(x_test_d7.values)\n",
    "pred=predictions_d7.copy()\n",
    "\n",
    "n_classes = 99\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(Y_test_d7[:, i], pred[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(Y_test_d7.ravel(), pred.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "    \n",
    "# Plot of a ROC curve for a specific class\n",
    "for i in range(n_classes):\n",
    "    plt.figure()\n",
    "    plt.plot(fpr[i], tpr[i], label='ROC curve (area = %0.2f)' % roc_auc[i])\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure()\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Some extension of Receiver operating characteristic to multi-class')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "x_train_d3.to_csv(\"x_train_d3_01.csv\")\n",
    "x_test_d3.to_csv(\"x_test_d3_01.csv\")\n",
    "pd.DataFrame(Y_train_d3).to_csv(\"Y_train_d3_01.csv\")\n",
    "pd.DataFrame(Y_test_d3).to_csv(\"Y_test_d3_01.csv\")\n",
    "\n",
    "x_train_d5.to_csv(\"x_train_d5_01.csv\")\n",
    "x_test_d5.to_csv(\"x_test_d5_01.csv\")\n",
    "pd.DataFrame(Y_train_d5).to_csv(\"Y_train_d5_01.csv\")\n",
    "pd.DataFrame(Y_test_d5).to_csv(\"Y_test_d5_01.csv\")\n",
    "\n",
    "x_train_d7.to_csv(\"x_train_d7_01.csv\")\n",
    "x_test_d7.to_csv(\"x_test_d7_01.csv\")\n",
    "pd.DataFrame(Y_train_d7).to_csv(\"Y_train_d7_01.csv\")\n",
    "pd.DataFrame(Y_test_d7).to_csv(\"Y_test_d7_01.csv\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "model_d7.save(\"model_d7_01.h5\")\n",
    "model_d5.save(\"model_d5_01.h5\")\n",
    "model.save(\"model_d3_01.h5\")\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
