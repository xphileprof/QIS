{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import time\n",
    "import random\n",
    "import copy\n",
    "\n",
    "from scipy import interp\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, ShuffleSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import precision_score,accuracy_score, f1_score, recall_score, hamming_loss\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "#%matplotlib inline\n",
    "#import mpld3\n",
    "#mpld3.enable_notebook()\n",
    "\n",
    "print (pd.__version__)\n",
    "\n",
    "######### DEFINITION OF GLOBAL VARIABLES #########\n",
    "RUN_CONFIGURATION_LOOP = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "#sys.path.append('/')\n",
    "import circuits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These functions needed to work with the GraphDecoder/MWPM module\n",
    "import time\n",
    "\n",
    "def add_measurement_errs(curr_syn, prob_err, x_syn, depth):\n",
    "    #x_syn is True if it is x syndrome, False if it is Z syndrome\n",
    "    total_time = 0\n",
    "    new_syn = []\n",
    "    if x_syn:\n",
    "        for i in curr_syn:\n",
    "            rand = random.random()\n",
    "            if rand > prob_err:\n",
    "                new_syn.append(i)\n",
    "        return (new_syn + return_xmeasurement_errs(depth, prob_err))\n",
    "    else:\n",
    "        for i in curr_syn:\n",
    "            rand = random.random()\n",
    "            if rand > prob_err:\n",
    "                new_syn.append(i)\n",
    "        return (new_syn + return_zmeasurement_errs(depth, prob_err))\n",
    "    \n",
    "def do_new_decoding(data, depth, prob_err):\n",
    "    decoder = circuits.GraphDecoder(depth,1)\n",
    "    G = decoder.S['Z']\n",
    "    #decoder.graph_2D(G,'distance')\n",
    "    df = pd.DataFrame()\n",
    "    syn = []\n",
    "    total_time = 0\n",
    "    \n",
    "    for row in data:\n",
    "        x_input = []\n",
    "        z_input = []\n",
    "        x_type = True\n",
    "        for col in row:\n",
    "            if not col == \"[]\":\n",
    "                col = eval(col)\n",
    "                for c in col:\n",
    "                    if x_type:\n",
    "                        x_input.append(c)\n",
    "                    else:\n",
    "                        z_input.append(c)\n",
    "            x_type = not x_type  \n",
    "            \n",
    "        if prob_err > 0:\n",
    "            syndromes_x = add_measurement_errs(x_input, prob_err, True, depth)\n",
    "            syndromes_z = add_measurement_errs(z_input, prob_err, False, depth)\n",
    "        else:\n",
    "            syndromes_x = x_input\n",
    "            syndromes_z = z_input\n",
    "\n",
    "        start = time.time_ns()\n",
    "        error_graph_x, paths_x = decoder.make_error_graph(syndromes_x,'X')\n",
    "        matching_graph_x = decoder.matching_graph(error_graph_x,'X')\n",
    "        matches_x = decoder.matching(matching_graph_x,'X')\n",
    "        flips_x = decoder.calculate_qubit_flips(matches_x, paths_x,'X')\n",
    "        syn_x = (translate_errors(flips_x))\n",
    "\n",
    "        error_graph_z, paths_z = decoder.make_error_graph(syndromes_z,'Z')\n",
    "        matching_graph_z = decoder.matching_graph(error_graph_z,'Z')\n",
    "        matches_z = decoder.matching(matching_graph_z,'Z')\n",
    "        flips_z = decoder.calculate_qubit_flips(matches_z, paths_z,'Z')\n",
    "        syn_z = translate_errors(flips_z)\n",
    "        df = df.append(pd.Series([syn_x, syn_z]), ignore_index=True)\n",
    "        end = time.time_ns()\n",
    "        total_time += (end - start)/ (10 ** 9)\n",
    "    return (df, total_time) \n",
    "\n",
    "import random\n",
    "def return_xmeasurement_errs(depth, prob):\n",
    "    \n",
    "    new_errs = []\n",
    "    \n",
    "    if depth == 3:\n",
    "        errs = [(0, -0.5, 0.5), (0, 0.5, 1.5), (0, 1.5, 0.5), (0, 2.5, 1.5)]\n",
    "    elif depth == 5:\n",
    "        errs = [(0, -0.5, 0.5), (0, 0.5, 1.5), (0, -0.5, 2.5), (0, 0.5, 3.5), (0, 1.5, 0.5), (0, 1.5, 2.5),\n",
    "                        (0, 2.5, 1.5), (0, 2.5, 3.5), (0, 3.5, 0.5), (0, 4.5, 1.5), (0, 3.5, 2.5), (0, 4.5, 3.5)]\n",
    "    else:\n",
    "        errs = [(0, -0.5, 0.5), (0, 0.5, 1.5), (0, -0.5, 2.5), (0, 0.5, 3.5), (0, -0.5, 4.5), (0, 0.5, 5.5),\n",
    "                        (0, 1.5, 0.5), (0, 1.5, 2.5), (0, 1.5, 4.5), (0, 2.5, 1.5), (0, 2.5, 3.5), (0, 2.5, 5.5),\n",
    "                        (0, 3.5, 0.5),  (0, 3.5, 2.5), (0, 3.5, 4.5), (0, 4.5, 1.5), (0, 4.5, 3.5), (0, 4.5, 5.5),\n",
    "                       (0, 5.5, 0.5), (0, 6.5, 1.5), (0, 5.5, 2.5), (0, 6.5, 3.5), (0, 5.5, 4.5), (0, 6.5, 5.5)]\n",
    "    for e in errs:\n",
    "        rand = random.random()\n",
    "        if rand <= prob:\n",
    "            new_errs.append(e)\n",
    "            \n",
    "    return new_errs\n",
    "            \n",
    "\n",
    "def return_zmeasurement_errs(depth, prob):\n",
    "    \n",
    "    new_errs = []\n",
    "    \n",
    "    if depth == 3:\n",
    "        errs = [(0, 0.5, 0.5), (0, 0.5, 2.5), (0, 1.5, -0.5), (0, 1.5, 1.5)]\n",
    "    elif depth == 5:\n",
    "        errs = [(0, 0.5, 0.5), (0, 0.5, 2.5), (0, 0.5, 4.5), (0, 1.5, -0.5), (0, 1.5, 1.5), (0, 1.5, 3.5),\n",
    "                        (0, 2.5, 0.5), (0, 2.5, 2.5), (0, 2.5, 4.5), (0, 3.5, -0.5), (0, 3.5, 1.5), (0, 3.5, 3.5)]\n",
    "    else:\n",
    "        errs = [(0, 0.5, 0.5), (0, 0.5, 2.5), (0, 0.5, 4.5), (0, 0.5, 6.5), (0, 1.5, -0.5), (0, 1.5, 1.5),\n",
    "                        (0, 1.5, 3.5), (0, 1.5, 5.5), (0, 2.5, 0.5), (0, 2.5, 2.5), (0, 2.5, 4.5), (0, 2.5, 6.5),\n",
    "                        (0, 3.5, -0.5),  (0, 3.5, 1.5), (0, 3.5, 3.5), (0, 3.5, 5.5), (0, 4.5, 0.5), (0, 4.5, 2.5),\n",
    "                       (0, 4.5, 4.5), (0, 4.5, 6.5), (0, 5.5, -0.5), (0, 5.5, 1.5), (0, 5.5, 3.5), (0, 5.5, 5.5)]\n",
    "        \n",
    "    for e in errs:\n",
    "        rand = random.random()\n",
    "        if rand <= prob:\n",
    "            new_errs.append(e)\n",
    "            \n",
    "    return new_errs\n",
    "\n",
    "def translate_errors (phys_errs):\n",
    "    flipX = np.array([(0, 1),(1, 0)])\n",
    "    flipZ = np.array([(1, 0), (0, -1)])\n",
    "    errs = []\n",
    "    str2 = \"\"\n",
    "    for qubit, flip in phys_errs.items():\n",
    "        row = int(qubit[1])\n",
    "        col = int(qubit[2])\n",
    "        if str(flip) == \"X\":\n",
    "            str1 = \"X\"\n",
    "        elif str(flip) == \"Z\":\n",
    "            str1 = \"Z\"\n",
    "        else:\n",
    "            str1 = \"X\"\n",
    "            str2 = \"Z\"\n",
    "        str1 += str(row) + str(col)\n",
    "        errs.append(str1)\n",
    "        if str2 != \"\":\n",
    "            str2 += str(row) +str(col)\n",
    "            errs.append(str2)\n",
    "            str2 = \"\"\n",
    "    return errs   \n",
    "\n",
    "def translate_to_graph(df_graph, labels, mlb):\n",
    "#go through labels given\n",
    "    indices = []\n",
    "    labels = mlb.inverse_transform(labels)\n",
    "    \n",
    "    for row in labels:\n",
    "        label_str = str(row)\n",
    "        for index, r in df_graph.iterrows():\n",
    "            if label_str == \"('',)\":\n",
    "                if str(r[\"Labels\"]) == \"[' ']\":\n",
    "                    indices.append([index])\n",
    "                    break\n",
    "            if set(row) == set(r[\"Labels\"]):\n",
    "                indices.append([index])\n",
    "                break\n",
    "\n",
    "    df_syns = df_graph.drop(['Labels'], axis=1)\n",
    "    return_df = pd.DataFrame()\n",
    "    for i in indices:\n",
    "        return_df = return_df.append(df_syns.loc[i])\n",
    "    return return_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function needed for preprocessing. CSV file reads in a string, needs to be a list for labels \n",
    "#for preprocessing csv files\n",
    "def create_list_from_string(err_list):\n",
    "    newstring = err_list.replace(\"'\", \"\")\n",
    "    new_err_list = newstring.strip('][').split(', ')\n",
    "    return sorted(set(new_err_list))\n",
    "\n",
    "\n",
    "def create_string_from_list(err_list):\n",
    "    return_string = \"[\"\n",
    "    if err_list[0] == \"''\":\n",
    "        return \"[' ']\"\n",
    "    else:\n",
    "        for index, i in enumerate(err_list):\n",
    "            return_string = return_string +  \"'\" + i + \"'\"\n",
    "            if index < (len(err_list)-1):\n",
    "                return_string += \", \"\n",
    "    return return_string + \"]\"\n",
    "        \n",
    "\n",
    "#take in two 2d arrays of predicted values, true values, and threshold to determine labels. \n",
    "#calculates the partial accuracy of the predicted values, averaged out for all obersvations\n",
    "def partial_accuracy(y_pred, y_true):\n",
    "    total = 0\n",
    "    rows = y_pred.shape[0]\n",
    "    cols = y_pred.shape[1]\n",
    "    for i in range(0, rows):\n",
    "        row_correct = 0\n",
    "        for j in range(0, cols):\n",
    "            if y_pred[i,j] == y_true[i,j]:\n",
    "                row_correct += 1\n",
    "        total += row_correct/cols\n",
    "    return total/rows\n",
    "\n",
    "def partial_accuracy_and_contingency(y_pred, y_true, mlb):\n",
    "    total = 0\n",
    "    a= np.zeros(shape=y_true.shape)\n",
    "    rows = y_pred.shape[0]\n",
    "    cols = y_pred.shape[1]\n",
    "    df = pd.DataFrame(a, columns = mlb.classes_)\n",
    "    for i in range(0, rows):\n",
    "        row_correct = 0\n",
    "        for j, label in enumerate(mlb.classes_):\n",
    "            if y_pred[i,j] == y_true[i,j]:\n",
    "                row_correct += 1\n",
    "                df.at[i, label] = 1\n",
    "            else:\n",
    "                df.at[i, label] = 0\n",
    "\n",
    "        total += row_correct/cols\n",
    "\n",
    "    return (total/rows, df)\n",
    "\n",
    "def contingency_table_and_t (clf1, clf2):\n",
    "    a = 0 #clf1 pos, clf2 pos\n",
    "    b = 0 #clf1 pos, clf2 neg\n",
    "    c = 0 #clf1 neg, clf2 pos\n",
    "    d = 0 #clf1 neg, clf2 neg\n",
    "    \n",
    "    for index, value in clf1.items():\n",
    "        if value == 1 and clf2[index] == 1:\n",
    "            a+=1\n",
    "        elif value == 1 and clf2[index] == 0: #classifier 1 right, classifier 2 wrong\n",
    "            b+=1\n",
    "        elif value == 0 and clf2[index] == 1: #classifier 1 wrong, classifier 2 right\n",
    "            c+=1\n",
    "        else:\n",
    "            d+=1\n",
    "    print(\"[\"+str(a)+\", \"+str(b)+\"]\")\n",
    "    print(\"[\"+str(c)+\", \"+str(d)+\"]\")\n",
    "    if b == 0 and c ==0:\n",
    "        print(\"both b and c are zero\")\n",
    "        t=0\n",
    "    else:\n",
    "        t = (((b-c)-1)**2)/(b+c)\n",
    "    return ([[a,b],[c,d]], t)\n",
    "\n",
    "def add_noise(val, noise_level):\n",
    "    rand = random.uniform(0, 1)\n",
    "    if rand <= noise_level:\n",
    "        if val == -1:\n",
    "            val = 1\n",
    "        elif val == 1:\n",
    "            val = -1\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are super long functions to be hard coded because i dont have time to properly fix them, sorry bout it\n",
    "#[(0, 0.5, 0.5), (0, 0.5, 2.5), (0, 1.5, -0.5), (0, 1.5, 1.5)]\n",
    "def graph_with_errs_d3(df):\n",
    "    x_data = []\n",
    "    z_data = []\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        x_data.append([])\n",
    "        z_data.append([])\n",
    "        \n",
    "        if df.loc[i].at[\"X0\"] == -1:\n",
    "            x_data[i].append((0, -0.5, 0.5))\n",
    "        if df.loc[i].at[\"Z1\"] == -1:\n",
    "            z_data[i].append((0, 0.5, 0.5))\n",
    "        if df.loc[i].at[\"X2\"] == -1:\n",
    "            x_data[i].append((0, 0.5, 1.5))\n",
    "        if df.loc[i].at[\"Z3\"] == -1:\n",
    "            z_data[i].append((0, 0.5, 2.5))\n",
    "        if df.loc[i].at[\"Z4\"] == -1:\n",
    "            z_data[i].append((0, 1.5, -0.5))\n",
    "        if df.loc[i].at[\"X5\"] == -1:\n",
    "            x_data[i].append((0, 1.5, 0.5))\n",
    "        if df.loc[i].at[\"X6\"] == -1:\n",
    "            x_data[i].append((0, 2.5, 1.5))\n",
    "        if df.loc[i].at[\"Z7\"] == -1:\n",
    "            z_data[i].append((0, 1.5, 1.5))\n",
    "        x_data[i] = str(x_data[i])\n",
    "        z_data[i] = str(z_data[i])\n",
    "    #graph_df = pd.DataFrame(df[\"Labels\"], x_data, z_data, columns=[\"Labels\", \"XSyn\", \"ZSyn\"])\n",
    "    graph_df = pd.DataFrame({\"XSyn\":x_data, \"ZSyn\":z_data})\n",
    "    return graph_df\n",
    "        \n",
    "\n",
    "def graph_with_errs_d5(df):\n",
    "    x_data = []\n",
    "    z_data = []\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        x_data.append([])\n",
    "        z_data.append([])\n",
    "        if df.loc[i].at[\"X0\"] == -1:\n",
    "             x_data[i].append((0, -0.5, 0.5))\n",
    "        if df.loc[i].at[\"Z1\"] == -1:\n",
    "            z_data[i].append((0, 0.5, 0.5))\n",
    "        if df.loc[i].at[\"X2\"] == -1:\n",
    "            x_data[i].append((0, 0.5, 1.5))\n",
    "        if df.loc[i].at[\"X3\"] == -1:\n",
    "            x_data[i].append((0, -0.5, 2.5))\n",
    "        if df.loc[i].at[\"Z4\"] == -1:\n",
    "            z_data[i].append((0, 0.5, 2.5))\n",
    "        if df.loc[i].at[\"X5\"] == -1:\n",
    "            x_data[i].append((0, 0.5, 3.5))\n",
    "        if df.loc[i].at[\"Z6\"] == -1:\n",
    "            z_data[i].append((0, 0.5, 4.5))\n",
    "        if df.loc[i].at[\"Z7\"] == -1:\n",
    "            z_data[i].append((0, 1.5, -0.5))\n",
    "        if df.loc[i].at[\"X8\"] == -1:\n",
    "            x_data[i].append((0, 1.5, 0.5))\n",
    "        if df.loc[i].at[\"Z9\"] == -1:\n",
    "            z_data[i].append((0, 1.5, 1.5))\n",
    "        if df.loc[i].at[\"X10\"] == -1:\n",
    "            x_data[i].append((0, 1.5, 2.5))\n",
    "        if df.loc[i].at[\"Z11\"] == -1:\n",
    "            z_data[i].append((0, 1.5, 3.5))\n",
    "        if df.loc[i].at[\"Z12\"] == -1:\n",
    "            z_data[i].append((0, 2.5, 0.5))\n",
    "        if df.loc[i].at[\"X13\"] == -1:\n",
    "            x_data[i].append((0, 2.5, 1.5))\n",
    "        if df.loc[i].at[\"Z14\"] == -1:\n",
    "            z_data[i].append((0, 2.5, 2.5))\n",
    "        if df.loc[i].at[\"X15\"] == -1:\n",
    "            x_data[i].append((0, 2.5, 3.5))\n",
    "        if df.loc[i].at[\"Z16\"] == -1:\n",
    "            z_data[i].append((0, 2.5, 4.5))\n",
    "        if df.loc[i].at[\"Z17\"] == -1:\n",
    "            z_data[i].append((0, 3.5, -0.5))\n",
    "        if df.loc[i].at[\"X18\"] == -1:\n",
    "            x_data[i].append((0, 3.5, 0.5))\n",
    "        if df.loc[i].at[\"X19\"] == -1:\n",
    "            x_data[i].append((0, 4.5, 1.5))\n",
    "        if df.loc[i].at[\"Z20\"] == -1:\n",
    "            z_data[i].append((0, 3.5, 1.5))\n",
    "        if df.loc[i].at[\"X21\"] == -1:\n",
    "            x_data[i].append((0, 3.5, 2.5))\n",
    "        if df.loc[i].at[\"X22\"] == -1:\n",
    "            x_data[i].append((0, 4.5, 3.5))\n",
    "        if df.loc[i].at[\"Z23\"] == -1:\n",
    "            z_data[i].append((0, 3.5, 3.5))\n",
    "        x_data[i] = str(x_data[i])\n",
    "        z_data[i] = str(z_data[i])\n",
    "            \n",
    "    graph_df = pd.DataFrame({\"XSyn\":x_data, \"ZSyn\":z_data})\n",
    "    return graph_df\n",
    "    \n",
    "def graph_with_errs_d7(df):\n",
    "    x_data = []\n",
    "    z_data = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        x_data.append([])\n",
    "        z_data.append([])\n",
    "        \n",
    "        if df.loc[i].at[\"X0\"] == -1:\n",
    "            x_data[i].append((0, -0.5, 0.5))\n",
    "        if df.loc[i].at[\"Z1\"] == -1:\n",
    "            z_data[i].append((0, 0.5, 0.5))\n",
    "        if df.loc[i].at[\"X2\"] == -1:\n",
    "            x_data[i].append((0, 0.5, 1.5))\n",
    "        if df.loc[i].at[\"X3\"] == -1:\n",
    "            x_data[i].append((0, -0.5, 2.5))\n",
    "        if df.loc[i].at[\"Z4\"] == -1:\n",
    "            z_data[i].append((0, 0.5, 2.5))\n",
    "        if df.loc[i].at[\"X5\"] == -1:\n",
    "            x_data[i].append((0, 0.5, 3.5))\n",
    "        if df.loc[i].at[\"X6\"] == -1:\n",
    "            x_data[i].append((0, -0.5, 4.5))\n",
    "        if df.loc[i].at[\"Z7\"] == -1:\n",
    "            z_data[i].append((0, 0.5, 4.5))\n",
    "        if df.loc[i].at[\"X8\"] == -1:\n",
    "            x_data[i].append((0, 0.5, 5.5))\n",
    "        if df.loc[i].at[\"Z9\"] == -1:\n",
    "            z_data[i].append((0, 0.5, 6.5))\n",
    "        if df.loc[i].at[\"Z10\"] == -1:\n",
    "            z_data[i].append((0, 1.5, -0.5))\n",
    "        if df.loc[i].at[\"X11\"] == -1:\n",
    "            x_data[i].append((0, 1.5, 0.5))\n",
    "        if df.loc[i].at[\"Z12\"] == -1:\n",
    "            z_data[i].append((0, 1.5, 1.5))\n",
    "        if df.loc[i].at[\"X13\"] == -1:\n",
    "            x_data[i].append((0, 1.5, 2.5))\n",
    "        if df.loc[i].at[\"Z14\"] == -1:\n",
    "            z_data[i].append((0, 1.5, 3.5))\n",
    "        if df.loc[i].at[\"X15\"] == -1:\n",
    "            x_data[i].append((0, 1.5, 4.5))\n",
    "        if df.loc[i].at[\"Z16\"] == -1:\n",
    "            z_data[i].append((0, 1.5, 5.5))\n",
    "        if df.loc[i].at[\"Z17\"] == -1:\n",
    "            z_data[i].append((0, 2.5, 0.5))\n",
    "        if df.loc[i].at[\"X18\"] == -1:\n",
    "            x_data[i].append((0, 2.5, 1.5))\n",
    "        if df.loc[i].at[\"Z19\"] == -1:\n",
    "            z_data[i].append((0, 2.5, 2.5))\n",
    "        if df.loc[i].at[\"X20\"] == -1:\n",
    "            x_data[i].append((0, 2.5, 3.5))\n",
    "        if df.loc[i].at[\"Z21\"] == -1:\n",
    "            z_data[i].append((0, 2.5, 4.5))\n",
    "        if df.loc[i].at[\"X22\"] == -1:\n",
    "            x_data[i].append((0, 2.5, 5.5))\n",
    "        if df.loc[i].at[\"Z23\"] == -1:\n",
    "            z_data[i].append((0, 2.5, 6.5))\n",
    "        if df.loc[i].at[\"Z24\"] == -1:\n",
    "            z_data[i].append((0, 3.5, -0.5))\n",
    "        if df.loc[i].at[\"X25\"] == -1:\n",
    "            x_data[i].append((0, 3.5, 0.5))\n",
    "        if df.loc[i].at[\"Z26\"] == -1:\n",
    "            z_data[i].append((0, 3.5, 1.5))\n",
    "        if df.loc[i].at[\"X27\"] == -1:\n",
    "            x_data[i].append((0, 3.5, 2.5))\n",
    "        if df.loc[i].at[\"Z28\"] == -1:\n",
    "            z_data[i].append((0, 3.5, 3.5))\n",
    "        if df.loc[i].at[\"X29\"] == -1:\n",
    "            x_data[i].append((0, 3.5, 4.5))\n",
    "        if df.loc[i].at[\"Z30\"] == -1:\n",
    "            z_data[i].append((0, 3.5, 5.5))\n",
    "        if df.loc[i].at[\"Z31\"] == -1:\n",
    "            z_data[i].append((0, 4.5, 0.5))\n",
    "        if df.loc[i].at[\"X32\"] == -1:\n",
    "            x_data[i].append((0, 4.5, 1.5))\n",
    "        if df.loc[i].at[\"Z33\"] == -1:\n",
    "            z_data[i].append((0, 4.5, 2.5))\n",
    "        if df.loc[i].at[\"X34\"] == -1:\n",
    "            x_data[i].append((0, 4.5, 3.5))\n",
    "        if df.loc[i].at[\"Z35\"] == -1:\n",
    "            z_data[i].append((0, 4.5, 4.5))\n",
    "        if df.loc[i].at[\"X36\"] == -1:\n",
    "            x_data[i].append((0, 4.5, 5.5))\n",
    "        if df.loc[i].at[\"Z37\"] == -1:\n",
    "            z_data[i].append((0, 4.5, 6.5))\n",
    "        if df.loc[i].at[\"Z38\"] == -1:\n",
    "            z_data[i].append((0, 5.5, -0.5))\n",
    "        if df.loc[i].at[\"X39\"] == -1:\n",
    "            x_data[i].append((0, 5.5, 0.5))\n",
    "        if df.loc[i].at[\"X40\"] == -1:\n",
    "            x_data[i].append((0, 6.5, 1.5))\n",
    "        if df.loc[i].at[\"Z41\"] == -1:\n",
    "            z_data[i].append((0, 5.5, 1.5))\n",
    "        if df.loc[i].at[\"X42\"] == -1:\n",
    "            x_data[i].append((0, 5.5, 2.5))\n",
    "        if df.loc[i].at[\"X43\"] == -1:\n",
    "            x_data[i].append((0, 6.5, 3.5))\n",
    "        if df.loc[i].at[\"Z44\"] == -1:\n",
    "            z_data[i].append((0, 5.5, 3.5))\n",
    "        if df.loc[i].at[\"X45\"] == -1:\n",
    "            x_data[i].append((0, 5.5, 4.5))\n",
    "        if df.loc[i].at[\"X46\"] == -1:\n",
    "            x_data[i].append((0, 6.5, 5.5))\n",
    "        if df.loc[i].at[\"Z47\"] == -1: \n",
    "            z_data[i].append((0, 5.5, 5.5))\n",
    "        x_data[i] = str(x_data[i])\n",
    "        z_data[i] = str(z_data[i])\n",
    "    graph_df = pd.DataFrame({\"XSyn\":x_data, \"ZSyn\":z_data})\n",
    "    return graph_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_d7= trainData_d7.dropna()\n",
    "#######################################################################################################\n",
    "\n",
    "trainData_d7 = pd.read_csv(\"depth7_all_combos.csv\")\n",
    "trainData_d7 = trainData_d7.applymap(lambda x: add_noise(x,.01))\n",
    "\n",
    "#These four lines remove duplicates\n",
    "trainData_d7['Labels'] = trainData_d7['Labels'].map(lambda x: create_list_from_string(x))\n",
    "trainData_d7['Labels'] = trainData_d7['Labels'].astype(str)\n",
    "trainData_d7 = trainData_d7.drop_duplicates('Labels', keep='first', ignore_index=True)\n",
    "trainData_d7['Labels'] = trainData_d7['Labels'].map(lambda x: create_list_from_string(x))\n",
    "\n",
    "testData_d7_MWPM = graph_with_errs_d7(trainData_d7)\n",
    "\n",
    "#transforms the data to encoding for ML\n",
    "mlb_d7 = MultiLabelBinarizer()\n",
    "mlb_d7.fit(trainData_d7['Labels'])\n",
    "df = pd.DataFrame(mlb_d7.transform(trainData_d7['Labels']))\n",
    "df['Labels']= df.values.tolist()\n",
    "trainData_d7 = trainData_d7.drop(['Labels'], axis=1)\n",
    "trainData_d7 = pd.concat([df[\"Labels\"],testData_d7_MWPM, trainData_d7], axis=1, ignore_index=True)\n",
    "trainData_d7.columns = [\"Labels\",\"XSyn\", \"ZSyn\",\"X0\", \"Z1\", \"X2\", \"X3\", \"Z4\", \"X5\", \"X6\", \"Z7\", \"X8\", \"Z9\", \"Z10\", \"X11\", \"Z12\", \"X13\", \"Z14\", \"X15\", \"Z16\", \"Z17\", \"X18\", \"Z19\",\"X20\", \"Z21\", \"X22\", \"Z23\", \"Z24\", \"X25\", \"Z26\", \"X27\", \"Z28\", \"X29\", \"Z30\", \"Z31\", \"X32\", \"Z33\", \"X34\", \"Z35\", \"X36\", \"Z37\", \"Z38\", \"X39\", \"X40\", \"Z41\", \"X42\", \"X43\", \"Z44\", \"X45\", \"X46\", \"Z47\"]\n",
    "\n",
    "#######################################################################################\n",
    "\n",
    "trainData_d5 = pd.read_csv(\"depth5_all_combos.csv\")\n",
    "trainData_d5 = trainData_d5.applymap(lambda x: add_noise(x,.01)) #was .05\n",
    "#These four lines remove duplicates\n",
    "trainData_d5['Labels'] = trainData_d5['Labels'].map(lambda x: create_list_from_string(x))\n",
    "trainData_d5['Labels'] = trainData_d5['Labels'].astype(str)\n",
    "trainData_d5 = trainData_d5.drop_duplicates('Labels', keep='first', ignore_index=True)\n",
    "trainData_d5['Labels'] = trainData_d5['Labels'].map(lambda x: create_list_from_string(x))\n",
    "\n",
    "\n",
    "testData_d5_MWPM = graph_with_errs_d5(trainData_d5)\n",
    "\n",
    "\n",
    "#transforms the data to encoding for ML\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(trainData_d5['Labels'])\n",
    "df = pd.DataFrame(mlb.transform(trainData_d5['Labels']))\n",
    "df['Labels']= df.values.tolist()\n",
    "trainData_d5 = trainData_d5.drop(['Labels'], axis=1)\n",
    "trainData_d5 = pd.concat([df[\"Labels\"], testData_d5_MWPM, trainData_d5], axis=1, ignore_index=True)\n",
    "trainData_d5.columns = [\"Labels\",\"XSyn\", \"ZSyn\",\"X0\",\"Z1\",\"X2\",\"X3\",\"Z4\",\"X5\",\"Z6\",\"Z7\",\"X8\",\"Z9\",\"X10\",\"Z11\",\"Z12\",\"X13\",\"Z14\",\"X15\",\"Z16\",\"Z17\",\"X18\",\"X19\",\"Z20\",\"X21\",\"X22\",\"Z23\"]\n",
    "#########################################################################################\n",
    "\n",
    "#Has no duplicates, small enough to check manually\n",
    "trainData_d3 = pd.read_csv(\"depth3_all_combos.csv\")\n",
    "\n",
    "trainData_d3[\"Labels\"] = trainData_d3['Labels'].map(lambda x: create_list_from_string(x))\n",
    "trainData_d3 = trainData_d3.applymap(lambda x: add_noise(x,.01))\n",
    "\n",
    "testData_d3_MWPM = graph_with_errs_d3(trainData_d3)\n",
    "\n",
    "mlb_d3 = MultiLabelBinarizer()\n",
    "mlb_d3.fit(trainData_d3[\"Labels\"])\n",
    "df = pd.DataFrame(mlb_d3.transform(trainData_d3['Labels']))\n",
    "df['Labels']= df.values.tolist()\n",
    "trainData_d3 = trainData_d3.drop(['Labels'], axis=1)\n",
    "trainData_d3 = pd.concat([df['Labels'], testData_d3_MWPM, trainData_d3], axis=1, ignore_index=True)\n",
    "trainData_d3.columns = [\"Labels\",\"XSyn\", \"ZSyn\", \"X0\", \"Z1\", \"X2\", \"Z3\", \"Z4\", \"X5\", \"X6\", \"Z7\"]\n",
    "#########################################################################################\n",
    "y_d3 = trainData_d3[\"Labels\"]\n",
    "x_d3 = trainData_d3.drop([\"Labels\"], axis=1)\n",
    "\n",
    "y_d5 = trainData_d5[\"Labels\"] \n",
    "x_d5 = trainData_d5.drop([\"Labels\"], axis=1) \n",
    "\n",
    "y_d7 = trainData_d7[\"Labels\"]\n",
    "x_d7 = trainData_d7.drop([\"Labels\"], axis=1)\n",
    "\n",
    "\n",
    "x_d3 = x_d3.replace([-1], 0)\n",
    "x_d5 = x_d5.replace([-1], 0)\n",
    "x_d7 = x_d7.replace([-1], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_d7 = trainData_d7[\"Labels\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions for creating lookup tables here:\n",
    "from collections import defaultdict\n",
    "from math import log2\n",
    "\n",
    "def generateAllBinaryStrings(n, arr, i, lookup):  \n",
    "    if i == n: \n",
    "        lookup.setBitStringArray(arr, n)  \n",
    "        return\n",
    "      \n",
    "    # First assign \"0\" at ith position  \n",
    "    # and try for all other permutations  \n",
    "    # for remaining positions  \n",
    "    arr[i] = 0\n",
    "    generateAllBinaryStrings(n, arr, i + 1, lookup)  \n",
    "  \n",
    "    # And then assign \"1\" at ith position  , \n",
    "    # and try for all other permutations  \n",
    "    # for remaining positions  \n",
    "    arr[i] = 1\n",
    "    generateAllBinaryStrings(n, arr, i + 1, lookup)\n",
    "\n",
    "class lookup_decoder:\n",
    "    \n",
    "    def __init__(self, depth):\n",
    "        #self.lookupTable = defaultdict()\n",
    "        self.lookupTable = {}\n",
    "        self.distributions = {}\n",
    "        self.depth = depth\n",
    "        #generating all possible syndrome observations\n",
    "        #arr = [None] * (depth**2 - 1)\n",
    "        #generateAllBinaryStrings((depth**2 - 1), arr, 0, self)\n",
    "        \n",
    "    def setBitStringArray(self, arr, n): \n",
    "        new_str = \"\"\n",
    "        for i in range(0, n):  \n",
    "            new_str += str(arr[i])\n",
    "        self.lookupTable.update({new_str:defaultdict()})  \n",
    "        \n",
    "    def update_table (self, syndrome, phys_errs):\n",
    "        #all the keys are made in the init, so simply update the physical error combinations for the given syndrome\n",
    "        if syndrome not in self.lookupTable:\n",
    "            self.lookupTable[syndrome] = {}\n",
    "            self.lookupTable[syndrome][phys_errs] = 1\n",
    "            return\n",
    "        \n",
    "        if phys_errs not in self.lookupTable[syndrome]:\n",
    "            self.lookupTable[syndrome].update({phys_errs: 1})\n",
    "        else:\n",
    "            self.lookupTable[syndrome][phys_errs] += 1\n",
    "     \n",
    "    def get_probable_error(self, syndrome):\n",
    "        return_key = []\n",
    "\n",
    "        if syndrome not in self.lookupTable.keys():\n",
    "            for i in range(2* int(self.depth**2) + 1):\n",
    "                return_key.append(0)\n",
    "            return return_key\n",
    "        \n",
    "        key, value = max(self.lookupTable[syndrome].items(), key=lambda x:x[1])\n",
    "        \n",
    "        for character in key:\n",
    "            if character == '0' or character == '1':\n",
    "                return_key.append(int(character))\n",
    "                \n",
    "        return return_key\n",
    "        \n",
    "    def make_distribution_graph(self, syn):\n",
    "        \n",
    "        plt.bar(list(self.lookupTable[syn].keys()), self.lookupTable[syn].values(), color='g')\n",
    "        plt.show()\n",
    "        \n",
    "    def syndrome_count_graph(self):\n",
    "        graph_dict = {}\n",
    "        for syn in self.lookupTable:\n",
    "            graph_dict.update({syn:sum(self.lookupTable[syn].values())})\n",
    "        plt.ylim((0,4))\n",
    "        plt.bar(graph_dict.keys(), graph_dict.values())\n",
    "        plt.show()\n",
    "        \n",
    "    \n",
    "    def length_of_lookup(self):\n",
    "        print(len(self.lookupTable))\n",
    "        \n",
    "            \n",
    "    def get_entropies(self):\n",
    "        entropies = {}\n",
    "        for syn in self.lookupTable:\n",
    "            total = sum(self.lookupTable[syn].values())\n",
    "            h = 0\n",
    "            for key in self.lookupTable[syn]:\n",
    "                p = self.lookupTable[syn][key]/total\n",
    "                h += p+log2(p)\n",
    "            entropies[syn] = -h\n",
    "        print(entropies)\n",
    "            \n",
    "    def get_syndromes(self):\n",
    "        return self.lookupTable.keys()\n",
    "                     \n",
    "    def print_lookup(self):\n",
    "        for syn in self.lookupTable:\n",
    "            print(self.lookupTable[syn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_plut(table, data_x, data_y):\n",
    "    i = 0\n",
    "    for index, x in enumerate(data_x):\n",
    "        syn = \"\".join([str(i) for i in x])\n",
    "        syn = syn.replace(\".0\",\"\")\n",
    "        labels = np.array2string(np.array(data_y[i]), precision=1, separator='',suppress_small=True).replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "        table.update_table(syn, labels)\n",
    "        i+=1\n",
    "    return table\n",
    "\n",
    "\n",
    "def test_plut(table, test_set):\n",
    "    predictions_lookup = []\n",
    "    for index, x in enumerate(test_set):\n",
    "        syn_x = \"\".join([str(i) for i in x])\n",
    "        syn_x = syn_x.replace(\".0\",\"\")\n",
    "        predictions_lookup.append(table.get_probable_error(syn_x))\n",
    "    return np.array(predictions_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_FFNN_model_DepthThree(depth):\n",
    "    model = Sequential()\n",
    "    layers = 2\n",
    "    #input layer\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    \n",
    "    #hidden layers go here\n",
    "    for i in range(layers):\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        \n",
    "    model.add(Dense(19 , activation='sigmoid'))\n",
    "    model.compile(loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.SGD(learning_rate=0.05),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "#make any needed changes here\n",
    "def compile_FFNN_model_DepthFive(depth):\n",
    "    model = Sequential()\n",
    "    layers = 4\n",
    "    \n",
    "    #input layer\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    \n",
    "    #hidden layers go here\n",
    "    for i in range(layers):\n",
    "        model.add(Dense(240, activation='relu'))\n",
    "    \n",
    "    #model.add(Dense((2*depth**2) , activation='sigmoid'))\n",
    "    model.add(Dense(51, activation='sigmoid'))\n",
    "    model.compile(loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.SGD(learning_rate=0.05),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "#make any needed changes here\n",
    "def compile_FFNN_model_DepthSeven(depth):\n",
    "    model = Sequential()\n",
    "    layers = 4\n",
    "    \n",
    "    #input layer\n",
    "    model.add(Dense(48, activation='relu'))\n",
    "    \n",
    "    #hidden layers go here\n",
    "    for i in range(layers):\n",
    "        model.add(Dense(400, activation='relu'))\n",
    "    \n",
    "    #model.add(Dense((2*depth**2) , activation='sigmoid'))\n",
    "    model.add(Dense(99, activation='sigmoid'))\n",
    "    model.compile(loss=keras.losses.binary_crossentropy, optimizer=tf.keras.optimizers.SGD(learning_rate=0.05),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Epoch 1/200\n",
      "1/1 [==============================] - 0s 411ms/step - loss: 0.7005 - accuracy: 0.0588 - val_loss: 0.6933 - val_accuracy: 0.2000\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6993 - accuracy: 0.0588 - val_loss: 0.6924 - val_accuracy: 0.2000\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6981 - accuracy: 0.0588 - val_loss: 0.6915 - val_accuracy: 0.2000\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6970 - accuracy: 0.0588 - val_loss: 0.6906 - val_accuracy: 0.2000\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6959 - accuracy: 0.0588 - val_loss: 0.6897 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6948 - accuracy: 0.0588 - val_loss: 0.6888 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6937 - accuracy: 0.0588 - val_loss: 0.6880 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6926 - accuracy: 0.0588 - val_loss: 0.6871 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6916 - accuracy: 0.0588 - val_loss: 0.6863 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6905 - accuracy: 0.0588 - val_loss: 0.6854 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6895 - accuracy: 0.0588 - val_loss: 0.6846 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6885 - accuracy: 0.0588 - val_loss: 0.6837 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6875 - accuracy: 0.0588 - val_loss: 0.6829 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6865 - accuracy: 0.0588 - val_loss: 0.6821 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6855 - accuracy: 0.0588 - val_loss: 0.6812 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6845 - accuracy: 0.0588 - val_loss: 0.6804 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6835 - accuracy: 0.0588 - val_loss: 0.6796 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6825 - accuracy: 0.0588 - val_loss: 0.6788 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6815 - accuracy: 0.0588 - val_loss: 0.6780 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6806 - accuracy: 0.0588 - val_loss: 0.6772 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6796 - accuracy: 0.0588 - val_loss: 0.6764 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6786 - accuracy: 0.0588 - val_loss: 0.6756 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6777 - accuracy: 0.0588 - val_loss: 0.6748 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6767 - accuracy: 0.0588 - val_loss: 0.6740 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6757 - accuracy: 0.0588 - val_loss: 0.6732 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6748 - accuracy: 0.0588 - val_loss: 0.6723 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6738 - accuracy: 0.0588 - val_loss: 0.6715 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6728 - accuracy: 0.0588 - val_loss: 0.6707 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6719 - accuracy: 0.0588 - val_loss: 0.6699 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6709 - accuracy: 0.0588 - val_loss: 0.6691 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6700 - accuracy: 0.0588 - val_loss: 0.6683 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6690 - accuracy: 0.0588 - val_loss: 0.6675 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6681 - accuracy: 0.0588 - val_loss: 0.6667 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6671 - accuracy: 0.0588 - val_loss: 0.6659 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6662 - accuracy: 0.0588 - val_loss: 0.6650 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6652 - accuracy: 0.0588 - val_loss: 0.6642 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6642 - accuracy: 0.0588 - val_loss: 0.6634 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6633 - accuracy: 0.0588 - val_loss: 0.6626 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6623 - accuracy: 0.1176 - val_loss: 0.6618 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6614 - accuracy: 0.1176 - val_loss: 0.6609 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6604 - accuracy: 0.1176 - val_loss: 0.6601 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6594 - accuracy: 0.1176 - val_loss: 0.6593 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6585 - accuracy: 0.1176 - val_loss: 0.6584 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6575 - accuracy: 0.1176 - val_loss: 0.6576 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6565 - accuracy: 0.0588 - val_loss: 0.6568 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6556 - accuracy: 0.0588 - val_loss: 0.6559 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6546 - accuracy: 0.0588 - val_loss: 0.6551 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6535 - accuracy: 0.0588 - val_loss: 0.6542 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6525 - accuracy: 0.0588 - val_loss: 0.6533 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6515 - accuracy: 0.0588 - val_loss: 0.6525 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.6505 - accuracy: 0.0588 - val_loss: 0.6516 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6494 - accuracy: 0.0588 - val_loss: 0.6507 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6484 - accuracy: 0.0588 - val_loss: 0.6499 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6473 - accuracy: 0.0588 - val_loss: 0.6490 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6463 - accuracy: 0.0588 - val_loss: 0.6481 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6452 - accuracy: 0.0588 - val_loss: 0.6472 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6441 - accuracy: 0.0588 - val_loss: 0.6463 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6431 - accuracy: 0.0588 - val_loss: 0.6454 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6420 - accuracy: 0.0588 - val_loss: 0.6445 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6409 - accuracy: 0.0588 - val_loss: 0.6435 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6398 - accuracy: 0.0588 - val_loss: 0.6426 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6387 - accuracy: 0.0588 - val_loss: 0.6416 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6376 - accuracy: 0.0588 - val_loss: 0.6407 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6364 - accuracy: 0.0588 - val_loss: 0.6397 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6353 - accuracy: 0.0588 - val_loss: 0.6387 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6341 - accuracy: 0.0588 - val_loss: 0.6378 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.6330 - accuracy: 0.0588 - val_loss: 0.6368 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6318 - accuracy: 0.0588 - val_loss: 0.6358 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6306 - accuracy: 0.0588 - val_loss: 0.6348 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6294 - accuracy: 0.1176 - val_loss: 0.6338 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6282 - accuracy: 0.1176 - val_loss: 0.6327 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6270 - accuracy: 0.1176 - val_loss: 0.6317 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6258 - accuracy: 0.1176 - val_loss: 0.6306 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.6246 - accuracy: 0.1176 - val_loss: 0.6295 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6233 - accuracy: 0.1176 - val_loss: 0.6284 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6220 - accuracy: 0.1176 - val_loss: 0.6273 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.6208 - accuracy: 0.1176 - val_loss: 0.6262 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6195 - accuracy: 0.1176 - val_loss: 0.6251 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6182 - accuracy: 0.1176 - val_loss: 0.6240 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6168 - accuracy: 0.1176 - val_loss: 0.6228 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6155 - accuracy: 0.1176 - val_loss: 0.6217 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6141 - accuracy: 0.1176 - val_loss: 0.6205 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6128 - accuracy: 0.1176 - val_loss: 0.6193 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6114 - accuracy: 0.1176 - val_loss: 0.6181 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6100 - accuracy: 0.1176 - val_loss: 0.6168 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6085 - accuracy: 0.1176 - val_loss: 0.6156 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6071 - accuracy: 0.1176 - val_loss: 0.6143 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6056 - accuracy: 0.1176 - val_loss: 0.6131 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6041 - accuracy: 0.1176 - val_loss: 0.6118 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6026 - accuracy: 0.1176 - val_loss: 0.6105 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6011 - accuracy: 0.1176 - val_loss: 0.6091 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5996 - accuracy: 0.1176 - val_loss: 0.6078 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5980 - accuracy: 0.1176 - val_loss: 0.6064 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5964 - accuracy: 0.1176 - val_loss: 0.6050 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5948 - accuracy: 0.1176 - val_loss: 0.6036 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5931 - accuracy: 0.1176 - val_loss: 0.6022 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5915 - accuracy: 0.1176 - val_loss: 0.6007 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5898 - accuracy: 0.1176 - val_loss: 0.5992 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5880 - accuracy: 0.1176 - val_loss: 0.5977 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5863 - accuracy: 0.1176 - val_loss: 0.5962 - val_accuracy: 0.0000e+00\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5845 - accuracy: 0.1176 - val_loss: 0.5947 - val_accuracy: 0.0000e+00\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5827 - accuracy: 0.1176 - val_loss: 0.5931 - val_accuracy: 0.0000e+00\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5809 - accuracy: 0.1176 - val_loss: 0.5915 - val_accuracy: 0.0000e+00\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5791 - accuracy: 0.1176 - val_loss: 0.5899 - val_accuracy: 0.0000e+00\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5772 - accuracy: 0.1176 - val_loss: 0.5883 - val_accuracy: 0.0000e+00\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5753 - accuracy: 0.1176 - val_loss: 0.5866 - val_accuracy: 0.0000e+00\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5733 - accuracy: 0.1176 - val_loss: 0.5849 - val_accuracy: 0.0000e+00\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5713 - accuracy: 0.1176 - val_loss: 0.5832 - val_accuracy: 0.0000e+00\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.5693 - accuracy: 0.1176 - val_loss: 0.5814 - val_accuracy: 0.0000e+00\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.5673 - accuracy: 0.1176 - val_loss: 0.5797 - val_accuracy: 0.0000e+00\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5652 - accuracy: 0.1176 - val_loss: 0.5778 - val_accuracy: 0.0000e+00\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5631 - accuracy: 0.1176 - val_loss: 0.5760 - val_accuracy: 0.0000e+00\n",
      "Epoch 113/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5610 - accuracy: 0.1176 - val_loss: 0.5741 - val_accuracy: 0.0000e+00\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5588 - accuracy: 0.1176 - val_loss: 0.5722 - val_accuracy: 0.0000e+00\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5566 - accuracy: 0.1176 - val_loss: 0.5703 - val_accuracy: 0.0000e+00\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5543 - accuracy: 0.1176 - val_loss: 0.5683 - val_accuracy: 0.0000e+00\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5520 - accuracy: 0.1176 - val_loss: 0.5663 - val_accuracy: 0.0000e+00\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5497 - accuracy: 0.1176 - val_loss: 0.5643 - val_accuracy: 0.0000e+00\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.5473 - accuracy: 0.1176 - val_loss: 0.5622 - val_accuracy: 0.0000e+00\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5449 - accuracy: 0.1176 - val_loss: 0.5601 - val_accuracy: 0.0000e+00\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5425 - accuracy: 0.1176 - val_loss: 0.5580 - val_accuracy: 0.0000e+00\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5400 - accuracy: 0.1176 - val_loss: 0.5558 - val_accuracy: 0.0000e+00\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5374 - accuracy: 0.1176 - val_loss: 0.5536 - val_accuracy: 0.0000e+00\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5349 - accuracy: 0.1176 - val_loss: 0.5513 - val_accuracy: 0.0000e+00\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5322 - accuracy: 0.1176 - val_loss: 0.5490 - val_accuracy: 0.0000e+00\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5296 - accuracy: 0.1176 - val_loss: 0.5467 - val_accuracy: 0.0000e+00\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5269 - accuracy: 0.1176 - val_loss: 0.5443 - val_accuracy: 0.0000e+00\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5241 - accuracy: 0.1176 - val_loss: 0.5419 - val_accuracy: 0.0000e+00\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5213 - accuracy: 0.1176 - val_loss: 0.5395 - val_accuracy: 0.0000e+00\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5184 - accuracy: 0.1176 - val_loss: 0.5371 - val_accuracy: 0.0000e+00\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5155 - accuracy: 0.1176 - val_loss: 0.5345 - val_accuracy: 0.0000e+00\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5125 - accuracy: 0.1176 - val_loss: 0.5320 - val_accuracy: 0.0000e+00\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5095 - accuracy: 0.1176 - val_loss: 0.5294 - val_accuracy: 0.0000e+00\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5064 - accuracy: 0.1176 - val_loss: 0.5267 - val_accuracy: 0.0000e+00\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5032 - accuracy: 0.1176 - val_loss: 0.5240 - val_accuracy: 0.0000e+00\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5000 - accuracy: 0.1176 - val_loss: 0.5213 - val_accuracy: 0.0000e+00\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4968 - accuracy: 0.1176 - val_loss: 0.5186 - val_accuracy: 0.0000e+00\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4935 - accuracy: 0.1176 - val_loss: 0.5158 - val_accuracy: 0.0000e+00\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.4901 - accuracy: 0.1176 - val_loss: 0.5129 - val_accuracy: 0.0000e+00\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4867 - accuracy: 0.1176 - val_loss: 0.5100 - val_accuracy: 0.0000e+00\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4833 - accuracy: 0.1176 - val_loss: 0.5071 - val_accuracy: 0.0000e+00\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4798 - accuracy: 0.1176 - val_loss: 0.5041 - val_accuracy: 0.0000e+00\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4762 - accuracy: 0.1176 - val_loss: 0.5011 - val_accuracy: 0.0000e+00\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4726 - accuracy: 0.1176 - val_loss: 0.4981 - val_accuracy: 0.0000e+00\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4690 - accuracy: 0.1176 - val_loss: 0.4950 - val_accuracy: 0.0000e+00\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4653 - accuracy: 0.1176 - val_loss: 0.4919 - val_accuracy: 0.0000e+00\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4616 - accuracy: 0.1176 - val_loss: 0.4888 - val_accuracy: 0.0000e+00\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4578 - accuracy: 0.1176 - val_loss: 0.4856 - val_accuracy: 0.0000e+00\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4540 - accuracy: 0.0588 - val_loss: 0.4824 - val_accuracy: 0.0000e+00\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4501 - accuracy: 0.0588 - val_loss: 0.4791 - val_accuracy: 0.0000e+00\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4462 - accuracy: 0.0588 - val_loss: 0.4759 - val_accuracy: 0.0000e+00\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4423 - accuracy: 0.0588 - val_loss: 0.4726 - val_accuracy: 0.0000e+00\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4383 - accuracy: 0.0588 - val_loss: 0.4693 - val_accuracy: 0.0000e+00\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4343 - accuracy: 0.0588 - val_loss: 0.4659 - val_accuracy: 0.0000e+00\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4303 - accuracy: 0.0588 - val_loss: 0.4626 - val_accuracy: 0.0000e+00\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4262 - accuracy: 0.0588 - val_loss: 0.4592 - val_accuracy: 0.0000e+00\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4221 - accuracy: 0.0588 - val_loss: 0.4558 - val_accuracy: 0.0000e+00\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4181 - accuracy: 0.0588 - val_loss: 0.4524 - val_accuracy: 0.0000e+00\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4139 - accuracy: 0.0588 - val_loss: 0.4490 - val_accuracy: 0.0000e+00\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4098 - accuracy: 0.0588 - val_loss: 0.4456 - val_accuracy: 0.0000e+00\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4057 - accuracy: 0.0588 - val_loss: 0.4422 - val_accuracy: 0.0000e+00\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4015 - accuracy: 0.0588 - val_loss: 0.4388 - val_accuracy: 0.0000e+00\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.3974 - accuracy: 0.0588 - val_loss: 0.4354 - val_accuracy: 0.0000e+00\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.3932 - accuracy: 0.0588 - val_loss: 0.4320 - val_accuracy: 0.0000e+00\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.3891 - accuracy: 0.0588 - val_loss: 0.4286 - val_accuracy: 0.0000e+00\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3850 - accuracy: 0.0588 - val_loss: 0.4253 - val_accuracy: 0.0000e+00\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.3808 - accuracy: 0.0588 - val_loss: 0.4219 - val_accuracy: 0.0000e+00\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.3767 - accuracy: 0.0588 - val_loss: 0.4186 - val_accuracy: 0.0000e+00\n",
      "Epoch 169/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step - loss: 0.3726 - accuracy: 0.0588 - val_loss: 0.4153 - val_accuracy: 0.0000e+00\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3685 - accuracy: 0.0588 - val_loss: 0.4120 - val_accuracy: 0.0000e+00\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.3645 - accuracy: 0.0588 - val_loss: 0.4088 - val_accuracy: 0.0000e+00\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.3605 - accuracy: 0.0588 - val_loss: 0.4056 - val_accuracy: 0.0000e+00\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3565 - accuracy: 0.0588 - val_loss: 0.4025 - val_accuracy: 0.0000e+00\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.3526 - accuracy: 0.0588 - val_loss: 0.3994 - val_accuracy: 0.0000e+00\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3487 - accuracy: 0.0588 - val_loss: 0.3963 - val_accuracy: 0.0000e+00\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.3448 - accuracy: 0.0588 - val_loss: 0.3933 - val_accuracy: 0.0000e+00\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.3410 - accuracy: 0.0588 - val_loss: 0.3903 - val_accuracy: 0.0000e+00\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.3373 - accuracy: 0.0588 - val_loss: 0.3874 - val_accuracy: 0.0000e+00\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.3336 - accuracy: 0.0588 - val_loss: 0.3845 - val_accuracy: 0.0000e+00\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.3300 - accuracy: 0.0588 - val_loss: 0.3817 - val_accuracy: 0.0000e+00\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3264 - accuracy: 0.0588 - val_loss: 0.3790 - val_accuracy: 0.0000e+00\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3229 - accuracy: 0.0588 - val_loss: 0.3763 - val_accuracy: 0.0000e+00\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.3195 - accuracy: 0.0588 - val_loss: 0.3737 - val_accuracy: 0.0000e+00\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.3161 - accuracy: 0.0588 - val_loss: 0.3712 - val_accuracy: 0.0000e+00\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.3128 - accuracy: 0.0588 - val_loss: 0.3688 - val_accuracy: 0.0000e+00\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.3096 - accuracy: 0.0588 - val_loss: 0.3664 - val_accuracy: 0.0000e+00\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.3065 - accuracy: 0.0588 - val_loss: 0.3641 - val_accuracy: 0.0000e+00\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.3034 - accuracy: 0.0588 - val_loss: 0.3618 - val_accuracy: 0.0000e+00\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.3005 - accuracy: 0.0588 - val_loss: 0.3596 - val_accuracy: 0.0000e+00\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2976 - accuracy: 0.0588 - val_loss: 0.3576 - val_accuracy: 0.0000e+00\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.2947 - accuracy: 0.0588 - val_loss: 0.3555 - val_accuracy: 0.0000e+00\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2920 - accuracy: 0.0588 - val_loss: 0.3536 - val_accuracy: 0.0000e+00\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2893 - accuracy: 0.0588 - val_loss: 0.3517 - val_accuracy: 0.0000e+00\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2868 - accuracy: 0.0000e+00 - val_loss: 0.3499 - val_accuracy: 0.0000e+00\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2843 - accuracy: 0.0000e+00 - val_loss: 0.3482 - val_accuracy: 0.0000e+00\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2819 - accuracy: 0.0000e+00 - val_loss: 0.3466 - val_accuracy: 0.0000e+00\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2795 - accuracy: 0.0000e+00 - val_loss: 0.3450 - val_accuracy: 0.0000e+00\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.2773 - accuracy: 0.0000e+00 - val_loss: 0.3435 - val_accuracy: 0.0000e+00\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2751 - accuracy: 0.0000e+00 - val_loss: 0.3421 - val_accuracy: 0.0000e+00\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2730 - accuracy: 0.0000e+00 - val_loss: 0.3407 - val_accuracy: 0.0000e+00\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-bc011529323e>:135: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "  tprs.append(interp(mean_fpr, fpr, tpr))\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 379ms/step - loss: 0.7027 - accuracy: 0.0000e+00 - val_loss: 0.7063 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.7015 - accuracy: 0.0000e+00 - val_loss: 0.7051 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7003 - accuracy: 0.0000e+00 - val_loss: 0.7038 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6990 - accuracy: 0.0000e+00 - val_loss: 0.7026 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6978 - accuracy: 0.0000e+00 - val_loss: 0.7013 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6966 - accuracy: 0.0000e+00 - val_loss: 0.7001 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6954 - accuracy: 0.0000e+00 - val_loss: 0.6989 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6942 - accuracy: 0.0000e+00 - val_loss: 0.6977 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6930 - accuracy: 0.0000e+00 - val_loss: 0.6965 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6919 - accuracy: 0.0000e+00 - val_loss: 0.6953 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6907 - accuracy: 0.0000e+00 - val_loss: 0.6941 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6896 - accuracy: 0.0000e+00 - val_loss: 0.6929 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6885 - accuracy: 0.0000e+00 - val_loss: 0.6918 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6873 - accuracy: 0.0000e+00 - val_loss: 0.6906 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6862 - accuracy: 0.0000e+00 - val_loss: 0.6895 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6851 - accuracy: 0.0000e+00 - val_loss: 0.6884 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6840 - accuracy: 0.0000e+00 - val_loss: 0.6873 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6830 - accuracy: 0.0000e+00 - val_loss: 0.6863 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6819 - accuracy: 0.0000e+00 - val_loss: 0.6852 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6809 - accuracy: 0.0000e+00 - val_loss: 0.6842 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6799 - accuracy: 0.0000e+00 - val_loss: 0.6832 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6788 - accuracy: 0.0000e+00 - val_loss: 0.6821 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6778 - accuracy: 0.0000e+00 - val_loss: 0.6811 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6768 - accuracy: 0.0000e+00 - val_loss: 0.6801 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.6758 - accuracy: 0.0000e+00 - val_loss: 0.6791 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6748 - accuracy: 0.0000e+00 - val_loss: 0.6781 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6738 - accuracy: 0.0000e+00 - val_loss: 0.6771 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6728 - accuracy: 0.0000e+00 - val_loss: 0.6761 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6718 - accuracy: 0.0000e+00 - val_loss: 0.6750 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6708 - accuracy: 0.0000e+00 - val_loss: 0.6740 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6698 - accuracy: 0.0000e+00 - val_loss: 0.6730 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6687 - accuracy: 0.0000e+00 - val_loss: 0.6720 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6678 - accuracy: 0.0000e+00 - val_loss: 0.6710 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6668 - accuracy: 0.0000e+00 - val_loss: 0.6700 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6658 - accuracy: 0.0000e+00 - val_loss: 0.6691 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6649 - accuracy: 0.0000e+00 - val_loss: 0.6681 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6639 - accuracy: 0.0000e+00 - val_loss: 0.6671 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6630 - accuracy: 0.0000e+00 - val_loss: 0.6662 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6620 - accuracy: 0.0000e+00 - val_loss: 0.6652 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6611 - accuracy: 0.0000e+00 - val_loss: 0.6642 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6601 - accuracy: 0.0000e+00 - val_loss: 0.6633 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6591 - accuracy: 0.0000e+00 - val_loss: 0.6623 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6582 - accuracy: 0.0000e+00 - val_loss: 0.6614 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6572 - accuracy: 0.0000e+00 - val_loss: 0.6604 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6562 - accuracy: 0.0000e+00 - val_loss: 0.6594 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6553 - accuracy: 0.0000e+00 - val_loss: 0.6585 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6543 - accuracy: 0.0000e+00 - val_loss: 0.6575 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6533 - accuracy: 0.0000e+00 - val_loss: 0.6566 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6524 - accuracy: 0.0000e+00 - val_loss: 0.6556 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6514 - accuracy: 0.0000e+00 - val_loss: 0.6546 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6504 - accuracy: 0.0000e+00 - val_loss: 0.6536 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6494 - accuracy: 0.0000e+00 - val_loss: 0.6527 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6484 - accuracy: 0.0000e+00 - val_loss: 0.6517 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6474 - accuracy: 0.0000e+00 - val_loss: 0.6507 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6464 - accuracy: 0.0000e+00 - val_loss: 0.6497 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6454 - accuracy: 0.0000e+00 - val_loss: 0.6487 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6444 - accuracy: 0.0000e+00 - val_loss: 0.6477 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.6434 - accuracy: 0.0000e+00 - val_loss: 0.6467 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6423 - accuracy: 0.0000e+00 - val_loss: 0.6457 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6413 - accuracy: 0.0000e+00 - val_loss: 0.6446 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6402 - accuracy: 0.0000e+00 - val_loss: 0.6436 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6392 - accuracy: 0.0000e+00 - val_loss: 0.6426 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6381 - accuracy: 0.0000e+00 - val_loss: 0.6415 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6371 - accuracy: 0.0000e+00 - val_loss: 0.6405 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6360 - accuracy: 0.0000e+00 - val_loss: 0.6394 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6349 - accuracy: 0.0000e+00 - val_loss: 0.6384 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6338 - accuracy: 0.0000e+00 - val_loss: 0.6373 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6327 - accuracy: 0.0000e+00 - val_loss: 0.6363 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6317 - accuracy: 0.0000e+00 - val_loss: 0.6352 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6306 - accuracy: 0.0000e+00 - val_loss: 0.6341 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6295 - accuracy: 0.0000e+00 - val_loss: 0.6331 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6284 - accuracy: 0.0000e+00 - val_loss: 0.6320 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6272 - accuracy: 0.0000e+00 - val_loss: 0.6309 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.6261 - accuracy: 0.0000e+00 - val_loss: 0.6298 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6250 - accuracy: 0.0000e+00 - val_loss: 0.6287 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.6238 - accuracy: 0.0000e+00 - val_loss: 0.6275 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.6227 - accuracy: 0.0000e+00 - val_loss: 0.6264 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6215 - accuracy: 0.0000e+00 - val_loss: 0.6253 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6204 - accuracy: 0.0000e+00 - val_loss: 0.6242 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6192 - accuracy: 0.0000e+00 - val_loss: 0.6230 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6180 - accuracy: 0.0000e+00 - val_loss: 0.6218 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6168 - accuracy: 0.0000e+00 - val_loss: 0.6207 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6156 - accuracy: 0.0000e+00 - val_loss: 0.6195 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6144 - accuracy: 0.0000e+00 - val_loss: 0.6183 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6132 - accuracy: 0.0000e+00 - val_loss: 0.6171 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6120 - accuracy: 0.0000e+00 - val_loss: 0.6158 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6107 - accuracy: 0.0000e+00 - val_loss: 0.6146 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6095 - accuracy: 0.0000e+00 - val_loss: 0.6134 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6082 - accuracy: 0.0000e+00 - val_loss: 0.6121 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6070 - accuracy: 0.0000e+00 - val_loss: 0.6109 - val_accuracy: 0.2000\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6057 - accuracy: 0.0000e+00 - val_loss: 0.6096 - val_accuracy: 0.2000\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6044 - accuracy: 0.0000e+00 - val_loss: 0.6083 - val_accuracy: 0.2000\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6031 - accuracy: 0.0000e+00 - val_loss: 0.6070 - val_accuracy: 0.2000\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6018 - accuracy: 0.0000e+00 - val_loss: 0.6057 - val_accuracy: 0.2000\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6005 - accuracy: 0.0000e+00 - val_loss: 0.6044 - val_accuracy: 0.2000\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5992 - accuracy: 0.0000e+00 - val_loss: 0.6031 - val_accuracy: 0.2000\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.5979 - accuracy: 0.0000e+00 - val_loss: 0.6018 - val_accuracy: 0.2000\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5966 - accuracy: 0.0000e+00 - val_loss: 0.6005 - val_accuracy: 0.2000\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5952 - accuracy: 0.0000e+00 - val_loss: 0.5992 - val_accuracy: 0.2000\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.5939 - accuracy: 0.0000e+00 - val_loss: 0.5978 - val_accuracy: 0.2000\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5925 - accuracy: 0.0000e+00 - val_loss: 0.5964 - val_accuracy: 0.2000\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.5911 - accuracy: 0.0000e+00 - val_loss: 0.5951 - val_accuracy: 0.2000\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5897 - accuracy: 0.0000e+00 - val_loss: 0.5937 - val_accuracy: 0.2000\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.5883 - accuracy: 0.0000e+00 - val_loss: 0.5923 - val_accuracy: 0.2000\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5869 - accuracy: 0.0000e+00 - val_loss: 0.5909 - val_accuracy: 0.2000\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.5855 - accuracy: 0.0000e+00 - val_loss: 0.5895 - val_accuracy: 0.2000\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5840 - accuracy: 0.0000e+00 - val_loss: 0.5881 - val_accuracy: 0.2000\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5826 - accuracy: 0.0000e+00 - val_loss: 0.5867 - val_accuracy: 0.2000\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5811 - accuracy: 0.0000e+00 - val_loss: 0.5852 - val_accuracy: 0.2000\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5796 - accuracy: 0.0000e+00 - val_loss: 0.5837 - val_accuracy: 0.2000\n",
      "Epoch 111/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5781 - accuracy: 0.0000e+00 - val_loss: 0.5823 - val_accuracy: 0.2000\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5765 - accuracy: 0.0000e+00 - val_loss: 0.5808 - val_accuracy: 0.2000\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5750 - accuracy: 0.0000e+00 - val_loss: 0.5793 - val_accuracy: 0.2000\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5734 - accuracy: 0.0000e+00 - val_loss: 0.5778 - val_accuracy: 0.2000\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5719 - accuracy: 0.0000e+00 - val_loss: 0.5763 - val_accuracy: 0.2000\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5703 - accuracy: 0.0000e+00 - val_loss: 0.5747 - val_accuracy: 0.2000\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5687 - accuracy: 0.0000e+00 - val_loss: 0.5732 - val_accuracy: 0.2000\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5671 - accuracy: 0.0000e+00 - val_loss: 0.5716 - val_accuracy: 0.2000\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5654 - accuracy: 0.0000e+00 - val_loss: 0.5700 - val_accuracy: 0.2000\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5638 - accuracy: 0.0000e+00 - val_loss: 0.5684 - val_accuracy: 0.2000\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5621 - accuracy: 0.0000e+00 - val_loss: 0.5668 - val_accuracy: 0.2000\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5604 - accuracy: 0.0000e+00 - val_loss: 0.5651 - val_accuracy: 0.2000\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5587 - accuracy: 0.0000e+00 - val_loss: 0.5635 - val_accuracy: 0.2000\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.5570 - accuracy: 0.0000e+00 - val_loss: 0.5618 - val_accuracy: 0.2000\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5552 - accuracy: 0.0000e+00 - val_loss: 0.5601 - val_accuracy: 0.2000\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5535 - accuracy: 0.0000e+00 - val_loss: 0.5584 - val_accuracy: 0.2000\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.5517 - accuracy: 0.0000e+00 - val_loss: 0.5567 - val_accuracy: 0.2000\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5499 - accuracy: 0.0000e+00 - val_loss: 0.5550 - val_accuracy: 0.2000\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.5481 - accuracy: 0.0000e+00 - val_loss: 0.5532 - val_accuracy: 0.2000\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5462 - accuracy: 0.0000e+00 - val_loss: 0.5514 - val_accuracy: 0.2000\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5443 - accuracy: 0.0000e+00 - val_loss: 0.5496 - val_accuracy: 0.2000\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5424 - accuracy: 0.0000e+00 - val_loss: 0.5478 - val_accuracy: 0.2000\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5405 - accuracy: 0.0000e+00 - val_loss: 0.5459 - val_accuracy: 0.2000\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5386 - accuracy: 0.0000e+00 - val_loss: 0.5441 - val_accuracy: 0.2000\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.5366 - accuracy: 0.0000e+00 - val_loss: 0.5422 - val_accuracy: 0.2000\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.5347 - accuracy: 0.0000e+00 - val_loss: 0.5403 - val_accuracy: 0.2000\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5326 - accuracy: 0.0000e+00 - val_loss: 0.5384 - val_accuracy: 0.2000\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5306 - accuracy: 0.0000e+00 - val_loss: 0.5365 - val_accuracy: 0.2000\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5286 - accuracy: 0.0000e+00 - val_loss: 0.5345 - val_accuracy: 0.2000\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5265 - accuracy: 0.0000e+00 - val_loss: 0.5325 - val_accuracy: 0.2000\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5244 - accuracy: 0.0000e+00 - val_loss: 0.5305 - val_accuracy: 0.2000\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5223 - accuracy: 0.0000e+00 - val_loss: 0.5285 - val_accuracy: 0.2000\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5201 - accuracy: 0.0000e+00 - val_loss: 0.5264 - val_accuracy: 0.2000\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5180 - accuracy: 0.0000e+00 - val_loss: 0.5244 - val_accuracy: 0.2000\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5158 - accuracy: 0.0000e+00 - val_loss: 0.5223 - val_accuracy: 0.2000\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5136 - accuracy: 0.0000e+00 - val_loss: 0.5202 - val_accuracy: 0.2000\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5113 - accuracy: 0.0000e+00 - val_loss: 0.5181 - val_accuracy: 0.2000\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5091 - accuracy: 0.0000e+00 - val_loss: 0.5160 - val_accuracy: 0.2000\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5068 - accuracy: 0.0000e+00 - val_loss: 0.5138 - val_accuracy: 0.2000\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5045 - accuracy: 0.0000e+00 - val_loss: 0.5116 - val_accuracy: 0.2000\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5021 - accuracy: 0.0000e+00 - val_loss: 0.5094 - val_accuracy: 0.2000\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4997 - accuracy: 0.0000e+00 - val_loss: 0.5072 - val_accuracy: 0.2000\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4973 - accuracy: 0.0000e+00 - val_loss: 0.5049 - val_accuracy: 0.2000\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4949 - accuracy: 0.0000e+00 - val_loss: 0.5027 - val_accuracy: 0.2000\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4924 - accuracy: 0.0000e+00 - val_loss: 0.5004 - val_accuracy: 0.2000\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4899 - accuracy: 0.0000e+00 - val_loss: 0.4981 - val_accuracy: 0.2000\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4874 - accuracy: 0.0000e+00 - val_loss: 0.4957 - val_accuracy: 0.2000\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4849 - accuracy: 0.0000e+00 - val_loss: 0.4934 - val_accuracy: 0.2000\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4822 - accuracy: 0.0000e+00 - val_loss: 0.4910 - val_accuracy: 0.2000\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4796 - accuracy: 0.0000e+00 - val_loss: 0.4886 - val_accuracy: 0.2000\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4769 - accuracy: 0.0000e+00 - val_loss: 0.4862 - val_accuracy: 0.2000\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4742 - accuracy: 0.0000e+00 - val_loss: 0.4837 - val_accuracy: 0.2000\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4715 - accuracy: 0.0000e+00 - val_loss: 0.4813 - val_accuracy: 0.2000\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4687 - accuracy: 0.0000e+00 - val_loss: 0.4788 - val_accuracy: 0.2000\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4659 - accuracy: 0.0000e+00 - val_loss: 0.4764 - val_accuracy: 0.2000\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4631 - accuracy: 0.0000e+00 - val_loss: 0.4739 - val_accuracy: 0.2000\n",
      "Epoch 167/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4602 - accuracy: 0.0000e+00 - val_loss: 0.4714 - val_accuracy: 0.2000\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4574 - accuracy: 0.0000e+00 - val_loss: 0.4688 - val_accuracy: 0.2000\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4545 - accuracy: 0.0000e+00 - val_loss: 0.4662 - val_accuracy: 0.2000\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4515 - accuracy: 0.0000e+00 - val_loss: 0.4636 - val_accuracy: 0.2000\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4485 - accuracy: 0.0000e+00 - val_loss: 0.4610 - val_accuracy: 0.2000\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4455 - accuracy: 0.0000e+00 - val_loss: 0.4583 - val_accuracy: 0.2000\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.4425 - accuracy: 0.0000e+00 - val_loss: 0.4556 - val_accuracy: 0.2000\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4395 - accuracy: 0.0000e+00 - val_loss: 0.4529 - val_accuracy: 0.2000\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4364 - accuracy: 0.0000e+00 - val_loss: 0.4502 - val_accuracy: 0.2000\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.4333 - accuracy: 0.0000e+00 - val_loss: 0.4475 - val_accuracy: 0.2000\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.4302 - accuracy: 0.0000e+00 - val_loss: 0.4447 - val_accuracy: 0.2000\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.4270 - accuracy: 0.0000e+00 - val_loss: 0.4420 - val_accuracy: 0.2000\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.4239 - accuracy: 0.0000e+00 - val_loss: 0.4393 - val_accuracy: 0.2000\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4208 - accuracy: 0.0000e+00 - val_loss: 0.4365 - val_accuracy: 0.2000\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.4176 - accuracy: 0.0000e+00 - val_loss: 0.4338 - val_accuracy: 0.2000\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4144 - accuracy: 0.0000e+00 - val_loss: 0.4310 - val_accuracy: 0.2000\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4113 - accuracy: 0.0000e+00 - val_loss: 0.4283 - val_accuracy: 0.2000\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4081 - accuracy: 0.0000e+00 - val_loss: 0.4256 - val_accuracy: 0.2000\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4049 - accuracy: 0.0000e+00 - val_loss: 0.4228 - val_accuracy: 0.2000\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4017 - accuracy: 0.0000e+00 - val_loss: 0.4201 - val_accuracy: 0.2000\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.3986 - accuracy: 0.0000e+00 - val_loss: 0.4174 - val_accuracy: 0.2000\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.3954 - accuracy: 0.0000e+00 - val_loss: 0.4147 - val_accuracy: 0.2000\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3922 - accuracy: 0.0000e+00 - val_loss: 0.4120 - val_accuracy: 0.2000\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.3890 - accuracy: 0.0000e+00 - val_loss: 0.4093 - val_accuracy: 0.2000\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.3859 - accuracy: 0.0000e+00 - val_loss: 0.4067 - val_accuracy: 0.2000\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3827 - accuracy: 0.0000e+00 - val_loss: 0.4040 - val_accuracy: 0.2000\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3795 - accuracy: 0.0000e+00 - val_loss: 0.4014 - val_accuracy: 0.2000\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.3764 - accuracy: 0.0000e+00 - val_loss: 0.3988 - val_accuracy: 0.2000\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.3732 - accuracy: 0.0000e+00 - val_loss: 0.3962 - val_accuracy: 0.2000\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3701 - accuracy: 0.0000e+00 - val_loss: 0.3936 - val_accuracy: 0.2000\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.3670 - accuracy: 0.0000e+00 - val_loss: 0.3911 - val_accuracy: 0.2000\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.3639 - accuracy: 0.0000e+00 - val_loss: 0.3886 - val_accuracy: 0.2000\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.3609 - accuracy: 0.0000e+00 - val_loss: 0.3861 - val_accuracy: 0.2000\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.3578 - accuracy: 0.0000e+00 - val_loss: 0.3836 - val_accuracy: 0.2000\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-bc011529323e>:135: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "  tprs.append(interp(mean_fpr, fpr, tpr))\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 378ms/step - loss: 0.6872 - accuracy: 0.1176 - val_loss: 0.6947 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6853 - accuracy: 0.1176 - val_loss: 0.6932 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6835 - accuracy: 0.1176 - val_loss: 0.6917 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6816 - accuracy: 0.1176 - val_loss: 0.6902 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6798 - accuracy: 0.1176 - val_loss: 0.6888 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6781 - accuracy: 0.1176 - val_loss: 0.6875 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6764 - accuracy: 0.1176 - val_loss: 0.6861 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6748 - accuracy: 0.1176 - val_loss: 0.6847 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6732 - accuracy: 0.1176 - val_loss: 0.6834 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6716 - accuracy: 0.1176 - val_loss: 0.6822 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6700 - accuracy: 0.1176 - val_loss: 0.6809 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6685 - accuracy: 0.1176 - val_loss: 0.6797 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.6670 - accuracy: 0.1176 - val_loss: 0.6784 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6655 - accuracy: 0.1176 - val_loss: 0.6772 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6641 - accuracy: 0.1176 - val_loss: 0.6760 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6626 - accuracy: 0.1176 - val_loss: 0.6747 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6611 - accuracy: 0.1176 - val_loss: 0.6735 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6596 - accuracy: 0.1176 - val_loss: 0.6722 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6581 - accuracy: 0.1176 - val_loss: 0.6709 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6566 - accuracy: 0.1176 - val_loss: 0.6697 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6551 - accuracy: 0.1176 - val_loss: 0.6685 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6536 - accuracy: 0.1176 - val_loss: 0.6672 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6521 - accuracy: 0.1176 - val_loss: 0.6660 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6507 - accuracy: 0.1176 - val_loss: 0.6647 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6492 - accuracy: 0.1176 - val_loss: 0.6635 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6476 - accuracy: 0.1176 - val_loss: 0.6622 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6461 - accuracy: 0.1176 - val_loss: 0.6609 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6445 - accuracy: 0.1176 - val_loss: 0.6596 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6430 - accuracy: 0.11 - 0s 15ms/step - loss: 0.6430 - accuracy: 0.1176 - val_loss: 0.6583 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6414 - accuracy: 0.1176 - val_loss: 0.6570 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6398 - accuracy: 0.1176 - val_loss: 0.6557 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6381 - accuracy: 0.1176 - val_loss: 0.6544 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6365 - accuracy: 0.1176 - val_loss: 0.6531 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6348 - accuracy: 0.1176 - val_loss: 0.6518 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6331 - accuracy: 0.1176 - val_loss: 0.6505 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6314 - accuracy: 0.1176 - val_loss: 0.6491 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6297 - accuracy: 0.1176 - val_loss: 0.6477 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6280 - accuracy: 0.1176 - val_loss: 0.6464 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6262 - accuracy: 0.1176 - val_loss: 0.6450 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6244 - accuracy: 0.1176 - val_loss: 0.6436 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6226 - accuracy: 0.1176 - val_loss: 0.6421 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6208 - accuracy: 0.1176 - val_loss: 0.6407 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6189 - accuracy: 0.1176 - val_loss: 0.6392 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6170 - accuracy: 0.1176 - val_loss: 0.6377 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6151 - accuracy: 0.1176 - val_loss: 0.6362 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6131 - accuracy: 0.1176 - val_loss: 0.6347 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6111 - accuracy: 0.1176 - val_loss: 0.6331 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6091 - accuracy: 0.1176 - val_loss: 0.6315 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6070 - accuracy: 0.1176 - val_loss: 0.6299 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.6049 - accuracy: 0.1176 - val_loss: 0.6282 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6028 - accuracy: 0.1176 - val_loss: 0.6266 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6006 - accuracy: 0.1176 - val_loss: 0.6249 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5984 - accuracy: 0.1176 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5962 - accuracy: 0.1176 - val_loss: 0.6214 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5939 - accuracy: 0.1176 - val_loss: 0.6197 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5916 - accuracy: 0.1176 - val_loss: 0.6179 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5892 - accuracy: 0.1176 - val_loss: 0.6161 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5869 - accuracy: 0.1176 - val_loss: 0.6142 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5844 - accuracy: 0.1176 - val_loss: 0.6123 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5819 - accuracy: 0.1176 - val_loss: 0.6104 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5794 - accuracy: 0.1176 - val_loss: 0.6085 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5768 - accuracy: 0.1176 - val_loss: 0.6065 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5742 - accuracy: 0.1176 - val_loss: 0.6045 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.5715 - accuracy: 0.1176 - val_loss: 0.6024 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5688 - accuracy: 0.1176 - val_loss: 0.6003 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.5660 - accuracy: 0.1176 - val_loss: 0.5982 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5632 - accuracy: 0.1176 - val_loss: 0.5960 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.5603 - accuracy: 0.1176 - val_loss: 0.5938 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5573 - accuracy: 0.1176 - val_loss: 0.5915 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5543 - accuracy: 0.1176 - val_loss: 0.5892 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5513 - accuracy: 0.1176 - val_loss: 0.5869 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5482 - accuracy: 0.1176 - val_loss: 0.5845 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5450 - accuracy: 0.1176 - val_loss: 0.5820 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5417 - accuracy: 0.1176 - val_loss: 0.5796 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5384 - accuracy: 0.1176 - val_loss: 0.5770 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5351 - accuracy: 0.1176 - val_loss: 0.5745 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5316 - accuracy: 0.1176 - val_loss: 0.5719 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5281 - accuracy: 0.1176 - val_loss: 0.5692 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5246 - accuracy: 0.1176 - val_loss: 0.5665 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5209 - accuracy: 0.1176 - val_loss: 0.5637 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5172 - accuracy: 0.1176 - val_loss: 0.5609 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5134 - accuracy: 0.1176 - val_loss: 0.5581 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5095 - accuracy: 0.1176 - val_loss: 0.5551 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5056 - accuracy: 0.1176 - val_loss: 0.5522 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5016 - accuracy: 0.1176 - val_loss: 0.5492 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4975 - accuracy: 0.1176 - val_loss: 0.5461 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4934 - accuracy: 0.1176 - val_loss: 0.5430 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.4892 - accuracy: 0.1176 - val_loss: 0.5398 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4849 - accuracy: 0.1176 - val_loss: 0.5366 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4806 - accuracy: 0.1176 - val_loss: 0.5334 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4762 - accuracy: 0.1176 - val_loss: 0.5301 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4717 - accuracy: 0.1176 - val_loss: 0.5268 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4672 - accuracy: 0.1176 - val_loss: 0.5234 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4626 - accuracy: 0.1176 - val_loss: 0.5200 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4579 - accuracy: 0.1176 - val_loss: 0.5165 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4532 - accuracy: 0.1176 - val_loss: 0.5130 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4485 - accuracy: 0.1176 - val_loss: 0.5095 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.4437 - accuracy: 0.1176 - val_loss: 0.5059 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4388 - accuracy: 0.1176 - val_loss: 0.5023 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4340 - accuracy: 0.1176 - val_loss: 0.4987 - val_accuracy: 0.0000e+00\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4290 - accuracy: 0.1176 - val_loss: 0.4950 - val_accuracy: 0.0000e+00\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4241 - accuracy: 0.1176 - val_loss: 0.4913 - val_accuracy: 0.0000e+00\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.4191 - accuracy: 0.1176 - val_loss: 0.4876 - val_accuracy: 0.0000e+00\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4141 - accuracy: 0.1176 - val_loss: 0.4839 - val_accuracy: 0.0000e+00\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4091 - accuracy: 0.1176 - val_loss: 0.4801 - val_accuracy: 0.0000e+00\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4040 - accuracy: 0.1176 - val_loss: 0.4763 - val_accuracy: 0.0000e+00\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.3989 - accuracy: 0.1176 - val_loss: 0.4725 - val_accuracy: 0.0000e+00\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.3939 - accuracy: 0.1176 - val_loss: 0.4687 - val_accuracy: 0.0000e+00\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.3888 - accuracy: 0.1176 - val_loss: 0.4648 - val_accuracy: 0.0000e+00\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.3837 - accuracy: 0.1176 - val_loss: 0.4610 - val_accuracy: 0.0000e+00\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.3787 - accuracy: 0.1176 - val_loss: 0.4572 - val_accuracy: 0.0000e+00\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3737 - accuracy: 0.1176 - val_loss: 0.4534 - val_accuracy: 0.0000e+00\n",
      "Epoch 113/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step - loss: 0.3686 - accuracy: 0.1176 - val_loss: 0.4496 - val_accuracy: 0.0000e+00\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3637 - accuracy: 0.1176 - val_loss: 0.4459 - val_accuracy: 0.0000e+00\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3587 - accuracy: 0.1176 - val_loss: 0.4421 - val_accuracy: 0.0000e+00\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3538 - accuracy: 0.1176 - val_loss: 0.4384 - val_accuracy: 0.0000e+00\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.3490 - accuracy: 0.1176 - val_loss: 0.4347 - val_accuracy: 0.0000e+00\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.3442 - accuracy: 0.1176 - val_loss: 0.4310 - val_accuracy: 0.0000e+00\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3395 - accuracy: 0.1176 - val_loss: 0.4274 - val_accuracy: 0.0000e+00\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3348 - accuracy: 0.1176 - val_loss: 0.4238 - val_accuracy: 0.0000e+00\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.3302 - accuracy: 0.1176 - val_loss: 0.4203 - val_accuracy: 0.0000e+00\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3257 - accuracy: 0.1176 - val_loss: 0.4169 - val_accuracy: 0.0000e+00\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.3212 - accuracy: 0.1176 - val_loss: 0.4135 - val_accuracy: 0.0000e+00\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.3169 - accuracy: 0.1176 - val_loss: 0.4102 - val_accuracy: 0.0000e+00\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3126 - accuracy: 0.1176 - val_loss: 0.4069 - val_accuracy: 0.0000e+00\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.3085 - accuracy: 0.1176 - val_loss: 0.4037 - val_accuracy: 0.0000e+00\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.3044 - accuracy: 0.1176 - val_loss: 0.4006 - val_accuracy: 0.0000e+00\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3005 - accuracy: 0.1176 - val_loss: 0.3976 - val_accuracy: 0.0000e+00\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2966 - accuracy: 0.1176 - val_loss: 0.3947 - val_accuracy: 0.0000e+00\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2929 - accuracy: 0.1176 - val_loss: 0.3918 - val_accuracy: 0.0000e+00\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2892 - accuracy: 0.1176 - val_loss: 0.3890 - val_accuracy: 0.0000e+00\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.2857 - accuracy: 0.1176 - val_loss: 0.3863 - val_accuracy: 0.0000e+00\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.2823 - accuracy: 0.1176 - val_loss: 0.3837 - val_accuracy: 0.0000e+00\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2790 - accuracy: 0.1176 - val_loss: 0.3812 - val_accuracy: 0.0000e+00\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2758 - accuracy: 0.1176 - val_loss: 0.3787 - val_accuracy: 0.0000e+00\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2727 - accuracy: 0.1176 - val_loss: 0.3763 - val_accuracy: 0.0000e+00\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.2698 - accuracy: 0.1176 - val_loss: 0.3741 - val_accuracy: 0.0000e+00\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2669 - accuracy: 0.1176 - val_loss: 0.3719 - val_accuracy: 0.0000e+00\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2642 - accuracy: 0.11 - 0s 15ms/step - loss: 0.2642 - accuracy: 0.1176 - val_loss: 0.3697 - val_accuracy: 0.0000e+00\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2615 - accuracy: 0.1176 - val_loss: 0.3677 - val_accuracy: 0.0000e+00\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2590 - accuracy: 0.1176 - val_loss: 0.3658 - val_accuracy: 0.0000e+00\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2566 - accuracy: 0.1176 - val_loss: 0.3639 - val_accuracy: 0.0000e+00\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2543 - accuracy: 0.1176 - val_loss: 0.3621 - val_accuracy: 0.0000e+00\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2521 - accuracy: 0.1176 - val_loss: 0.3604 - val_accuracy: 0.0000e+00\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.2499 - accuracy: 0.1176 - val_loss: 0.3588 - val_accuracy: 0.0000e+00\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2479 - accuracy: 0.1176 - val_loss: 0.3572 - val_accuracy: 0.0000e+00\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2460 - accuracy: 0.1176 - val_loss: 0.3557 - val_accuracy: 0.0000e+00\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2442 - accuracy: 0.1176 - val_loss: 0.3543 - val_accuracy: 0.0000e+00\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.2424 - accuracy: 0.1176 - val_loss: 0.3530 - val_accuracy: 0.0000e+00\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2408 - accuracy: 0.1176 - val_loss: 0.3518 - val_accuracy: 0.0000e+00\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2392 - accuracy: 0.1176 - val_loss: 0.3506 - val_accuracy: 0.0000e+00\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2377 - accuracy: 0.1176 - val_loss: 0.3494 - val_accuracy: 0.0000e+00\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.2363 - accuracy: 0.1176 - val_loss: 0.3484 - val_accuracy: 0.0000e+00\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.2349 - accuracy: 0.1176 - val_loss: 0.3474 - val_accuracy: 0.0000e+00\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2336 - accuracy: 0.1176 - val_loss: 0.3464 - val_accuracy: 0.0000e+00\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2324 - accuracy: 0.1176 - val_loss: 0.3455 - val_accuracy: 0.0000e+00\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2312 - accuracy: 0.1176 - val_loss: 0.3446 - val_accuracy: 0.0000e+00\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2301 - accuracy: 0.1176 - val_loss: 0.3438 - val_accuracy: 0.0000e+00\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.2291 - accuracy: 0.1176 - val_loss: 0.3431 - val_accuracy: 0.0000e+00\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2281 - accuracy: 0.1176 - val_loss: 0.3423 - val_accuracy: 0.0000e+00\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.2271 - accuracy: 0.1176 - val_loss: 0.3417 - val_accuracy: 0.0000e+00\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.2262 - accuracy: 0.1176 - val_loss: 0.3410 - val_accuracy: 0.0000e+00\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2254 - accuracy: 0.1176 - val_loss: 0.3404 - val_accuracy: 0.0000e+00\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.2246 - accuracy: 0.1176 - val_loss: 0.3398 - val_accuracy: 0.0000e+00\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.2238 - accuracy: 0.1176 - val_loss: 0.3393 - val_accuracy: 0.0000e+00\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.2231 - accuracy: 0.1176 - val_loss: 0.3388 - val_accuracy: 0.0000e+00\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.2224 - accuracy: 0.1176 - val_loss: 0.3383 - val_accuracy: 0.0000e+00\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2218 - accuracy: 0.1176 - val_loss: 0.3379 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.2211 - accuracy: 0.1176 - val_loss: 0.3374 - val_accuracy: 0.0000e+00\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.2206 - accuracy: 0.1176 - val_loss: 0.3371 - val_accuracy: 0.0000e+00\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.2200 - accuracy: 0.1176 - val_loss: 0.3367 - val_accuracy: 0.0000e+00\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.2195 - accuracy: 0.1176 - val_loss: 0.3364 - val_accuracy: 0.0000e+00\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2190 - accuracy: 0.1176 - val_loss: 0.3360 - val_accuracy: 0.0000e+00\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2185 - accuracy: 0.1176 - val_loss: 0.3357 - val_accuracy: 0.0000e+00\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.2181 - accuracy: 0.1176 - val_loss: 0.3355 - val_accuracy: 0.0000e+00\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.2176 - accuracy: 0.1176 - val_loss: 0.3352 - val_accuracy: 0.0000e+00\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2172 - accuracy: 0.1176 - val_loss: 0.3350 - val_accuracy: 0.0000e+00\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.2168 - accuracy: 0.1176 - val_loss: 0.3347 - val_accuracy: 0.0000e+00\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.2165 - accuracy: 0.1176 - val_loss: 0.3345 - val_accuracy: 0.0000e+00\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2161 - accuracy: 0.0588 - val_loss: 0.3343 - val_accuracy: 0.0000e+00\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2158 - accuracy: 0.0588 - val_loss: 0.3341 - val_accuracy: 0.0000e+00\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.2155 - accuracy: 0.0588 - val_loss: 0.3340 - val_accuracy: 0.0000e+00\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2151 - accuracy: 0.0588 - val_loss: 0.3338 - val_accuracy: 0.0000e+00\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.2149 - accuracy: 0.0588 - val_loss: 0.3337 - val_accuracy: 0.0000e+00\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2146 - accuracy: 0.0588 - val_loss: 0.3336 - val_accuracy: 0.0000e+00\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.2143 - accuracy: 0.1176 - val_loss: 0.3334 - val_accuracy: 0.0000e+00\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.2141 - accuracy: 0.1176 - val_loss: 0.3333 - val_accuracy: 0.0000e+00\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2138 - accuracy: 0.1176 - val_loss: 0.3332 - val_accuracy: 0.0000e+00\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.2136 - accuracy: 0.1176 - val_loss: 0.3331 - val_accuracy: 0.0000e+00\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.2134 - accuracy: 0.1176 - val_loss: 0.3331 - val_accuracy: 0.0000e+00\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.2132 - accuracy: 0.1176 - val_loss: 0.3330 - val_accuracy: 0.0000e+00\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.2130 - accuracy: 0.1176 - val_loss: 0.3329 - val_accuracy: 0.0000e+00\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2128 - accuracy: 0.1176 - val_loss: 0.3329 - val_accuracy: 0.0000e+00\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2126 - accuracy: 0.1176 - val_loss: 0.3328 - val_accuracy: 0.0000e+00\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.2124 - accuracy: 0.1176 - val_loss: 0.3328 - val_accuracy: 0.0000e+00\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2123 - accuracy: 0.1176 - val_loss: 0.3327 - val_accuracy: 0.0000e+00\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.2121 - accuracy: 0.0588 - val_loss: 0.3327 - val_accuracy: 0.0000e+00\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.2119 - accuracy: 0.0588 - val_loss: 0.3327 - val_accuracy: 0.0000e+00\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2118 - accuracy: 0.0588 - val_loss: 0.3326 - val_accuracy: 0.0000e+00\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.2116 - accuracy: 0.0588 - val_loss: 0.3326 - val_accuracy: 0.0000e+00\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-bc011529323e>:135: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "  tprs.append(interp(mean_fpr, fpr, tpr))\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 448ms/step - loss: 0.6917 - accuracy: 0.0556 - val_loss: 0.6915 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6901 - accuracy: 0.0556 - val_loss: 0.6903 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6884 - accuracy: 0.0556 - val_loss: 0.6891 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6868 - accuracy: 0.0556 - val_loss: 0.6879 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6852 - accuracy: 0.0556 - val_loss: 0.6867 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6836 - accuracy: 0.0556 - val_loss: 0.6855 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6819 - accuracy: 0.0556 - val_loss: 0.6843 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6803 - accuracy: 0.0556 - val_loss: 0.6831 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.6786 - accuracy: 0.0556 - val_loss: 0.6818 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6769 - accuracy: 0.0556 - val_loss: 0.6806 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6753 - accuracy: 0.0556 - val_loss: 0.6794 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6736 - accuracy: 0.0556 - val_loss: 0.6782 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6719 - accuracy: 0.0556 - val_loss: 0.6770 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6702 - accuracy: 0.0556 - val_loss: 0.6758 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.6685 - accuracy: 0.0556 - val_loss: 0.6746 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.6668 - accuracy: 0.0556 - val_loss: 0.6734 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6651 - accuracy: 0.0556 - val_loss: 0.6721 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.6634 - accuracy: 0.0556 - val_loss: 0.6709 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6616 - accuracy: 0.0556 - val_loss: 0.6696 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.6599 - accuracy: 0.0556 - val_loss: 0.6684 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6582 - accuracy: 0.0556 - val_loss: 0.6671 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6565 - accuracy: 0.0556 - val_loss: 0.6658 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.6548 - accuracy: 0.0556 - val_loss: 0.6646 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.6531 - accuracy: 0.0556 - val_loss: 0.6633 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6514 - accuracy: 0.0556 - val_loss: 0.6621 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6497 - accuracy: 0.0556 - val_loss: 0.6608 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6480 - accuracy: 0.0556 - val_loss: 0.6596 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6464 - accuracy: 0.0556 - val_loss: 0.6583 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.6447 - accuracy: 0.0556 - val_loss: 0.6570 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6430 - accuracy: 0.0556 - val_loss: 0.6558 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.6413 - accuracy: 0.0556 - val_loss: 0.6545 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6396 - accuracy: 0.0556 - val_loss: 0.6532 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.6379 - accuracy: 0.0556 - val_loss: 0.6520 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6361 - accuracy: 0.0556 - val_loss: 0.6507 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6344 - accuracy: 0.0556 - val_loss: 0.6495 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6327 - accuracy: 0.0556 - val_loss: 0.6482 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6309 - accuracy: 0.0556 - val_loss: 0.6470 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.6291 - accuracy: 0.0556 - val_loss: 0.6457 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6273 - accuracy: 0.0556 - val_loss: 0.6444 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.6255 - accuracy: 0.0556 - val_loss: 0.6431 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6237 - accuracy: 0.0556 - val_loss: 0.6417 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6218 - accuracy: 0.0556 - val_loss: 0.6404 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6199 - accuracy: 0.0556 - val_loss: 0.6390 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.6181 - accuracy: 0.0556 - val_loss: 0.6376 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6162 - accuracy: 0.0556 - val_loss: 0.6362 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.6143 - accuracy: 0.0556 - val_loss: 0.6348 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6123 - accuracy: 0.0556 - val_loss: 0.6334 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6104 - accuracy: 0.0556 - val_loss: 0.6319 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6084 - accuracy: 0.0556 - val_loss: 0.6305 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6064 - accuracy: 0.0556 - val_loss: 0.6290 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.6044 - accuracy: 0.0556 - val_loss: 0.6275 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6024 - accuracy: 0.0556 - val_loss: 0.6260 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6003 - accuracy: 0.0556 - val_loss: 0.6245 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5982 - accuracy: 0.0556 - val_loss: 0.6229 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5961 - accuracy: 0.0556 - val_loss: 0.6213 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5939 - accuracy: 0.0556 - val_loss: 0.6198 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5917 - accuracy: 0.0556 - val_loss: 0.6181 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5894 - accuracy: 0.0556 - val_loss: 0.6165 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.5871 - accuracy: 0.0556 - val_loss: 0.6149 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5848 - accuracy: 0.0556 - val_loss: 0.6132 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5824 - accuracy: 0.0556 - val_loss: 0.6114 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.5800 - accuracy: 0.0556 - val_loss: 0.6096 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5775 - accuracy: 0.0556 - val_loss: 0.6077 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.5750 - accuracy: 0.0556 - val_loss: 0.6058 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5724 - accuracy: 0.0556 - val_loss: 0.6039 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.5698 - accuracy: 0.0556 - val_loss: 0.6019 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5671 - accuracy: 0.0556 - val_loss: 0.5999 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5644 - accuracy: 0.0556 - val_loss: 0.5978 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5616 - accuracy: 0.0556 - val_loss: 0.5957 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5587 - accuracy: 0.0556 - val_loss: 0.5935 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5558 - accuracy: 0.0556 - val_loss: 0.5914 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5529 - accuracy: 0.0556 - val_loss: 0.5891 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.5499 - accuracy: 0.0556 - val_loss: 0.5868 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5468 - accuracy: 0.0556 - val_loss: 0.5845 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.5438 - accuracy: 0.0556 - val_loss: 0.5822 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5407 - accuracy: 0.0556 - val_loss: 0.5798 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5375 - accuracy: 0.0556 - val_loss: 0.5774 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5342 - accuracy: 0.0556 - val_loss: 0.5749 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.5309 - accuracy: 0.0556 - val_loss: 0.5724 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5276 - accuracy: 0.0556 - val_loss: 0.5699 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5243 - accuracy: 0.0556 - val_loss: 0.5674 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5209 - accuracy: 0.0556 - val_loss: 0.5648 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5174 - accuracy: 0.0556 - val_loss: 0.5622 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.5140 - accuracy: 0.0556 - val_loss: 0.5596 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5105 - accuracy: 0.0556 - val_loss: 0.5570 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5069 - accuracy: 0.0556 - val_loss: 0.5543 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5034 - accuracy: 0.0556 - val_loss: 0.5516 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4998 - accuracy: 0.0556 - val_loss: 0.5489 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4961 - accuracy: 0.0556 - val_loss: 0.5462 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4924 - accuracy: 0.0556 - val_loss: 0.5434 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.4887 - accuracy: 0.0556 - val_loss: 0.5406 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4849 - accuracy: 0.0556 - val_loss: 0.5378 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4812 - accuracy: 0.0556 - val_loss: 0.5350 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4773 - accuracy: 0.0556 - val_loss: 0.5322 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.4735 - accuracy: 0.0556 - val_loss: 0.5293 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4696 - accuracy: 0.0556 - val_loss: 0.5264 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4657 - accuracy: 0.0556 - val_loss: 0.5235 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4618 - accuracy: 0.0556 - val_loss: 0.5205 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4578 - accuracy: 0.0556 - val_loss: 0.5176 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4539 - accuracy: 0.0556 - val_loss: 0.5146 - val_accuracy: 0.0000e+00\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4499 - accuracy: 0.0556 - val_loss: 0.5115 - val_accuracy: 0.0000e+00\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.4459 - accuracy: 0.0556 - val_loss: 0.5085 - val_accuracy: 0.0000e+00\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4418 - accuracy: 0.0556 - val_loss: 0.5055 - val_accuracy: 0.0000e+00\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4378 - accuracy: 0.0556 - val_loss: 0.5025 - val_accuracy: 0.0000e+00\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4338 - accuracy: 0.0556 - val_loss: 0.4994 - val_accuracy: 0.0000e+00\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4297 - accuracy: 0.0556 - val_loss: 0.4963 - val_accuracy: 0.0000e+00\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4257 - accuracy: 0.0556 - val_loss: 0.4933 - val_accuracy: 0.0000e+00\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.4216 - accuracy: 0.0556 - val_loss: 0.4902 - val_accuracy: 0.0000e+00\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.4176 - accuracy: 0.0556 - val_loss: 0.4871 - val_accuracy: 0.0000e+00\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.4135 - accuracy: 0.0556 - val_loss: 0.4841 - val_accuracy: 0.0000e+00\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4095 - accuracy: 0.0556 - val_loss: 0.4810 - val_accuracy: 0.0000e+00\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.4054 - accuracy: 0.0556 - val_loss: 0.4780 - val_accuracy: 0.0000e+00\n",
      "Epoch 113/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step - loss: 0.4014 - accuracy: 0.0556 - val_loss: 0.4749 - val_accuracy: 0.0000e+00\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.3974 - accuracy: 0.0556 - val_loss: 0.4719 - val_accuracy: 0.0000e+00\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.3934 - accuracy: 0.0556 - val_loss: 0.4689 - val_accuracy: 0.0000e+00\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.3894 - accuracy: 0.0556 - val_loss: 0.4658 - val_accuracy: 0.0000e+00\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.3855 - accuracy: 0.0556 - val_loss: 0.4629 - val_accuracy: 0.0000e+00\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3816 - accuracy: 0.0556 - val_loss: 0.4599 - val_accuracy: 0.0000e+00\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3777 - accuracy: 0.0556 - val_loss: 0.4569 - val_accuracy: 0.0000e+00\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3738 - accuracy: 0.0556 - val_loss: 0.4540 - val_accuracy: 0.0000e+00\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3700 - accuracy: 0.0556 - val_loss: 0.4511 - val_accuracy: 0.0000e+00\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3662 - accuracy: 0.0556 - val_loss: 0.4482 - val_accuracy: 0.0000e+00\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3624 - accuracy: 0.0556 - val_loss: 0.4454 - val_accuracy: 0.0000e+00\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3587 - accuracy: 0.0556 - val_loss: 0.4426 - val_accuracy: 0.0000e+00\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3550 - accuracy: 0.0556 - val_loss: 0.4398 - val_accuracy: 0.0000e+00\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3514 - accuracy: 0.0556 - val_loss: 0.4371 - val_accuracy: 0.0000e+00\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.3478 - accuracy: 0.0556 - val_loss: 0.4344 - val_accuracy: 0.0000e+00\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3442 - accuracy: 0.0556 - val_loss: 0.4317 - val_accuracy: 0.0000e+00\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.3408 - accuracy: 0.0556 - val_loss: 0.4291 - val_accuracy: 0.0000e+00\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3373 - accuracy: 0.0556 - val_loss: 0.4266 - val_accuracy: 0.0000e+00\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.3340 - accuracy: 0.0556 - val_loss: 0.4240 - val_accuracy: 0.0000e+00\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3307 - accuracy: 0.0556 - val_loss: 0.4216 - val_accuracy: 0.0000e+00\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.3274 - accuracy: 0.0556 - val_loss: 0.4191 - val_accuracy: 0.0000e+00\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3242 - accuracy: 0.0556 - val_loss: 0.4167 - val_accuracy: 0.0000e+00\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3211 - accuracy: 0.0556 - val_loss: 0.4144 - val_accuracy: 0.0000e+00\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.3180 - accuracy: 0.0556 - val_loss: 0.4121 - val_accuracy: 0.0000e+00\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3150 - accuracy: 0.0556 - val_loss: 0.4098 - val_accuracy: 0.0000e+00\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.3120 - accuracy: 0.0556 - val_loss: 0.4076 - val_accuracy: 0.0000e+00\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3091 - accuracy: 0.0556 - val_loss: 0.4055 - val_accuracy: 0.0000e+00\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3063 - accuracy: 0.0556 - val_loss: 0.4033 - val_accuracy: 0.0000e+00\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3035 - accuracy: 0.0556 - val_loss: 0.4013 - val_accuracy: 0.0000e+00\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.3008 - accuracy: 0.0556 - val_loss: 0.3993 - val_accuracy: 0.0000e+00\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2982 - accuracy: 0.0556 - val_loss: 0.3974 - val_accuracy: 0.0000e+00\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.2956 - accuracy: 0.0556 - val_loss: 0.3955 - val_accuracy: 0.0000e+00\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2931 - accuracy: 0.0556 - val_loss: 0.3937 - val_accuracy: 0.0000e+00\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2907 - accuracy: 0.0556 - val_loss: 0.3919 - val_accuracy: 0.0000e+00\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.2883 - accuracy: 0.0556 - val_loss: 0.3902 - val_accuracy: 0.0000e+00\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2860 - accuracy: 0.0556 - val_loss: 0.3885 - val_accuracy: 0.0000e+00\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2837 - accuracy: 0.0556 - val_loss: 0.3869 - val_accuracy: 0.0000e+00\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.2816 - accuracy: 0.0556 - val_loss: 0.3854 - val_accuracy: 0.0000e+00\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2795 - accuracy: 0.0556 - val_loss: 0.3839 - val_accuracy: 0.0000e+00\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.2775 - accuracy: 0.0556 - val_loss: 0.3825 - val_accuracy: 0.0000e+00\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2755 - accuracy: 0.0556 - val_loss: 0.3811 - val_accuracy: 0.0000e+00\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.2737 - accuracy: 0.0556 - val_loss: 0.3797 - val_accuracy: 0.0000e+00\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2719 - accuracy: 0.0556 - val_loss: 0.3784 - val_accuracy: 0.0000e+00\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2701 - accuracy: 0.0556 - val_loss: 0.3771 - val_accuracy: 0.0000e+00\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.2684 - accuracy: 0.0556 - val_loss: 0.3759 - val_accuracy: 0.0000e+00\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.2668 - accuracy: 0.0556 - val_loss: 0.3747 - val_accuracy: 0.0000e+00\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.2653 - accuracy: 0.0556 - val_loss: 0.3736 - val_accuracy: 0.0000e+00\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2638 - accuracy: 0.0556 - val_loss: 0.3726 - val_accuracy: 0.0000e+00\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2623 - accuracy: 0.0556 - val_loss: 0.3716 - val_accuracy: 0.0000e+00\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.2609 - accuracy: 0.0556 - val_loss: 0.3706 - val_accuracy: 0.0000e+00\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2596 - accuracy: 0.0556 - val_loss: 0.3697 - val_accuracy: 0.0000e+00\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2583 - accuracy: 0.0556 - val_loss: 0.3688 - val_accuracy: 0.0000e+00\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2571 - accuracy: 0.0556 - val_loss: 0.3679 - val_accuracy: 0.0000e+00\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.2560 - accuracy: 0.0556 - val_loss: 0.3671 - val_accuracy: 0.0000e+00\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.2548 - accuracy: 0.0556 - val_loss: 0.3664 - val_accuracy: 0.0000e+00\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2537 - accuracy: 0.0556 - val_loss: 0.3657 - val_accuracy: 0.0000e+00\n",
      "Epoch 169/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 14ms/step - loss: 0.2527 - accuracy: 0.0556 - val_loss: 0.3650 - val_accuracy: 0.0000e+00\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2517 - accuracy: 0.0556 - val_loss: 0.3643 - val_accuracy: 0.0000e+00\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.2508 - accuracy: 0.0556 - val_loss: 0.3637 - val_accuracy: 0.0000e+00\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2499 - accuracy: 0.0556 - val_loss: 0.3631 - val_accuracy: 0.0000e+00\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.2490 - accuracy: 0.0556 - val_loss: 0.3626 - val_accuracy: 0.0000e+00\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.2481 - accuracy: 0.0556 - val_loss: 0.3620 - val_accuracy: 0.0000e+00\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.2473 - accuracy: 0.0556 - val_loss: 0.3615 - val_accuracy: 0.0000e+00\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2466 - accuracy: 0.0556 - val_loss: 0.3611 - val_accuracy: 0.0000e+00\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.2458 - accuracy: 0.0556 - val_loss: 0.3606 - val_accuracy: 0.0000e+00\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2451 - accuracy: 0.0556 - val_loss: 0.3602 - val_accuracy: 0.0000e+00\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2445 - accuracy: 0.0556 - val_loss: 0.3598 - val_accuracy: 0.0000e+00\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2438 - accuracy: 0.0556 - val_loss: 0.3595 - val_accuracy: 0.0000e+00\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.2432 - accuracy: 0.0556 - val_loss: 0.3591 - val_accuracy: 0.0000e+00\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.2426 - accuracy: 0.0556 - val_loss: 0.3588 - val_accuracy: 0.0000e+00\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2420 - accuracy: 0.0556 - val_loss: 0.3584 - val_accuracy: 0.0000e+00\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2415 - accuracy: 0.0556 - val_loss: 0.3581 - val_accuracy: 0.0000e+00\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2409 - accuracy: 0.0556 - val_loss: 0.3579 - val_accuracy: 0.0000e+00\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.2404 - accuracy: 0.0556 - val_loss: 0.3576 - val_accuracy: 0.0000e+00\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2400 - accuracy: 0.0556 - val_loss: 0.3574 - val_accuracy: 0.0000e+00\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2395 - accuracy: 0.0556 - val_loss: 0.3571 - val_accuracy: 0.0000e+00\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2390 - accuracy: 0.0556 - val_loss: 0.3569 - val_accuracy: 0.0000e+00\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2386 - accuracy: 0.0556 - val_loss: 0.3567 - val_accuracy: 0.0000e+00\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2382 - accuracy: 0.0556 - val_loss: 0.3566 - val_accuracy: 0.0000e+00\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2378 - accuracy: 0.0556 - val_loss: 0.3564 - val_accuracy: 0.0000e+00\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.2374 - accuracy: 0.0556 - val_loss: 0.3562 - val_accuracy: 0.0000e+00\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2371 - accuracy: 0.0556 - val_loss: 0.3561 - val_accuracy: 0.0000e+00\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2367 - accuracy: 0.0556 - val_loss: 0.3560 - val_accuracy: 0.0000e+00\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2364 - accuracy: 0.0556 - val_loss: 0.3558 - val_accuracy: 0.0000e+00\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2360 - accuracy: 0.0556 - val_loss: 0.3557 - val_accuracy: 0.0000e+00\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.2357 - accuracy: 0.0556 - val_loss: 0.3556 - val_accuracy: 0.0000e+00\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.2354 - accuracy: 0.0556 - val_loss: 0.3555 - val_accuracy: 0.0000e+00\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.2351 - accuracy: 0.0556 - val_loss: 0.3554 - val_accuracy: 0.0000e+00\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-bc011529323e>:135: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "  tprs.append(interp(mean_fpr, fpr, tpr))\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 389ms/step - loss: 0.7173 - accuracy: 0.0556 - val_loss: 0.7118 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7153 - accuracy: 0.0556 - val_loss: 0.7102 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.7134 - accuracy: 0.0556 - val_loss: 0.7086 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.7115 - accuracy: 0.0556 - val_loss: 0.7071 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.7097 - accuracy: 0.0556 - val_loss: 0.7055 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.7079 - accuracy: 0.0556 - val_loss: 0.7040 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.7062 - accuracy: 0.0556 - val_loss: 0.7026 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.7045 - accuracy: 0.0556 - val_loss: 0.7011 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7028 - accuracy: 0.0556 - val_loss: 0.6997 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.7011 - accuracy: 0.0556 - val_loss: 0.6983 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6995 - accuracy: 0.0000e+00 - val_loss: 0.6969 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6979 - accuracy: 0.0000e+00 - val_loss: 0.6956 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6963 - accuracy: 0.0000e+00 - val_loss: 0.6942 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6947 - accuracy: 0.0000e+00 - val_loss: 0.6929 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6932 - accuracy: 0.0000e+00 - val_loss: 0.6916 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6916 - accuracy: 0.0000e+00 - val_loss: 0.6903 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6901 - accuracy: 0.0000e+00 - val_loss: 0.6890 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6886 - accuracy: 0.0000e+00 - val_loss: 0.6878 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6872 - accuracy: 0.0000e+00 - val_loss: 0.6865 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6857 - accuracy: 0.0000e+00 - val_loss: 0.6853 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6843 - accuracy: 0.0000e+00 - val_loss: 0.6841 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6828 - accuracy: 0.0000e+00 - val_loss: 0.6828 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6814 - accuracy: 0.0000e+00 - val_loss: 0.6816 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6800 - accuracy: 0.0000e+00 - val_loss: 0.6804 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6786 - accuracy: 0.0000e+00 - val_loss: 0.6793 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6772 - accuracy: 0.0000e+00 - val_loss: 0.6781 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6758 - accuracy: 0.0000e+00 - val_loss: 0.6769 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6744 - accuracy: 0.0000e+00 - val_loss: 0.6758 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6730 - accuracy: 0.0000e+00 - val_loss: 0.6746 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6716 - accuracy: 0.0000e+00 - val_loss: 0.6734 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.6703 - accuracy: 0.0000e+00 - val_loss: 0.6723 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6689 - accuracy: 0.0000e+00 - val_loss: 0.6711 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6675 - accuracy: 0.0000e+00 - val_loss: 0.6700 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6662 - accuracy: 0.0000e+00 - val_loss: 0.6688 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6648 - accuracy: 0.0000e+00 - val_loss: 0.6677 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6634 - accuracy: 0.0000e+00 - val_loss: 0.6665 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6621 - accuracy: 0.0000e+00 - val_loss: 0.6654 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6607 - accuracy: 0.0000e+00 - val_loss: 0.6642 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6594 - accuracy: 0.0000e+00 - val_loss: 0.6631 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6580 - accuracy: 0.0000e+00 - val_loss: 0.6619 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6566 - accuracy: 0.0000e+00 - val_loss: 0.6608 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6552 - accuracy: 0.0000e+00 - val_loss: 0.6596 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6539 - accuracy: 0.0000e+00 - val_loss: 0.6585 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6525 - accuracy: 0.0000e+00 - val_loss: 0.6573 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6511 - accuracy: 0.0000e+00 - val_loss: 0.6562 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6497 - accuracy: 0.0000e+00 - val_loss: 0.6550 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6482 - accuracy: 0.0000e+00 - val_loss: 0.6538 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6468 - accuracy: 0.0000e+00 - val_loss: 0.6526 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6454 - accuracy: 0.0000e+00 - val_loss: 0.6515 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6439 - accuracy: 0.0000e+00 - val_loss: 0.6503 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6425 - accuracy: 0.0000e+00 - val_loss: 0.6491 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6410 - accuracy: 0.0000e+00 - val_loss: 0.6479 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6396 - accuracy: 0.0000e+00 - val_loss: 0.6467 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6381 - accuracy: 0.0000e+00 - val_loss: 0.6455 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6366 - accuracy: 0.0000e+00 - val_loss: 0.6442 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6351 - accuracy: 0.0556 - val_loss: 0.6430 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6336 - accuracy: 0.0556 - val_loss: 0.6418 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6320 - accuracy: 0.0556 - val_loss: 0.6405 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6305 - accuracy: 0.0556 - val_loss: 0.6393 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6289 - accuracy: 0.0556 - val_loss: 0.6380 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6273 - accuracy: 0.0556 - val_loss: 0.6367 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6257 - accuracy: 0.0556 - val_loss: 0.6354 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6241 - accuracy: 0.0556 - val_loss: 0.6341 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6225 - accuracy: 0.0556 - val_loss: 0.6328 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6208 - accuracy: 0.05 - 0s 17ms/step - loss: 0.6208 - accuracy: 0.0556 - val_loss: 0.6315 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6191 - accuracy: 0.0556 - val_loss: 0.6302 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6174 - accuracy: 0.0556 - val_loss: 0.6288 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6157 - accuracy: 0.0556 - val_loss: 0.6274 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6139 - accuracy: 0.0556 - val_loss: 0.6260 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6121 - accuracy: 0.0556 - val_loss: 0.6246 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6103 - accuracy: 0.0556 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6085 - accuracy: 0.0556 - val_loss: 0.6218 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6066 - accuracy: 0.0556 - val_loss: 0.6203 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6047 - accuracy: 0.0556 - val_loss: 0.6188 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6028 - accuracy: 0.0556 - val_loss: 0.6173 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6008 - accuracy: 0.0556 - val_loss: 0.6158 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5988 - accuracy: 0.0556 - val_loss: 0.6142 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5967 - accuracy: 0.0556 - val_loss: 0.6127 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5947 - accuracy: 0.0556 - val_loss: 0.6111 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5925 - accuracy: 0.0556 - val_loss: 0.6095 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5904 - accuracy: 0.0556 - val_loss: 0.6078 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5882 - accuracy: 0.0556 - val_loss: 0.6061 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5860 - accuracy: 0.0556 - val_loss: 0.6044 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5837 - accuracy: 0.0556 - val_loss: 0.6027 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5813 - accuracy: 0.0556 - val_loss: 0.6010 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5790 - accuracy: 0.0556 - val_loss: 0.5992 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5765 - accuracy: 0.0556 - val_loss: 0.5974 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5740 - accuracy: 0.0556 - val_loss: 0.5955 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5715 - accuracy: 0.0556 - val_loss: 0.5936 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5689 - accuracy: 0.0556 - val_loss: 0.5917 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5663 - accuracy: 0.0556 - val_loss: 0.5898 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5636 - accuracy: 0.0556 - val_loss: 0.5878 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5609 - accuracy: 0.0556 - val_loss: 0.5858 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5581 - accuracy: 0.0556 - val_loss: 0.5837 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5553 - accuracy: 0.0556 - val_loss: 0.5816 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5524 - accuracy: 0.0556 - val_loss: 0.5794 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5494 - accuracy: 0.0556 - val_loss: 0.5772 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5464 - accuracy: 0.0556 - val_loss: 0.5750 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5434 - accuracy: 0.0556 - val_loss: 0.5726 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.5402 - accuracy: 0.0556 - val_loss: 0.5703 - val_accuracy: 0.0000e+00\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5370 - accuracy: 0.0556 - val_loss: 0.5679 - val_accuracy: 0.0000e+00\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5337 - accuracy: 0.0556 - val_loss: 0.5654 - val_accuracy: 0.0000e+00\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5304 - accuracy: 0.0556 - val_loss: 0.5629 - val_accuracy: 0.0000e+00\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5270 - accuracy: 0.0556 - val_loss: 0.5604 - val_accuracy: 0.0000e+00\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5235 - accuracy: 0.0556 - val_loss: 0.5578 - val_accuracy: 0.0000e+00\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5199 - accuracy: 0.0556 - val_loss: 0.5551 - val_accuracy: 0.0000e+00\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.5163 - accuracy: 0.0556 - val_loss: 0.5524 - val_accuracy: 0.0000e+00\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5126 - accuracy: 0.0556 - val_loss: 0.5497 - val_accuracy: 0.0000e+00\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5088 - accuracy: 0.0556 - val_loss: 0.5469 - val_accuracy: 0.0000e+00\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5050 - accuracy: 0.0556 - val_loss: 0.5440 - val_accuracy: 0.0000e+00\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.5011 - accuracy: 0.0556 - val_loss: 0.5411 - val_accuracy: 0.0000e+00\n",
      "Epoch 112/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4971 - accuracy: 0.0556 - val_loss: 0.5382 - val_accuracy: 0.0000e+00\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4931 - accuracy: 0.05 - 0s 17ms/step - loss: 0.4931 - accuracy: 0.0556 - val_loss: 0.5351 - val_accuracy: 0.0000e+00\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4890 - accuracy: 0.0556 - val_loss: 0.5321 - val_accuracy: 0.0000e+00\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4848 - accuracy: 0.0556 - val_loss: 0.5290 - val_accuracy: 0.0000e+00\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4806 - accuracy: 0.0556 - val_loss: 0.5259 - val_accuracy: 0.0000e+00\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4763 - accuracy: 0.0556 - val_loss: 0.5227 - val_accuracy: 0.0000e+00\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4719 - accuracy: 0.0556 - val_loss: 0.5195 - val_accuracy: 0.0000e+00\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.4675 - accuracy: 0.0556 - val_loss: 0.5162 - val_accuracy: 0.0000e+00\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4630 - accuracy: 0.0556 - val_loss: 0.5129 - val_accuracy: 0.0000e+00\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4585 - accuracy: 0.0556 - val_loss: 0.5096 - val_accuracy: 0.0000e+00\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4539 - accuracy: 0.0556 - val_loss: 0.5062 - val_accuracy: 0.0000e+00\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4493 - accuracy: 0.0556 - val_loss: 0.5028 - val_accuracy: 0.0000e+00\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4446 - accuracy: 0.0556 - val_loss: 0.4994 - val_accuracy: 0.0000e+00\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4399 - accuracy: 0.0556 - val_loss: 0.4959 - val_accuracy: 0.0000e+00\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4352 - accuracy: 0.0556 - val_loss: 0.4924 - val_accuracy: 0.0000e+00\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.4304 - accuracy: 0.0556 - val_loss: 0.4889 - val_accuracy: 0.0000e+00\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.4256 - accuracy: 0.0556 - val_loss: 0.4854 - val_accuracy: 0.0000e+00\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4208 - accuracy: 0.0556 - val_loss: 0.4819 - val_accuracy: 0.0000e+00\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4159 - accuracy: 0.0556 - val_loss: 0.4783 - val_accuracy: 0.0000e+00\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4110 - accuracy: 0.0556 - val_loss: 0.4748 - val_accuracy: 0.0000e+00\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4061 - accuracy: 0.0556 - val_loss: 0.4712 - val_accuracy: 0.0000e+00\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.4012 - accuracy: 0.0556 - val_loss: 0.4677 - val_accuracy: 0.0000e+00\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.3963 - accuracy: 0.0556 - val_loss: 0.4641 - val_accuracy: 0.0000e+00\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.3914 - accuracy: 0.0556 - val_loss: 0.4605 - val_accuracy: 0.0000e+00\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.3865 - accuracy: 0.0556 - val_loss: 0.4569 - val_accuracy: 0.0000e+00\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.3817 - accuracy: 0.0556 - val_loss: 0.4534 - val_accuracy: 0.0000e+00\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.3769 - accuracy: 0.0556 - val_loss: 0.4499 - val_accuracy: 0.0000e+00\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.3721 - accuracy: 0.0556 - val_loss: 0.4464 - val_accuracy: 0.0000e+00\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.3674 - accuracy: 0.0556 - val_loss: 0.4430 - val_accuracy: 0.0000e+00\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.3627 - accuracy: 0.0556 - val_loss: 0.4396 - val_accuracy: 0.0000e+00\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.3581 - accuracy: 0.0556 - val_loss: 0.4362 - val_accuracy: 0.0000e+00\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.3535 - accuracy: 0.0556 - val_loss: 0.4329 - val_accuracy: 0.0000e+00\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3490 - accuracy: 0.0556 - val_loss: 0.4296 - val_accuracy: 0.0000e+00\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.3445 - accuracy: 0.0556 - val_loss: 0.4265 - val_accuracy: 0.0000e+00\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.3402 - accuracy: 0.0556 - val_loss: 0.4233 - val_accuracy: 0.0000e+00\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3359 - accuracy: 0.0556 - val_loss: 0.4203 - val_accuracy: 0.0000e+00\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.3317 - accuracy: 0.0556 - val_loss: 0.4173 - val_accuracy: 0.0000e+00\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.3276 - accuracy: 0.0556 - val_loss: 0.4143 - val_accuracy: 0.0000e+00\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.3236 - accuracy: 0.0556 - val_loss: 0.4115 - val_accuracy: 0.0000e+00\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.3197 - accuracy: 0.0556 - val_loss: 0.4087 - val_accuracy: 0.0000e+00\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.3159 - accuracy: 0.0556 - val_loss: 0.4060 - val_accuracy: 0.0000e+00\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.3122 - accuracy: 0.0556 - val_loss: 0.4033 - val_accuracy: 0.0000e+00\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.3086 - accuracy: 0.0556 - val_loss: 0.4008 - val_accuracy: 0.0000e+00\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.3051 - accuracy: 0.0556 - val_loss: 0.3983 - val_accuracy: 0.0000e+00\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.3017 - accuracy: 0.0556 - val_loss: 0.3959 - val_accuracy: 0.0000e+00\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2985 - accuracy: 0.0556 - val_loss: 0.3936 - val_accuracy: 0.0000e+00\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.2953 - accuracy: 0.0556 - val_loss: 0.3914 - val_accuracy: 0.0000e+00\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.2922 - accuracy: 0.0556 - val_loss: 0.3893 - val_accuracy: 0.0000e+00\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2893 - accuracy: 0.0556 - val_loss: 0.3872 - val_accuracy: 0.0000e+00\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.2865 - accuracy: 0.0556 - val_loss: 0.3853 - val_accuracy: 0.0000e+00\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2837 - accuracy: 0.0556 - val_loss: 0.3834 - val_accuracy: 0.0000e+00\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2811 - accuracy: 0.0556 - val_loss: 0.3816 - val_accuracy: 0.0000e+00\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2786 - accuracy: 0.0556 - val_loss: 0.3799 - val_accuracy: 0.0000e+00\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.2762 - accuracy: 0.0556 - val_loss: 0.3783 - val_accuracy: 0.0000e+00\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2739 - accuracy: 0.0556 - val_loss: 0.3767 - val_accuracy: 0.0000e+00\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2717 - accuracy: 0.0556 - val_loss: 0.3752 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.2696 - accuracy: 0.0556 - val_loss: 0.3738 - val_accuracy: 0.0000e+00\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2676 - accuracy: 0.0556 - val_loss: 0.3725 - val_accuracy: 0.0000e+00\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.2656 - accuracy: 0.0556 - val_loss: 0.3712 - val_accuracy: 0.0000e+00\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.2638 - accuracy: 0.0556 - val_loss: 0.3701 - val_accuracy: 0.0000e+00\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2620 - accuracy: 0.0556 - val_loss: 0.3689 - val_accuracy: 0.0000e+00\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.2604 - accuracy: 0.0556 - val_loss: 0.3679 - val_accuracy: 0.0000e+00\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2588 - accuracy: 0.0556 - val_loss: 0.3669 - val_accuracy: 0.0000e+00\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.2572 - accuracy: 0.0556 - val_loss: 0.3659 - val_accuracy: 0.0000e+00\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2558 - accuracy: 0.0556 - val_loss: 0.3651 - val_accuracy: 0.0000e+00\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2544 - accuracy: 0.0556 - val_loss: 0.3642 - val_accuracy: 0.0000e+00\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.2531 - accuracy: 0.0556 - val_loss: 0.3634 - val_accuracy: 0.0000e+00\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2519 - accuracy: 0.0556 - val_loss: 0.3627 - val_accuracy: 0.0000e+00\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.2507 - accuracy: 0.0556 - val_loss: 0.3620 - val_accuracy: 0.0000e+00\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2496 - accuracy: 0.0556 - val_loss: 0.3614 - val_accuracy: 0.0000e+00\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2485 - accuracy: 0.0556 - val_loss: 0.3608 - val_accuracy: 0.0000e+00\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.2475 - accuracy: 0.0556 - val_loss: 0.3603 - val_accuracy: 0.0000e+00\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2465 - accuracy: 0.0556 - val_loss: 0.3598 - val_accuracy: 0.0000e+00\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.2456 - accuracy: 0.0556 - val_loss: 0.3593 - val_accuracy: 0.0000e+00\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.2447 - accuracy: 0.0556 - val_loss: 0.3589 - val_accuracy: 0.0000e+00\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2439 - accuracy: 0.0556 - val_loss: 0.3584 - val_accuracy: 0.0000e+00\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2431 - accuracy: 0.0556 - val_loss: 0.3581 - val_accuracy: 0.0000e+00\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2424 - accuracy: 0.0556 - val_loss: 0.3577 - val_accuracy: 0.0000e+00\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2417 - accuracy: 0.0556 - val_loss: 0.3574 - val_accuracy: 0.0000e+00\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2410 - accuracy: 0.0556 - val_loss: 0.3571 - val_accuracy: 0.0000e+00\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2404 - accuracy: 0.0556 - val_loss: 0.3569 - val_accuracy: 0.0000e+00\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.2398 - accuracy: 0.0556 - val_loss: 0.3567 - val_accuracy: 0.0000e+00\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.2392 - accuracy: 0.0556 - val_loss: 0.3564 - val_accuracy: 0.0000e+00\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2386 - accuracy: 0.0556 - val_loss: 0.3563 - val_accuracy: 0.0000e+00\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.2381 - accuracy: 0.0556 - val_loss: 0.3561 - val_accuracy: 0.0000e+00\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.2376 - accuracy: 0.0556 - val_loss: 0.3559 - val_accuracy: 0.0000e+00\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2371 - accuracy: 0.0556 - val_loss: 0.3558 - val_accuracy: 0.0000e+00\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2366 - accuracy: 0.0556 - val_loss: 0.3557 - val_accuracy: 0.0000e+00\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.2362 - accuracy: 0.0556 - val_loss: 0.3556 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001A50CF60430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[3, 0]\n",
      "[1, 1]\n",
      "[3, 0]\n",
      "[1, 1]\n",
      "[0, 0]\n",
      "[5, 0]\n",
      "[0, 0]\n",
      "[4, 1]\n",
      "[0, 0]\n",
      "[5, 0]\n",
      "[0, 0]\n",
      "[5, 0]\n",
      "[4, 0]\n",
      "[1, 0]\n",
      "[4, 0]\n",
      "[1, 0]\n",
      "[3, 0]\n",
      "[2, 0]\n",
      "[3, 0]\n",
      "[2, 0]\n",
      "[4, 0]\n",
      "[1, 0]\n",
      "[4, 0]\n",
      "[0, 1]\n",
      "both b and c are zero\n",
      "[5, 0]\n",
      "[0, 0]\n",
      "both b and c are zero\n",
      "[5, 0]\n",
      "[0, 0]\n",
      "both b and c are zero\n",
      "[2, 0]\n",
      "[3, 0]\n",
      "[2, 0]\n",
      "[3, 0]\n",
      "[4, 0]\n",
      "[1, 0]\n",
      "[4, 0]\n",
      "[0, 1]\n",
      "both b and c are zero\n",
      "[3, 0]\n",
      "[2, 0]\n",
      "[3, 0]\n",
      "[2, 0]\n",
      "[2, 0]\n",
      "[3, 0]\n",
      "[2, 0]\n",
      "[2, 1]\n",
      "[3, 0]\n",
      "[2, 0]\n",
      "[2, 1]\n",
      "[2, 0]\n",
      "[0, 0]\n",
      "[5, 0]\n",
      "[0, 0]\n",
      "[5, 0]\n",
      "[4, 0]\n",
      "[1, 0]\n",
      "[4, 0]\n",
      "[0, 1]\n",
      "both b and c are zero\n",
      "[4, 0]\n",
      "[1, 0]\n",
      "[4, 0]\n",
      "[1, 0]\n",
      "[4, 0]\n",
      "[1, 0]\n",
      "[4, 0]\n",
      "[1, 0]\n",
      "[1, 0]\n",
      "[4, 0]\n",
      "[1, 0]\n",
      "[4, 0]\n",
      "[3, 0]\n",
      "[2, 0]\n",
      "[3, 0]\n",
      "[2, 0]\n",
      "[3, 0]\n",
      "[2, 0]\n",
      "[3, 0]\n",
      "[2, 0]\n",
      "------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds of MWPM:\n",
      "> Accuracy: 0.9571929824561405 (+- 0.02226342747960048)\n",
      "> F1: 0.6898520811833505(+- 0.16851412343654842)\n",
      "> Time: 0.028815959999999995 (+- 0.003038892813246297)\n",
      "#####################################################################################\n",
      "Average scores for all folds of PLUT:\n",
      "> Accuracy: 0.9154385964912279 (+- 0.00739344123007209)\n",
      "> F1: 0.03333333333333333(+- 0.06666666666666667)\n",
      "> Time: 0.0 (+- 0.0)\n",
      "#####################################################################################\n",
      "Average scores for all folds of NN:\n",
      "> Accuracy: 0.1094736842105263 (+- 0.2189473684210526)\n",
      "> F1: 0.07445960092377277(+- 0.06406422968254094)\n",
      "> Time: 0.06115345999999999 (+- 0.011413067949434105)\n",
      "#####################################################################################\n",
      "> AUC for class : 0.0 (+- 0.0)\n",
      "X^2 for MWPM and NN: 4.0\n",
      "X^2 for PLUT and NN: 4.0\n",
      "> AUC for class X00: 0.25 (+- 0.0)\n",
      "X^2 for MWPM and NN: 7.2\n",
      "X^2 for PLUT and NN: 6.25\n",
      "> AUC for class X01: 0.5 (+- 0.5)\n",
      "X^2 for MWPM and NN: 7.2\n",
      "X^2 for PLUT and NN: 7.2\n",
      "> AUC for class X02: 0.2625 (+- 0.1375)\n",
      "X^2 for MWPM and NN: 4.0\n",
      "X^2 for PLUT and NN: 4.0\n",
      "> AUC for class X10: 0.44999999999999996 (+- 0.15)\n",
      "X^2 for MWPM and NN: 4.5\n",
      "X^2 for PLUT and NN: 4.5\n",
      "> AUC for class X11: 0.775 (+- 0.025000000000000022)\n",
      "X^2 for MWPM and NN: 4.0\n",
      "X^2 for PLUT and NN: 0\n",
      "> AUC for class X12: 0.5833333333333333 (+- 0.0)\n",
      "X^2 for MWPM and NN: 0\n",
      "X^2 for PLUT and NN: 0\n",
      "> AUC for class X20: 0.14999999999999997 (+- 0.04999999999999999)\n",
      "X^2 for MWPM and NN: 5.333333333333333\n",
      "X^2 for PLUT and NN: 5.333333333333333\n",
      "> AUC for class X21: 1.0 (+- 0.0)\n",
      "X^2 for MWPM and NN: 4.0\n",
      "X^2 for PLUT and NN: 0\n",
      "> AUC for class X22: 0.7 (+- 0.3)\n",
      "X^2 for MWPM and NN: 4.5\n",
      "X^2 for PLUT and NN: 4.5\n",
      "> AUC for class Z00: 0.55 (+- 0.04999999999999999)\n",
      "X^2 for MWPM and NN: 5.333333333333333\n",
      "X^2 for PLUT and NN: 4.5\n",
      "> AUC for class Z01: 1.0 (+- 0.0)\n",
      "X^2 for MWPM and NN: 4.5\n",
      "X^2 for PLUT and NN: 1.3333333333333333\n",
      "> AUC for class Z02: 0.5 (+- 0.09999999999999998)\n",
      "X^2 for MWPM and NN: 7.2\n",
      "X^2 for PLUT and NN: 7.2\n",
      "> AUC for class Z10: 0.425 (+- 0.175)\n",
      "X^2 for MWPM and NN: 4.0\n",
      "X^2 for PLUT and NN: 0\n",
      "> AUC for class Z11: 0.8 (+- 0.0)\n",
      "X^2 for MWPM and NN: 4.0\n",
      "X^2 for PLUT and NN: 4.0\n",
      "> AUC for class Z12: 0.8333333333333334 (+- 0.0)\n",
      "X^2 for MWPM and NN: 4.0\n",
      "X^2 for PLUT and NN: 4.0\n",
      "> AUC for class Z20: 0.0 (+- 0.0)\n",
      "X^2 for MWPM and NN: 6.25\n",
      "X^2 for PLUT and NN: 6.25\n",
      "> AUC for class Z21: 0.75 (+- 0.25)\n",
      "X^2 for MWPM and NN: 4.5\n",
      "X^2 for PLUT and NN: 4.5\n",
      "> AUC for class Z22: 0.0 (+- 0.0)\n",
      "X^2 for MWPM and NN: 4.5\n",
      "X^2 for PLUT and NN: 4.5\n",
      "###################################################################################\n",
      "TOTAL F1 NN: [0.15841584158415842, 0.11864406779661016, 0.09523809523809523, 0.0, 0.0]\n",
      "TOTAL F1 PLUT: [0.16666666666666666, 0.0, 0.0, 0.0, 0.0]\n",
      "TOTAL F1 MWPM: [0.823529411764706, 0.42857142857142855, 0.7368421052631577, 0.5714285714285715, 0.888888888888889]\n",
      "TOTAL ACC NN: [0.0, 0.0, 0.0, 0.0, 0.5473684210526315]\n",
      "TOTAL ACC PLUT: [0.9122807017543858, 0.9210526315789472, 0.9122807017543858, 0.9052631578947368, 0.9263157894736841]\n",
      "TOTAL ACC MWPM: [0.9736842105263158, 0.9298245614035087, 0.956140350877193, 0.9368421052631579, 0.9894736842105264]\n",
      "TOTAL TIME NN: [0.0836335, 0.0586357, 0.0566085, 0.0536636, 0.053226]\n",
      "TOTAL TIME PLUT: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "TOTAL TIME MWPM: [0.0300751, 0.0289201, 0.0329118, 0.0286262, 0.0235466]\n",
      "------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-bc011529323e>:135: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "  tprs.append(interp(mean_fpr, fpr, tpr))\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "inputs = x_d3.copy()\n",
    "inputs = np.array(inputs)\n",
    "inputs = np.stack(inputs)\n",
    "targets = y_d3.copy()\n",
    "targets = np.array([np.array(xi) for xi in targets])\n",
    "\n",
    "n_classes = 19\n",
    "\n",
    "acc_per_fold = []\n",
    "f1_per_fold = []\n",
    "time_per_fold = []\n",
    "\n",
    "\n",
    "acc_per_fold_plut = []\n",
    "f1_per_fold_plut = []\n",
    "time_per_fold_plut = []\n",
    "\n",
    "acc_per_fold_mwpm = []\n",
    "f1_per_fold_mwpm = []\n",
    "time_per_fold_mwpm = []\n",
    "\n",
    "\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "aucs_classes = {}\n",
    "for i in mlb_d3.classes_:\n",
    "    aucs_classes[i] = []\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "#for i in range(5):\n",
    "\n",
    "    # K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "for i, (train, test) in enumerate(kfold.split(inputs, targets)):\n",
    "    i_train = inputs[train].copy()\n",
    "    i_test = inputs[test].copy()\n",
    "    x_test_d3 = i_test[:,:2]\n",
    "    \n",
    "    \n",
    "    inputs_train = i_train[:,3:]\n",
    "    inputs_test = i_test[:,3:]\n",
    "    \n",
    "    \n",
    "    ####################################################################################################\n",
    "    #test MWPM decoder for this fold\n",
    "    #labels = targets[train], features = inputs[train]\n",
    "    #x_test_d3 = translate_to_graph(testData_d3_MWPM, targets[test], mlb_d3)\n",
    "    decoding_d3, time_mwpm = do_new_decoding(x_test_d3, 3, .03)\n",
    "    decoding_d3['combine'] = decoding_d3[[0, 1]].values.tolist()\n",
    "    decoding_d3['combine'].apply(lambda x: x[0].extend(x[1]))\n",
    "    decoding_d3 = np.array(decoding_d3[0])\n",
    "\n",
    "    time_per_fold_mwpm.append(time_mwpm)\n",
    "\n",
    "    pred_mwpm = mlb_d3.transform(decoding_d3)\n",
    "    if fold_no < 5:\n",
    "        acc = partial_accuracy(targets[test], pred_mwpm)\n",
    "    else:\n",
    "        acc, contingency_mwpm = partial_accuracy_and_contingency(targets[test], pred_mwpm, mlb_d3)\n",
    "        \n",
    "    acc_per_fold_mwpm.append(acc)\n",
    "    f1_per_fold_mwpm.append(f1_score(targets[test], pred_mwpm, average='micro'))\n",
    "\n",
    "\n",
    "    #####################################################################################################\n",
    "    #test the plut decoder for this fold\n",
    "\n",
    "    lookup_d3 = lookup_decoder(3)\n",
    "\n",
    "    lookup_d3 = train_plut(lookup_d3, inputs_train, targets[train])\n",
    "\n",
    "    start = time.time_ns()\n",
    "    pred_plut_d3 = test_plut(lookup_d3, inputs_test)\n",
    "    end = time.time_ns() \n",
    "    time_per_fold_plut.append((end - start)/ (10 ** 9))\n",
    "    \n",
    "    if fold_no < 5:\n",
    "        acc = partial_accuracy(targets[test], pred_plut_d3)\n",
    "    else:\n",
    "        acc, contingency_plut = partial_accuracy_and_contingency(targets[test], pred_plut_d3, mlb_d3)\n",
    "\n",
    "    acc_per_fold_plut.append(acc)\n",
    "    f1_per_fold_plut.append(f1_score(targets[test], pred_plut_d3, average='micro'))\n",
    "\n",
    "    #####################################################################################################\n",
    "    #Test the NN decoder for this fold\n",
    "\n",
    "    \n",
    "    model = compile_FFNN_model_DepthThree(3)\n",
    "    # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "    inputs_train = np.asarray(inputs_train).astype('int32')\n",
    "    inputs_test = np.asarray(inputs_test).astype('int32')\n",
    "\n",
    "    # Fit data to model\n",
    "\n",
    "    history = model.fit(\n",
    "        inputs_train, targets[train],\n",
    "        validation_split=.2,\n",
    "        epochs=200,\n",
    "        verbose=1)\n",
    "\n",
    "   # Generate generalization metrics\n",
    "    scores = model.evaluate(inputs_test, targets[test], verbose=0)\n",
    "\n",
    "    #get the time to predicting test\n",
    "    start = time.time_ns()\n",
    "    predictions_d3 = model.predict(inputs_test) #change here\n",
    "    end = time.time_ns()\n",
    "    time_per_fold.append((end - start)/ (10 ** 9))\n",
    "\n",
    "    #threshold based on previous tests with train_test_split in hyperparameter tuning\n",
    "    pred=predictions_d3.copy() #change here\n",
    "    pred[pred>=.1]=1 \n",
    "    pred[pred<.1]=0\n",
    "    \n",
    "    if fold_no <5:\n",
    "        acc = scores[1]\n",
    "    else:\n",
    "        acc, contingency_nn = partial_accuracy_and_contingency(targets[test], pred, mlb_d3)\n",
    "\n",
    "    acc_per_fold.append(acc)\n",
    "    f1_per_fold.append(f1_score(targets[test], pred, average='micro'))\n",
    "\n",
    "    #####################################################################################################\n",
    "    #comput ROC AUC for classes and the mircoaverage\n",
    "    fpr, tpr, thresholds = roc_curve(targets[test].ravel(), predictions_d3.ravel())\n",
    "    tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "    tprs[-1][0] = 0.0\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "    plt.plot(fpr, tpr, lw=1, alpha=0.3, label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
    "\n",
    "    #get the AUCs of each class, used to get average AUC of each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(targets[test][:, i], predictions_d3[:, i]) #change here\n",
    "        aucs_classes[mlb_d3.classes_[i]].append(auc(fpr[i], tpr[i]))\n",
    "        \n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n",
    "        \n",
    "#########################################################################################################\n",
    "#compute McNemar's statistic on results of last fold\n",
    "\n",
    "mcnemar_results_mwpm = {}\n",
    "mcnemar_results_plut = {}\n",
    "\n",
    "for class_ in mlb_d3.classes_:\n",
    "    #compute the x^2 for NN and MWPM\n",
    "    mcnemar_results_mwpm[class_]=contingency_table_and_t(contingency_nn[class_], contingency_mwpm[class_])[1]    \n",
    "    #comput the x^2 for NN and PLUT\n",
    "    mcnemar_results_plut[class_] = contingency_table_and_t(contingency_nn[class_], contingency_plut[class_])[1]\n",
    "        \n",
    "    \n",
    "############print mean and stdev of AUC of each class#####################      \n",
    "    \n",
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds of MWPM:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold_mwpm)} (+- {np.std(acc_per_fold_mwpm)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold_mwpm)}(+- {np.std(f1_per_fold_mwpm)})')\n",
    "print(f'> Time: {np.mean(time_per_fold_mwpm)} (+- {np.std(time_per_fold_mwpm)})')\n",
    "print(\"#####################################################################################\")\n",
    "print('Average scores for all folds of PLUT:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold_plut)} (+- {np.std(acc_per_fold_plut)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold_plut)}(+- {np.std(f1_per_fold_plut)})')\n",
    "print(f'> Time: {np.mean(time_per_fold_plut)} (+- {np.std(time_per_fold_plut)})')\n",
    "print(\"#####################################################################################\")\n",
    "print('Average scores for all folds of NN:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold)}(+- {np.std(f1_per_fold)})')\n",
    "print(f'> Time: {np.mean(time_per_fold)} (+- {np.std(time_per_fold)})')\n",
    "print(\"#####################################################################################\")\n",
    "for key in aucs_classes:\n",
    "    cleanedList = [x for x in aucs_classes[key] if str(x) != 'nan']\n",
    "    print(f'> AUC for class {key}: {np.mean(cleanedList)} (+- {np.std(cleanedList)})') #this has to be a for loop\n",
    "    print(\"X^2 for MWPM and NN: \" + str(mcnemar_results_mwpm[key]))  \n",
    "    print(\"X^2 for PLUT and NN: \" + str(mcnemar_results_plut[key]))\n",
    "print(\"###################################################################################\")\n",
    "print(\"TOTAL F1 NN: \" + str(f1_per_fold))\n",
    "print(\"TOTAL F1 PLUT: \" + str(f1_per_fold_plut))\n",
    "print(\"TOTAL F1 MWPM: \" + str(f1_per_fold_mwpm))\n",
    "print(\"TOTAL ACC NN: \" + str(acc_per_fold))\n",
    "print(\"TOTAL ACC PLUT: \" + str(acc_per_fold_plut))\n",
    "print(\"TOTAL ACC MWPM: \" + str(acc_per_fold_mwpm))\n",
    "print(\"TOTAL TIME NN: \" + str(time_per_fold))\n",
    "print(\"TOTAL TIME PLUT: \" + str(time_per_fold_plut))\n",
    "print(\"TOTAL TIME MWPM: \" + str(time_per_fold_mwpm))\n",
    "print('------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "         label='Chance', alpha=.8)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "         label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "         lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                 label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "plt.xlim([-0.01, 1.01])\n",
    "plt.ylim([-0.01, 1.01])\n",
    "plt.xlabel('False Positive Rate',fontsize=18)\n",
    "plt.ylabel('True Positive Rate',fontsize=18)\n",
    "plt.title('Cross-Validation ROC of Depth 3 NN',fontsize=18)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit model on training data\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'x_train_d3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-67e2e66d47b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Fit model on training data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m history = model.fit(\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx_train_d3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mY_train_d3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m.25\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_train_d3' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "\n",
    "model = compile_FFNN_model_DepthThree(3)\n",
    "print(\"Fit model on training data\")\n",
    "history = model.fit(\n",
    "    x=x_train_d3.values,\n",
    "    y=Y_train_d3,\n",
    "    validation_split=.25,\n",
    "    epochs=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Make a quick epoch vs. accuracy plot\\nplt.figure(figsize=(10,10))\\nplt.grid()\\nplt.plot(history.history['accuracy'])\\nplt.plot(history.history['val_accuracy'])\\nplt.title('Depth 3 Model Accuracy')\\nplt.xlabel('Epochs')\\nplt.ylabel('Accuracy')\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Make a quick epoch vs. accuracy plot\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.grid()\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Depth 3 Model Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nstart = time.time()\\npredictions_d3 = model.predict(x_test_d3.values)\\nend = time.time()\\nprint(\"Time: \" + str(end - start))\\n# predict\\n\\nthresholds=[0.1, .2, .3, .4, .5, .6, .7, .8, .9]\\nfor val in thresholds:\\n    pred=predictions_d3.copy()\\n  \\n    pred[pred>=val]=1\\n    pred[pred<val]=0\\n\\n    precision = precision_score(Y_test_d3, pred, average=\\'micro\\')\\n    recall = recall_score(Y_test_d3, pred, average=\\'micro\\')\\n    f1 = f1_score(Y_test_d3, pred, average=\\'micro\\')\\n   \\n    print(\"Micro-average quality numbers\")\\n    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\\n    print(\"Accuracy = \",accuracy_score(Y_test_d3, pred))\\n    print(\"Partial Accuracy = \",partial_accuracy(Y_test_d3, pred))\\n    print(\"\\n\")'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "start = time.time()\n",
    "predictions_d3 = model.predict(x_test_d3.values)\n",
    "end = time.time()\n",
    "print(\"Time: \" + str(end - start))\n",
    "# predict\n",
    "\n",
    "thresholds=[0.1, .2, .3, .4, .5, .6, .7, .8, .9]\n",
    "for val in thresholds:\n",
    "    pred=predictions_d3.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "\n",
    "    precision = precision_score(Y_test_d3, pred, average='micro')\n",
    "    recall = recall_score(Y_test_d3, pred, average='micro')\n",
    "    f1 = f1_score(Y_test_d3, pred, average='micro')\n",
    "   \n",
    "    print(\"Micro-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "    print(\"Accuracy = \",accuracy_score(Y_test_d3, pred))\n",
    "    print(\"Partial Accuracy = \",partial_accuracy(Y_test_d3, pred))\n",
    "    print(\"\\n\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Epoch 1/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.6433 - accuracy: 0.0210 - val_loss: 0.5634 - val_accuracy: 0.0252\n",
      "Epoch 2/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.3369 - accuracy: 0.0216 - val_loss: 0.2582 - val_accuracy: 0.0252\n",
      "Epoch 3/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1913 - accuracy: 0.0228 - val_loss: 0.2525 - val_accuracy: 0.0198\n",
      "Epoch 4/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1877 - accuracy: 0.0264 - val_loss: 0.2511 - val_accuracy: 0.0216\n",
      "Epoch 5/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1868 - accuracy: 0.0240 - val_loss: 0.2498 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1864 - accuracy: 0.0090 - val_loss: 0.2495 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1862 - accuracy: 0.0156 - val_loss: 0.2494 - val_accuracy: 0.0018\n",
      "Epoch 8/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1860 - accuracy: 0.0348 - val_loss: 0.2495 - val_accuracy: 0.0450\n",
      "Epoch 9/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1859 - accuracy: 0.0444 - val_loss: 0.2487 - val_accuracy: 0.0342\n",
      "Epoch 10/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1858 - accuracy: 0.0348 - val_loss: 0.2486 - val_accuracy: 0.0216\n",
      "Epoch 11/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1857 - accuracy: 0.0228 - val_loss: 0.2484 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1856 - accuracy: 0.0144 - val_loss: 0.2489 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1855 - accuracy: 0.0120 - val_loss: 0.2485 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1854 - accuracy: 0.0192 - val_loss: 0.2477 - val_accuracy: 0.0595\n",
      "Epoch 15/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1853 - accuracy: 0.0408 - val_loss: 0.2476 - val_accuracy: 0.0378\n",
      "Epoch 16/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1852 - accuracy: 0.0318 - val_loss: 0.2481 - val_accuracy: 0.0090\n",
      "Epoch 17/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1851 - accuracy: 0.0336 - val_loss: 0.2482 - val_accuracy: 0.0072\n",
      "Epoch 18/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1850 - accuracy: 0.0462 - val_loss: 0.2480 - val_accuracy: 0.0036\n",
      "Epoch 19/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1849 - accuracy: 0.0264 - val_loss: 0.2477 - val_accuracy: 0.0541\n",
      "Epoch 20/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1848 - accuracy: 0.0511 - val_loss: 0.2478 - val_accuracy: 0.0018\n",
      "Epoch 21/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1847 - accuracy: 0.0198 - val_loss: 0.2470 - val_accuracy: 0.0468\n",
      "Epoch 22/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1846 - accuracy: 0.0553 - val_loss: 0.2468 - val_accuracy: 0.0378\n",
      "Epoch 23/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1845 - accuracy: 0.0414 - val_loss: 0.2469 - val_accuracy: 0.0342\n",
      "Epoch 24/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1845 - accuracy: 0.0384 - val_loss: 0.2463 - val_accuracy: 0.0360\n",
      "Epoch 25/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1843 - accuracy: 0.0318 - val_loss: 0.2453 - val_accuracy: 0.0018\n",
      "Epoch 26/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1843 - accuracy: 0.0300 - val_loss: 0.2460 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1842 - accuracy: 0.0306 - val_loss: 0.2463 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1840 - accuracy: 0.0324 - val_loss: 0.2464 - val_accuracy: 0.0126\n",
      "Epoch 29/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1839 - accuracy: 0.0282 - val_loss: 0.2460 - val_accuracy: 0.0378\n",
      "Epoch 30/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1838 - accuracy: 0.0396 - val_loss: 0.2450 - val_accuracy: 0.0414\n",
      "Epoch 31/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1837 - accuracy: 0.0511 - val_loss: 0.2450 - val_accuracy: 0.0432\n",
      "Epoch 32/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1836 - accuracy: 0.0360 - val_loss: 0.2456 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1835 - accuracy: 0.0330 - val_loss: 0.2451 - val_accuracy: 0.0739\n",
      "Epoch 34/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1834 - accuracy: 0.0679 - val_loss: 0.2452 - val_accuracy: 0.0486\n",
      "Epoch 35/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1833 - accuracy: 0.0583 - val_loss: 0.2451 - val_accuracy: 0.0811\n",
      "Epoch 36/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1831 - accuracy: 0.0841 - val_loss: 0.2441 - val_accuracy: 0.0090\n",
      "Epoch 37/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1830 - accuracy: 0.0565 - val_loss: 0.2449 - val_accuracy: 0.0018\n",
      "Epoch 38/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1829 - accuracy: 0.0270 - val_loss: 0.2439 - val_accuracy: 0.0559\n",
      "Epoch 39/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1827 - accuracy: 0.0547 - val_loss: 0.2439 - val_accuracy: 0.0577\n",
      "Epoch 40/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1826 - accuracy: 0.0589 - val_loss: 0.2437 - val_accuracy: 0.0613\n",
      "Epoch 41/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1825 - accuracy: 0.0721 - val_loss: 0.2439 - val_accuracy: 0.0505\n",
      "Epoch 42/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1823 - accuracy: 0.0577 - val_loss: 0.2429 - val_accuracy: 0.0775\n",
      "Epoch 43/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1822 - accuracy: 0.0715 - val_loss: 0.2428 - val_accuracy: 0.0559\n",
      "Epoch 44/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1820 - accuracy: 0.0583 - val_loss: 0.2426 - val_accuracy: 0.0541\n",
      "Epoch 45/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1819 - accuracy: 0.0697 - val_loss: 0.2422 - val_accuracy: 0.0991\n",
      "Epoch 46/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1817 - accuracy: 0.0913 - val_loss: 0.2420 - val_accuracy: 0.0811\n",
      "Epoch 47/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1815 - accuracy: 0.0721 - val_loss: 0.2424 - val_accuracy: 0.0432\n",
      "Epoch 48/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1813 - accuracy: 0.0649 - val_loss: 0.2423 - val_accuracy: 0.0270\n",
      "Epoch 49/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1813 - accuracy: 0.0498 - val_loss: 0.2422 - val_accuracy: 0.0865\n",
      "Epoch 50/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1810 - accuracy: 0.0799 - val_loss: 0.2418 - val_accuracy: 0.0559\n",
      "Epoch 51/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1808 - accuracy: 0.0691 - val_loss: 0.2409 - val_accuracy: 0.0631\n",
      "Epoch 52/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1806 - accuracy: 0.0799 - val_loss: 0.2406 - val_accuracy: 0.0847\n",
      "Epoch 53/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1805 - accuracy: 0.1081 - val_loss: 0.2400 - val_accuracy: 0.0414\n",
      "Epoch 54/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1803 - accuracy: 0.0739 - val_loss: 0.2400 - val_accuracy: 0.0919\n",
      "Epoch 55/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1801 - accuracy: 0.1171 - val_loss: 0.2391 - val_accuracy: 0.1369\n",
      "Epoch 56/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1798 - accuracy: 0.1039 - val_loss: 0.2392 - val_accuracy: 0.1099\n",
      "Epoch 57/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1796 - accuracy: 0.1123 - val_loss: 0.2387 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1793 - accuracy: 0.1225 - val_loss: 0.2382 - val_accuracy: 0.1730\n",
      "Epoch 59/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1791 - accuracy: 0.1297 - val_loss: 0.2381 - val_accuracy: 0.0991\n",
      "Epoch 60/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1788 - accuracy: 0.1117 - val_loss: 0.2379 - val_accuracy: 0.0505\n",
      "Epoch 61/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1785 - accuracy: 0.0889 - val_loss: 0.2373 - val_accuracy: 0.1514\n",
      "Epoch 62/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1782 - accuracy: 0.1141 - val_loss: 0.2376 - val_accuracy: 0.1063\n",
      "Epoch 63/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1779 - accuracy: 0.1225 - val_loss: 0.2367 - val_accuracy: 0.1117\n",
      "Epoch 64/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1776 - accuracy: 0.1333 - val_loss: 0.2360 - val_accuracy: 0.1099\n",
      "Epoch 65/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1773 - accuracy: 0.1195 - val_loss: 0.2351 - val_accuracy: 0.1423\n",
      "Epoch 66/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1769 - accuracy: 0.1105 - val_loss: 0.2346 - val_accuracy: 0.1802\n",
      "Epoch 67/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1766 - accuracy: 0.1201 - val_loss: 0.2338 - val_accuracy: 0.1766\n",
      "Epoch 68/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1762 - accuracy: 0.1369 - val_loss: 0.2336 - val_accuracy: 0.0973\n",
      "Epoch 69/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1759 - accuracy: 0.1171 - val_loss: 0.2330 - val_accuracy: 0.1153\n",
      "Epoch 70/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1755 - accuracy: 0.1225 - val_loss: 0.2325 - val_accuracy: 0.1928\n",
      "Epoch 71/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1750 - accuracy: 0.1393 - val_loss: 0.2324 - val_accuracy: 0.1712\n",
      "Epoch 72/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1746 - accuracy: 0.1508 - val_loss: 0.2321 - val_accuracy: 0.1099\n",
      "Epoch 73/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1742 - accuracy: 0.1279 - val_loss: 0.2308 - val_accuracy: 0.1694\n",
      "Epoch 74/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1738 - accuracy: 0.1405 - val_loss: 0.2308 - val_accuracy: 0.2018\n",
      "Epoch 75/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1733 - accuracy: 0.1520 - val_loss: 0.2300 - val_accuracy: 0.1820\n",
      "Epoch 76/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1728 - accuracy: 0.1423 - val_loss: 0.2286 - val_accuracy: 0.2234\n",
      "Epoch 77/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1723 - accuracy: 0.1544 - val_loss: 0.2287 - val_accuracy: 0.1514\n",
      "Epoch 78/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1719 - accuracy: 0.1441 - val_loss: 0.2271 - val_accuracy: 0.1676\n",
      "Epoch 79/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1713 - accuracy: 0.1387 - val_loss: 0.2267 - val_accuracy: 0.1532\n",
      "Epoch 80/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1708 - accuracy: 0.1351 - val_loss: 0.2248 - val_accuracy: 0.2162\n",
      "Epoch 81/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1702 - accuracy: 0.1556 - val_loss: 0.2246 - val_accuracy: 0.1640\n",
      "Epoch 82/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1696 - accuracy: 0.1273 - val_loss: 0.2237 - val_accuracy: 0.2054\n",
      "Epoch 83/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1690 - accuracy: 0.1574 - val_loss: 0.2233 - val_accuracy: 0.1928\n",
      "Epoch 84/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1684 - accuracy: 0.1646 - val_loss: 0.2208 - val_accuracy: 0.2126\n",
      "Epoch 85/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1679 - accuracy: 0.1628 - val_loss: 0.2206 - val_accuracy: 0.1640\n",
      "Epoch 86/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1673 - accuracy: 0.1489 - val_loss: 0.2202 - val_accuracy: 0.1838\n",
      "Epoch 87/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1666 - accuracy: 0.1598 - val_loss: 0.2200 - val_accuracy: 0.1964\n",
      "Epoch 88/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1660 - accuracy: 0.1634 - val_loss: 0.2185 - val_accuracy: 0.2144\n",
      "Epoch 89/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1654 - accuracy: 0.1556 - val_loss: 0.2178 - val_accuracy: 0.2559\n",
      "Epoch 90/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1648 - accuracy: 0.1868 - val_loss: 0.2172 - val_accuracy: 0.2396\n",
      "Epoch 91/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1641 - accuracy: 0.1736 - val_loss: 0.2169 - val_accuracy: 0.1910\n",
      "Epoch 92/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1636 - accuracy: 0.1658 - val_loss: 0.2154 - val_accuracy: 0.1568\n",
      "Epoch 93/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1630 - accuracy: 0.1520 - val_loss: 0.2144 - val_accuracy: 0.2396\n",
      "Epoch 94/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1623 - accuracy: 0.1748 - val_loss: 0.2138 - val_accuracy: 0.2036\n",
      "Epoch 95/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1618 - accuracy: 0.1628 - val_loss: 0.2128 - val_accuracy: 0.1766\n",
      "Epoch 96/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1611 - accuracy: 0.1514 - val_loss: 0.2122 - val_accuracy: 0.2396\n",
      "Epoch 97/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1606 - accuracy: 0.1814 - val_loss: 0.2110 - val_accuracy: 0.2090\n",
      "Epoch 98/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1601 - accuracy: 0.1718 - val_loss: 0.2099 - val_accuracy: 0.2216\n",
      "Epoch 99/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1595 - accuracy: 0.1736 - val_loss: 0.2099 - val_accuracy: 0.2505\n",
      "Epoch 100/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1589 - accuracy: 0.1844 - val_loss: 0.2088 - val_accuracy: 0.2216\n",
      "Epoch 101/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1584 - accuracy: 0.1790 - val_loss: 0.2084 - val_accuracy: 0.2342\n",
      "Epoch 102/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1578 - accuracy: 0.1952 - val_loss: 0.2080 - val_accuracy: 0.2198\n",
      "Epoch 103/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1573 - accuracy: 0.1754 - val_loss: 0.2073 - val_accuracy: 0.2198\n",
      "Epoch 104/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1567 - accuracy: 0.1850 - val_loss: 0.2055 - val_accuracy: 0.1892\n",
      "Epoch 105/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1563 - accuracy: 0.1778 - val_loss: 0.2046 - val_accuracy: 0.2324\n",
      "Epoch 106/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1557 - accuracy: 0.1850 - val_loss: 0.2071 - val_accuracy: 0.2036\n",
      "Epoch 107/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1554 - accuracy: 0.1868 - val_loss: 0.2042 - val_accuracy: 0.2396\n",
      "Epoch 108/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1547 - accuracy: 0.1838 - val_loss: 0.2030 - val_accuracy: 0.2468\n",
      "Epoch 109/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1542 - accuracy: 0.1940 - val_loss: 0.2013 - val_accuracy: 0.2108\n",
      "Epoch 110/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1538 - accuracy: 0.1868 - val_loss: 0.2024 - val_accuracy: 0.2126\n",
      "Epoch 111/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1533 - accuracy: 0.1808 - val_loss: 0.2023 - val_accuracy: 0.2198\n",
      "Epoch 112/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1528 - accuracy: 0.1880 - val_loss: 0.2016 - val_accuracy: 0.1946\n",
      "Epoch 113/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1524 - accuracy: 0.1868 - val_loss: 0.1999 - val_accuracy: 0.2523\n",
      "Epoch 114/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1518 - accuracy: 0.2000 - val_loss: 0.2006 - val_accuracy: 0.2505\n",
      "Epoch 115/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1515 - accuracy: 0.1886 - val_loss: 0.1987 - val_accuracy: 0.2523\n",
      "Epoch 116/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1509 - accuracy: 0.1952 - val_loss: 0.1984 - val_accuracy: 0.2324\n",
      "Epoch 117/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1504 - accuracy: 0.1916 - val_loss: 0.1980 - val_accuracy: 0.2505\n",
      "Epoch 118/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1498 - accuracy: 0.1940 - val_loss: 0.1970 - val_accuracy: 0.2162\n",
      "Epoch 119/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1494 - accuracy: 0.1898 - val_loss: 0.1960 - val_accuracy: 0.2018\n",
      "Epoch 120/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1490 - accuracy: 0.1874 - val_loss: 0.1978 - val_accuracy: 0.2523\n",
      "Epoch 121/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1486 - accuracy: 0.2006 - val_loss: 0.1996 - val_accuracy: 0.1982\n",
      "Epoch 122/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1481 - accuracy: 0.1946 - val_loss: 0.1941 - val_accuracy: 0.2685\n",
      "Epoch 123/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1474 - accuracy: 0.2018 - val_loss: 0.1943 - val_accuracy: 0.2342\n",
      "Epoch 124/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1469 - accuracy: 0.1994 - val_loss: 0.1931 - val_accuracy: 0.2198\n",
      "Epoch 125/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1465 - accuracy: 0.1922 - val_loss: 0.1950 - val_accuracy: 0.2595\n",
      "Epoch 126/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1460 - accuracy: 0.2048 - val_loss: 0.1918 - val_accuracy: 0.2685\n",
      "Epoch 127/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1454 - accuracy: 0.2018 - val_loss: 0.1921 - val_accuracy: 0.2486\n",
      "Epoch 128/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1450 - accuracy: 0.1994 - val_loss: 0.1905 - val_accuracy: 0.2252\n",
      "Epoch 129/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1445 - accuracy: 0.1988 - val_loss: 0.1900 - val_accuracy: 0.2216\n",
      "Epoch 130/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1440 - accuracy: 0.2042 - val_loss: 0.1904 - val_accuracy: 0.2703\n",
      "Epoch 131/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1434 - accuracy: 0.2120 - val_loss: 0.1921 - val_accuracy: 0.2090\n",
      "Epoch 132/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1431 - accuracy: 0.1946 - val_loss: 0.1886 - val_accuracy: 0.2739\n",
      "Epoch 133/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1425 - accuracy: 0.2090 - val_loss: 0.1902 - val_accuracy: 0.2450\n",
      "Epoch 134/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1421 - accuracy: 0.2120 - val_loss: 0.1909 - val_accuracy: 0.2505\n",
      "Epoch 135/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1416 - accuracy: 0.2096 - val_loss: 0.1870 - val_accuracy: 0.2468\n",
      "Epoch 136/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1410 - accuracy: 0.2120 - val_loss: 0.1867 - val_accuracy: 0.2757\n",
      "Epoch 137/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1405 - accuracy: 0.2084 - val_loss: 0.1861 - val_accuracy: 0.2901\n",
      "Epoch 138/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1400 - accuracy: 0.2126 - val_loss: 0.1847 - val_accuracy: 0.2486\n",
      "Epoch 139/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1395 - accuracy: 0.2108 - val_loss: 0.1894 - val_accuracy: 0.2270\n",
      "Epoch 140/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1392 - accuracy: 0.2192 - val_loss: 0.1849 - val_accuracy: 0.2252\n",
      "Epoch 141/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1385 - accuracy: 0.2096 - val_loss: 0.1830 - val_accuracy: 0.2595\n",
      "Epoch 142/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1380 - accuracy: 0.2216 - val_loss: 0.1830 - val_accuracy: 0.2595\n",
      "Epoch 143/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1374 - accuracy: 0.2150 - val_loss: 0.1828 - val_accuracy: 0.2613\n",
      "Epoch 144/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1370 - accuracy: 0.2288 - val_loss: 0.1806 - val_accuracy: 0.2523\n",
      "Epoch 145/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1364 - accuracy: 0.2132 - val_loss: 0.1800 - val_accuracy: 0.2739\n",
      "Epoch 146/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1358 - accuracy: 0.2240 - val_loss: 0.1794 - val_accuracy: 0.2829\n",
      "Epoch 147/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1353 - accuracy: 0.2336 - val_loss: 0.1787 - val_accuracy: 0.2667\n",
      "Epoch 148/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1348 - accuracy: 0.2204 - val_loss: 0.1786 - val_accuracy: 0.3117\n",
      "Epoch 149/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1343 - accuracy: 0.2318 - val_loss: 0.1793 - val_accuracy: 0.2883\n",
      "Epoch 150/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1339 - accuracy: 0.2276 - val_loss: 0.1775 - val_accuracy: 0.2955\n",
      "Epoch 151/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1333 - accuracy: 0.2324 - val_loss: 0.1758 - val_accuracy: 0.2703\n",
      "Epoch 152/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1328 - accuracy: 0.2270 - val_loss: 0.1762 - val_accuracy: 0.2793\n",
      "Epoch 153/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1322 - accuracy: 0.2360 - val_loss: 0.1758 - val_accuracy: 0.2721\n",
      "Epoch 154/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1318 - accuracy: 0.2414 - val_loss: 0.1747 - val_accuracy: 0.2667\n",
      "Epoch 155/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1312 - accuracy: 0.2330 - val_loss: 0.1733 - val_accuracy: 0.2703\n",
      "Epoch 156/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1307 - accuracy: 0.2366 - val_loss: 0.1741 - val_accuracy: 0.2865\n",
      "Epoch 157/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1302 - accuracy: 0.2402 - val_loss: 0.1739 - val_accuracy: 0.2757\n",
      "Epoch 158/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1297 - accuracy: 0.2426 - val_loss: 0.1722 - val_accuracy: 0.2595\n",
      "Epoch 159/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1292 - accuracy: 0.2348 - val_loss: 0.1702 - val_accuracy: 0.2703\n",
      "Epoch 160/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1288 - accuracy: 0.2318 - val_loss: 0.1700 - val_accuracy: 0.2919\n",
      "Epoch 161/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1282 - accuracy: 0.2420 - val_loss: 0.1701 - val_accuracy: 0.2649\n",
      "Epoch 162/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1279 - accuracy: 0.2462 - val_loss: 0.1749 - val_accuracy: 0.2703\n",
      "Epoch 163/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1276 - accuracy: 0.2426 - val_loss: 0.1696 - val_accuracy: 0.3153\n",
      "Epoch 164/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1268 - accuracy: 0.2438 - val_loss: 0.1685 - val_accuracy: 0.2937\n",
      "Epoch 165/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1264 - accuracy: 0.2456 - val_loss: 0.1684 - val_accuracy: 0.3045\n",
      "Epoch 166/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1260 - accuracy: 0.2505 - val_loss: 0.1668 - val_accuracy: 0.3009\n",
      "Epoch 167/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1255 - accuracy: 0.2517 - val_loss: 0.1672 - val_accuracy: 0.3207\n",
      "Epoch 168/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1250 - accuracy: 0.2486 - val_loss: 0.1648 - val_accuracy: 0.2811\n",
      "Epoch 169/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1245 - accuracy: 0.2511 - val_loss: 0.1658 - val_accuracy: 0.2775\n",
      "Epoch 170/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1241 - accuracy: 0.2565 - val_loss: 0.1669 - val_accuracy: 0.3171\n",
      "Epoch 171/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1238 - accuracy: 0.2462 - val_loss: 0.1648 - val_accuracy: 0.3189\n",
      "Epoch 172/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1232 - accuracy: 0.2517 - val_loss: 0.1639 - val_accuracy: 0.3261\n",
      "Epoch 173/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1230 - accuracy: 0.2505 - val_loss: 0.1655 - val_accuracy: 0.3153\n",
      "Epoch 174/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1224 - accuracy: 0.2625 - val_loss: 0.1642 - val_accuracy: 0.3171\n",
      "Epoch 175/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1220 - accuracy: 0.2547 - val_loss: 0.1622 - val_accuracy: 0.3099\n",
      "Epoch 176/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1216 - accuracy: 0.2553 - val_loss: 0.1617 - val_accuracy: 0.3135\n",
      "Epoch 177/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1212 - accuracy: 0.2583 - val_loss: 0.1623 - val_accuracy: 0.3045\n",
      "Epoch 178/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1209 - accuracy: 0.2583 - val_loss: 0.1614 - val_accuracy: 0.3207\n",
      "Epoch 179/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1205 - accuracy: 0.2601 - val_loss: 0.1609 - val_accuracy: 0.2937\n",
      "Epoch 180/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1201 - accuracy: 0.2571 - val_loss: 0.1593 - val_accuracy: 0.2919\n",
      "Epoch 181/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1196 - accuracy: 0.2505 - val_loss: 0.1599 - val_accuracy: 0.3369\n",
      "Epoch 182/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1193 - accuracy: 0.2577 - val_loss: 0.1636 - val_accuracy: 0.3369\n",
      "Epoch 183/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1192 - accuracy: 0.2601 - val_loss: 0.1643 - val_accuracy: 0.3441\n",
      "Epoch 184/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1189 - accuracy: 0.2601 - val_loss: 0.1586 - val_accuracy: 0.2847\n",
      "Epoch 185/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1183 - accuracy: 0.2595 - val_loss: 0.1581 - val_accuracy: 0.2865\n",
      "Epoch 186/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1180 - accuracy: 0.2492 - val_loss: 0.1578 - val_accuracy: 0.3045\n",
      "Epoch 187/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1176 - accuracy: 0.2625 - val_loss: 0.1574 - val_accuracy: 0.3099\n",
      "Epoch 188/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1173 - accuracy: 0.2679 - val_loss: 0.1579 - val_accuracy: 0.3333\n",
      "Epoch 189/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1170 - accuracy: 0.2703 - val_loss: 0.1564 - val_accuracy: 0.3225\n",
      "Epoch 190/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1167 - accuracy: 0.2559 - val_loss: 0.1598 - val_accuracy: 0.3009\n",
      "Epoch 191/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1165 - accuracy: 0.2679 - val_loss: 0.1559 - val_accuracy: 0.3081\n",
      "Epoch 192/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1160 - accuracy: 0.2607 - val_loss: 0.1548 - val_accuracy: 0.3045\n",
      "Epoch 193/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1157 - accuracy: 0.2613 - val_loss: 0.1573 - val_accuracy: 0.3081\n",
      "Epoch 194/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1154 - accuracy: 0.2673 - val_loss: 0.1556 - val_accuracy: 0.3369\n",
      "Epoch 195/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1151 - accuracy: 0.2679 - val_loss: 0.1558 - val_accuracy: 0.3423\n",
      "Epoch 196/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1148 - accuracy: 0.2679 - val_loss: 0.1545 - val_accuracy: 0.3225\n",
      "Epoch 197/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1145 - accuracy: 0.2751 - val_loss: 0.1539 - val_accuracy: 0.3135\n",
      "Epoch 198/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1141 - accuracy: 0.2673 - val_loss: 0.1551 - val_accuracy: 0.3171\n",
      "Epoch 199/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1139 - accuracy: 0.2649 - val_loss: 0.1546 - val_accuracy: 0.3387\n",
      "Epoch 200/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1135 - accuracy: 0.2721 - val_loss: 0.1601 - val_accuracy: 0.2685\n",
      "Epoch 201/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1136 - accuracy: 0.2625 - val_loss: 0.1515 - val_accuracy: 0.3099\n",
      "Epoch 202/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1130 - accuracy: 0.2715 - val_loss: 0.1589 - val_accuracy: 0.3081\n",
      "Epoch 203/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1129 - accuracy: 0.2751 - val_loss: 0.1521 - val_accuracy: 0.3315\n",
      "Epoch 204/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1123 - accuracy: 0.2745 - val_loss: 0.1530 - val_accuracy: 0.3189\n",
      "Epoch 205/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1121 - accuracy: 0.2769 - val_loss: 0.1515 - val_accuracy: 0.2973\n",
      "Epoch 206/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1118 - accuracy: 0.2667 - val_loss: 0.1524 - val_accuracy: 0.3351\n",
      "Epoch 207/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1115 - accuracy: 0.2775 - val_loss: 0.1547 - val_accuracy: 0.3171\n",
      "Epoch 208/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1114 - accuracy: 0.2745 - val_loss: 0.1502 - val_accuracy: 0.3207\n",
      "Epoch 209/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1109 - accuracy: 0.2727 - val_loss: 0.1504 - val_accuracy: 0.3333\n",
      "Epoch 210/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1105 - accuracy: 0.2727 - val_loss: 0.1508 - val_accuracy: 0.3261\n",
      "Epoch 211/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1104 - accuracy: 0.2805 - val_loss: 0.1570 - val_accuracy: 0.2468\n",
      "Epoch 212/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1106 - accuracy: 0.2679 - val_loss: 0.1500 - val_accuracy: 0.3405\n",
      "Epoch 213/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1097 - accuracy: 0.2799 - val_loss: 0.1504 - val_accuracy: 0.3387\n",
      "Epoch 214/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1096 - accuracy: 0.2835 - val_loss: 0.1510 - val_accuracy: 0.3676\n",
      "Epoch 215/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1093 - accuracy: 0.2811 - val_loss: 0.1514 - val_accuracy: 0.3189\n",
      "Epoch 216/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1090 - accuracy: 0.2853 - val_loss: 0.1482 - val_accuracy: 0.3297\n",
      "Epoch 217/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1086 - accuracy: 0.2835 - val_loss: 0.1490 - val_accuracy: 0.3369\n",
      "Epoch 218/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1085 - accuracy: 0.2811 - val_loss: 0.1482 - val_accuracy: 0.3171\n",
      "Epoch 219/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1081 - accuracy: 0.2745 - val_loss: 0.1481 - val_accuracy: 0.3586\n",
      "Epoch 220/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1078 - accuracy: 0.2811 - val_loss: 0.1480 - val_accuracy: 0.3459\n",
      "Epoch 221/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1074 - accuracy: 0.2877 - val_loss: 0.1476 - val_accuracy: 0.3532\n",
      "Epoch 222/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1072 - accuracy: 0.2931 - val_loss: 0.1474 - val_accuracy: 0.3441\n",
      "Epoch 223/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1069 - accuracy: 0.2907 - val_loss: 0.1475 - val_accuracy: 0.3459\n",
      "Epoch 224/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1065 - accuracy: 0.2877 - val_loss: 0.1460 - val_accuracy: 0.3532\n",
      "Epoch 225/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1064 - accuracy: 0.2913 - val_loss: 0.1465 - val_accuracy: 0.3261\n",
      "Epoch 226/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1061 - accuracy: 0.2865 - val_loss: 0.1450 - val_accuracy: 0.3261\n",
      "Epoch 227/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1057 - accuracy: 0.2805 - val_loss: 0.1453 - val_accuracy: 0.3459\n",
      "Epoch 228/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1056 - accuracy: 0.2889 - val_loss: 0.1465 - val_accuracy: 0.3387\n",
      "Epoch 229/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1052 - accuracy: 0.2871 - val_loss: 0.1458 - val_accuracy: 0.3387\n",
      "Epoch 230/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1049 - accuracy: 0.2913 - val_loss: 0.1482 - val_accuracy: 0.3568\n",
      "Epoch 231/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1048 - accuracy: 0.2889 - val_loss: 0.1447 - val_accuracy: 0.3351\n",
      "Epoch 232/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1042 - accuracy: 0.2883 - val_loss: 0.1439 - val_accuracy: 0.3766\n",
      "Epoch 233/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1039 - accuracy: 0.2925 - val_loss: 0.1446 - val_accuracy: 0.3550\n",
      "Epoch 234/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1040 - accuracy: 0.2907 - val_loss: 0.1440 - val_accuracy: 0.3568\n",
      "Epoch 235/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1034 - accuracy: 0.2877 - val_loss: 0.1474 - val_accuracy: 0.3784\n",
      "Epoch 236/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1036 - accuracy: 0.2973 - val_loss: 0.1465 - val_accuracy: 0.3712\n",
      "Epoch 237/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1033 - accuracy: 0.2949 - val_loss: 0.1445 - val_accuracy: 0.3387\n",
      "Epoch 238/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1027 - accuracy: 0.2949 - val_loss: 0.1429 - val_accuracy: 0.3622\n",
      "Epoch 239/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1024 - accuracy: 0.2967 - val_loss: 0.1418 - val_accuracy: 0.3622\n",
      "Epoch 240/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1021 - accuracy: 0.2931 - val_loss: 0.1424 - val_accuracy: 0.3171\n",
      "Epoch 241/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1019 - accuracy: 0.2805 - val_loss: 0.1416 - val_accuracy: 0.3676\n",
      "Epoch 242/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1016 - accuracy: 0.2895 - val_loss: 0.1425 - val_accuracy: 0.3838\n",
      "Epoch 243/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1013 - accuracy: 0.3075 - val_loss: 0.1428 - val_accuracy: 0.3694\n",
      "Epoch 244/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1011 - accuracy: 0.3027 - val_loss: 0.1421 - val_accuracy: 0.3514\n",
      "Epoch 245/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1008 - accuracy: 0.2973 - val_loss: 0.1411 - val_accuracy: 0.3640\n",
      "Epoch 246/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1005 - accuracy: 0.2955 - val_loss: 0.1414 - val_accuracy: 0.3730\n",
      "Epoch 247/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1003 - accuracy: 0.2895 - val_loss: 0.1420 - val_accuracy: 0.3622\n",
      "Epoch 248/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1001 - accuracy: 0.2955 - val_loss: 0.1484 - val_accuracy: 0.2432\n",
      "Epoch 249/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1003 - accuracy: 0.2847 - val_loss: 0.1441 - val_accuracy: 0.3694\n",
      "Epoch 250/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0997 - accuracy: 0.2955 - val_loss: 0.1422 - val_accuracy: 0.3766\n",
      "Epoch 251/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0993 - accuracy: 0.2979 - val_loss: 0.1439 - val_accuracy: 0.2847\n",
      "Epoch 252/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0995 - accuracy: 0.2985 - val_loss: 0.1500 - val_accuracy: 0.2721\n",
      "Epoch 253/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0993 - accuracy: 0.2871 - val_loss: 0.1403 - val_accuracy: 0.3748\n",
      "Epoch 254/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0987 - accuracy: 0.2991 - val_loss: 0.1392 - val_accuracy: 0.3802\n",
      "Epoch 255/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0983 - accuracy: 0.3045 - val_loss: 0.1397 - val_accuracy: 0.3532\n",
      "Epoch 256/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0983 - accuracy: 0.2955 - val_loss: 0.1416 - val_accuracy: 0.3514\n",
      "Epoch 257/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0980 - accuracy: 0.3009 - val_loss: 0.1390 - val_accuracy: 0.3874\n",
      "Epoch 258/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0976 - accuracy: 0.2997 - val_loss: 0.1382 - val_accuracy: 0.3550\n",
      "Epoch 259/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0976 - accuracy: 0.3015 - val_loss: 0.1393 - val_accuracy: 0.3784\n",
      "Epoch 260/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0975 - accuracy: 0.3069 - val_loss: 0.1397 - val_accuracy: 0.3856\n",
      "Epoch 261/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0972 - accuracy: 0.2979 - val_loss: 0.1379 - val_accuracy: 0.3640\n",
      "Epoch 262/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0968 - accuracy: 0.2955 - val_loss: 0.1399 - val_accuracy: 0.3802\n",
      "Epoch 263/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0967 - accuracy: 0.3057 - val_loss: 0.1376 - val_accuracy: 0.3676\n",
      "Epoch 264/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0965 - accuracy: 0.2955 - val_loss: 0.1379 - val_accuracy: 0.3910\n",
      "Epoch 265/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0963 - accuracy: 0.3009 - val_loss: 0.1396 - val_accuracy: 0.3676\n",
      "Epoch 266/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0960 - accuracy: 0.3051 - val_loss: 0.1416 - val_accuracy: 0.2991\n",
      "Epoch 267/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0964 - accuracy: 0.2943 - val_loss: 0.1390 - val_accuracy: 0.3928\n",
      "Epoch 268/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0957 - accuracy: 0.3069 - val_loss: 0.1373 - val_accuracy: 0.3712\n",
      "Epoch 269/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0954 - accuracy: 0.3069 - val_loss: 0.1380 - val_accuracy: 0.3784\n",
      "Epoch 270/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0952 - accuracy: 0.3141 - val_loss: 0.1362 - val_accuracy: 0.3640\n",
      "Epoch 271/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0950 - accuracy: 0.3039 - val_loss: 0.1366 - val_accuracy: 0.3622\n",
      "Epoch 272/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0949 - accuracy: 0.3057 - val_loss: 0.1374 - val_accuracy: 0.3640\n",
      "Epoch 273/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0947 - accuracy: 0.3057 - val_loss: 0.1365 - val_accuracy: 0.3550\n",
      "Epoch 274/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0946 - accuracy: 0.3081 - val_loss: 0.1359 - val_accuracy: 0.3856\n",
      "Epoch 275/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0943 - accuracy: 0.2973 - val_loss: 0.1376 - val_accuracy: 0.3766\n",
      "Epoch 276/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0944 - accuracy: 0.3093 - val_loss: 0.1356 - val_accuracy: 0.3838\n",
      "Epoch 277/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0940 - accuracy: 0.2979 - val_loss: 0.1368 - val_accuracy: 0.3856\n",
      "Epoch 278/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0939 - accuracy: 0.3057 - val_loss: 0.1362 - val_accuracy: 0.3820\n",
      "Epoch 279/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0937 - accuracy: 0.3117 - val_loss: 0.1356 - val_accuracy: 0.3730\n",
      "Epoch 280/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0935 - accuracy: 0.3009 - val_loss: 0.1360 - val_accuracy: 0.3640\n",
      "Epoch 281/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0933 - accuracy: 0.3087 - val_loss: 0.1359 - val_accuracy: 0.3712\n",
      "Epoch 282/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0931 - accuracy: 0.3081 - val_loss: 0.1354 - val_accuracy: 0.3532\n",
      "Epoch 283/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0931 - accuracy: 0.3051 - val_loss: 0.1466 - val_accuracy: 0.3423\n",
      "Epoch 284/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0937 - accuracy: 0.3075 - val_loss: 0.1354 - val_accuracy: 0.3802\n",
      "Epoch 285/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0928 - accuracy: 0.2979 - val_loss: 0.1373 - val_accuracy: 0.3550\n",
      "Epoch 286/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0928 - accuracy: 0.3009 - val_loss: 0.1360 - val_accuracy: 0.3982\n",
      "Epoch 287/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0926 - accuracy: 0.3105 - val_loss: 0.1342 - val_accuracy: 0.3856\n",
      "Epoch 288/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0922 - accuracy: 0.3027 - val_loss: 0.1353 - val_accuracy: 0.3495\n",
      "Epoch 289/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0921 - accuracy: 0.3009 - val_loss: 0.1351 - val_accuracy: 0.3477\n",
      "Epoch 290/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0920 - accuracy: 0.3111 - val_loss: 0.1341 - val_accuracy: 0.3748\n",
      "Epoch 291/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0918 - accuracy: 0.3015 - val_loss: 0.1353 - val_accuracy: 0.3802\n",
      "Epoch 292/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0917 - accuracy: 0.2997 - val_loss: 0.1388 - val_accuracy: 0.3730\n",
      "Epoch 293/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0916 - accuracy: 0.3171 - val_loss: 0.1345 - val_accuracy: 0.3730\n",
      "Epoch 294/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0913 - accuracy: 0.3057 - val_loss: 0.1327 - val_accuracy: 0.3838\n",
      "Epoch 295/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0912 - accuracy: 0.3033 - val_loss: 0.1345 - val_accuracy: 0.3856\n",
      "Epoch 296/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0910 - accuracy: 0.3147 - val_loss: 0.1342 - val_accuracy: 0.3730\n",
      "Epoch 297/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0910 - accuracy: 0.3111 - val_loss: 0.1340 - val_accuracy: 0.3802\n",
      "Epoch 298/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0908 - accuracy: 0.3051 - val_loss: 0.1330 - val_accuracy: 0.3730\n",
      "Epoch 299/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0905 - accuracy: 0.3087 - val_loss: 0.1349 - val_accuracy: 0.3856\n",
      "Epoch 300/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0906 - accuracy: 0.3087 - val_loss: 0.1351 - val_accuracy: 0.3802\n",
      "Epoch 301/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0907 - accuracy: 0.3087 - val_loss: 0.1330 - val_accuracy: 0.3568\n",
      "Epoch 302/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0903 - accuracy: 0.3021 - val_loss: 0.1388 - val_accuracy: 0.3784\n",
      "Epoch 303/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0904 - accuracy: 0.3021 - val_loss: 0.1339 - val_accuracy: 0.3766\n",
      "Epoch 304/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0899 - accuracy: 0.3033 - val_loss: 0.1359 - val_accuracy: 0.3640\n",
      "Epoch 305/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0899 - accuracy: 0.3117 - val_loss: 0.1340 - val_accuracy: 0.3766\n",
      "Epoch 306/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0898 - accuracy: 0.3111 - val_loss: 0.1324 - val_accuracy: 0.3820\n",
      "Epoch 307/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0896 - accuracy: 0.3093 - val_loss: 0.1316 - val_accuracy: 0.3766\n",
      "Epoch 308/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0893 - accuracy: 0.3039 - val_loss: 0.1352 - val_accuracy: 0.3874\n",
      "Epoch 309/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0894 - accuracy: 0.3117 - val_loss: 0.1349 - val_accuracy: 0.3748\n",
      "Epoch 310/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0894 - accuracy: 0.3069 - val_loss: 0.1325 - val_accuracy: 0.3802\n",
      "Epoch 311/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0890 - accuracy: 0.3063 - val_loss: 0.1343 - val_accuracy: 0.3874\n",
      "Epoch 312/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0891 - accuracy: 0.3021 - val_loss: 0.1333 - val_accuracy: 0.3784\n",
      "Epoch 313/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0888 - accuracy: 0.3093 - val_loss: 0.1332 - val_accuracy: 0.3604\n",
      "Epoch 314/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0888 - accuracy: 0.3039 - val_loss: 0.1321 - val_accuracy: 0.3658\n",
      "Epoch 315/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0886 - accuracy: 0.3075 - val_loss: 0.1311 - val_accuracy: 0.3676\n",
      "Epoch 316/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0885 - accuracy: 0.3069 - val_loss: 0.1305 - val_accuracy: 0.3730\n",
      "Epoch 317/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0883 - accuracy: 0.3063 - val_loss: 0.1314 - val_accuracy: 0.3784\n",
      "Epoch 318/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0882 - accuracy: 0.3123 - val_loss: 0.1324 - val_accuracy: 0.3982\n",
      "Epoch 319/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0881 - accuracy: 0.3069 - val_loss: 0.1390 - val_accuracy: 0.3622\n",
      "Epoch 320/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0885 - accuracy: 0.3087 - val_loss: 0.1316 - val_accuracy: 0.3730\n",
      "Epoch 321/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0879 - accuracy: 0.3129 - val_loss: 0.1307 - val_accuracy: 0.3874\n",
      "Epoch 322/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0878 - accuracy: 0.3063 - val_loss: 0.1472 - val_accuracy: 0.3117\n",
      "Epoch 323/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0883 - accuracy: 0.3165 - val_loss: 0.1312 - val_accuracy: 0.3820\n",
      "Epoch 324/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0874 - accuracy: 0.3087 - val_loss: 0.1325 - val_accuracy: 0.3730\n",
      "Epoch 325/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0874 - accuracy: 0.3165 - val_loss: 0.1450 - val_accuracy: 0.3459\n",
      "Epoch 326/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0879 - accuracy: 0.3075 - val_loss: 0.1323 - val_accuracy: 0.3568\n",
      "Epoch 327/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0873 - accuracy: 0.3117 - val_loss: 0.1301 - val_accuracy: 0.3856\n",
      "Epoch 328/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0870 - accuracy: 0.3069 - val_loss: 0.1327 - val_accuracy: 0.3676\n",
      "Epoch 329/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0872 - accuracy: 0.3075 - val_loss: 0.1309 - val_accuracy: 0.3676\n",
      "Epoch 330/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0868 - accuracy: 0.3069 - val_loss: 0.1416 - val_accuracy: 0.4144\n",
      "Epoch 331/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0871 - accuracy: 0.3201 - val_loss: 0.1302 - val_accuracy: 0.3928\n",
      "Epoch 332/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0866 - accuracy: 0.3129 - val_loss: 0.1302 - val_accuracy: 0.3802\n",
      "Epoch 333/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0864 - accuracy: 0.3105 - val_loss: 0.1342 - val_accuracy: 0.3640\n",
      "Epoch 334/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0866 - accuracy: 0.3135 - val_loss: 0.1286 - val_accuracy: 0.3586\n",
      "Epoch 335/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0862 - accuracy: 0.3051 - val_loss: 0.1310 - val_accuracy: 0.3928\n",
      "Epoch 336/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0861 - accuracy: 0.3063 - val_loss: 0.1315 - val_accuracy: 0.3856\n",
      "Epoch 337/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0860 - accuracy: 0.3261 - val_loss: 0.1317 - val_accuracy: 0.3676\n",
      "Epoch 338/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0859 - accuracy: 0.3135 - val_loss: 0.1302 - val_accuracy: 0.3730\n",
      "Epoch 339/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0858 - accuracy: 0.3171 - val_loss: 0.1301 - val_accuracy: 0.3495\n",
      "Epoch 340/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0856 - accuracy: 0.3129 - val_loss: 0.1307 - val_accuracy: 0.3622\n",
      "Epoch 341/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0856 - accuracy: 0.3063 - val_loss: 0.1308 - val_accuracy: 0.3694\n",
      "Epoch 342/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0855 - accuracy: 0.3171 - val_loss: 0.1345 - val_accuracy: 0.3405\n",
      "Epoch 343/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0860 - accuracy: 0.3183 - val_loss: 0.1286 - val_accuracy: 0.3748\n",
      "Epoch 344/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0853 - accuracy: 0.3159 - val_loss: 0.1280 - val_accuracy: 0.3784\n",
      "Epoch 345/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0852 - accuracy: 0.3135 - val_loss: 0.1292 - val_accuracy: 0.3784\n",
      "Epoch 346/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0849 - accuracy: 0.3117 - val_loss: 0.1285 - val_accuracy: 0.3532\n",
      "Epoch 347/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0850 - accuracy: 0.3165 - val_loss: 0.1309 - val_accuracy: 0.3820\n",
      "Epoch 348/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0848 - accuracy: 0.3219 - val_loss: 0.1414 - val_accuracy: 0.3658\n",
      "Epoch 349/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0851 - accuracy: 0.3243 - val_loss: 0.1280 - val_accuracy: 0.3910\n",
      "Epoch 350/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0844 - accuracy: 0.3255 - val_loss: 0.1290 - val_accuracy: 0.3856\n",
      "Epoch 351/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0844 - accuracy: 0.3213 - val_loss: 0.1287 - val_accuracy: 0.3874\n",
      "Epoch 352/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0844 - accuracy: 0.3201 - val_loss: 0.1375 - val_accuracy: 0.3928\n",
      "Epoch 353/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0848 - accuracy: 0.3297 - val_loss: 0.1291 - val_accuracy: 0.3892\n",
      "Epoch 354/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0841 - accuracy: 0.3231 - val_loss: 0.1282 - val_accuracy: 0.4018\n",
      "Epoch 355/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0840 - accuracy: 0.3243 - val_loss: 0.1293 - val_accuracy: 0.4000\n",
      "Epoch 356/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0839 - accuracy: 0.3285 - val_loss: 0.1287 - val_accuracy: 0.3892\n",
      "Epoch 357/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0840 - accuracy: 0.3273 - val_loss: 0.1287 - val_accuracy: 0.3910\n",
      "Epoch 358/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0837 - accuracy: 0.3183 - val_loss: 0.1286 - val_accuracy: 0.3856\n",
      "Epoch 359/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0835 - accuracy: 0.3153 - val_loss: 0.1429 - val_accuracy: 0.3333\n",
      "Epoch 360/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0841 - accuracy: 0.3231 - val_loss: 0.1280 - val_accuracy: 0.3838\n",
      "Epoch 361/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0834 - accuracy: 0.3237 - val_loss: 0.1272 - val_accuracy: 0.3802\n",
      "Epoch 362/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0832 - accuracy: 0.3219 - val_loss: 0.1275 - val_accuracy: 0.3892\n",
      "Epoch 363/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0830 - accuracy: 0.3237 - val_loss: 0.1281 - val_accuracy: 0.3748\n",
      "Epoch 364/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0832 - accuracy: 0.3261 - val_loss: 0.1276 - val_accuracy: 0.3676\n",
      "Epoch 365/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0829 - accuracy: 0.3177 - val_loss: 0.1284 - val_accuracy: 0.3459\n",
      "Epoch 366/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0832 - accuracy: 0.3189 - val_loss: 0.1277 - val_accuracy: 0.3928\n",
      "Epoch 367/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0827 - accuracy: 0.3249 - val_loss: 0.1273 - val_accuracy: 0.3694\n",
      "Epoch 368/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0827 - accuracy: 0.3231 - val_loss: 0.1281 - val_accuracy: 0.3910\n",
      "Epoch 369/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0825 - accuracy: 0.3219 - val_loss: 0.1257 - val_accuracy: 0.3640\n",
      "Epoch 370/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0825 - accuracy: 0.3213 - val_loss: 0.1390 - val_accuracy: 0.3405\n",
      "Epoch 371/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0831 - accuracy: 0.3327 - val_loss: 0.1271 - val_accuracy: 0.3676\n",
      "Epoch 372/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0825 - accuracy: 0.3195 - val_loss: 0.1262 - val_accuracy: 0.3730\n",
      "Epoch 373/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0821 - accuracy: 0.3255 - val_loss: 0.1442 - val_accuracy: 0.3586\n",
      "Epoch 374/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0828 - accuracy: 0.3297 - val_loss: 0.1259 - val_accuracy: 0.3766\n",
      "Epoch 375/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0818 - accuracy: 0.3219 - val_loss: 0.1267 - val_accuracy: 0.3928\n",
      "Epoch 376/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0819 - accuracy: 0.3267 - val_loss: 0.1258 - val_accuracy: 0.4018\n",
      "Epoch 377/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0817 - accuracy: 0.3345 - val_loss: 0.1272 - val_accuracy: 0.3838\n",
      "Epoch 378/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0815 - accuracy: 0.3219 - val_loss: 0.1315 - val_accuracy: 0.3712\n",
      "Epoch 379/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0817 - accuracy: 0.3207 - val_loss: 0.1281 - val_accuracy: 0.3748\n",
      "Epoch 380/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0815 - accuracy: 0.3231 - val_loss: 0.1268 - val_accuracy: 0.3550\n",
      "Epoch 381/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0813 - accuracy: 0.3243 - val_loss: 0.1266 - val_accuracy: 0.3640\n",
      "Epoch 382/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0813 - accuracy: 0.3267 - val_loss: 0.1275 - val_accuracy: 0.3712\n",
      "Epoch 383/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0813 - accuracy: 0.3261 - val_loss: 0.1267 - val_accuracy: 0.3892\n",
      "Epoch 384/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0810 - accuracy: 0.3171 - val_loss: 0.1242 - val_accuracy: 0.3568\n",
      "Epoch 385/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0811 - accuracy: 0.3219 - val_loss: 0.1253 - val_accuracy: 0.3730\n",
      "Epoch 386/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0807 - accuracy: 0.3309 - val_loss: 0.1253 - val_accuracy: 0.3550\n",
      "Epoch 387/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0809 - accuracy: 0.3141 - val_loss: 0.1250 - val_accuracy: 0.3730\n",
      "Epoch 388/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0806 - accuracy: 0.3255 - val_loss: 0.1259 - val_accuracy: 0.3694\n",
      "Epoch 389/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0805 - accuracy: 0.3303 - val_loss: 0.1244 - val_accuracy: 0.3712\n",
      "Epoch 390/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0803 - accuracy: 0.3237 - val_loss: 0.1271 - val_accuracy: 0.3748\n",
      "Epoch 391/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0805 - accuracy: 0.3273 - val_loss: 0.1244 - val_accuracy: 0.3658\n",
      "Epoch 392/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0802 - accuracy: 0.3333 - val_loss: 0.1268 - val_accuracy: 0.3838\n",
      "Epoch 393/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0801 - accuracy: 0.3243 - val_loss: 0.1255 - val_accuracy: 0.3874\n",
      "Epoch 394/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0801 - accuracy: 0.3315 - val_loss: 0.1243 - val_accuracy: 0.3928\n",
      "Epoch 395/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0799 - accuracy: 0.3285 - val_loss: 0.1245 - val_accuracy: 0.3730\n",
      "Epoch 396/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0798 - accuracy: 0.3291 - val_loss: 0.1227 - val_accuracy: 0.3838\n",
      "Epoch 397/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0796 - accuracy: 0.3201 - val_loss: 0.1251 - val_accuracy: 0.3802\n",
      "Epoch 398/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0796 - accuracy: 0.3291 - val_loss: 0.1253 - val_accuracy: 0.3694\n",
      "Epoch 399/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0796 - accuracy: 0.3267 - val_loss: 0.1276 - val_accuracy: 0.3982\n",
      "Epoch 400/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0797 - accuracy: 0.3285 - val_loss: 0.1252 - val_accuracy: 0.3838\n",
      "Epoch 401/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0793 - accuracy: 0.3279 - val_loss: 0.1264 - val_accuracy: 0.3946\n",
      "Epoch 402/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0793 - accuracy: 0.3213 - val_loss: 0.1261 - val_accuracy: 0.3802\n",
      "Epoch 403/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0793 - accuracy: 0.3339 - val_loss: 0.1272 - val_accuracy: 0.4018\n",
      "Epoch 404/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0793 - accuracy: 0.3243 - val_loss: 0.1253 - val_accuracy: 0.3477\n",
      "Epoch 405/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0792 - accuracy: 0.3285 - val_loss: 0.1242 - val_accuracy: 0.3964\n",
      "Epoch 406/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0789 - accuracy: 0.3249 - val_loss: 0.1247 - val_accuracy: 0.4036\n",
      "Epoch 407/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0787 - accuracy: 0.3357 - val_loss: 0.1221 - val_accuracy: 0.3802\n",
      "Epoch 408/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0787 - accuracy: 0.3333 - val_loss: 0.1237 - val_accuracy: 0.3676\n",
      "Epoch 409/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0788 - accuracy: 0.3321 - val_loss: 0.1240 - val_accuracy: 0.3748\n",
      "Epoch 410/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0784 - accuracy: 0.3363 - val_loss: 0.1247 - val_accuracy: 0.4108\n",
      "Epoch 411/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0785 - accuracy: 0.3303 - val_loss: 0.1242 - val_accuracy: 0.3928\n",
      "Epoch 412/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0782 - accuracy: 0.3363 - val_loss: 0.1226 - val_accuracy: 0.3676\n",
      "Epoch 413/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0783 - accuracy: 0.3309 - val_loss: 0.1234 - val_accuracy: 0.3622\n",
      "Epoch 414/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0781 - accuracy: 0.3291 - val_loss: 0.1235 - val_accuracy: 0.3892\n",
      "Epoch 415/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0782 - accuracy: 0.3339 - val_loss: 0.1256 - val_accuracy: 0.3856\n",
      "Epoch 416/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0781 - accuracy: 0.3225 - val_loss: 0.1227 - val_accuracy: 0.3838\n",
      "Epoch 417/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0777 - accuracy: 0.3303 - val_loss: 0.1247 - val_accuracy: 0.3910\n",
      "Epoch 418/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0779 - accuracy: 0.3291 - val_loss: 0.1273 - val_accuracy: 0.4036\n",
      "Epoch 419/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0784 - accuracy: 0.3225 - val_loss: 0.1272 - val_accuracy: 0.3694\n",
      "Epoch 420/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0779 - accuracy: 0.3303 - val_loss: 0.1225 - val_accuracy: 0.3838\n",
      "Epoch 421/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0775 - accuracy: 0.3273 - val_loss: 0.1229 - val_accuracy: 0.3622\n",
      "Epoch 422/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0777 - accuracy: 0.3285 - val_loss: 0.1237 - val_accuracy: 0.3874\n",
      "Epoch 423/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0774 - accuracy: 0.3351 - val_loss: 0.1270 - val_accuracy: 0.3297\n",
      "Epoch 424/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0778 - accuracy: 0.3291 - val_loss: 0.1277 - val_accuracy: 0.3946\n",
      "Epoch 425/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0776 - accuracy: 0.3219 - val_loss: 0.1233 - val_accuracy: 0.3946\n",
      "Epoch 426/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0772 - accuracy: 0.3279 - val_loss: 0.1223 - val_accuracy: 0.3892\n",
      "Epoch 427/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0769 - accuracy: 0.3339 - val_loss: 0.1236 - val_accuracy: 0.3946\n",
      "Epoch 428/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0770 - accuracy: 0.3351 - val_loss: 0.1227 - val_accuracy: 0.3730\n",
      "Epoch 429/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0769 - accuracy: 0.3267 - val_loss: 0.1236 - val_accuracy: 0.3784\n",
      "Epoch 430/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0770 - accuracy: 0.3231 - val_loss: 0.1210 - val_accuracy: 0.3694\n",
      "Epoch 431/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0766 - accuracy: 0.3315 - val_loss: 0.1225 - val_accuracy: 0.3982\n",
      "Epoch 432/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0767 - accuracy: 0.3267 - val_loss: 0.1358 - val_accuracy: 0.3514\n",
      "Epoch 433/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0772 - accuracy: 0.3243 - val_loss: 0.1218 - val_accuracy: 0.3712\n",
      "Epoch 434/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0766 - accuracy: 0.3327 - val_loss: 0.1214 - val_accuracy: 0.4018\n",
      "Epoch 435/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0764 - accuracy: 0.3267 - val_loss: 0.1291 - val_accuracy: 0.3766\n",
      "Epoch 436/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0765 - accuracy: 0.3333 - val_loss: 0.1226 - val_accuracy: 0.3910\n",
      "Epoch 437/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0763 - accuracy: 0.3399 - val_loss: 0.1272 - val_accuracy: 0.3802\n",
      "Epoch 438/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0763 - accuracy: 0.3333 - val_loss: 0.1217 - val_accuracy: 0.3820\n",
      "Epoch 439/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0763 - accuracy: 0.3387 - val_loss: 0.1212 - val_accuracy: 0.3874\n",
      "Epoch 440/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0761 - accuracy: 0.3327 - val_loss: 0.1214 - val_accuracy: 0.3622\n",
      "Epoch 441/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0761 - accuracy: 0.3279 - val_loss: 0.1218 - val_accuracy: 0.3604\n",
      "Epoch 442/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0758 - accuracy: 0.3249 - val_loss: 0.1231 - val_accuracy: 0.3910\n",
      "Epoch 443/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0758 - accuracy: 0.3333 - val_loss: 0.1208 - val_accuracy: 0.3748\n",
      "Epoch 444/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0757 - accuracy: 0.3357 - val_loss: 0.1203 - val_accuracy: 0.3766\n",
      "Epoch 445/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0756 - accuracy: 0.3315 - val_loss: 0.1208 - val_accuracy: 0.3802\n",
      "Epoch 446/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0756 - accuracy: 0.3261 - val_loss: 0.1229 - val_accuracy: 0.4072\n",
      "Epoch 447/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0756 - accuracy: 0.3249 - val_loss: 0.1210 - val_accuracy: 0.3784\n",
      "Epoch 448/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0752 - accuracy: 0.3255 - val_loss: 0.1262 - val_accuracy: 0.3568\n",
      "Epoch 449/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0755 - accuracy: 0.3219 - val_loss: 0.1206 - val_accuracy: 0.3892\n",
      "Epoch 450/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0751 - accuracy: 0.3303 - val_loss: 0.1212 - val_accuracy: 0.3856\n",
      "Epoch 451/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0751 - accuracy: 0.3315 - val_loss: 0.1219 - val_accuracy: 0.3658\n",
      "Epoch 452/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0750 - accuracy: 0.3315 - val_loss: 0.1216 - val_accuracy: 0.3838\n",
      "Epoch 453/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0749 - accuracy: 0.3267 - val_loss: 0.1197 - val_accuracy: 0.3658\n",
      "Epoch 454/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0749 - accuracy: 0.3267 - val_loss: 0.1215 - val_accuracy: 0.3928\n",
      "Epoch 455/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0749 - accuracy: 0.3189 - val_loss: 0.1201 - val_accuracy: 0.3730\n",
      "Epoch 456/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0746 - accuracy: 0.3315 - val_loss: 0.1192 - val_accuracy: 0.3694\n",
      "Epoch 457/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0746 - accuracy: 0.3315 - val_loss: 0.1358 - val_accuracy: 0.3514\n",
      "Epoch 458/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0754 - accuracy: 0.3393 - val_loss: 0.1197 - val_accuracy: 0.3784\n",
      "Epoch 459/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0746 - accuracy: 0.3321 - val_loss: 0.1166 - val_accuracy: 0.3532\n",
      "Epoch 460/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0745 - accuracy: 0.3279 - val_loss: 0.1208 - val_accuracy: 0.3892\n",
      "Epoch 461/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0742 - accuracy: 0.3261 - val_loss: 0.1229 - val_accuracy: 0.3784\n",
      "Epoch 462/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0743 - accuracy: 0.3357 - val_loss: 0.1214 - val_accuracy: 0.3766\n",
      "Epoch 463/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0744 - accuracy: 0.3261 - val_loss: 0.1184 - val_accuracy: 0.3640\n",
      "Epoch 464/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0740 - accuracy: 0.3333 - val_loss: 0.1205 - val_accuracy: 0.3748\n",
      "Epoch 465/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0741 - accuracy: 0.3339 - val_loss: 0.1209 - val_accuracy: 0.3982\n",
      "Epoch 466/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0739 - accuracy: 0.3297 - val_loss: 0.1269 - val_accuracy: 0.3640\n",
      "Epoch 467/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0744 - accuracy: 0.3267 - val_loss: 0.1190 - val_accuracy: 0.3802\n",
      "Epoch 468/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0738 - accuracy: 0.3363 - val_loss: 0.1210 - val_accuracy: 0.4036\n",
      "Epoch 469/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0738 - accuracy: 0.3309 - val_loss: 0.1359 - val_accuracy: 0.3676\n",
      "Epoch 470/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0744 - accuracy: 0.3321 - val_loss: 0.1235 - val_accuracy: 0.3297\n",
      "Epoch 471/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0739 - accuracy: 0.3279 - val_loss: 0.1198 - val_accuracy: 0.3946\n",
      "Epoch 472/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0736 - accuracy: 0.3285 - val_loss: 0.1200 - val_accuracy: 0.3423\n",
      "Epoch 473/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0737 - accuracy: 0.3369 - val_loss: 0.1207 - val_accuracy: 0.3856\n",
      "Epoch 474/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0734 - accuracy: 0.3279 - val_loss: 0.1189 - val_accuracy: 0.3658\n",
      "Epoch 475/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0736 - accuracy: 0.3285 - val_loss: 0.1200 - val_accuracy: 0.3748\n",
      "Epoch 476/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0733 - accuracy: 0.3303 - val_loss: 0.1209 - val_accuracy: 0.3946\n",
      "Epoch 477/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0733 - accuracy: 0.3309 - val_loss: 0.1198 - val_accuracy: 0.3892\n",
      "Epoch 478/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0732 - accuracy: 0.3237 - val_loss: 0.1354 - val_accuracy: 0.3568\n",
      "Epoch 479/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0742 - accuracy: 0.3291 - val_loss: 0.1234 - val_accuracy: 0.3712\n",
      "Epoch 480/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0732 - accuracy: 0.3333 - val_loss: 0.1216 - val_accuracy: 0.3784\n",
      "Epoch 481/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0731 - accuracy: 0.3375 - val_loss: 0.1268 - val_accuracy: 0.3586\n",
      "Epoch 482/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0735 - accuracy: 0.3285 - val_loss: 0.1174 - val_accuracy: 0.3676\n",
      "Epoch 483/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0730 - accuracy: 0.3309 - val_loss: 0.1188 - val_accuracy: 0.4000\n",
      "Epoch 484/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0726 - accuracy: 0.3411 - val_loss: 0.1182 - val_accuracy: 0.3802\n",
      "Epoch 485/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0727 - accuracy: 0.3315 - val_loss: 0.1191 - val_accuracy: 0.3784\n",
      "Epoch 486/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0726 - accuracy: 0.3303 - val_loss: 0.1201 - val_accuracy: 0.3874\n",
      "Epoch 487/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0727 - accuracy: 0.3363 - val_loss: 0.1203 - val_accuracy: 0.3730\n",
      "Epoch 488/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0724 - accuracy: 0.3387 - val_loss: 0.1167 - val_accuracy: 0.3856\n",
      "Epoch 489/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0724 - accuracy: 0.3309 - val_loss: 0.1173 - val_accuracy: 0.3892\n",
      "Epoch 490/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0724 - accuracy: 0.3303 - val_loss: 0.1175 - val_accuracy: 0.3892\n",
      "Epoch 491/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0722 - accuracy: 0.3267 - val_loss: 0.1225 - val_accuracy: 0.4198\n",
      "Epoch 492/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0723 - accuracy: 0.3327 - val_loss: 0.1167 - val_accuracy: 0.3622\n",
      "Epoch 493/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0720 - accuracy: 0.3339 - val_loss: 0.1219 - val_accuracy: 0.3820\n",
      "Epoch 494/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0722 - accuracy: 0.3309 - val_loss: 0.1167 - val_accuracy: 0.3658\n",
      "Epoch 495/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0725 - accuracy: 0.3405 - val_loss: 0.1171 - val_accuracy: 0.3405\n",
      "Epoch 496/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0722 - accuracy: 0.3243 - val_loss: 0.1203 - val_accuracy: 0.3964\n",
      "Epoch 497/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0720 - accuracy: 0.3291 - val_loss: 0.1259 - val_accuracy: 0.4054\n",
      "Epoch 498/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0723 - accuracy: 0.3375 - val_loss: 0.1204 - val_accuracy: 0.3748\n",
      "Epoch 499/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0717 - accuracy: 0.3411 - val_loss: 0.1177 - val_accuracy: 0.3946\n",
      "Epoch 500/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0717 - accuracy: 0.3321 - val_loss: 0.1184 - val_accuracy: 0.3820\n",
      "Epoch 501/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0716 - accuracy: 0.3321 - val_loss: 0.1187 - val_accuracy: 0.4018\n",
      "Epoch 502/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0715 - accuracy: 0.3315 - val_loss: 0.1339 - val_accuracy: 0.4216\n",
      "Epoch 503/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0721 - accuracy: 0.3309 - val_loss: 0.1251 - val_accuracy: 0.3910\n",
      "Epoch 504/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0720 - accuracy: 0.3339 - val_loss: 0.1185 - val_accuracy: 0.3982\n",
      "Epoch 505/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0712 - accuracy: 0.3321 - val_loss: 0.1154 - val_accuracy: 0.3892\n",
      "Epoch 506/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0713 - accuracy: 0.3321 - val_loss: 0.1377 - val_accuracy: 0.2973\n",
      "Epoch 507/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0723 - accuracy: 0.3309 - val_loss: 0.1341 - val_accuracy: 0.3225\n",
      "Epoch 508/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0721 - accuracy: 0.3315 - val_loss: 0.1160 - val_accuracy: 0.3802\n",
      "Epoch 509/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0711 - accuracy: 0.3357 - val_loss: 0.1156 - val_accuracy: 0.3658\n",
      "Epoch 510/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0711 - accuracy: 0.3267 - val_loss: 0.1201 - val_accuracy: 0.4018\n",
      "Epoch 511/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0710 - accuracy: 0.3351 - val_loss: 0.1158 - val_accuracy: 0.3748\n",
      "Epoch 512/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0708 - accuracy: 0.3381 - val_loss: 0.1184 - val_accuracy: 0.3892\n",
      "Epoch 513/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0708 - accuracy: 0.3351 - val_loss: 0.1295 - val_accuracy: 0.3477\n",
      "Epoch 514/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0716 - accuracy: 0.3285 - val_loss: 0.1198 - val_accuracy: 0.4108\n",
      "Epoch 515/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0708 - accuracy: 0.3273 - val_loss: 0.1171 - val_accuracy: 0.3838\n",
      "Epoch 516/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0707 - accuracy: 0.3285 - val_loss: 0.1411 - val_accuracy: 0.3207\n",
      "Epoch 517/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0722 - accuracy: 0.3309 - val_loss: 0.1165 - val_accuracy: 0.3874\n",
      "Epoch 518/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0704 - accuracy: 0.3303 - val_loss: 0.1172 - val_accuracy: 0.3802\n",
      "Epoch 519/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0704 - accuracy: 0.3267 - val_loss: 0.1185 - val_accuracy: 0.4234\n",
      "Epoch 520/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0705 - accuracy: 0.3417 - val_loss: 0.1161 - val_accuracy: 0.3928\n",
      "Epoch 521/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0702 - accuracy: 0.3333 - val_loss: 0.1162 - val_accuracy: 0.3928\n",
      "Epoch 522/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0702 - accuracy: 0.3273 - val_loss: 0.1168 - val_accuracy: 0.4090\n",
      "Epoch 523/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0702 - accuracy: 0.3345 - val_loss: 0.1176 - val_accuracy: 0.3658\n",
      "Epoch 524/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0703 - accuracy: 0.3303 - val_loss: 0.1345 - val_accuracy: 0.3117\n",
      "Epoch 525/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0712 - accuracy: 0.3279 - val_loss: 0.1168 - val_accuracy: 0.3874\n",
      "Epoch 526/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0700 - accuracy: 0.3369 - val_loss: 0.1159 - val_accuracy: 0.3910\n",
      "Epoch 527/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0698 - accuracy: 0.3345 - val_loss: 0.1165 - val_accuracy: 0.3946\n",
      "Epoch 528/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0699 - accuracy: 0.3321 - val_loss: 0.1174 - val_accuracy: 0.3802\n",
      "Epoch 529/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0700 - accuracy: 0.3237 - val_loss: 0.1247 - val_accuracy: 0.3892\n",
      "Epoch 530/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0700 - accuracy: 0.3315 - val_loss: 0.1185 - val_accuracy: 0.3892\n",
      "Epoch 531/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0699 - accuracy: 0.3363 - val_loss: 0.1178 - val_accuracy: 0.3622\n",
      "Epoch 532/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0699 - accuracy: 0.3369 - val_loss: 0.1169 - val_accuracy: 0.4018\n",
      "Epoch 533/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0694 - accuracy: 0.3405 - val_loss: 0.1178 - val_accuracy: 0.3802\n",
      "Epoch 534/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0696 - accuracy: 0.3303 - val_loss: 0.1174 - val_accuracy: 0.3730\n",
      "Epoch 535/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0693 - accuracy: 0.3333 - val_loss: 0.1164 - val_accuracy: 0.3946\n",
      "Epoch 536/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0693 - accuracy: 0.3357 - val_loss: 0.1140 - val_accuracy: 0.3766\n",
      "Epoch 537/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0692 - accuracy: 0.3237 - val_loss: 0.1151 - val_accuracy: 0.3874\n",
      "Epoch 538/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0692 - accuracy: 0.3363 - val_loss: 0.1215 - val_accuracy: 0.3712\n",
      "Epoch 539/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0695 - accuracy: 0.3375 - val_loss: 0.1186 - val_accuracy: 0.3459\n",
      "Epoch 540/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0694 - accuracy: 0.3315 - val_loss: 0.1172 - val_accuracy: 0.3820\n",
      "Epoch 541/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0695 - accuracy: 0.3333 - val_loss: 0.1207 - val_accuracy: 0.3550\n",
      "Epoch 542/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0693 - accuracy: 0.3387 - val_loss: 0.1252 - val_accuracy: 0.3315\n",
      "Epoch 543/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0694 - accuracy: 0.3363 - val_loss: 0.1183 - val_accuracy: 0.4162\n",
      "Epoch 544/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0691 - accuracy: 0.3231 - val_loss: 0.1162 - val_accuracy: 0.4108\n",
      "Epoch 545/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0690 - accuracy: 0.3375 - val_loss: 0.1153 - val_accuracy: 0.3874\n",
      "Epoch 546/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0690 - accuracy: 0.3423 - val_loss: 0.1161 - val_accuracy: 0.4036\n",
      "Epoch 547/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0687 - accuracy: 0.3315 - val_loss: 0.1148 - val_accuracy: 0.4000\n",
      "Epoch 548/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0688 - accuracy: 0.3393 - val_loss: 0.1153 - val_accuracy: 0.3856\n",
      "Epoch 549/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0686 - accuracy: 0.3357 - val_loss: 0.1160 - val_accuracy: 0.4090\n",
      "Epoch 550/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0685 - accuracy: 0.3447 - val_loss: 0.1191 - val_accuracy: 0.3622\n",
      "Epoch 551/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0688 - accuracy: 0.3369 - val_loss: 0.1156 - val_accuracy: 0.3874\n",
      "Epoch 552/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0688 - accuracy: 0.3363 - val_loss: 0.1158 - val_accuracy: 0.4054\n",
      "Epoch 553/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0685 - accuracy: 0.3381 - val_loss: 0.1185 - val_accuracy: 0.3712\n",
      "Epoch 554/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0687 - accuracy: 0.3369 - val_loss: 0.1151 - val_accuracy: 0.3928\n",
      "Epoch 555/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0683 - accuracy: 0.3339 - val_loss: 0.1188 - val_accuracy: 0.4198\n",
      "Epoch 556/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0683 - accuracy: 0.3369 - val_loss: 0.1171 - val_accuracy: 0.4000\n",
      "Epoch 557/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0681 - accuracy: 0.3411 - val_loss: 0.1165 - val_accuracy: 0.3171\n",
      "Epoch 558/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0683 - accuracy: 0.3297 - val_loss: 0.1163 - val_accuracy: 0.3838\n",
      "Epoch 559/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0681 - accuracy: 0.3285 - val_loss: 0.1166 - val_accuracy: 0.4108\n",
      "Epoch 560/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0679 - accuracy: 0.3399 - val_loss: 0.1130 - val_accuracy: 0.4144\n",
      "Epoch 561/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0679 - accuracy: 0.3351 - val_loss: 0.1284 - val_accuracy: 0.3550\n",
      "Epoch 562/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0687 - accuracy: 0.3303 - val_loss: 0.1167 - val_accuracy: 0.3622\n",
      "Epoch 563/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0678 - accuracy: 0.3321 - val_loss: 0.1179 - val_accuracy: 0.4018\n",
      "Epoch 564/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0680 - accuracy: 0.3339 - val_loss: 0.1149 - val_accuracy: 0.3928\n",
      "Epoch 565/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0675 - accuracy: 0.3375 - val_loss: 0.1138 - val_accuracy: 0.4018\n",
      "Epoch 566/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0676 - accuracy: 0.3369 - val_loss: 0.1186 - val_accuracy: 0.4216\n",
      "Epoch 567/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0678 - accuracy: 0.3393 - val_loss: 0.1143 - val_accuracy: 0.3856\n",
      "Epoch 568/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0673 - accuracy: 0.3333 - val_loss: 0.1131 - val_accuracy: 0.3820\n",
      "Epoch 569/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0672 - accuracy: 0.3303 - val_loss: 0.1169 - val_accuracy: 0.3640\n",
      "Epoch 570/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0676 - accuracy: 0.3279 - val_loss: 0.1152 - val_accuracy: 0.4072\n",
      "Epoch 571/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0672 - accuracy: 0.3363 - val_loss: 0.1131 - val_accuracy: 0.3820\n",
      "Epoch 572/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0671 - accuracy: 0.3345 - val_loss: 0.1158 - val_accuracy: 0.3946\n",
      "Epoch 573/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0675 - accuracy: 0.3309 - val_loss: 0.1169 - val_accuracy: 0.3946\n",
      "Epoch 574/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0674 - accuracy: 0.3381 - val_loss: 0.1366 - val_accuracy: 0.2703\n",
      "Epoch 575/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0687 - accuracy: 0.3345 - val_loss: 0.1127 - val_accuracy: 0.4018\n",
      "Epoch 576/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0670 - accuracy: 0.3363 - val_loss: 0.1133 - val_accuracy: 0.4018\n",
      "Epoch 577/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0671 - accuracy: 0.3303 - val_loss: 0.1136 - val_accuracy: 0.3784\n",
      "Epoch 578/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0668 - accuracy: 0.3411 - val_loss: 0.1160 - val_accuracy: 0.3856\n",
      "Epoch 579/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0673 - accuracy: 0.3417 - val_loss: 0.1143 - val_accuracy: 0.3784\n",
      "Epoch 580/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0669 - accuracy: 0.3363 - val_loss: 0.1141 - val_accuracy: 0.4036\n",
      "Epoch 581/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0667 - accuracy: 0.3309 - val_loss: 0.1145 - val_accuracy: 0.3964\n",
      "Epoch 582/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0670 - accuracy: 0.3357 - val_loss: 0.1160 - val_accuracy: 0.3964\n",
      "Epoch 583/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0666 - accuracy: 0.3351 - val_loss: 0.1136 - val_accuracy: 0.4018\n",
      "Epoch 584/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0665 - accuracy: 0.3393 - val_loss: 0.1146 - val_accuracy: 0.4054\n",
      "Epoch 585/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0666 - accuracy: 0.3321 - val_loss: 0.1116 - val_accuracy: 0.3946\n",
      "Epoch 586/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0665 - accuracy: 0.3333 - val_loss: 0.1137 - val_accuracy: 0.4018\n",
      "Epoch 587/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0662 - accuracy: 0.3297 - val_loss: 0.1144 - val_accuracy: 0.3838\n",
      "Epoch 588/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0663 - accuracy: 0.3351 - val_loss: 0.1194 - val_accuracy: 0.3820\n",
      "Epoch 589/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0666 - accuracy: 0.3279 - val_loss: 0.1167 - val_accuracy: 0.3874\n",
      "Epoch 590/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0666 - accuracy: 0.3345 - val_loss: 0.1124 - val_accuracy: 0.3928\n",
      "Epoch 591/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0661 - accuracy: 0.3363 - val_loss: 0.1115 - val_accuracy: 0.3748\n",
      "Epoch 592/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0661 - accuracy: 0.3369 - val_loss: 0.1141 - val_accuracy: 0.3514\n",
      "Epoch 593/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0664 - accuracy: 0.3375 - val_loss: 0.1148 - val_accuracy: 0.3982\n",
      "Epoch 594/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0660 - accuracy: 0.3315 - val_loss: 0.1137 - val_accuracy: 0.3550\n",
      "Epoch 595/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0662 - accuracy: 0.3267 - val_loss: 0.1239 - val_accuracy: 0.3676\n",
      "Epoch 596/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0665 - accuracy: 0.3441 - val_loss: 0.1122 - val_accuracy: 0.3964\n",
      "Epoch 597/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0657 - accuracy: 0.3393 - val_loss: 0.1143 - val_accuracy: 0.4144\n",
      "Epoch 598/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0659 - accuracy: 0.3345 - val_loss: 0.1153 - val_accuracy: 0.4288\n",
      "Epoch 599/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0660 - accuracy: 0.3309 - val_loss: 0.1134 - val_accuracy: 0.3856\n",
      "Epoch 600/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0657 - accuracy: 0.3393 - val_loss: 0.1152 - val_accuracy: 0.4000\n",
      "Epoch 601/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0662 - accuracy: 0.3423 - val_loss: 0.1132 - val_accuracy: 0.3892\n",
      "Epoch 602/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0656 - accuracy: 0.3411 - val_loss: 0.1119 - val_accuracy: 0.3856\n",
      "Epoch 603/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0654 - accuracy: 0.3321 - val_loss: 0.1160 - val_accuracy: 0.4198\n",
      "Epoch 604/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0657 - accuracy: 0.3381 - val_loss: 0.1116 - val_accuracy: 0.3928\n",
      "Epoch 605/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0655 - accuracy: 0.3339 - val_loss: 0.1176 - val_accuracy: 0.4216\n",
      "Epoch 606/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0656 - accuracy: 0.3351 - val_loss: 0.1266 - val_accuracy: 0.3964\n",
      "Epoch 607/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0662 - accuracy: 0.3441 - val_loss: 0.1132 - val_accuracy: 0.3820\n",
      "Epoch 608/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0655 - accuracy: 0.3393 - val_loss: 0.1114 - val_accuracy: 0.4018\n",
      "Epoch 609/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0651 - accuracy: 0.3339 - val_loss: 0.1119 - val_accuracy: 0.4162\n",
      "Epoch 610/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0652 - accuracy: 0.3363 - val_loss: 0.1128 - val_accuracy: 0.4018\n",
      "Epoch 611/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0651 - accuracy: 0.3381 - val_loss: 0.1132 - val_accuracy: 0.3838\n",
      "Epoch 612/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0653 - accuracy: 0.3345 - val_loss: 0.1131 - val_accuracy: 0.3207\n",
      "Epoch 613/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0653 - accuracy: 0.3339 - val_loss: 0.1119 - val_accuracy: 0.3892\n",
      "Epoch 614/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0649 - accuracy: 0.3369 - val_loss: 0.1294 - val_accuracy: 0.3117\n",
      "Epoch 615/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0658 - accuracy: 0.3399 - val_loss: 0.1119 - val_accuracy: 0.4054\n",
      "Epoch 616/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0649 - accuracy: 0.3369 - val_loss: 0.1103 - val_accuracy: 0.3910\n",
      "Epoch 617/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0648 - accuracy: 0.3393 - val_loss: 0.1117 - val_accuracy: 0.3820\n",
      "Epoch 618/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0646 - accuracy: 0.3339 - val_loss: 0.1132 - val_accuracy: 0.3910\n",
      "Epoch 619/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0647 - accuracy: 0.3369 - val_loss: 0.1142 - val_accuracy: 0.4054\n",
      "Epoch 620/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0650 - accuracy: 0.3381 - val_loss: 0.1116 - val_accuracy: 0.4036\n",
      "Epoch 621/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0644 - accuracy: 0.3369 - val_loss: 0.1149 - val_accuracy: 0.3514\n",
      "Epoch 622/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0649 - accuracy: 0.3333 - val_loss: 0.1136 - val_accuracy: 0.3964\n",
      "Epoch 623/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0644 - accuracy: 0.3279 - val_loss: 0.1103 - val_accuracy: 0.3928\n",
      "Epoch 624/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0643 - accuracy: 0.3399 - val_loss: 0.1101 - val_accuracy: 0.3838\n",
      "Epoch 625/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0645 - accuracy: 0.3387 - val_loss: 0.1132 - val_accuracy: 0.4180\n",
      "Epoch 626/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0644 - accuracy: 0.3357 - val_loss: 0.1103 - val_accuracy: 0.3459\n",
      "Epoch 627/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0643 - accuracy: 0.3315 - val_loss: 0.1133 - val_accuracy: 0.3748\n",
      "Epoch 628/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0645 - accuracy: 0.3399 - val_loss: 0.1162 - val_accuracy: 0.4252\n",
      "Epoch 629/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0642 - accuracy: 0.3393 - val_loss: 0.1238 - val_accuracy: 0.3604\n",
      "Epoch 630/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0650 - accuracy: 0.3429 - val_loss: 0.1138 - val_accuracy: 0.3820\n",
      "Epoch 631/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0643 - accuracy: 0.3321 - val_loss: 0.1129 - val_accuracy: 0.3928\n",
      "Epoch 632/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0642 - accuracy: 0.3327 - val_loss: 0.1100 - val_accuracy: 0.3568\n",
      "Epoch 633/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0641 - accuracy: 0.3453 - val_loss: 0.1118 - val_accuracy: 0.4036\n",
      "Epoch 634/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0640 - accuracy: 0.3339 - val_loss: 0.1113 - val_accuracy: 0.3766\n",
      "Epoch 635/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0640 - accuracy: 0.3411 - val_loss: 0.1124 - val_accuracy: 0.3856\n",
      "Epoch 636/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0640 - accuracy: 0.3393 - val_loss: 0.1137 - val_accuracy: 0.4036\n",
      "Epoch 637/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0640 - accuracy: 0.3453 - val_loss: 0.1102 - val_accuracy: 0.3856\n",
      "Epoch 638/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0638 - accuracy: 0.3303 - val_loss: 0.1113 - val_accuracy: 0.4072\n",
      "Epoch 639/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0636 - accuracy: 0.3459 - val_loss: 0.1101 - val_accuracy: 0.3964\n",
      "Epoch 640/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0636 - accuracy: 0.3405 - val_loss: 0.1112 - val_accuracy: 0.3712\n",
      "Epoch 641/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0637 - accuracy: 0.3405 - val_loss: 0.1128 - val_accuracy: 0.3892\n",
      "Epoch 642/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0636 - accuracy: 0.3321 - val_loss: 0.1124 - val_accuracy: 0.3730\n",
      "Epoch 643/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0635 - accuracy: 0.3471 - val_loss: 0.1106 - val_accuracy: 0.4018\n",
      "Epoch 644/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0634 - accuracy: 0.3471 - val_loss: 0.1099 - val_accuracy: 0.3748\n",
      "Epoch 645/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0633 - accuracy: 0.3381 - val_loss: 0.1128 - val_accuracy: 0.3640\n",
      "Epoch 646/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0637 - accuracy: 0.3417 - val_loss: 0.1220 - val_accuracy: 0.4090\n",
      "Epoch 647/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0636 - accuracy: 0.3369 - val_loss: 0.1110 - val_accuracy: 0.4036\n",
      "Epoch 648/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0632 - accuracy: 0.3321 - val_loss: 0.1102 - val_accuracy: 0.3946\n",
      "Epoch 649/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0632 - accuracy: 0.3369 - val_loss: 0.1086 - val_accuracy: 0.3838\n",
      "Epoch 650/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0631 - accuracy: 0.3489 - val_loss: 0.1110 - val_accuracy: 0.3568\n",
      "Epoch 651/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0632 - accuracy: 0.3351 - val_loss: 0.1179 - val_accuracy: 0.3928\n",
      "Epoch 652/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0633 - accuracy: 0.3441 - val_loss: 0.1094 - val_accuracy: 0.3964\n",
      "Epoch 653/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0631 - accuracy: 0.3411 - val_loss: 0.1114 - val_accuracy: 0.3946\n",
      "Epoch 654/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0629 - accuracy: 0.3411 - val_loss: 0.1143 - val_accuracy: 0.3441\n",
      "Epoch 655/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0633 - accuracy: 0.3363 - val_loss: 0.1145 - val_accuracy: 0.3550\n",
      "Epoch 656/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0634 - accuracy: 0.3435 - val_loss: 0.1117 - val_accuracy: 0.3982\n",
      "Epoch 657/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0630 - accuracy: 0.3429 - val_loss: 0.1297 - val_accuracy: 0.2793\n",
      "Epoch 658/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0640 - accuracy: 0.3405 - val_loss: 0.1112 - val_accuracy: 0.4000\n",
      "Epoch 659/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0628 - accuracy: 0.3399 - val_loss: 0.1193 - val_accuracy: 0.3568\n",
      "Epoch 660/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0631 - accuracy: 0.3435 - val_loss: 0.1124 - val_accuracy: 0.4018\n",
      "Epoch 661/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0627 - accuracy: 0.3417 - val_loss: 0.1118 - val_accuracy: 0.3892\n",
      "Epoch 662/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0625 - accuracy: 0.3441 - val_loss: 0.1322 - val_accuracy: 0.3261\n",
      "Epoch 663/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0636 - accuracy: 0.3357 - val_loss: 0.1113 - val_accuracy: 0.4036\n",
      "Epoch 664/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0625 - accuracy: 0.3483 - val_loss: 0.1107 - val_accuracy: 0.3820\n",
      "Epoch 665/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0624 - accuracy: 0.3357 - val_loss: 0.1147 - val_accuracy: 0.4000\n",
      "Epoch 666/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0624 - accuracy: 0.3369 - val_loss: 0.1103 - val_accuracy: 0.4054\n",
      "Epoch 667/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0624 - accuracy: 0.3381 - val_loss: 0.1099 - val_accuracy: 0.3928\n",
      "Epoch 668/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0623 - accuracy: 0.3393 - val_loss: 0.1092 - val_accuracy: 0.4072\n",
      "Epoch 669/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0621 - accuracy: 0.3411 - val_loss: 0.1164 - val_accuracy: 0.4288\n",
      "Epoch 670/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0625 - accuracy: 0.3399 - val_loss: 0.1110 - val_accuracy: 0.3946\n",
      "Epoch 671/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0621 - accuracy: 0.3399 - val_loss: 0.1114 - val_accuracy: 0.3892\n",
      "Epoch 672/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0620 - accuracy: 0.3375 - val_loss: 0.1101 - val_accuracy: 0.4054\n",
      "Epoch 673/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0619 - accuracy: 0.3357 - val_loss: 0.1106 - val_accuracy: 0.3946\n",
      "Epoch 674/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0620 - accuracy: 0.3363 - val_loss: 0.1112 - val_accuracy: 0.3946\n",
      "Epoch 675/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0619 - accuracy: 0.3339 - val_loss: 0.1111 - val_accuracy: 0.3856\n",
      "Epoch 676/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0619 - accuracy: 0.3339 - val_loss: 0.1103 - val_accuracy: 0.3766\n",
      "Epoch 677/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0621 - accuracy: 0.3417 - val_loss: 0.1117 - val_accuracy: 0.3712\n",
      "Epoch 678/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0618 - accuracy: 0.3441 - val_loss: 0.1115 - val_accuracy: 0.3892\n",
      "Epoch 679/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0620 - accuracy: 0.3399 - val_loss: 0.1094 - val_accuracy: 0.3856\n",
      "Epoch 680/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0619 - accuracy: 0.3453 - val_loss: 0.1105 - val_accuracy: 0.4036\n",
      "Epoch 681/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0617 - accuracy: 0.3399 - val_loss: 0.1109 - val_accuracy: 0.3874\n",
      "Epoch 682/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0617 - accuracy: 0.3417 - val_loss: 0.1097 - val_accuracy: 0.3658\n",
      "Epoch 683/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0618 - accuracy: 0.3423 - val_loss: 0.1126 - val_accuracy: 0.3982\n",
      "Epoch 684/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0618 - accuracy: 0.3339 - val_loss: 0.1098 - val_accuracy: 0.3802\n",
      "Epoch 685/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0615 - accuracy: 0.3351 - val_loss: 0.1082 - val_accuracy: 0.4072\n",
      "Epoch 686/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0614 - accuracy: 0.3471 - val_loss: 0.1131 - val_accuracy: 0.4000\n",
      "Epoch 687/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0614 - accuracy: 0.3387 - val_loss: 0.1168 - val_accuracy: 0.3405\n",
      "Epoch 688/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0619 - accuracy: 0.3315 - val_loss: 0.1102 - val_accuracy: 0.3892\n",
      "Epoch 689/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0615 - accuracy: 0.3502 - val_loss: 0.1101 - val_accuracy: 0.3910\n",
      "Epoch 690/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0612 - accuracy: 0.3399 - val_loss: 0.1085 - val_accuracy: 0.3802\n",
      "Epoch 691/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0612 - accuracy: 0.3429 - val_loss: 0.1109 - val_accuracy: 0.3766\n",
      "Epoch 692/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0614 - accuracy: 0.3327 - val_loss: 0.1106 - val_accuracy: 0.4270\n",
      "Epoch 693/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0612 - accuracy: 0.3441 - val_loss: 0.1115 - val_accuracy: 0.3928\n",
      "Epoch 694/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0612 - accuracy: 0.3393 - val_loss: 0.1138 - val_accuracy: 0.4090\n",
      "Epoch 695/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0610 - accuracy: 0.3381 - val_loss: 0.1127 - val_accuracy: 0.3441\n",
      "Epoch 696/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0613 - accuracy: 0.3345 - val_loss: 0.1115 - val_accuracy: 0.4036\n",
      "Epoch 697/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0612 - accuracy: 0.3405 - val_loss: 0.1113 - val_accuracy: 0.3928\n",
      "Epoch 698/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0611 - accuracy: 0.3429 - val_loss: 0.1105 - val_accuracy: 0.3712\n",
      "Epoch 699/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0610 - accuracy: 0.3321 - val_loss: 0.1092 - val_accuracy: 0.3892\n",
      "Epoch 700/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0608 - accuracy: 0.3291 - val_loss: 0.1107 - val_accuracy: 0.3838\n",
      "Epoch 701/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0607 - accuracy: 0.3405 - val_loss: 0.1113 - val_accuracy: 0.3964\n",
      "Epoch 702/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0609 - accuracy: 0.3411 - val_loss: 0.1134 - val_accuracy: 0.3676\n",
      "Epoch 703/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0611 - accuracy: 0.3387 - val_loss: 0.1223 - val_accuracy: 0.4306\n",
      "Epoch 704/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0615 - accuracy: 0.3532 - val_loss: 0.1231 - val_accuracy: 0.3081\n",
      "Epoch 705/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0617 - accuracy: 0.3369 - val_loss: 0.1139 - val_accuracy: 0.3514\n",
      "Epoch 706/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0609 - accuracy: 0.3441 - val_loss: 0.1117 - val_accuracy: 0.3676\n",
      "Epoch 707/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0605 - accuracy: 0.3459 - val_loss: 0.1097 - val_accuracy: 0.3676\n",
      "Epoch 708/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0606 - accuracy: 0.3339 - val_loss: 0.1211 - val_accuracy: 0.4018\n",
      "Epoch 709/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0608 - accuracy: 0.3471 - val_loss: 0.1101 - val_accuracy: 0.3946\n",
      "Epoch 710/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0603 - accuracy: 0.3393 - val_loss: 0.1127 - val_accuracy: 0.3946\n",
      "Epoch 711/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0602 - accuracy: 0.3429 - val_loss: 0.1131 - val_accuracy: 0.3874\n",
      "Epoch 712/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0604 - accuracy: 0.3411 - val_loss: 0.1117 - val_accuracy: 0.3441\n",
      "Epoch 713/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0608 - accuracy: 0.3387 - val_loss: 0.1093 - val_accuracy: 0.3874\n",
      "Epoch 714/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0602 - accuracy: 0.3411 - val_loss: 0.1085 - val_accuracy: 0.3766\n",
      "Epoch 715/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0601 - accuracy: 0.3357 - val_loss: 0.1445 - val_accuracy: 0.2955\n",
      "Epoch 716/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0627 - accuracy: 0.3423 - val_loss: 0.1145 - val_accuracy: 0.4000\n",
      "Epoch 717/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0605 - accuracy: 0.3459 - val_loss: 0.1113 - val_accuracy: 0.3730\n",
      "Epoch 718/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0604 - accuracy: 0.3387 - val_loss: 0.1107 - val_accuracy: 0.3946\n",
      "Epoch 719/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0600 - accuracy: 0.3429 - val_loss: 0.1087 - val_accuracy: 0.4054\n",
      "Epoch 720/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0600 - accuracy: 0.3423 - val_loss: 0.1099 - val_accuracy: 0.3748\n",
      "Epoch 721/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0598 - accuracy: 0.3363 - val_loss: 0.1082 - val_accuracy: 0.3712\n",
      "Epoch 722/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0599 - accuracy: 0.3453 - val_loss: 0.1092 - val_accuracy: 0.3856\n",
      "Epoch 723/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0597 - accuracy: 0.3387 - val_loss: 0.1112 - val_accuracy: 0.4216\n",
      "Epoch 724/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0598 - accuracy: 0.3477 - val_loss: 0.1088 - val_accuracy: 0.4018\n",
      "Epoch 725/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0599 - accuracy: 0.3405 - val_loss: 0.1246 - val_accuracy: 0.4198\n",
      "Epoch 726/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0607 - accuracy: 0.3447 - val_loss: 0.1069 - val_accuracy: 0.3928\n",
      "Epoch 727/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0598 - accuracy: 0.3393 - val_loss: 0.1083 - val_accuracy: 0.3928\n",
      "Epoch 728/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0596 - accuracy: 0.3453 - val_loss: 0.1095 - val_accuracy: 0.3946\n",
      "Epoch 729/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0597 - accuracy: 0.3351 - val_loss: 0.1105 - val_accuracy: 0.3730\n",
      "Epoch 730/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0597 - accuracy: 0.3351 - val_loss: 0.1085 - val_accuracy: 0.3910\n",
      "Epoch 731/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0594 - accuracy: 0.3399 - val_loss: 0.1097 - val_accuracy: 0.4000\n",
      "Epoch 732/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0595 - accuracy: 0.3333 - val_loss: 0.1104 - val_accuracy: 0.3964\n",
      "Epoch 733/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0594 - accuracy: 0.3423 - val_loss: 0.1078 - val_accuracy: 0.3838\n",
      "Epoch 734/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0592 - accuracy: 0.3411 - val_loss: 0.1085 - val_accuracy: 0.3712\n",
      "Epoch 735/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0594 - accuracy: 0.3423 - val_loss: 0.1109 - val_accuracy: 0.3910\n",
      "Epoch 736/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0593 - accuracy: 0.3405 - val_loss: 0.1078 - val_accuracy: 0.3784\n",
      "Epoch 737/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0592 - accuracy: 0.3411 - val_loss: 0.1095 - val_accuracy: 0.3892\n",
      "Epoch 738/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0590 - accuracy: 0.3369 - val_loss: 0.1085 - val_accuracy: 0.3820\n",
      "Epoch 739/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0592 - accuracy: 0.3369 - val_loss: 0.1102 - val_accuracy: 0.4072\n",
      "Epoch 740/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0590 - accuracy: 0.3369 - val_loss: 0.1099 - val_accuracy: 0.3712\n",
      "Epoch 741/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0591 - accuracy: 0.3327 - val_loss: 0.1088 - val_accuracy: 0.3838\n",
      "Epoch 742/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0590 - accuracy: 0.3447 - val_loss: 0.1106 - val_accuracy: 0.3982\n",
      "Epoch 743/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0592 - accuracy: 0.3315 - val_loss: 0.1134 - val_accuracy: 0.4054\n",
      "Epoch 744/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0593 - accuracy: 0.3429 - val_loss: 0.1127 - val_accuracy: 0.4144\n",
      "Epoch 745/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0591 - accuracy: 0.3417 - val_loss: 0.1666 - val_accuracy: 0.2613\n",
      "Epoch 746/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0612 - accuracy: 0.3315 - val_loss: 0.1081 - val_accuracy: 0.3964\n",
      "Epoch 747/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0589 - accuracy: 0.3411 - val_loss: 0.1116 - val_accuracy: 0.3568\n",
      "Epoch 748/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0595 - accuracy: 0.3285 - val_loss: 0.1091 - val_accuracy: 0.4072\n",
      "Epoch 749/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0586 - accuracy: 0.3429 - val_loss: 0.1110 - val_accuracy: 0.3658\n",
      "Epoch 750/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0595 - accuracy: 0.3321 - val_loss: 0.1169 - val_accuracy: 0.4108\n",
      "Epoch 751/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0589 - accuracy: 0.3357 - val_loss: 0.1106 - val_accuracy: 0.4072\n",
      "Epoch 752/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0588 - accuracy: 0.3399 - val_loss: 0.1162 - val_accuracy: 0.4018\n",
      "Epoch 753/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0592 - accuracy: 0.3375 - val_loss: 0.1084 - val_accuracy: 0.3928\n",
      "Epoch 754/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0585 - accuracy: 0.3417 - val_loss: 0.1090 - val_accuracy: 0.3910\n",
      "Epoch 755/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0584 - accuracy: 0.3435 - val_loss: 0.1099 - val_accuracy: 0.3892\n",
      "Epoch 756/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0589 - accuracy: 0.3375 - val_loss: 0.1074 - val_accuracy: 0.3910\n",
      "Epoch 757/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0583 - accuracy: 0.3429 - val_loss: 0.1087 - val_accuracy: 0.3892\n",
      "Epoch 758/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0584 - accuracy: 0.3357 - val_loss: 0.1095 - val_accuracy: 0.4072\n",
      "Epoch 759/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0583 - accuracy: 0.3429 - val_loss: 0.1345 - val_accuracy: 0.3550\n",
      "Epoch 760/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0598 - accuracy: 0.3363 - val_loss: 0.1232 - val_accuracy: 0.3820\n",
      "Epoch 761/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0588 - accuracy: 0.3459 - val_loss: 0.1151 - val_accuracy: 0.4144\n",
      "Epoch 762/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0586 - accuracy: 0.3465 - val_loss: 0.1128 - val_accuracy: 0.4180\n",
      "Epoch 763/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0585 - accuracy: 0.3387 - val_loss: 0.1097 - val_accuracy: 0.3982\n",
      "Epoch 764/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0582 - accuracy: 0.3291 - val_loss: 0.1134 - val_accuracy: 0.3892\n",
      "Epoch 765/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0582 - accuracy: 0.3369 - val_loss: 0.1113 - val_accuracy: 0.4072\n",
      "Epoch 766/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0585 - accuracy: 0.3435 - val_loss: 0.1198 - val_accuracy: 0.3802\n",
      "Epoch 767/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0587 - accuracy: 0.3321 - val_loss: 0.1075 - val_accuracy: 0.3838\n",
      "Epoch 768/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0579 - accuracy: 0.3381 - val_loss: 0.1268 - val_accuracy: 0.2991\n",
      "Epoch 769/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0592 - accuracy: 0.3387 - val_loss: 0.1081 - val_accuracy: 0.4018\n",
      "Epoch 770/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0578 - accuracy: 0.3441 - val_loss: 0.1331 - val_accuracy: 0.4000\n",
      "Epoch 771/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0591 - accuracy: 0.3441 - val_loss: 0.1088 - val_accuracy: 0.3928\n",
      "Epoch 772/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0580 - accuracy: 0.3435 - val_loss: 0.1093 - val_accuracy: 0.4126\n",
      "Epoch 773/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0578 - accuracy: 0.3447 - val_loss: 0.1130 - val_accuracy: 0.3856\n",
      "Epoch 774/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0581 - accuracy: 0.3333 - val_loss: 0.1108 - val_accuracy: 0.3982\n",
      "Epoch 775/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0577 - accuracy: 0.3387 - val_loss: 0.1103 - val_accuracy: 0.3982\n",
      "Epoch 776/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0576 - accuracy: 0.3381 - val_loss: 0.1087 - val_accuracy: 0.3730\n",
      "Epoch 777/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0576 - accuracy: 0.3375 - val_loss: 0.1095 - val_accuracy: 0.4234\n",
      "Epoch 778/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0577 - accuracy: 0.3417 - val_loss: 0.1088 - val_accuracy: 0.4000\n",
      "Epoch 779/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0574 - accuracy: 0.3369 - val_loss: 0.1121 - val_accuracy: 0.3640\n",
      "Epoch 780/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0580 - accuracy: 0.3381 - val_loss: 0.1085 - val_accuracy: 0.3820\n",
      "Epoch 781/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0574 - accuracy: 0.3423 - val_loss: 0.1101 - val_accuracy: 0.4108\n",
      "Epoch 782/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0575 - accuracy: 0.3339 - val_loss: 0.1080 - val_accuracy: 0.3766\n",
      "Epoch 783/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0573 - accuracy: 0.3303 - val_loss: 0.1071 - val_accuracy: 0.3982\n",
      "Epoch 784/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0572 - accuracy: 0.3381 - val_loss: 0.1097 - val_accuracy: 0.3928\n",
      "Epoch 785/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0573 - accuracy: 0.3351 - val_loss: 0.1176 - val_accuracy: 0.4072\n",
      "Epoch 786/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0579 - accuracy: 0.3387 - val_loss: 0.1168 - val_accuracy: 0.3802\n",
      "Epoch 787/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0575 - accuracy: 0.3399 - val_loss: 0.1082 - val_accuracy: 0.3946\n",
      "Epoch 788/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0571 - accuracy: 0.3375 - val_loss: 0.1107 - val_accuracy: 0.3928\n",
      "Epoch 789/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0578 - accuracy: 0.3435 - val_loss: 0.1115 - val_accuracy: 0.3477\n",
      "Epoch 790/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0578 - accuracy: 0.3375 - val_loss: 0.1131 - val_accuracy: 0.3550\n",
      "Epoch 791/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0576 - accuracy: 0.3393 - val_loss: 0.1090 - val_accuracy: 0.3964\n",
      "Epoch 792/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0569 - accuracy: 0.3399 - val_loss: 0.1106 - val_accuracy: 0.4054\n",
      "Epoch 793/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0571 - accuracy: 0.3423 - val_loss: 0.1115 - val_accuracy: 0.3892\n",
      "Epoch 794/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0570 - accuracy: 0.3459 - val_loss: 0.1115 - val_accuracy: 0.3550\n",
      "Epoch 795/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0574 - accuracy: 0.3381 - val_loss: 0.1124 - val_accuracy: 0.3892\n",
      "Epoch 796/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0574 - accuracy: 0.3381 - val_loss: 0.1085 - val_accuracy: 0.4054\n",
      "Epoch 797/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0568 - accuracy: 0.3447 - val_loss: 0.1113 - val_accuracy: 0.3838\n",
      "Epoch 798/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0571 - accuracy: 0.3375 - val_loss: 0.1119 - val_accuracy: 0.4198\n",
      "Epoch 799/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0568 - accuracy: 0.3447 - val_loss: 0.1076 - val_accuracy: 0.4054\n",
      "Epoch 800/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0566 - accuracy: 0.3429 - val_loss: 0.1114 - val_accuracy: 0.3802\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001FB8DA93820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-5f8a8e11cd4a>:128: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "  tprs.append(interp(mean_fpr, fpr, tpr))\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Epoch 1/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.6229 - accuracy: 0.0024 - val_loss: 0.5201 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.2973 - accuracy: 0.0024 - val_loss: 0.2542 - val_accuracy: 0.0414\n",
      "Epoch 3/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1917 - accuracy: 0.0342 - val_loss: 0.2502 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1881 - accuracy: 0.0108 - val_loss: 0.2493 - val_accuracy: 0.0108\n",
      "Epoch 5/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1872 - accuracy: 0.0138 - val_loss: 0.2498 - val_accuracy: 0.0216\n",
      "Epoch 6/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1868 - accuracy: 0.0180 - val_loss: 0.2490 - val_accuracy: 0.0773\n",
      "Epoch 7/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1867 - accuracy: 0.0456 - val_loss: 0.2494 - val_accuracy: 0.0558\n",
      "Epoch 8/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1866 - accuracy: 0.0324 - val_loss: 0.2497 - val_accuracy: 0.0378\n",
      "Epoch 9/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1864 - accuracy: 0.0318 - val_loss: 0.2492 - val_accuracy: 0.0378\n",
      "Epoch 10/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1864 - accuracy: 0.0342 - val_loss: 0.2490 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1863 - accuracy: 0.0132 - val_loss: 0.2489 - val_accuracy: 0.0018\n",
      "Epoch 12/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1862 - accuracy: 0.0162 - val_loss: 0.2492 - val_accuracy: 0.0396\n",
      "Epoch 13/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1861 - accuracy: 0.0414 - val_loss: 0.2485 - val_accuracy: 0.0522\n",
      "Epoch 14/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1861 - accuracy: 0.0408 - val_loss: 0.2487 - val_accuracy: 0.0629\n",
      "Epoch 15/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1860 - accuracy: 0.0492 - val_loss: 0.2486 - val_accuracy: 0.0414\n",
      "Epoch 16/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1859 - accuracy: 0.0396 - val_loss: 0.2484 - val_accuracy: 0.0252\n",
      "Epoch 17/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1859 - accuracy: 0.0228 - val_loss: 0.2485 - val_accuracy: 0.0360\n",
      "Epoch 18/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1858 - accuracy: 0.0378 - val_loss: 0.2487 - val_accuracy: 0.0576\n",
      "Epoch 19/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1857 - accuracy: 0.0486 - val_loss: 0.2484 - val_accuracy: 0.0432\n",
      "Epoch 20/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1857 - accuracy: 0.0378 - val_loss: 0.2481 - val_accuracy: 0.0324\n",
      "Epoch 21/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1856 - accuracy: 0.0294 - val_loss: 0.2477 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1855 - accuracy: 0.0186 - val_loss: 0.2476 - val_accuracy: 0.0737\n",
      "Epoch 23/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1854 - accuracy: 0.0523 - val_loss: 0.2473 - val_accuracy: 0.0629\n",
      "Epoch 24/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1853 - accuracy: 0.0517 - val_loss: 0.2472 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1852 - accuracy: 0.0324 - val_loss: 0.2471 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1852 - accuracy: 0.0258 - val_loss: 0.2470 - val_accuracy: 0.0252\n",
      "Epoch 27/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1851 - accuracy: 0.0234 - val_loss: 0.2467 - val_accuracy: 0.0504\n",
      "Epoch 28/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1850 - accuracy: 0.0523 - val_loss: 0.2472 - val_accuracy: 0.0576\n",
      "Epoch 29/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1849 - accuracy: 0.0444 - val_loss: 0.2471 - val_accuracy: 0.0432\n",
      "Epoch 30/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1848 - accuracy: 0.0438 - val_loss: 0.2469 - val_accuracy: 0.0252\n",
      "Epoch 31/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1847 - accuracy: 0.0288 - val_loss: 0.2471 - val_accuracy: 0.0414\n",
      "Epoch 32/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1846 - accuracy: 0.0348 - val_loss: 0.2464 - val_accuracy: 0.0468\n",
      "Epoch 33/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1846 - accuracy: 0.0414 - val_loss: 0.2463 - val_accuracy: 0.0270\n",
      "Epoch 34/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1844 - accuracy: 0.0378 - val_loss: 0.2458 - val_accuracy: 0.0234\n",
      "Epoch 35/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1843 - accuracy: 0.0432 - val_loss: 0.2463 - val_accuracy: 0.0701\n",
      "Epoch 36/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1842 - accuracy: 0.0462 - val_loss: 0.2464 - val_accuracy: 0.0522\n",
      "Epoch 37/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1842 - accuracy: 0.0535 - val_loss: 0.2458 - val_accuracy: 0.0342\n",
      "Epoch 38/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1841 - accuracy: 0.0396 - val_loss: 0.2460 - val_accuracy: 0.0773\n",
      "Epoch 39/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1839 - accuracy: 0.0727 - val_loss: 0.2462 - val_accuracy: 0.0360\n",
      "Epoch 40/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1838 - accuracy: 0.0408 - val_loss: 0.2455 - val_accuracy: 0.0558\n",
      "Epoch 41/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1837 - accuracy: 0.0553 - val_loss: 0.2457 - val_accuracy: 0.0288\n",
      "Epoch 42/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1836 - accuracy: 0.0468 - val_loss: 0.2455 - val_accuracy: 0.0468\n",
      "Epoch 43/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1835 - accuracy: 0.0511 - val_loss: 0.2454 - val_accuracy: 0.0773\n",
      "Epoch 44/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1834 - accuracy: 0.0727 - val_loss: 0.2455 - val_accuracy: 0.0396\n",
      "Epoch 45/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1833 - accuracy: 0.0396 - val_loss: 0.2447 - val_accuracy: 0.0378\n",
      "Epoch 46/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1832 - accuracy: 0.0432 - val_loss: 0.2450 - val_accuracy: 0.0522\n",
      "Epoch 47/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1830 - accuracy: 0.0613 - val_loss: 0.2441 - val_accuracy: 0.0719\n",
      "Epoch 48/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1829 - accuracy: 0.0655 - val_loss: 0.2445 - val_accuracy: 0.0216\n",
      "Epoch 49/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1827 - accuracy: 0.0354 - val_loss: 0.2438 - val_accuracy: 0.0737\n",
      "Epoch 50/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1826 - accuracy: 0.0745 - val_loss: 0.2441 - val_accuracy: 0.0719\n",
      "Epoch 51/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1824 - accuracy: 0.0949 - val_loss: 0.2441 - val_accuracy: 0.0054\n",
      "Epoch 52/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1823 - accuracy: 0.0607 - val_loss: 0.2443 - val_accuracy: 0.0755\n",
      "Epoch 53/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1821 - accuracy: 0.0691 - val_loss: 0.2435 - val_accuracy: 0.0414\n",
      "Epoch 54/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1820 - accuracy: 0.0793 - val_loss: 0.2431 - val_accuracy: 0.0486\n",
      "Epoch 55/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1818 - accuracy: 0.0667 - val_loss: 0.2434 - val_accuracy: 0.0540\n",
      "Epoch 56/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1816 - accuracy: 0.0721 - val_loss: 0.2433 - val_accuracy: 0.0072\n",
      "Epoch 57/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1815 - accuracy: 0.0517 - val_loss: 0.2424 - val_accuracy: 0.0719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1813 - accuracy: 0.0781 - val_loss: 0.2424 - val_accuracy: 0.0594\n",
      "Epoch 59/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1811 - accuracy: 0.0697 - val_loss: 0.2418 - val_accuracy: 0.0018\n",
      "Epoch 60/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1809 - accuracy: 0.0577 - val_loss: 0.2416 - val_accuracy: 0.0899\n",
      "Epoch 61/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1807 - accuracy: 0.0937 - val_loss: 0.2416 - val_accuracy: 0.0540\n",
      "Epoch 62/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1804 - accuracy: 0.0721 - val_loss: 0.2409 - val_accuracy: 0.0540\n",
      "Epoch 63/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1802 - accuracy: 0.0715 - val_loss: 0.2408 - val_accuracy: 0.0629\n",
      "Epoch 64/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1800 - accuracy: 0.0877 - val_loss: 0.2408 - val_accuracy: 0.0773\n",
      "Epoch 65/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1798 - accuracy: 0.0961 - val_loss: 0.2402 - val_accuracy: 0.0270\n",
      "Epoch 66/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1796 - accuracy: 0.0697 - val_loss: 0.2396 - val_accuracy: 0.0378\n",
      "Epoch 67/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1793 - accuracy: 0.0913 - val_loss: 0.2398 - val_accuracy: 0.0701\n",
      "Epoch 68/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1790 - accuracy: 0.0835 - val_loss: 0.2390 - val_accuracy: 0.0701\n",
      "Epoch 69/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1788 - accuracy: 0.0907 - val_loss: 0.2386 - val_accuracy: 0.0827\n",
      "Epoch 70/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1784 - accuracy: 0.0823 - val_loss: 0.2384 - val_accuracy: 0.0737\n",
      "Epoch 71/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1782 - accuracy: 0.0787 - val_loss: 0.2383 - val_accuracy: 0.0755\n",
      "Epoch 72/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1778 - accuracy: 0.0871 - val_loss: 0.2371 - val_accuracy: 0.0845\n",
      "Epoch 73/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1775 - accuracy: 0.1063 - val_loss: 0.2368 - val_accuracy: 0.0809\n",
      "Epoch 74/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1772 - accuracy: 0.0817 - val_loss: 0.2368 - val_accuracy: 0.0971\n",
      "Epoch 75/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1768 - accuracy: 0.0997 - val_loss: 0.2369 - val_accuracy: 0.0935\n",
      "Epoch 76/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1765 - accuracy: 0.1147 - val_loss: 0.2363 - val_accuracy: 0.0827\n",
      "Epoch 77/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1761 - accuracy: 0.0853 - val_loss: 0.2355 - val_accuracy: 0.0935\n",
      "Epoch 78/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1757 - accuracy: 0.1051 - val_loss: 0.2351 - val_accuracy: 0.0755\n",
      "Epoch 79/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1753 - accuracy: 0.0997 - val_loss: 0.2342 - val_accuracy: 0.0719\n",
      "Epoch 80/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1749 - accuracy: 0.0883 - val_loss: 0.2340 - val_accuracy: 0.0719\n",
      "Epoch 81/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1745 - accuracy: 0.1039 - val_loss: 0.2327 - val_accuracy: 0.1043\n",
      "Epoch 82/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1741 - accuracy: 0.1159 - val_loss: 0.2323 - val_accuracy: 0.0809\n",
      "Epoch 83/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1736 - accuracy: 0.1009 - val_loss: 0.2319 - val_accuracy: 0.0612\n",
      "Epoch 84/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1732 - accuracy: 0.0991 - val_loss: 0.2316 - val_accuracy: 0.1331\n",
      "Epoch 85/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1727 - accuracy: 0.1159 - val_loss: 0.2306 - val_accuracy: 0.1007\n",
      "Epoch 86/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1723 - accuracy: 0.1033 - val_loss: 0.2304 - val_accuracy: 0.1007\n",
      "Epoch 87/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1718 - accuracy: 0.1105 - val_loss: 0.2296 - val_accuracy: 0.0558\n",
      "Epoch 88/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1713 - accuracy: 0.1039 - val_loss: 0.2293 - val_accuracy: 0.1025\n",
      "Epoch 89/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1708 - accuracy: 0.1219 - val_loss: 0.2288 - val_accuracy: 0.1367\n",
      "Epoch 90/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1703 - accuracy: 0.1273 - val_loss: 0.2277 - val_accuracy: 0.1151\n",
      "Epoch 91/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1698 - accuracy: 0.1177 - val_loss: 0.2275 - val_accuracy: 0.1583\n",
      "Epoch 92/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1693 - accuracy: 0.1279 - val_loss: 0.2270 - val_accuracy: 0.1349\n",
      "Epoch 93/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1688 - accuracy: 0.1285 - val_loss: 0.2255 - val_accuracy: 0.1385\n",
      "Epoch 94/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1684 - accuracy: 0.1213 - val_loss: 0.2254 - val_accuracy: 0.1061\n",
      "Epoch 95/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1679 - accuracy: 0.1243 - val_loss: 0.2244 - val_accuracy: 0.1205\n",
      "Epoch 96/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1673 - accuracy: 0.1279 - val_loss: 0.2238 - val_accuracy: 0.1151\n",
      "Epoch 97/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1669 - accuracy: 0.1177 - val_loss: 0.2236 - val_accuracy: 0.1457\n",
      "Epoch 98/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1663 - accuracy: 0.1255 - val_loss: 0.2228 - val_accuracy: 0.1187\n",
      "Epoch 99/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1658 - accuracy: 0.1297 - val_loss: 0.2223 - val_accuracy: 0.1385\n",
      "Epoch 100/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1652 - accuracy: 0.1447 - val_loss: 0.2223 - val_accuracy: 0.1097\n",
      "Epoch 101/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1647 - accuracy: 0.1303 - val_loss: 0.2212 - val_accuracy: 0.1331\n",
      "Epoch 102/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1642 - accuracy: 0.1399 - val_loss: 0.2202 - val_accuracy: 0.1421\n",
      "Epoch 103/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1637 - accuracy: 0.1447 - val_loss: 0.2210 - val_accuracy: 0.1115\n",
      "Epoch 104/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1632 - accuracy: 0.1441 - val_loss: 0.2191 - val_accuracy: 0.1205\n",
      "Epoch 105/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1626 - accuracy: 0.1435 - val_loss: 0.2182 - val_accuracy: 0.1655\n",
      "Epoch 106/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1621 - accuracy: 0.1441 - val_loss: 0.2181 - val_accuracy: 0.0917\n",
      "Epoch 107/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1616 - accuracy: 0.1441 - val_loss: 0.2165 - val_accuracy: 0.1547\n",
      "Epoch 108/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1611 - accuracy: 0.1502 - val_loss: 0.2164 - val_accuracy: 0.1673\n",
      "Epoch 109/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1605 - accuracy: 0.1646 - val_loss: 0.2155 - val_accuracy: 0.1439\n",
      "Epoch 110/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1601 - accuracy: 0.1616 - val_loss: 0.2149 - val_accuracy: 0.1673\n",
      "Epoch 111/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1595 - accuracy: 0.1586 - val_loss: 0.2144 - val_accuracy: 0.1493\n",
      "Epoch 112/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1591 - accuracy: 0.1562 - val_loss: 0.2133 - val_accuracy: 0.1583\n",
      "Epoch 113/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1586 - accuracy: 0.1580 - val_loss: 0.2132 - val_accuracy: 0.1439\n",
      "Epoch 114/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1580 - accuracy: 0.1592 - val_loss: 0.2117 - val_accuracy: 0.1547\n",
      "Epoch 115/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1574 - accuracy: 0.1616 - val_loss: 0.2107 - val_accuracy: 0.1637\n",
      "Epoch 116/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1569 - accuracy: 0.1724 - val_loss: 0.2114 - val_accuracy: 0.1583\n",
      "Epoch 117/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1564 - accuracy: 0.1628 - val_loss: 0.2107 - val_accuracy: 0.1727\n",
      "Epoch 118/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1559 - accuracy: 0.1634 - val_loss: 0.2114 - val_accuracy: 0.1601\n",
      "Epoch 119/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1554 - accuracy: 0.1700 - val_loss: 0.2100 - val_accuracy: 0.1367\n",
      "Epoch 120/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1549 - accuracy: 0.1574 - val_loss: 0.2088 - val_accuracy: 0.1601\n",
      "Epoch 121/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1543 - accuracy: 0.1694 - val_loss: 0.2078 - val_accuracy: 0.1529\n",
      "Epoch 122/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1538 - accuracy: 0.1694 - val_loss: 0.2068 - val_accuracy: 0.1942\n",
      "Epoch 123/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1532 - accuracy: 0.1772 - val_loss: 0.2066 - val_accuracy: 0.1745\n",
      "Epoch 124/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1528 - accuracy: 0.1790 - val_loss: 0.2055 - val_accuracy: 0.1835\n",
      "Epoch 125/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1522 - accuracy: 0.1790 - val_loss: 0.2058 - val_accuracy: 0.1745\n",
      "Epoch 126/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1517 - accuracy: 0.1880 - val_loss: 0.2046 - val_accuracy: 0.2086\n",
      "Epoch 127/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1511 - accuracy: 0.1820 - val_loss: 0.2040 - val_accuracy: 0.1817\n",
      "Epoch 128/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1506 - accuracy: 0.1826 - val_loss: 0.2034 - val_accuracy: 0.1709\n",
      "Epoch 129/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1501 - accuracy: 0.1886 - val_loss: 0.2026 - val_accuracy: 0.2248\n",
      "Epoch 130/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1495 - accuracy: 0.1916 - val_loss: 0.2008 - val_accuracy: 0.2158\n",
      "Epoch 131/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1490 - accuracy: 0.1970 - val_loss: 0.2000 - val_accuracy: 0.2194\n",
      "Epoch 132/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1485 - accuracy: 0.1928 - val_loss: 0.1992 - val_accuracy: 0.1493\n",
      "Epoch 133/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1478 - accuracy: 0.1964 - val_loss: 0.1992 - val_accuracy: 0.2194\n",
      "Epoch 134/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1474 - accuracy: 0.1988 - val_loss: 0.1983 - val_accuracy: 0.1960\n",
      "Epoch 135/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1467 - accuracy: 0.2042 - val_loss: 0.1972 - val_accuracy: 0.2122\n",
      "Epoch 136/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1462 - accuracy: 0.1910 - val_loss: 0.1973 - val_accuracy: 0.2212\n",
      "Epoch 137/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1458 - accuracy: 0.2006 - val_loss: 0.1967 - val_accuracy: 0.2050\n",
      "Epoch 138/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1451 - accuracy: 0.2036 - val_loss: 0.1943 - val_accuracy: 0.2122\n",
      "Epoch 139/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1446 - accuracy: 0.2048 - val_loss: 0.1961 - val_accuracy: 0.1745\n",
      "Epoch 140/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1441 - accuracy: 0.2042 - val_loss: 0.1941 - val_accuracy: 0.1924\n",
      "Epoch 141/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1434 - accuracy: 0.2102 - val_loss: 0.1938 - val_accuracy: 0.2068\n",
      "Epoch 142/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1429 - accuracy: 0.2012 - val_loss: 0.1923 - val_accuracy: 0.2212\n",
      "Epoch 143/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1423 - accuracy: 0.2042 - val_loss: 0.1929 - val_accuracy: 0.1960\n",
      "Epoch 144/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1417 - accuracy: 0.2156 - val_loss: 0.1915 - val_accuracy: 0.2338\n",
      "Epoch 145/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1412 - accuracy: 0.2258 - val_loss: 0.1906 - val_accuracy: 0.2068\n",
      "Epoch 146/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1406 - accuracy: 0.2234 - val_loss: 0.1897 - val_accuracy: 0.1960\n",
      "Epoch 147/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1400 - accuracy: 0.2246 - val_loss: 0.1895 - val_accuracy: 0.2338\n",
      "Epoch 148/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1395 - accuracy: 0.2132 - val_loss: 0.1887 - val_accuracy: 0.2266\n",
      "Epoch 149/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1389 - accuracy: 0.2228 - val_loss: 0.1875 - val_accuracy: 0.2536\n",
      "Epoch 150/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1383 - accuracy: 0.2258 - val_loss: 0.1870 - val_accuracy: 0.2230\n",
      "Epoch 151/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1378 - accuracy: 0.2174 - val_loss: 0.1879 - val_accuracy: 0.2122\n",
      "Epoch 152/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1373 - accuracy: 0.2174 - val_loss: 0.1857 - val_accuracy: 0.2194\n",
      "Epoch 153/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1366 - accuracy: 0.2324 - val_loss: 0.1852 - val_accuracy: 0.2392\n",
      "Epoch 154/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1360 - accuracy: 0.2228 - val_loss: 0.1847 - val_accuracy: 0.2086\n",
      "Epoch 155/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1356 - accuracy: 0.2246 - val_loss: 0.1846 - val_accuracy: 0.2050\n",
      "Epoch 156/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1350 - accuracy: 0.2222 - val_loss: 0.1893 - val_accuracy: 0.1601\n",
      "Epoch 157/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1348 - accuracy: 0.2270 - val_loss: 0.1814 - val_accuracy: 0.2428\n",
      "Epoch 158/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1340 - accuracy: 0.2294 - val_loss: 0.1867 - val_accuracy: 0.1871\n",
      "Epoch 159/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1338 - accuracy: 0.2294 - val_loss: 0.1817 - val_accuracy: 0.2086\n",
      "Epoch 160/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1330 - accuracy: 0.2228 - val_loss: 0.1812 - val_accuracy: 0.2410\n",
      "Epoch 161/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1325 - accuracy: 0.2294 - val_loss: 0.1814 - val_accuracy: 0.2428\n",
      "Epoch 162/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1321 - accuracy: 0.2222 - val_loss: 0.1801 - val_accuracy: 0.2212\n",
      "Epoch 163/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1318 - accuracy: 0.2186 - val_loss: 0.1790 - val_accuracy: 0.2302\n",
      "Epoch 164/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1311 - accuracy: 0.2360 - val_loss: 0.1861 - val_accuracy: 0.2410\n",
      "Epoch 165/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1310 - accuracy: 0.2354 - val_loss: 0.1777 - val_accuracy: 0.2446\n",
      "Epoch 166/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1302 - accuracy: 0.2366 - val_loss: 0.1774 - val_accuracy: 0.2374\n",
      "Epoch 167/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1298 - accuracy: 0.2456 - val_loss: 0.1782 - val_accuracy: 0.2572\n",
      "Epoch 168/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1294 - accuracy: 0.2444 - val_loss: 0.1760 - val_accuracy: 0.2374\n",
      "Epoch 169/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1290 - accuracy: 0.2294 - val_loss: 0.1753 - val_accuracy: 0.2680\n",
      "Epoch 170/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1286 - accuracy: 0.2372 - val_loss: 0.1745 - val_accuracy: 0.2680\n",
      "Epoch 171/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1280 - accuracy: 0.2330 - val_loss: 0.1746 - val_accuracy: 0.2536\n",
      "Epoch 172/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1278 - accuracy: 0.2396 - val_loss: 0.1750 - val_accuracy: 0.2716\n",
      "Epoch 173/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1273 - accuracy: 0.2396 - val_loss: 0.1762 - val_accuracy: 0.2374\n",
      "Epoch 174/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1271 - accuracy: 0.2312 - val_loss: 0.1739 - val_accuracy: 0.2896\n",
      "Epoch 175/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1266 - accuracy: 0.2396 - val_loss: 0.1731 - val_accuracy: 0.2536\n",
      "Epoch 176/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1262 - accuracy: 0.2414 - val_loss: 0.1733 - val_accuracy: 0.2896\n",
      "Epoch 177/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1258 - accuracy: 0.2408 - val_loss: 0.1738 - val_accuracy: 0.2662\n",
      "Epoch 178/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1255 - accuracy: 0.2432 - val_loss: 0.1734 - val_accuracy: 0.2608\n",
      "Epoch 179/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1251 - accuracy: 0.2354 - val_loss: 0.1721 - val_accuracy: 0.2212\n",
      "Epoch 180/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1247 - accuracy: 0.2366 - val_loss: 0.1713 - val_accuracy: 0.2734\n",
      "Epoch 181/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1243 - accuracy: 0.2462 - val_loss: 0.1717 - val_accuracy: 0.2932\n",
      "Epoch 182/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1240 - accuracy: 0.2505 - val_loss: 0.1706 - val_accuracy: 0.2428\n",
      "Epoch 183/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1236 - accuracy: 0.2547 - val_loss: 0.1705 - val_accuracy: 0.2824\n",
      "Epoch 184/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1233 - accuracy: 0.2529 - val_loss: 0.1723 - val_accuracy: 0.2878\n",
      "Epoch 185/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1231 - accuracy: 0.2462 - val_loss: 0.1750 - val_accuracy: 0.2068\n",
      "Epoch 186/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1228 - accuracy: 0.2414 - val_loss: 0.1691 - val_accuracy: 0.2572\n",
      "Epoch 187/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1224 - accuracy: 0.2480 - val_loss: 0.1697 - val_accuracy: 0.2662\n",
      "Epoch 188/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1221 - accuracy: 0.2450 - val_loss: 0.1702 - val_accuracy: 0.2374\n",
      "Epoch 189/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1216 - accuracy: 0.2505 - val_loss: 0.1691 - val_accuracy: 0.2302\n",
      "Epoch 190/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1214 - accuracy: 0.2462 - val_loss: 0.1681 - val_accuracy: 0.2698\n",
      "Epoch 191/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1210 - accuracy: 0.2511 - val_loss: 0.1680 - val_accuracy: 0.2662\n",
      "Epoch 192/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1208 - accuracy: 0.2613 - val_loss: 0.1683 - val_accuracy: 0.2446\n",
      "Epoch 193/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1204 - accuracy: 0.2486 - val_loss: 0.1685 - val_accuracy: 0.2716\n",
      "Epoch 194/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1201 - accuracy: 0.2456 - val_loss: 0.1670 - val_accuracy: 0.2950\n",
      "Epoch 195/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1198 - accuracy: 0.2498 - val_loss: 0.1662 - val_accuracy: 0.2752\n",
      "Epoch 196/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1195 - accuracy: 0.2553 - val_loss: 0.1672 - val_accuracy: 0.2698\n",
      "Epoch 197/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1192 - accuracy: 0.2535 - val_loss: 0.1664 - val_accuracy: 0.2590\n",
      "Epoch 198/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1187 - accuracy: 0.2667 - val_loss: 0.1706 - val_accuracy: 0.2374\n",
      "Epoch 199/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1189 - accuracy: 0.2601 - val_loss: 0.1664 - val_accuracy: 0.2734\n",
      "Epoch 200/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1183 - accuracy: 0.2619 - val_loss: 0.1656 - val_accuracy: 0.2608\n",
      "Epoch 201/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1179 - accuracy: 0.2535 - val_loss: 0.1646 - val_accuracy: 0.3022\n",
      "Epoch 202/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1176 - accuracy: 0.2547 - val_loss: 0.1644 - val_accuracy: 0.2770\n",
      "Epoch 203/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1173 - accuracy: 0.2559 - val_loss: 0.1648 - val_accuracy: 0.2770\n",
      "Epoch 204/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1171 - accuracy: 0.2589 - val_loss: 0.1645 - val_accuracy: 0.2446\n",
      "Epoch 205/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1167 - accuracy: 0.2661 - val_loss: 0.1637 - val_accuracy: 0.2500\n",
      "Epoch 206/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1164 - accuracy: 0.2535 - val_loss: 0.1652 - val_accuracy: 0.2590\n",
      "Epoch 207/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1162 - accuracy: 0.2571 - val_loss: 0.1626 - val_accuracy: 0.2932\n",
      "Epoch 208/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1158 - accuracy: 0.2601 - val_loss: 0.1638 - val_accuracy: 0.2896\n",
      "Epoch 209/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1155 - accuracy: 0.2667 - val_loss: 0.1624 - val_accuracy: 0.2806\n",
      "Epoch 210/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1152 - accuracy: 0.2619 - val_loss: 0.1624 - val_accuracy: 0.2860\n",
      "Epoch 211/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1149 - accuracy: 0.2631 - val_loss: 0.1612 - val_accuracy: 0.2788\n",
      "Epoch 212/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1146 - accuracy: 0.2619 - val_loss: 0.1612 - val_accuracy: 0.2644\n",
      "Epoch 213/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1143 - accuracy: 0.2697 - val_loss: 0.1618 - val_accuracy: 0.3004\n",
      "Epoch 214/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1140 - accuracy: 0.2733 - val_loss: 0.1609 - val_accuracy: 0.2554\n",
      "Epoch 215/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1138 - accuracy: 0.2613 - val_loss: 0.1625 - val_accuracy: 0.2554\n",
      "Epoch 216/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1136 - accuracy: 0.2637 - val_loss: 0.1600 - val_accuracy: 0.3112\n",
      "Epoch 217/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1130 - accuracy: 0.2637 - val_loss: 0.1603 - val_accuracy: 0.2824\n",
      "Epoch 218/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1130 - accuracy: 0.2673 - val_loss: 0.1603 - val_accuracy: 0.2806\n",
      "Epoch 219/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1126 - accuracy: 0.2643 - val_loss: 0.1616 - val_accuracy: 0.3094\n",
      "Epoch 220/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1124 - accuracy: 0.2631 - val_loss: 0.1589 - val_accuracy: 0.3094\n",
      "Epoch 221/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1121 - accuracy: 0.2787 - val_loss: 0.1617 - val_accuracy: 0.2374\n",
      "Epoch 222/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1118 - accuracy: 0.2685 - val_loss: 0.1676 - val_accuracy: 0.2248\n",
      "Epoch 223/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1118 - accuracy: 0.2679 - val_loss: 0.1574 - val_accuracy: 0.2914\n",
      "Epoch 224/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1112 - accuracy: 0.2769 - val_loss: 0.1586 - val_accuracy: 0.3094\n",
      "Epoch 225/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1109 - accuracy: 0.2637 - val_loss: 0.1589 - val_accuracy: 0.2950\n",
      "Epoch 226/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1106 - accuracy: 0.2727 - val_loss: 0.1562 - val_accuracy: 0.3129\n",
      "Epoch 227/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1102 - accuracy: 0.2751 - val_loss: 0.1571 - val_accuracy: 0.3363\n",
      "Epoch 228/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1100 - accuracy: 0.2697 - val_loss: 0.1568 - val_accuracy: 0.3183\n",
      "Epoch 229/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1098 - accuracy: 0.2709 - val_loss: 0.1593 - val_accuracy: 0.2968\n",
      "Epoch 230/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1095 - accuracy: 0.2799 - val_loss: 0.1585 - val_accuracy: 0.2788\n",
      "Epoch 231/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1094 - accuracy: 0.2763 - val_loss: 0.1555 - val_accuracy: 0.3237\n",
      "Epoch 232/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1089 - accuracy: 0.2787 - val_loss: 0.1552 - val_accuracy: 0.3094\n",
      "Epoch 233/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1087 - accuracy: 0.2817 - val_loss: 0.1547 - val_accuracy: 0.3022\n",
      "Epoch 234/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1083 - accuracy: 0.2859 - val_loss: 0.1551 - val_accuracy: 0.3076\n",
      "Epoch 235/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1081 - accuracy: 0.2751 - val_loss: 0.1553 - val_accuracy: 0.3022\n",
      "Epoch 236/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1079 - accuracy: 0.2787 - val_loss: 0.1550 - val_accuracy: 0.2752\n",
      "Epoch 237/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1076 - accuracy: 0.2799 - val_loss: 0.1549 - val_accuracy: 0.3147\n",
      "Epoch 238/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1074 - accuracy: 0.2709 - val_loss: 0.1536 - val_accuracy: 0.3076\n",
      "Epoch 239/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1070 - accuracy: 0.2889 - val_loss: 0.1671 - val_accuracy: 0.2248\n",
      "Epoch 240/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1074 - accuracy: 0.2817 - val_loss: 0.1526 - val_accuracy: 0.3147\n",
      "Epoch 241/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1066 - accuracy: 0.2739 - val_loss: 0.1537 - val_accuracy: 0.3381\n",
      "Epoch 242/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1064 - accuracy: 0.2877 - val_loss: 0.1554 - val_accuracy: 0.3183\n",
      "Epoch 243/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1062 - accuracy: 0.2871 - val_loss: 0.1531 - val_accuracy: 0.3165\n",
      "Epoch 244/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1059 - accuracy: 0.2799 - val_loss: 0.1520 - val_accuracy: 0.3147\n",
      "Epoch 245/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1056 - accuracy: 0.2757 - val_loss: 0.1528 - val_accuracy: 0.2896\n",
      "Epoch 246/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1054 - accuracy: 0.2715 - val_loss: 0.1515 - val_accuracy: 0.3112\n",
      "Epoch 247/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1052 - accuracy: 0.2709 - val_loss: 0.1507 - val_accuracy: 0.3058\n",
      "Epoch 248/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1049 - accuracy: 0.2823 - val_loss: 0.1556 - val_accuracy: 0.3022\n",
      "Epoch 249/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1049 - accuracy: 0.2859 - val_loss: 0.1523 - val_accuracy: 0.3183\n",
      "Epoch 250/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1046 - accuracy: 0.2835 - val_loss: 0.1506 - val_accuracy: 0.3094\n",
      "Epoch 251/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1043 - accuracy: 0.2817 - val_loss: 0.1504 - val_accuracy: 0.2950\n",
      "Epoch 252/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1042 - accuracy: 0.2817 - val_loss: 0.1509 - val_accuracy: 0.3381\n",
      "Epoch 253/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1039 - accuracy: 0.2817 - val_loss: 0.1496 - val_accuracy: 0.3219\n",
      "Epoch 254/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1037 - accuracy: 0.2841 - val_loss: 0.1498 - val_accuracy: 0.3273\n",
      "Epoch 255/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1036 - accuracy: 0.2733 - val_loss: 0.1509 - val_accuracy: 0.3309\n",
      "Epoch 256/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1035 - accuracy: 0.2871 - val_loss: 0.1492 - val_accuracy: 0.3435\n",
      "Epoch 257/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1032 - accuracy: 0.2811 - val_loss: 0.1492 - val_accuracy: 0.3147\n",
      "Epoch 258/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1028 - accuracy: 0.2769 - val_loss: 0.1565 - val_accuracy: 0.2716\n",
      "Epoch 259/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1032 - accuracy: 0.2763 - val_loss: 0.1476 - val_accuracy: 0.3219\n",
      "Epoch 260/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1026 - accuracy: 0.2811 - val_loss: 0.1611 - val_accuracy: 0.2860\n",
      "Epoch 261/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1029 - accuracy: 0.2823 - val_loss: 0.1575 - val_accuracy: 0.3273\n",
      "Epoch 262/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1028 - accuracy: 0.2865 - val_loss: 0.1473 - val_accuracy: 0.3309\n",
      "Epoch 263/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1021 - accuracy: 0.2835 - val_loss: 0.1474 - val_accuracy: 0.3183\n",
      "Epoch 264/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1019 - accuracy: 0.2799 - val_loss: 0.1477 - val_accuracy: 0.3345\n",
      "Epoch 265/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1018 - accuracy: 0.2895 - val_loss: 0.1463 - val_accuracy: 0.3094\n",
      "Epoch 266/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1016 - accuracy: 0.2793 - val_loss: 0.1482 - val_accuracy: 0.3363\n",
      "Epoch 267/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1015 - accuracy: 0.2847 - val_loss: 0.1662 - val_accuracy: 0.2392\n",
      "Epoch 268/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1018 - accuracy: 0.2943 - val_loss: 0.1571 - val_accuracy: 0.2824\n",
      "Epoch 269/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1017 - accuracy: 0.2811 - val_loss: 0.1486 - val_accuracy: 0.3525\n",
      "Epoch 270/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1010 - accuracy: 0.2841 - val_loss: 0.1464 - val_accuracy: 0.3345\n",
      "Epoch 271/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1006 - accuracy: 0.3003 - val_loss: 0.1474 - val_accuracy: 0.3219\n",
      "Epoch 272/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1006 - accuracy: 0.2895 - val_loss: 0.1476 - val_accuracy: 0.3201\n",
      "Epoch 273/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1005 - accuracy: 0.2811 - val_loss: 0.1470 - val_accuracy: 0.3435\n",
      "Epoch 274/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1002 - accuracy: 0.2901 - val_loss: 0.1460 - val_accuracy: 0.3201\n",
      "Epoch 275/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1002 - accuracy: 0.2871 - val_loss: 0.1468 - val_accuracy: 0.3165\n",
      "Epoch 276/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1001 - accuracy: 0.2877 - val_loss: 0.1460 - val_accuracy: 0.3255\n",
      "Epoch 277/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1000 - accuracy: 0.2793 - val_loss: 0.1452 - val_accuracy: 0.3022\n",
      "Epoch 278/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0998 - accuracy: 0.2793 - val_loss: 0.1456 - val_accuracy: 0.3453\n",
      "Epoch 279/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0997 - accuracy: 0.2925 - val_loss: 0.1464 - val_accuracy: 0.3507\n",
      "Epoch 280/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0994 - accuracy: 0.2907 - val_loss: 0.1450 - val_accuracy: 0.3471\n",
      "Epoch 281/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0993 - accuracy: 0.2877 - val_loss: 0.1455 - val_accuracy: 0.3165\n",
      "Epoch 282/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0991 - accuracy: 0.2871 - val_loss: 0.1461 - val_accuracy: 0.3183\n",
      "Epoch 283/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0990 - accuracy: 0.2895 - val_loss: 0.1441 - val_accuracy: 0.3165\n",
      "Epoch 284/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0988 - accuracy: 0.2853 - val_loss: 0.1485 - val_accuracy: 0.3597\n",
      "Epoch 285/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0989 - accuracy: 0.2931 - val_loss: 0.1448 - val_accuracy: 0.3165\n",
      "Epoch 286/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0986 - accuracy: 0.2871 - val_loss: 0.1449 - val_accuracy: 0.3112\n",
      "Epoch 287/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0985 - accuracy: 0.2823 - val_loss: 0.1450 - val_accuracy: 0.3489\n",
      "Epoch 288/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0984 - accuracy: 0.2949 - val_loss: 0.1451 - val_accuracy: 0.3489\n",
      "Epoch 289/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0982 - accuracy: 0.2979 - val_loss: 0.1447 - val_accuracy: 0.3291\n",
      "Epoch 290/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0983 - accuracy: 0.2961 - val_loss: 0.1573 - val_accuracy: 0.2950\n",
      "Epoch 291/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0988 - accuracy: 0.2883 - val_loss: 0.1443 - val_accuracy: 0.3417\n",
      "Epoch 292/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0978 - accuracy: 0.2931 - val_loss: 0.1437 - val_accuracy: 0.3076\n",
      "Epoch 293/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0978 - accuracy: 0.2913 - val_loss: 0.1466 - val_accuracy: 0.3004\n",
      "Epoch 294/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0979 - accuracy: 0.2841 - val_loss: 0.1440 - val_accuracy: 0.3507\n",
      "Epoch 295/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0973 - accuracy: 0.2973 - val_loss: 0.1436 - val_accuracy: 0.3345\n",
      "Epoch 296/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0972 - accuracy: 0.2961 - val_loss: 0.1451 - val_accuracy: 0.3327\n",
      "Epoch 297/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0972 - accuracy: 0.2961 - val_loss: 0.1445 - val_accuracy: 0.3453\n",
      "Epoch 298/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0971 - accuracy: 0.3039 - val_loss: 0.1439 - val_accuracy: 0.3183\n",
      "Epoch 299/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0970 - accuracy: 0.2925 - val_loss: 0.1432 - val_accuracy: 0.3022\n",
      "Epoch 300/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0968 - accuracy: 0.2859 - val_loss: 0.1546 - val_accuracy: 0.2932\n",
      "Epoch 301/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0974 - accuracy: 0.2925 - val_loss: 0.1415 - val_accuracy: 0.3201\n",
      "Epoch 302/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0966 - accuracy: 0.2913 - val_loss: 0.1426 - val_accuracy: 0.3201\n",
      "Epoch 303/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0965 - accuracy: 0.2877 - val_loss: 0.1433 - val_accuracy: 0.3219\n",
      "Epoch 304/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0963 - accuracy: 0.2973 - val_loss: 0.1424 - val_accuracy: 0.3345\n",
      "Epoch 305/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0961 - accuracy: 0.2907 - val_loss: 0.1421 - val_accuracy: 0.3399\n",
      "Epoch 306/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0961 - accuracy: 0.2889 - val_loss: 0.1427 - val_accuracy: 0.3219\n",
      "Epoch 307/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0959 - accuracy: 0.2973 - val_loss: 0.1434 - val_accuracy: 0.3363\n",
      "Epoch 308/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0960 - accuracy: 0.2985 - val_loss: 0.1439 - val_accuracy: 0.3417\n",
      "Epoch 309/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0957 - accuracy: 0.3045 - val_loss: 0.1588 - val_accuracy: 0.2608\n",
      "Epoch 310/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0961 - accuracy: 0.2913 - val_loss: 0.1418 - val_accuracy: 0.3489\n",
      "Epoch 311/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0953 - accuracy: 0.2985 - val_loss: 0.1423 - val_accuracy: 0.3651\n",
      "Epoch 312/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0952 - accuracy: 0.3063 - val_loss: 0.1426 - val_accuracy: 0.3381\n",
      "Epoch 313/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0952 - accuracy: 0.2997 - val_loss: 0.1422 - val_accuracy: 0.2968\n",
      "Epoch 314/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0950 - accuracy: 0.2991 - val_loss: 0.1411 - val_accuracy: 0.3255\n",
      "Epoch 315/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0949 - accuracy: 0.2907 - val_loss: 0.1412 - val_accuracy: 0.3417\n",
      "Epoch 316/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0948 - accuracy: 0.2967 - val_loss: 0.1395 - val_accuracy: 0.3255\n",
      "Epoch 317/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0945 - accuracy: 0.3009 - val_loss: 0.1407 - val_accuracy: 0.3543\n",
      "Epoch 318/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0944 - accuracy: 0.2967 - val_loss: 0.1422 - val_accuracy: 0.3363\n",
      "Epoch 319/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0945 - accuracy: 0.2931 - val_loss: 0.1423 - val_accuracy: 0.3327\n",
      "Epoch 320/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0943 - accuracy: 0.2991 - val_loss: 0.1418 - val_accuracy: 0.3309\n",
      "Epoch 321/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0943 - accuracy: 0.3033 - val_loss: 0.1531 - val_accuracy: 0.3112\n",
      "Epoch 322/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0947 - accuracy: 0.3045 - val_loss: 0.1425 - val_accuracy: 0.3147\n",
      "Epoch 323/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0939 - accuracy: 0.2979 - val_loss: 0.1402 - val_accuracy: 0.3291\n",
      "Epoch 324/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0937 - accuracy: 0.2973 - val_loss: 0.1390 - val_accuracy: 0.3255\n",
      "Epoch 325/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0937 - accuracy: 0.2955 - val_loss: 0.1389 - val_accuracy: 0.3022\n",
      "Epoch 326/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0936 - accuracy: 0.2895 - val_loss: 0.1405 - val_accuracy: 0.3094\n",
      "Epoch 327/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0936 - accuracy: 0.2937 - val_loss: 0.1501 - val_accuracy: 0.2590\n",
      "Epoch 328/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0940 - accuracy: 0.2877 - val_loss: 0.1420 - val_accuracy: 0.3381\n",
      "Epoch 329/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0932 - accuracy: 0.3039 - val_loss: 0.1393 - val_accuracy: 0.3040\n",
      "Epoch 330/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0933 - accuracy: 0.2997 - val_loss: 0.1396 - val_accuracy: 0.3273\n",
      "Epoch 331/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0930 - accuracy: 0.2973 - val_loss: 0.1408 - val_accuracy: 0.3094\n",
      "Epoch 332/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0929 - accuracy: 0.2961 - val_loss: 0.1387 - val_accuracy: 0.3201\n",
      "Epoch 333/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0926 - accuracy: 0.2961 - val_loss: 0.1393 - val_accuracy: 0.3399\n",
      "Epoch 334/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0925 - accuracy: 0.2967 - val_loss: 0.1385 - val_accuracy: 0.2788\n",
      "Epoch 335/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0925 - accuracy: 0.2883 - val_loss: 0.1399 - val_accuracy: 0.3363\n",
      "Epoch 336/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0925 - accuracy: 0.2997 - val_loss: 0.1401 - val_accuracy: 0.2824\n",
      "Epoch 337/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0923 - accuracy: 0.3039 - val_loss: 0.1403 - val_accuracy: 0.2842\n",
      "Epoch 338/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0924 - accuracy: 0.2961 - val_loss: 0.1537 - val_accuracy: 0.2644\n",
      "Epoch 339/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0927 - accuracy: 0.3021 - val_loss: 0.1410 - val_accuracy: 0.3076\n",
      "Epoch 340/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0920 - accuracy: 0.2991 - val_loss: 0.1386 - val_accuracy: 0.3112\n",
      "Epoch 341/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0918 - accuracy: 0.2943 - val_loss: 0.1400 - val_accuracy: 0.3363\n",
      "Epoch 342/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0917 - accuracy: 0.3075 - val_loss: 0.1428 - val_accuracy: 0.2770\n",
      "Epoch 343/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0920 - accuracy: 0.2949 - val_loss: 0.1393 - val_accuracy: 0.3687\n",
      "Epoch 344/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0915 - accuracy: 0.3015 - val_loss: 0.1382 - val_accuracy: 0.3112\n",
      "Epoch 345/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0913 - accuracy: 0.2991 - val_loss: 0.1382 - val_accuracy: 0.3201\n",
      "Epoch 346/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0912 - accuracy: 0.3075 - val_loss: 0.1469 - val_accuracy: 0.3094\n",
      "Epoch 347/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0917 - accuracy: 0.2997 - val_loss: 0.1394 - val_accuracy: 0.2788\n",
      "Epoch 348/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0913 - accuracy: 0.2931 - val_loss: 0.1462 - val_accuracy: 0.3040\n",
      "Epoch 349/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0912 - accuracy: 0.3081 - val_loss: 0.1374 - val_accuracy: 0.3219\n",
      "Epoch 350/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0907 - accuracy: 0.3003 - val_loss: 0.1378 - val_accuracy: 0.3309\n",
      "Epoch 351/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0905 - accuracy: 0.3039 - val_loss: 0.1382 - val_accuracy: 0.3129\n",
      "Epoch 352/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0906 - accuracy: 0.2997 - val_loss: 0.1380 - val_accuracy: 0.3255\n",
      "Epoch 353/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0906 - accuracy: 0.2979 - val_loss: 0.1400 - val_accuracy: 0.3201\n",
      "Epoch 354/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0904 - accuracy: 0.3015 - val_loss: 0.1378 - val_accuracy: 0.3076\n",
      "Epoch 355/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0899 - accuracy: 0.3027 - val_loss: 0.1374 - val_accuracy: 0.3147\n",
      "Epoch 356/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0899 - accuracy: 0.3045 - val_loss: 0.1380 - val_accuracy: 0.2914\n",
      "Epoch 357/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0899 - accuracy: 0.3003 - val_loss: 0.1370 - val_accuracy: 0.3417\n",
      "Epoch 358/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0897 - accuracy: 0.3063 - val_loss: 0.1385 - val_accuracy: 0.3345\n",
      "Epoch 359/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0897 - accuracy: 0.3087 - val_loss: 0.1361 - val_accuracy: 0.3273\n",
      "Epoch 360/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0895 - accuracy: 0.3039 - val_loss: 0.1383 - val_accuracy: 0.3273\n",
      "Epoch 361/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0896 - accuracy: 0.3033 - val_loss: 0.1382 - val_accuracy: 0.3183\n",
      "Epoch 362/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0893 - accuracy: 0.3117 - val_loss: 0.1375 - val_accuracy: 0.3219\n",
      "Epoch 363/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0894 - accuracy: 0.3057 - val_loss: 0.1384 - val_accuracy: 0.3165\n",
      "Epoch 364/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0892 - accuracy: 0.2991 - val_loss: 0.1406 - val_accuracy: 0.3381\n",
      "Epoch 365/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0891 - accuracy: 0.3087 - val_loss: 0.1363 - val_accuracy: 0.3183\n",
      "Epoch 366/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0888 - accuracy: 0.3045 - val_loss: 0.1369 - val_accuracy: 0.3489\n",
      "Epoch 367/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0888 - accuracy: 0.3099 - val_loss: 0.1351 - val_accuracy: 0.3183\n",
      "Epoch 368/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0885 - accuracy: 0.3015 - val_loss: 0.1392 - val_accuracy: 0.3058\n",
      "Epoch 369/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0886 - accuracy: 0.3075 - val_loss: 0.1355 - val_accuracy: 0.3201\n",
      "Epoch 370/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0882 - accuracy: 0.3051 - val_loss: 0.1352 - val_accuracy: 0.3183\n",
      "Epoch 371/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0881 - accuracy: 0.3093 - val_loss: 0.1348 - val_accuracy: 0.2914\n",
      "Epoch 372/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0881 - accuracy: 0.3015 - val_loss: 0.1344 - val_accuracy: 0.3094\n",
      "Epoch 373/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0879 - accuracy: 0.3027 - val_loss: 0.1491 - val_accuracy: 0.3129\n",
      "Epoch 374/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0884 - accuracy: 0.3003 - val_loss: 0.1345 - val_accuracy: 0.3058\n",
      "Epoch 375/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0878 - accuracy: 0.2997 - val_loss: 0.1355 - val_accuracy: 0.3165\n",
      "Epoch 376/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0877 - accuracy: 0.2997 - val_loss: 0.1389 - val_accuracy: 0.3309\n",
      "Epoch 377/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0878 - accuracy: 0.3039 - val_loss: 0.1358 - val_accuracy: 0.3255\n",
      "Epoch 378/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0875 - accuracy: 0.3027 - val_loss: 0.1353 - val_accuracy: 0.3507\n",
      "Epoch 379/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0872 - accuracy: 0.3057 - val_loss: 0.1355 - val_accuracy: 0.3076\n",
      "Epoch 380/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0873 - accuracy: 0.3069 - val_loss: 0.1360 - val_accuracy: 0.3183\n",
      "Epoch 381/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0871 - accuracy: 0.3063 - val_loss: 0.1343 - val_accuracy: 0.2860\n",
      "Epoch 382/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0871 - accuracy: 0.3051 - val_loss: 0.1360 - val_accuracy: 0.3309\n",
      "Epoch 383/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0870 - accuracy: 0.3051 - val_loss: 0.1423 - val_accuracy: 0.2770\n",
      "Epoch 384/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0871 - accuracy: 0.3087 - val_loss: 0.1352 - val_accuracy: 0.3309\n",
      "Epoch 385/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0866 - accuracy: 0.3093 - val_loss: 0.1361 - val_accuracy: 0.3112\n",
      "Epoch 386/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0866 - accuracy: 0.3027 - val_loss: 0.1339 - val_accuracy: 0.3076\n",
      "Epoch 387/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0865 - accuracy: 0.3075 - val_loss: 0.1338 - val_accuracy: 0.3147\n",
      "Epoch 388/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0865 - accuracy: 0.3033 - val_loss: 0.1529 - val_accuracy: 0.3489\n",
      "Epoch 389/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0871 - accuracy: 0.3045 - val_loss: 0.1339 - val_accuracy: 0.2914\n",
      "Epoch 390/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0862 - accuracy: 0.3009 - val_loss: 0.1347 - val_accuracy: 0.3129\n",
      "Epoch 391/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0861 - accuracy: 0.3045 - val_loss: 0.1355 - val_accuracy: 0.2644\n",
      "Epoch 392/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0860 - accuracy: 0.2967 - val_loss: 0.1330 - val_accuracy: 0.2896\n",
      "Epoch 393/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0860 - accuracy: 0.2985 - val_loss: 0.1339 - val_accuracy: 0.3022\n",
      "Epoch 394/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0857 - accuracy: 0.3063 - val_loss: 0.1341 - val_accuracy: 0.3076\n",
      "Epoch 395/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0856 - accuracy: 0.2979 - val_loss: 0.1344 - val_accuracy: 0.3255\n",
      "Epoch 396/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0857 - accuracy: 0.3039 - val_loss: 0.1418 - val_accuracy: 0.3399\n",
      "Epoch 397/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0857 - accuracy: 0.3027 - val_loss: 0.1387 - val_accuracy: 0.3076\n",
      "Epoch 398/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0856 - accuracy: 0.2973 - val_loss: 0.1339 - val_accuracy: 0.3022\n",
      "Epoch 399/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0854 - accuracy: 0.3081 - val_loss: 0.1356 - val_accuracy: 0.3255\n",
      "Epoch 400/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0852 - accuracy: 0.3135 - val_loss: 0.1336 - val_accuracy: 0.3273\n",
      "Epoch 401/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0851 - accuracy: 0.3087 - val_loss: 0.1352 - val_accuracy: 0.3058\n",
      "Epoch 402/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0851 - accuracy: 0.2997 - val_loss: 0.1346 - val_accuracy: 0.3273\n",
      "Epoch 403/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0849 - accuracy: 0.3057 - val_loss: 0.1342 - val_accuracy: 0.2662\n",
      "Epoch 404/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0848 - accuracy: 0.3021 - val_loss: 0.1336 - val_accuracy: 0.3094\n",
      "Epoch 405/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0848 - accuracy: 0.3057 - val_loss: 0.1559 - val_accuracy: 0.2356\n",
      "Epoch 406/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0858 - accuracy: 0.3057 - val_loss: 0.1335 - val_accuracy: 0.3147\n",
      "Epoch 407/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0848 - accuracy: 0.3021 - val_loss: 0.1331 - val_accuracy: 0.3129\n",
      "Epoch 408/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0844 - accuracy: 0.3087 - val_loss: 0.1366 - val_accuracy: 0.2734\n",
      "Epoch 409/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0845 - accuracy: 0.3009 - val_loss: 0.1372 - val_accuracy: 0.2608\n",
      "Epoch 410/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0846 - accuracy: 0.2925 - val_loss: 0.1346 - val_accuracy: 0.3183\n",
      "Epoch 411/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0843 - accuracy: 0.3033 - val_loss: 0.1341 - val_accuracy: 0.3004\n",
      "Epoch 412/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0842 - accuracy: 0.3003 - val_loss: 0.1333 - val_accuracy: 0.3219\n",
      "Epoch 413/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0840 - accuracy: 0.3081 - val_loss: 0.1375 - val_accuracy: 0.2986\n",
      "Epoch 414/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0841 - accuracy: 0.2979 - val_loss: 0.1356 - val_accuracy: 0.3219\n",
      "Epoch 415/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0838 - accuracy: 0.3015 - val_loss: 0.1326 - val_accuracy: 0.3129\n",
      "Epoch 416/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0837 - accuracy: 0.3015 - val_loss: 0.1331 - val_accuracy: 0.2896\n",
      "Epoch 417/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0837 - accuracy: 0.2991 - val_loss: 0.1326 - val_accuracy: 0.3147\n",
      "Epoch 418/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0834 - accuracy: 0.3045 - val_loss: 0.1352 - val_accuracy: 0.2932\n",
      "Epoch 419/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0836 - accuracy: 0.3063 - val_loss: 0.1319 - val_accuracy: 0.3076\n",
      "Epoch 420/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0832 - accuracy: 0.2949 - val_loss: 0.1504 - val_accuracy: 0.2356\n",
      "Epoch 421/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0838 - accuracy: 0.2991 - val_loss: 0.1331 - val_accuracy: 0.3040\n",
      "Epoch 422/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0831 - accuracy: 0.3045 - val_loss: 0.1364 - val_accuracy: 0.2986\n",
      "Epoch 423/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0831 - accuracy: 0.3015 - val_loss: 0.1325 - val_accuracy: 0.3183\n",
      "Epoch 424/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0828 - accuracy: 0.3003 - val_loss: 0.1330 - val_accuracy: 0.2986\n",
      "Epoch 425/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0828 - accuracy: 0.3099 - val_loss: 0.1330 - val_accuracy: 0.2986\n",
      "Epoch 426/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0827 - accuracy: 0.2967 - val_loss: 0.1355 - val_accuracy: 0.3076\n",
      "Epoch 427/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0829 - accuracy: 0.3069 - val_loss: 0.1321 - val_accuracy: 0.3058\n",
      "Epoch 428/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0826 - accuracy: 0.3015 - val_loss: 0.1337 - val_accuracy: 0.3417\n",
      "Epoch 429/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0825 - accuracy: 0.3033 - val_loss: 0.1338 - val_accuracy: 0.3040\n",
      "Epoch 430/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0823 - accuracy: 0.3015 - val_loss: 0.1356 - val_accuracy: 0.3147\n",
      "Epoch 431/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0824 - accuracy: 0.2997 - val_loss: 0.1337 - val_accuracy: 0.2428\n",
      "Epoch 432/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0825 - accuracy: 0.3057 - val_loss: 0.1324 - val_accuracy: 0.2932\n",
      "Epoch 433/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0821 - accuracy: 0.3051 - val_loss: 0.1333 - val_accuracy: 0.3165\n",
      "Epoch 434/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0821 - accuracy: 0.3087 - val_loss: 0.1362 - val_accuracy: 0.3165\n",
      "Epoch 435/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0822 - accuracy: 0.3111 - val_loss: 0.1332 - val_accuracy: 0.3040\n",
      "Epoch 436/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0822 - accuracy: 0.3063 - val_loss: 0.1316 - val_accuracy: 0.3237\n",
      "Epoch 437/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0818 - accuracy: 0.3033 - val_loss: 0.1336 - val_accuracy: 0.3094\n",
      "Epoch 438/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0818 - accuracy: 0.2991 - val_loss: 0.1314 - val_accuracy: 0.2842\n",
      "Epoch 439/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0817 - accuracy: 0.2973 - val_loss: 0.1328 - val_accuracy: 0.3201\n",
      "Epoch 440/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0818 - accuracy: 0.3039 - val_loss: 0.1320 - val_accuracy: 0.3058\n",
      "Epoch 441/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0815 - accuracy: 0.3033 - val_loss: 0.1386 - val_accuracy: 0.3219\n",
      "Epoch 442/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0818 - accuracy: 0.3069 - val_loss: 0.1318 - val_accuracy: 0.2986\n",
      "Epoch 443/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0815 - accuracy: 0.3081 - val_loss: 0.1332 - val_accuracy: 0.3058\n",
      "Epoch 444/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0813 - accuracy: 0.3009 - val_loss: 0.1320 - val_accuracy: 0.3058\n",
      "Epoch 445/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0811 - accuracy: 0.2985 - val_loss: 0.1325 - val_accuracy: 0.2626\n",
      "Epoch 446/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0813 - accuracy: 0.2937 - val_loss: 0.1306 - val_accuracy: 0.3040\n",
      "Epoch 447/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0810 - accuracy: 0.2949 - val_loss: 0.1322 - val_accuracy: 0.3112\n",
      "Epoch 448/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0810 - accuracy: 0.3021 - val_loss: 0.1397 - val_accuracy: 0.3112\n",
      "Epoch 449/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0811 - accuracy: 0.3039 - val_loss: 0.1336 - val_accuracy: 0.3201\n",
      "Epoch 450/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0808 - accuracy: 0.2979 - val_loss: 0.1312 - val_accuracy: 0.3129\n",
      "Epoch 451/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0806 - accuracy: 0.3081 - val_loss: 0.1325 - val_accuracy: 0.3076\n",
      "Epoch 452/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0806 - accuracy: 0.3075 - val_loss: 0.1323 - val_accuracy: 0.3022\n",
      "Epoch 453/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0805 - accuracy: 0.3009 - val_loss: 0.1424 - val_accuracy: 0.3058\n",
      "Epoch 454/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0810 - accuracy: 0.3039 - val_loss: 0.1448 - val_accuracy: 0.3327\n",
      "Epoch 455/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0810 - accuracy: 0.3069 - val_loss: 0.1408 - val_accuracy: 0.2626\n",
      "Epoch 456/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0810 - accuracy: 0.3093 - val_loss: 0.1320 - val_accuracy: 0.3094\n",
      "Epoch 457/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0802 - accuracy: 0.2949 - val_loss: 0.1304 - val_accuracy: 0.2914\n",
      "Epoch 458/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0801 - accuracy: 0.3045 - val_loss: 0.1387 - val_accuracy: 0.2626\n",
      "Epoch 459/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0805 - accuracy: 0.3045 - val_loss: 0.1309 - val_accuracy: 0.2770\n",
      "Epoch 460/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0801 - accuracy: 0.3033 - val_loss: 0.1316 - val_accuracy: 0.3076\n",
      "Epoch 461/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0799 - accuracy: 0.3045 - val_loss: 0.1310 - val_accuracy: 0.2914\n",
      "Epoch 462/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0800 - accuracy: 0.3021 - val_loss: 0.1317 - val_accuracy: 0.3076\n",
      "Epoch 463/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0797 - accuracy: 0.3039 - val_loss: 0.1436 - val_accuracy: 0.2788\n",
      "Epoch 464/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0804 - accuracy: 0.3045 - val_loss: 0.1327 - val_accuracy: 0.2842\n",
      "Epoch 465/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0796 - accuracy: 0.3039 - val_loss: 0.1302 - val_accuracy: 0.2770\n",
      "Epoch 466/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0794 - accuracy: 0.3045 - val_loss: 0.1294 - val_accuracy: 0.2914\n",
      "Epoch 467/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0795 - accuracy: 0.2991 - val_loss: 0.1311 - val_accuracy: 0.2896\n",
      "Epoch 468/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0793 - accuracy: 0.3081 - val_loss: 0.1324 - val_accuracy: 0.2968\n",
      "Epoch 469/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0793 - accuracy: 0.2997 - val_loss: 0.1307 - val_accuracy: 0.2986\n",
      "Epoch 470/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0790 - accuracy: 0.3003 - val_loss: 0.1304 - val_accuracy: 0.2788\n",
      "Epoch 471/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0790 - accuracy: 0.2949 - val_loss: 0.1323 - val_accuracy: 0.3058\n",
      "Epoch 472/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0789 - accuracy: 0.3009 - val_loss: 0.1308 - val_accuracy: 0.3004\n",
      "Epoch 473/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0789 - accuracy: 0.3027 - val_loss: 0.1325 - val_accuracy: 0.2770\n",
      "Epoch 474/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0789 - accuracy: 0.3087 - val_loss: 0.1310 - val_accuracy: 0.2716\n",
      "Epoch 475/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0788 - accuracy: 0.3045 - val_loss: 0.1327 - val_accuracy: 0.3183\n",
      "Epoch 476/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0786 - accuracy: 0.2985 - val_loss: 0.1329 - val_accuracy: 0.2914\n",
      "Epoch 477/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0788 - accuracy: 0.3087 - val_loss: 0.1315 - val_accuracy: 0.3237\n",
      "Epoch 478/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0785 - accuracy: 0.3069 - val_loss: 0.1313 - val_accuracy: 0.3004\n",
      "Epoch 479/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0787 - accuracy: 0.3063 - val_loss: 0.1301 - val_accuracy: 0.2896\n",
      "Epoch 480/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0782 - accuracy: 0.3081 - val_loss: 0.1308 - val_accuracy: 0.3022\n",
      "Epoch 481/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0783 - accuracy: 0.3033 - val_loss: 0.1310 - val_accuracy: 0.2896\n",
      "Epoch 482/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0783 - accuracy: 0.3009 - val_loss: 0.1322 - val_accuracy: 0.2950\n",
      "Epoch 483/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0782 - accuracy: 0.2985 - val_loss: 0.1309 - val_accuracy: 0.2914\n",
      "Epoch 484/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0780 - accuracy: 0.3111 - val_loss: 0.1461 - val_accuracy: 0.2896\n",
      "Epoch 485/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0787 - accuracy: 0.3123 - val_loss: 0.1309 - val_accuracy: 0.3147\n",
      "Epoch 486/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0778 - accuracy: 0.3039 - val_loss: 0.1310 - val_accuracy: 0.2932\n",
      "Epoch 487/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0780 - accuracy: 0.3099 - val_loss: 0.1313 - val_accuracy: 0.3237\n",
      "Epoch 488/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0778 - accuracy: 0.3063 - val_loss: 0.1319 - val_accuracy: 0.3129\n",
      "Epoch 489/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0777 - accuracy: 0.3033 - val_loss: 0.1299 - val_accuracy: 0.2806\n",
      "Epoch 490/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0777 - accuracy: 0.3075 - val_loss: 0.1300 - val_accuracy: 0.2968\n",
      "Epoch 491/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0776 - accuracy: 0.3003 - val_loss: 0.1303 - val_accuracy: 0.2986\n",
      "Epoch 492/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0774 - accuracy: 0.3051 - val_loss: 0.1296 - val_accuracy: 0.3040\n",
      "Epoch 493/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0774 - accuracy: 0.3021 - val_loss: 0.1299 - val_accuracy: 0.3129\n",
      "Epoch 494/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0772 - accuracy: 0.3129 - val_loss: 0.1407 - val_accuracy: 0.2086\n",
      "Epoch 495/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0779 - accuracy: 0.3027 - val_loss: 0.1320 - val_accuracy: 0.3040\n",
      "Epoch 496/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0772 - accuracy: 0.3075 - val_loss: 0.1352 - val_accuracy: 0.3165\n",
      "Epoch 497/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0776 - accuracy: 0.3069 - val_loss: 0.1316 - val_accuracy: 0.2878\n",
      "Epoch 498/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0771 - accuracy: 0.3087 - val_loss: 0.1291 - val_accuracy: 0.2896\n",
      "Epoch 499/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0771 - accuracy: 0.3075 - val_loss: 0.1310 - val_accuracy: 0.2806\n",
      "Epoch 500/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0769 - accuracy: 0.3057 - val_loss: 0.1307 - val_accuracy: 0.2914\n",
      "Epoch 501/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0770 - accuracy: 0.3075 - val_loss: 0.1288 - val_accuracy: 0.2914\n",
      "Epoch 502/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0768 - accuracy: 0.3075 - val_loss: 0.1296 - val_accuracy: 0.3129\n",
      "Epoch 503/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0766 - accuracy: 0.3111 - val_loss: 0.1341 - val_accuracy: 0.2500\n",
      "Epoch 504/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0769 - accuracy: 0.3027 - val_loss: 0.1276 - val_accuracy: 0.2950\n",
      "Epoch 505/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0766 - accuracy: 0.3141 - val_loss: 0.1297 - val_accuracy: 0.3022\n",
      "Epoch 506/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0764 - accuracy: 0.3075 - val_loss: 0.1305 - val_accuracy: 0.3040\n",
      "Epoch 507/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0764 - accuracy: 0.3129 - val_loss: 0.1304 - val_accuracy: 0.2896\n",
      "Epoch 508/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0762 - accuracy: 0.3117 - val_loss: 0.1361 - val_accuracy: 0.3597\n",
      "Epoch 509/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0764 - accuracy: 0.3237 - val_loss: 0.1301 - val_accuracy: 0.3040\n",
      "Epoch 510/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0760 - accuracy: 0.3111 - val_loss: 0.1285 - val_accuracy: 0.2950\n",
      "Epoch 511/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0762 - accuracy: 0.3159 - val_loss: 0.1295 - val_accuracy: 0.2896\n",
      "Epoch 512/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0760 - accuracy: 0.3105 - val_loss: 0.1289 - val_accuracy: 0.2752\n",
      "Epoch 513/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0758 - accuracy: 0.3075 - val_loss: 0.1294 - val_accuracy: 0.2950\n",
      "Epoch 514/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0760 - accuracy: 0.3087 - val_loss: 0.1285 - val_accuracy: 0.2860\n",
      "Epoch 515/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0756 - accuracy: 0.3063 - val_loss: 0.1294 - val_accuracy: 0.2824\n",
      "Epoch 516/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0757 - accuracy: 0.3087 - val_loss: 0.1300 - val_accuracy: 0.2716\n",
      "Epoch 517/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0757 - accuracy: 0.3087 - val_loss: 0.1295 - val_accuracy: 0.3058\n",
      "Epoch 518/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0756 - accuracy: 0.3129 - val_loss: 0.1294 - val_accuracy: 0.2878\n",
      "Epoch 519/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0755 - accuracy: 0.3057 - val_loss: 0.1286 - val_accuracy: 0.2770\n",
      "Epoch 520/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0755 - accuracy: 0.3045 - val_loss: 0.1297 - val_accuracy: 0.2932\n",
      "Epoch 521/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0755 - accuracy: 0.3021 - val_loss: 0.1298 - val_accuracy: 0.2788\n",
      "Epoch 522/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0754 - accuracy: 0.3045 - val_loss: 0.1295 - val_accuracy: 0.2788\n",
      "Epoch 523/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0752 - accuracy: 0.3057 - val_loss: 0.1460 - val_accuracy: 0.3237\n",
      "Epoch 524/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0757 - accuracy: 0.3141 - val_loss: 0.1271 - val_accuracy: 0.3076\n",
      "Epoch 525/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0751 - accuracy: 0.3117 - val_loss: 0.1316 - val_accuracy: 0.2824\n",
      "Epoch 526/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0751 - accuracy: 0.3159 - val_loss: 0.1309 - val_accuracy: 0.3058\n",
      "Epoch 527/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0749 - accuracy: 0.3123 - val_loss: 0.1302 - val_accuracy: 0.2824\n",
      "Epoch 528/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0749 - accuracy: 0.3069 - val_loss: 0.1311 - val_accuracy: 0.3076\n",
      "Epoch 529/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0747 - accuracy: 0.3141 - val_loss: 0.1309 - val_accuracy: 0.2968\n",
      "Epoch 530/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0750 - accuracy: 0.3117 - val_loss: 0.1285 - val_accuracy: 0.2806\n",
      "Epoch 531/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0747 - accuracy: 0.3111 - val_loss: 0.1289 - val_accuracy: 0.2932\n",
      "Epoch 532/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0744 - accuracy: 0.3141 - val_loss: 0.1281 - val_accuracy: 0.2716\n",
      "Epoch 533/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0745 - accuracy: 0.3105 - val_loss: 0.1304 - val_accuracy: 0.3022\n",
      "Epoch 534/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0743 - accuracy: 0.3153 - val_loss: 0.1284 - val_accuracy: 0.2806\n",
      "Epoch 535/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0742 - accuracy: 0.3153 - val_loss: 0.1288 - val_accuracy: 0.2932\n",
      "Epoch 536/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0742 - accuracy: 0.3117 - val_loss: 0.1289 - val_accuracy: 0.3112\n",
      "Epoch 537/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0741 - accuracy: 0.3123 - val_loss: 0.1265 - val_accuracy: 0.2788\n",
      "Epoch 538/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0741 - accuracy: 0.3105 - val_loss: 0.1280 - val_accuracy: 0.2896\n",
      "Epoch 539/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0742 - accuracy: 0.3141 - val_loss: 0.1277 - val_accuracy: 0.2572\n",
      "Epoch 540/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0741 - accuracy: 0.3093 - val_loss: 0.1316 - val_accuracy: 0.2896\n",
      "Epoch 541/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0739 - accuracy: 0.3225 - val_loss: 0.1298 - val_accuracy: 0.3040\n",
      "Epoch 542/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0738 - accuracy: 0.3153 - val_loss: 0.1484 - val_accuracy: 0.3040\n",
      "Epoch 543/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0750 - accuracy: 0.3147 - val_loss: 0.1267 - val_accuracy: 0.2878\n",
      "Epoch 544/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0737 - accuracy: 0.3129 - val_loss: 0.1418 - val_accuracy: 0.3129\n",
      "Epoch 545/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0745 - accuracy: 0.3219 - val_loss: 0.1342 - val_accuracy: 0.2986\n",
      "Epoch 546/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0738 - accuracy: 0.3243 - val_loss: 0.1489 - val_accuracy: 0.2824\n",
      "Epoch 547/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0746 - accuracy: 0.3081 - val_loss: 0.1264 - val_accuracy: 0.2626\n",
      "Epoch 548/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0735 - accuracy: 0.3099 - val_loss: 0.1275 - val_accuracy: 0.2932\n",
      "Epoch 549/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0734 - accuracy: 0.3135 - val_loss: 0.1305 - val_accuracy: 0.2950\n",
      "Epoch 550/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0733 - accuracy: 0.3177 - val_loss: 0.1276 - val_accuracy: 0.3004\n",
      "Epoch 551/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0734 - accuracy: 0.3159 - val_loss: 0.1275 - val_accuracy: 0.2968\n",
      "Epoch 552/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0731 - accuracy: 0.3111 - val_loss: 0.1283 - val_accuracy: 0.2644\n",
      "Epoch 553/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0734 - accuracy: 0.3123 - val_loss: 0.1273 - val_accuracy: 0.2860\n",
      "Epoch 554/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0729 - accuracy: 0.3237 - val_loss: 0.1277 - val_accuracy: 0.2914\n",
      "Epoch 555/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0728 - accuracy: 0.3147 - val_loss: 0.1259 - val_accuracy: 0.2878\n",
      "Epoch 556/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0727 - accuracy: 0.3177 - val_loss: 0.1287 - val_accuracy: 0.2986\n",
      "Epoch 557/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0726 - accuracy: 0.3135 - val_loss: 0.1272 - val_accuracy: 0.2860\n",
      "Epoch 558/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0726 - accuracy: 0.3105 - val_loss: 0.1303 - val_accuracy: 0.3076\n",
      "Epoch 559/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0728 - accuracy: 0.3135 - val_loss: 0.1267 - val_accuracy: 0.3040\n",
      "Epoch 560/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0724 - accuracy: 0.3189 - val_loss: 0.1278 - val_accuracy: 0.2986\n",
      "Epoch 561/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0725 - accuracy: 0.3171 - val_loss: 0.1267 - val_accuracy: 0.2914\n",
      "Epoch 562/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0723 - accuracy: 0.3171 - val_loss: 0.1289 - val_accuracy: 0.3094\n",
      "Epoch 563/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0725 - accuracy: 0.3135 - val_loss: 0.1284 - val_accuracy: 0.3022\n",
      "Epoch 564/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0724 - accuracy: 0.3171 - val_loss: 0.1274 - val_accuracy: 0.2932\n",
      "Epoch 565/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0723 - accuracy: 0.3243 - val_loss: 0.1285 - val_accuracy: 0.2770\n",
      "Epoch 566/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0722 - accuracy: 0.3135 - val_loss: 0.1310 - val_accuracy: 0.2716\n",
      "Epoch 567/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0724 - accuracy: 0.3111 - val_loss: 0.1279 - val_accuracy: 0.3022\n",
      "Epoch 568/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0721 - accuracy: 0.3213 - val_loss: 0.1275 - val_accuracy: 0.2896\n",
      "Epoch 569/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0719 - accuracy: 0.3129 - val_loss: 0.1415 - val_accuracy: 0.3004\n",
      "Epoch 570/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0725 - accuracy: 0.3207 - val_loss: 0.1259 - val_accuracy: 0.2842\n",
      "Epoch 571/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0719 - accuracy: 0.3153 - val_loss: 0.1266 - val_accuracy: 0.3022\n",
      "Epoch 572/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0719 - accuracy: 0.3207 - val_loss: 0.1243 - val_accuracy: 0.2842\n",
      "Epoch 573/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0718 - accuracy: 0.3165 - val_loss: 0.1261 - val_accuracy: 0.3165\n",
      "Epoch 574/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0716 - accuracy: 0.3255 - val_loss: 0.1268 - val_accuracy: 0.3022\n",
      "Epoch 575/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0716 - accuracy: 0.3159 - val_loss: 0.1316 - val_accuracy: 0.2932\n",
      "Epoch 576/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0717 - accuracy: 0.3123 - val_loss: 0.1355 - val_accuracy: 0.2716\n",
      "Epoch 577/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0722 - accuracy: 0.3039 - val_loss: 0.1261 - val_accuracy: 0.2842\n",
      "Epoch 578/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0718 - accuracy: 0.3165 - val_loss: 0.1306 - val_accuracy: 0.3094\n",
      "Epoch 579/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0714 - accuracy: 0.3159 - val_loss: 0.1296 - val_accuracy: 0.3112\n",
      "Epoch 580/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0714 - accuracy: 0.3159 - val_loss: 0.1259 - val_accuracy: 0.2842\n",
      "Epoch 581/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0712 - accuracy: 0.3207 - val_loss: 0.1268 - val_accuracy: 0.2770\n",
      "Epoch 582/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0711 - accuracy: 0.3177 - val_loss: 0.1367 - val_accuracy: 0.3076\n",
      "Epoch 583/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0714 - accuracy: 0.3315 - val_loss: 0.1266 - val_accuracy: 0.3004\n",
      "Epoch 584/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0710 - accuracy: 0.3195 - val_loss: 0.1267 - val_accuracy: 0.2968\n",
      "Epoch 585/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0710 - accuracy: 0.3171 - val_loss: 0.1274 - val_accuracy: 0.3022\n",
      "Epoch 586/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0708 - accuracy: 0.3201 - val_loss: 0.1265 - val_accuracy: 0.2860\n",
      "Epoch 587/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0708 - accuracy: 0.3141 - val_loss: 0.1288 - val_accuracy: 0.2878\n",
      "Epoch 588/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0714 - accuracy: 0.3165 - val_loss: 0.1255 - val_accuracy: 0.2986\n",
      "Epoch 589/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0709 - accuracy: 0.3159 - val_loss: 0.1280 - val_accuracy: 0.3094\n",
      "Epoch 590/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0707 - accuracy: 0.3117 - val_loss: 0.1255 - val_accuracy: 0.3040\n",
      "Epoch 591/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0704 - accuracy: 0.3105 - val_loss: 0.1265 - val_accuracy: 0.3147\n",
      "Epoch 592/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0706 - accuracy: 0.3267 - val_loss: 0.1348 - val_accuracy: 0.2140\n",
      "Epoch 593/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0715 - accuracy: 0.3039 - val_loss: 0.1276 - val_accuracy: 0.2914\n",
      "Epoch 594/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0704 - accuracy: 0.3231 - val_loss: 0.1274 - val_accuracy: 0.2770\n",
      "Epoch 595/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0704 - accuracy: 0.3183 - val_loss: 0.1257 - val_accuracy: 0.2842\n",
      "Epoch 596/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0703 - accuracy: 0.3207 - val_loss: 0.1265 - val_accuracy: 0.3219\n",
      "Epoch 597/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0701 - accuracy: 0.3171 - val_loss: 0.1273 - val_accuracy: 0.3022\n",
      "Epoch 598/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0703 - accuracy: 0.3243 - val_loss: 0.1279 - val_accuracy: 0.2806\n",
      "Epoch 599/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0701 - accuracy: 0.3141 - val_loss: 0.1279 - val_accuracy: 0.3040\n",
      "Epoch 600/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0699 - accuracy: 0.3195 - val_loss: 0.1261 - val_accuracy: 0.3004\n",
      "Epoch 601/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0698 - accuracy: 0.3123 - val_loss: 0.1270 - val_accuracy: 0.2932\n",
      "Epoch 602/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0700 - accuracy: 0.3111 - val_loss: 0.1313 - val_accuracy: 0.2896\n",
      "Epoch 603/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0701 - accuracy: 0.3219 - val_loss: 0.1262 - val_accuracy: 0.2986\n",
      "Epoch 604/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0696 - accuracy: 0.3153 - val_loss: 0.1273 - val_accuracy: 0.3004\n",
      "Epoch 605/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0696 - accuracy: 0.3189 - val_loss: 0.1257 - val_accuracy: 0.2860\n",
      "Epoch 606/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0695 - accuracy: 0.3189 - val_loss: 0.1292 - val_accuracy: 0.3076\n",
      "Epoch 607/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0699 - accuracy: 0.3165 - val_loss: 0.1258 - val_accuracy: 0.3183\n",
      "Epoch 608/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0696 - accuracy: 0.3183 - val_loss: 0.1300 - val_accuracy: 0.3094\n",
      "Epoch 609/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0698 - accuracy: 0.3177 - val_loss: 0.1424 - val_accuracy: 0.2320\n",
      "Epoch 610/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0706 - accuracy: 0.3075 - val_loss: 0.1250 - val_accuracy: 0.3076\n",
      "Epoch 611/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0695 - accuracy: 0.3183 - val_loss: 0.1250 - val_accuracy: 0.2734\n",
      "Epoch 612/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0692 - accuracy: 0.3183 - val_loss: 0.1257 - val_accuracy: 0.2950\n",
      "Epoch 613/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0692 - accuracy: 0.3195 - val_loss: 0.1227 - val_accuracy: 0.2896\n",
      "Epoch 614/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0692 - accuracy: 0.3219 - val_loss: 0.1276 - val_accuracy: 0.3201\n",
      "Epoch 615/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0691 - accuracy: 0.3177 - val_loss: 0.1300 - val_accuracy: 0.2950\n",
      "Epoch 616/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0691 - accuracy: 0.3105 - val_loss: 0.1253 - val_accuracy: 0.2788\n",
      "Epoch 617/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0688 - accuracy: 0.3177 - val_loss: 0.1267 - val_accuracy: 0.3058\n",
      "Epoch 618/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0688 - accuracy: 0.3147 - val_loss: 0.1251 - val_accuracy: 0.2950\n",
      "Epoch 619/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0689 - accuracy: 0.3111 - val_loss: 0.1242 - val_accuracy: 0.2896\n",
      "Epoch 620/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0686 - accuracy: 0.3249 - val_loss: 0.1255 - val_accuracy: 0.2968\n",
      "Epoch 621/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0686 - accuracy: 0.3159 - val_loss: 0.1269 - val_accuracy: 0.3094\n",
      "Epoch 622/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0687 - accuracy: 0.3141 - val_loss: 0.1268 - val_accuracy: 0.2914\n",
      "Epoch 623/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0686 - accuracy: 0.3159 - val_loss: 0.1396 - val_accuracy: 0.2140\n",
      "Epoch 624/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0695 - accuracy: 0.3153 - val_loss: 0.1252 - val_accuracy: 0.3129\n",
      "Epoch 625/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0683 - accuracy: 0.3237 - val_loss: 0.1232 - val_accuracy: 0.2914\n",
      "Epoch 626/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0684 - accuracy: 0.3189 - val_loss: 0.1308 - val_accuracy: 0.3094\n",
      "Epoch 627/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0685 - accuracy: 0.3183 - val_loss: 0.1272 - val_accuracy: 0.2752\n",
      "Epoch 628/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0684 - accuracy: 0.3231 - val_loss: 0.1242 - val_accuracy: 0.2770\n",
      "Epoch 629/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0683 - accuracy: 0.3225 - val_loss: 0.1244 - val_accuracy: 0.2842\n",
      "Epoch 630/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0682 - accuracy: 0.3135 - val_loss: 0.1272 - val_accuracy: 0.3040\n",
      "Epoch 631/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0681 - accuracy: 0.3243 - val_loss: 0.1250 - val_accuracy: 0.2770\n",
      "Epoch 632/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0681 - accuracy: 0.3195 - val_loss: 0.1240 - val_accuracy: 0.3112\n",
      "Epoch 633/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0681 - accuracy: 0.3099 - val_loss: 0.1250 - val_accuracy: 0.2950\n",
      "Epoch 634/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0680 - accuracy: 0.3225 - val_loss: 0.1482 - val_accuracy: 0.1960\n",
      "Epoch 635/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0692 - accuracy: 0.3171 - val_loss: 0.1252 - val_accuracy: 0.2968\n",
      "Epoch 636/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0681 - accuracy: 0.3153 - val_loss: 0.1297 - val_accuracy: 0.3076\n",
      "Epoch 637/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0680 - accuracy: 0.3231 - val_loss: 0.1254 - val_accuracy: 0.3022\n",
      "Epoch 638/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0676 - accuracy: 0.3213 - val_loss: 0.1288 - val_accuracy: 0.3076\n",
      "Epoch 639/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0676 - accuracy: 0.3195 - val_loss: 0.1237 - val_accuracy: 0.2986\n",
      "Epoch 640/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0676 - accuracy: 0.3195 - val_loss: 0.1362 - val_accuracy: 0.1996\n",
      "Epoch 641/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0681 - accuracy: 0.3105 - val_loss: 0.1243 - val_accuracy: 0.3022\n",
      "Epoch 642/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0674 - accuracy: 0.3159 - val_loss: 0.1251 - val_accuracy: 0.2896\n",
      "Epoch 643/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0675 - accuracy: 0.3231 - val_loss: 0.1266 - val_accuracy: 0.2914\n",
      "Epoch 644/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0673 - accuracy: 0.3153 - val_loss: 0.1272 - val_accuracy: 0.3058\n",
      "Epoch 645/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0674 - accuracy: 0.3201 - val_loss: 0.1229 - val_accuracy: 0.3112\n",
      "Epoch 646/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0671 - accuracy: 0.3165 - val_loss: 0.1250 - val_accuracy: 0.3112\n",
      "Epoch 647/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0672 - accuracy: 0.3207 - val_loss: 0.1251 - val_accuracy: 0.3040\n",
      "Epoch 648/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0671 - accuracy: 0.3207 - val_loss: 0.1244 - val_accuracy: 0.3112\n",
      "Epoch 649/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0670 - accuracy: 0.3105 - val_loss: 0.1277 - val_accuracy: 0.3273\n",
      "Epoch 650/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0673 - accuracy: 0.3297 - val_loss: 0.1247 - val_accuracy: 0.3040\n",
      "Epoch 651/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0669 - accuracy: 0.3219 - val_loss: 0.1244 - val_accuracy: 0.3076\n",
      "Epoch 652/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0667 - accuracy: 0.3267 - val_loss: 0.1231 - val_accuracy: 0.2914\n",
      "Epoch 653/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0667 - accuracy: 0.3219 - val_loss: 0.1236 - val_accuracy: 0.2860\n",
      "Epoch 654/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0668 - accuracy: 0.3135 - val_loss: 0.1366 - val_accuracy: 0.2050\n",
      "Epoch 655/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0673 - accuracy: 0.3129 - val_loss: 0.1257 - val_accuracy: 0.2986\n",
      "Epoch 656/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0671 - accuracy: 0.3213 - val_loss: 0.1250 - val_accuracy: 0.2752\n",
      "Epoch 657/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0673 - accuracy: 0.3219 - val_loss: 0.1244 - val_accuracy: 0.2860\n",
      "Epoch 658/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0666 - accuracy: 0.3105 - val_loss: 0.1286 - val_accuracy: 0.3201\n",
      "Epoch 659/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0666 - accuracy: 0.3165 - val_loss: 0.1269 - val_accuracy: 0.3183\n",
      "Epoch 660/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0665 - accuracy: 0.3189 - val_loss: 0.1246 - val_accuracy: 0.2644\n",
      "Epoch 661/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0665 - accuracy: 0.3201 - val_loss: 0.1252 - val_accuracy: 0.3004\n",
      "Epoch 662/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0663 - accuracy: 0.3231 - val_loss: 0.1249 - val_accuracy: 0.3076\n",
      "Epoch 663/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0663 - accuracy: 0.3153 - val_loss: 0.1218 - val_accuracy: 0.2914\n",
      "Epoch 664/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0664 - accuracy: 0.3153 - val_loss: 0.1337 - val_accuracy: 0.3147\n",
      "Epoch 665/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0670 - accuracy: 0.3087 - val_loss: 0.1246 - val_accuracy: 0.3147\n",
      "Epoch 666/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0662 - accuracy: 0.3261 - val_loss: 0.1223 - val_accuracy: 0.2878\n",
      "Epoch 667/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0662 - accuracy: 0.3189 - val_loss: 0.1259 - val_accuracy: 0.3022\n",
      "Epoch 668/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0660 - accuracy: 0.3153 - val_loss: 0.1272 - val_accuracy: 0.3345\n",
      "Epoch 669/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0664 - accuracy: 0.3261 - val_loss: 0.1272 - val_accuracy: 0.3040\n",
      "Epoch 670/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0660 - accuracy: 0.3159 - val_loss: 0.1276 - val_accuracy: 0.2824\n",
      "Epoch 671/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0661 - accuracy: 0.3171 - val_loss: 0.1245 - val_accuracy: 0.3040\n",
      "Epoch 672/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0656 - accuracy: 0.3147 - val_loss: 0.1231 - val_accuracy: 0.2950\n",
      "Epoch 673/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0656 - accuracy: 0.3177 - val_loss: 0.1243 - val_accuracy: 0.3417\n",
      "Epoch 674/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0657 - accuracy: 0.3261 - val_loss: 0.1237 - val_accuracy: 0.2950\n",
      "Epoch 675/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0656 - accuracy: 0.3153 - val_loss: 0.1254 - val_accuracy: 0.3112\n",
      "Epoch 676/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0657 - accuracy: 0.3255 - val_loss: 0.1304 - val_accuracy: 0.3147\n",
      "Epoch 677/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0661 - accuracy: 0.3165 - val_loss: 0.1249 - val_accuracy: 0.2932\n",
      "Epoch 678/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0658 - accuracy: 0.3153 - val_loss: 0.1280 - val_accuracy: 0.3147\n",
      "Epoch 679/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0656 - accuracy: 0.3147 - val_loss: 0.1260 - val_accuracy: 0.3147\n",
      "Epoch 680/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0653 - accuracy: 0.3195 - val_loss: 0.1235 - val_accuracy: 0.2986\n",
      "Epoch 681/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0652 - accuracy: 0.3135 - val_loss: 0.1264 - val_accuracy: 0.2932\n",
      "Epoch 682/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0655 - accuracy: 0.3117 - val_loss: 0.1223 - val_accuracy: 0.3076\n",
      "Epoch 683/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0651 - accuracy: 0.3153 - val_loss: 0.1220 - val_accuracy: 0.3417\n",
      "Epoch 684/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0652 - accuracy: 0.3153 - val_loss: 0.1229 - val_accuracy: 0.2950\n",
      "Epoch 685/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0651 - accuracy: 0.3153 - val_loss: 0.1275 - val_accuracy: 0.3183\n",
      "Epoch 686/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0653 - accuracy: 0.3147 - val_loss: 0.1258 - val_accuracy: 0.3040\n",
      "Epoch 687/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0652 - accuracy: 0.3129 - val_loss: 0.1216 - val_accuracy: 0.3058\n",
      "Epoch 688/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0648 - accuracy: 0.3135 - val_loss: 0.1267 - val_accuracy: 0.3201\n",
      "Epoch 689/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0649 - accuracy: 0.3165 - val_loss: 0.1221 - val_accuracy: 0.3094\n",
      "Epoch 690/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0647 - accuracy: 0.3165 - val_loss: 0.1256 - val_accuracy: 0.2950\n",
      "Epoch 691/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0649 - accuracy: 0.3147 - val_loss: 0.1224 - val_accuracy: 0.3022\n",
      "Epoch 692/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0647 - accuracy: 0.3195 - val_loss: 0.1253 - val_accuracy: 0.3201\n",
      "Epoch 693/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0647 - accuracy: 0.3213 - val_loss: 0.1240 - val_accuracy: 0.3183\n",
      "Epoch 694/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0648 - accuracy: 0.3219 - val_loss: 0.1266 - val_accuracy: 0.3076\n",
      "Epoch 695/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0648 - accuracy: 0.3135 - val_loss: 0.1304 - val_accuracy: 0.2914\n",
      "Epoch 696/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0651 - accuracy: 0.3195 - val_loss: 0.1253 - val_accuracy: 0.2266\n",
      "Epoch 697/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0652 - accuracy: 0.3111 - val_loss: 0.1232 - val_accuracy: 0.2842\n",
      "Epoch 698/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0647 - accuracy: 0.3069 - val_loss: 0.1265 - val_accuracy: 0.3094\n",
      "Epoch 699/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0646 - accuracy: 0.3135 - val_loss: 0.1224 - val_accuracy: 0.3112\n",
      "Epoch 700/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0643 - accuracy: 0.3135 - val_loss: 0.1235 - val_accuracy: 0.2986\n",
      "Epoch 701/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0645 - accuracy: 0.3249 - val_loss: 0.1286 - val_accuracy: 0.2950\n",
      "Epoch 702/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0646 - accuracy: 0.3231 - val_loss: 0.1254 - val_accuracy: 0.3112\n",
      "Epoch 703/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0645 - accuracy: 0.3165 - val_loss: 0.1316 - val_accuracy: 0.3022\n",
      "Epoch 704/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0647 - accuracy: 0.3123 - val_loss: 0.1285 - val_accuracy: 0.2644\n",
      "Epoch 705/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0646 - accuracy: 0.3189 - val_loss: 0.1225 - val_accuracy: 0.3094\n",
      "Epoch 706/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0639 - accuracy: 0.3189 - val_loss: 0.1244 - val_accuracy: 0.3147\n",
      "Epoch 707/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0641 - accuracy: 0.3183 - val_loss: 0.1230 - val_accuracy: 0.3040\n",
      "Epoch 708/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0642 - accuracy: 0.3213 - val_loss: 0.1238 - val_accuracy: 0.2968\n",
      "Epoch 709/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0641 - accuracy: 0.3105 - val_loss: 0.1220 - val_accuracy: 0.3040\n",
      "Epoch 710/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0638 - accuracy: 0.3141 - val_loss: 0.1246 - val_accuracy: 0.3147\n",
      "Epoch 711/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0639 - accuracy: 0.3213 - val_loss: 0.1495 - val_accuracy: 0.2608\n",
      "Epoch 712/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0649 - accuracy: 0.3165 - val_loss: 0.1233 - val_accuracy: 0.3165\n",
      "Epoch 713/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0635 - accuracy: 0.3177 - val_loss: 0.1270 - val_accuracy: 0.3058\n",
      "Epoch 714/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0638 - accuracy: 0.3111 - val_loss: 0.1230 - val_accuracy: 0.3040\n",
      "Epoch 715/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0637 - accuracy: 0.3045 - val_loss: 0.1273 - val_accuracy: 0.2968\n",
      "Epoch 716/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0639 - accuracy: 0.3105 - val_loss: 0.1259 - val_accuracy: 0.2914\n",
      "Epoch 717/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0636 - accuracy: 0.3135 - val_loss: 0.1218 - val_accuracy: 0.3022\n",
      "Epoch 718/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0635 - accuracy: 0.3171 - val_loss: 0.1257 - val_accuracy: 0.2770\n",
      "Epoch 719/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0638 - accuracy: 0.3141 - val_loss: 0.1229 - val_accuracy: 0.2932\n",
      "Epoch 720/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0633 - accuracy: 0.3171 - val_loss: 0.1239 - val_accuracy: 0.3147\n",
      "Epoch 721/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0635 - accuracy: 0.3207 - val_loss: 0.1221 - val_accuracy: 0.3112\n",
      "Epoch 722/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0631 - accuracy: 0.3129 - val_loss: 0.1283 - val_accuracy: 0.3112\n",
      "Epoch 723/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0636 - accuracy: 0.3183 - val_loss: 0.1201 - val_accuracy: 0.2860\n",
      "Epoch 724/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0632 - accuracy: 0.3213 - val_loss: 0.1357 - val_accuracy: 0.2554\n",
      "Epoch 725/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0644 - accuracy: 0.3177 - val_loss: 0.1260 - val_accuracy: 0.3219\n",
      "Epoch 726/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0631 - accuracy: 0.3225 - val_loss: 0.1278 - val_accuracy: 0.3094\n",
      "Epoch 727/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0632 - accuracy: 0.3255 - val_loss: 0.1264 - val_accuracy: 0.3417\n",
      "Epoch 728/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0632 - accuracy: 0.3201 - val_loss: 0.1215 - val_accuracy: 0.3058\n",
      "Epoch 729/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0629 - accuracy: 0.3207 - val_loss: 0.1220 - val_accuracy: 0.3022\n",
      "Epoch 730/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0629 - accuracy: 0.3165 - val_loss: 0.1316 - val_accuracy: 0.3147\n",
      "Epoch 731/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0635 - accuracy: 0.3183 - val_loss: 0.1250 - val_accuracy: 0.2968\n",
      "Epoch 732/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0628 - accuracy: 0.3159 - val_loss: 0.1236 - val_accuracy: 0.3183\n",
      "Epoch 733/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0628 - accuracy: 0.3231 - val_loss: 0.1221 - val_accuracy: 0.3058\n",
      "Epoch 734/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0628 - accuracy: 0.3141 - val_loss: 0.1256 - val_accuracy: 0.2986\n",
      "Epoch 735/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0628 - accuracy: 0.3135 - val_loss: 0.1226 - val_accuracy: 0.3147\n",
      "Epoch 736/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0625 - accuracy: 0.3135 - val_loss: 0.1230 - val_accuracy: 0.3255\n",
      "Epoch 737/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0624 - accuracy: 0.3225 - val_loss: 0.1472 - val_accuracy: 0.3507\n",
      "Epoch 738/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0640 - accuracy: 0.3111 - val_loss: 0.1207 - val_accuracy: 0.3094\n",
      "Epoch 739/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0623 - accuracy: 0.3141 - val_loss: 0.1229 - val_accuracy: 0.3094\n",
      "Epoch 740/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0624 - accuracy: 0.3099 - val_loss: 0.1223 - val_accuracy: 0.3022\n",
      "Epoch 741/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0623 - accuracy: 0.3183 - val_loss: 0.1224 - val_accuracy: 0.3094\n",
      "Epoch 742/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0624 - accuracy: 0.3129 - val_loss: 0.1207 - val_accuracy: 0.3129\n",
      "Epoch 743/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0621 - accuracy: 0.3087 - val_loss: 0.1233 - val_accuracy: 0.3076\n",
      "Epoch 744/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0622 - accuracy: 0.3165 - val_loss: 0.1241 - val_accuracy: 0.3058\n",
      "Epoch 745/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0624 - accuracy: 0.3099 - val_loss: 0.1222 - val_accuracy: 0.3076\n",
      "Epoch 746/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0620 - accuracy: 0.3141 - val_loss: 0.1218 - val_accuracy: 0.3147\n",
      "Epoch 747/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0621 - accuracy: 0.3129 - val_loss: 0.1221 - val_accuracy: 0.2950\n",
      "Epoch 748/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0619 - accuracy: 0.3147 - val_loss: 0.1260 - val_accuracy: 0.3112\n",
      "Epoch 749/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0621 - accuracy: 0.3099 - val_loss: 0.1228 - val_accuracy: 0.3219\n",
      "Epoch 750/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0619 - accuracy: 0.3201 - val_loss: 0.1237 - val_accuracy: 0.3165\n",
      "Epoch 751/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0620 - accuracy: 0.3177 - val_loss: 0.1224 - val_accuracy: 0.3112\n",
      "Epoch 752/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0617 - accuracy: 0.3207 - val_loss: 0.1236 - val_accuracy: 0.3094\n",
      "Epoch 753/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0619 - accuracy: 0.3129 - val_loss: 0.1208 - val_accuracy: 0.3129\n",
      "Epoch 754/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0616 - accuracy: 0.3129 - val_loss: 0.1243 - val_accuracy: 0.3040\n",
      "Epoch 755/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0618 - accuracy: 0.3135 - val_loss: 0.1239 - val_accuracy: 0.2896\n",
      "Epoch 756/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0620 - accuracy: 0.3087 - val_loss: 0.1267 - val_accuracy: 0.2896\n",
      "Epoch 757/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0625 - accuracy: 0.3123 - val_loss: 0.1208 - val_accuracy: 0.3165\n",
      "Epoch 758/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0615 - accuracy: 0.3129 - val_loss: 0.1261 - val_accuracy: 0.3219\n",
      "Epoch 759/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0614 - accuracy: 0.3159 - val_loss: 0.1259 - val_accuracy: 0.3237\n",
      "Epoch 760/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0615 - accuracy: 0.3135 - val_loss: 0.1216 - val_accuracy: 0.3129\n",
      "Epoch 761/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0613 - accuracy: 0.3189 - val_loss: 0.1301 - val_accuracy: 0.3435\n",
      "Epoch 762/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0616 - accuracy: 0.3117 - val_loss: 0.1236 - val_accuracy: 0.3022\n",
      "Epoch 763/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0614 - accuracy: 0.3075 - val_loss: 0.1231 - val_accuracy: 0.3255\n",
      "Epoch 764/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0613 - accuracy: 0.3171 - val_loss: 0.1229 - val_accuracy: 0.3219\n",
      "Epoch 765/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0612 - accuracy: 0.3153 - val_loss: 0.1289 - val_accuracy: 0.3129\n",
      "Epoch 766/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0614 - accuracy: 0.3183 - val_loss: 0.1203 - val_accuracy: 0.3183\n",
      "Epoch 767/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0611 - accuracy: 0.3117 - val_loss: 0.1223 - val_accuracy: 0.2968\n",
      "Epoch 768/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0613 - accuracy: 0.3141 - val_loss: 0.1218 - val_accuracy: 0.3040\n",
      "Epoch 769/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0613 - accuracy: 0.3063 - val_loss: 0.1253 - val_accuracy: 0.3363\n",
      "Epoch 770/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0610 - accuracy: 0.3171 - val_loss: 0.1212 - val_accuracy: 0.3112\n",
      "Epoch 771/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0608 - accuracy: 0.3135 - val_loss: 0.1209 - val_accuracy: 0.2950\n",
      "Epoch 772/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0609 - accuracy: 0.3027 - val_loss: 0.1220 - val_accuracy: 0.3183\n",
      "Epoch 773/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0609 - accuracy: 0.3063 - val_loss: 0.1226 - val_accuracy: 0.2986\n",
      "Epoch 774/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0610 - accuracy: 0.3111 - val_loss: 0.1221 - val_accuracy: 0.3183\n",
      "Epoch 775/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0608 - accuracy: 0.3147 - val_loss: 0.1280 - val_accuracy: 0.3237\n",
      "Epoch 776/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0612 - accuracy: 0.3225 - val_loss: 0.1218 - val_accuracy: 0.3004\n",
      "Epoch 777/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0606 - accuracy: 0.3153 - val_loss: 0.1221 - val_accuracy: 0.3183\n",
      "Epoch 778/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0608 - accuracy: 0.3129 - val_loss: 0.1217 - val_accuracy: 0.3022\n",
      "Epoch 779/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0604 - accuracy: 0.3165 - val_loss: 0.1215 - val_accuracy: 0.3165\n",
      "Epoch 780/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0606 - accuracy: 0.3069 - val_loss: 0.1203 - val_accuracy: 0.2824\n",
      "Epoch 781/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0606 - accuracy: 0.3117 - val_loss: 0.1215 - val_accuracy: 0.3058\n",
      "Epoch 782/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0605 - accuracy: 0.3177 - val_loss: 0.1228 - val_accuracy: 0.3022\n",
      "Epoch 783/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0605 - accuracy: 0.3075 - val_loss: 0.1272 - val_accuracy: 0.2878\n",
      "Epoch 784/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0606 - accuracy: 0.3165 - val_loss: 0.1236 - val_accuracy: 0.3183\n",
      "Epoch 785/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0601 - accuracy: 0.3135 - val_loss: 0.1294 - val_accuracy: 0.3129\n",
      "Epoch 786/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0607 - accuracy: 0.3165 - val_loss: 0.1221 - val_accuracy: 0.3147\n",
      "Epoch 787/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0601 - accuracy: 0.3087 - val_loss: 0.1217 - val_accuracy: 0.2716\n",
      "Epoch 788/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0603 - accuracy: 0.3081 - val_loss: 0.1203 - val_accuracy: 0.3022\n",
      "Epoch 789/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0601 - accuracy: 0.3111 - val_loss: 0.1217 - val_accuracy: 0.3201\n",
      "Epoch 790/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0601 - accuracy: 0.3105 - val_loss: 0.1238 - val_accuracy: 0.3076\n",
      "Epoch 791/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0601 - accuracy: 0.3135 - val_loss: 0.1262 - val_accuracy: 0.3094\n",
      "Epoch 792/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0605 - accuracy: 0.3027 - val_loss: 0.1224 - val_accuracy: 0.3022\n",
      "Epoch 793/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0602 - accuracy: 0.3009 - val_loss: 0.1223 - val_accuracy: 0.3255\n",
      "Epoch 794/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0598 - accuracy: 0.3165 - val_loss: 0.1225 - val_accuracy: 0.3165\n",
      "Epoch 795/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0597 - accuracy: 0.3123 - val_loss: 0.1215 - val_accuracy: 0.3219\n",
      "Epoch 796/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0596 - accuracy: 0.3141 - val_loss: 0.1271 - val_accuracy: 0.3004\n",
      "Epoch 797/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0600 - accuracy: 0.3141 - val_loss: 0.1278 - val_accuracy: 0.2788\n",
      "Epoch 798/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0606 - accuracy: 0.3153 - val_loss: 0.1191 - val_accuracy: 0.3094\n",
      "Epoch 799/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0596 - accuracy: 0.3117 - val_loss: 0.1504 - val_accuracy: 0.2770\n",
      "Epoch 800/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0610 - accuracy: 0.3045 - val_loss: 0.1220 - val_accuracy: 0.3255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-5f8a8e11cd4a>:128: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "  tprs.append(interp(mean_fpr, fpr, tpr))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Epoch 1/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.6415 - accuracy: 0.0348 - val_loss: 0.5646 - val_accuracy: 0.0558\n",
      "Epoch 2/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.3396 - accuracy: 0.0408 - val_loss: 0.2600 - val_accuracy: 0.0558\n",
      "Epoch 3/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1913 - accuracy: 0.0348 - val_loss: 0.2525 - val_accuracy: 0.0126\n",
      "Epoch 4/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1874 - accuracy: 0.0192 - val_loss: 0.2514 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1867 - accuracy: 0.0126 - val_loss: 0.2506 - val_accuracy: 0.0234\n",
      "Epoch 6/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1865 - accuracy: 0.0144 - val_loss: 0.2506 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1863 - accuracy: 0.0156 - val_loss: 0.2500 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1863 - accuracy: 0.0210 - val_loss: 0.2493 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1861 - accuracy: 0.0228 - val_loss: 0.2496 - val_accuracy: 0.0342\n",
      "Epoch 10/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1861 - accuracy: 0.0270 - val_loss: 0.2497 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1859 - accuracy: 0.0216 - val_loss: 0.2495 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1859 - accuracy: 0.0192 - val_loss: 0.2497 - val_accuracy: 0.0450\n",
      "Epoch 13/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1858 - accuracy: 0.0240 - val_loss: 0.2493 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1858 - accuracy: 0.0210 - val_loss: 0.2490 - val_accuracy: 0.0576\n",
      "Epoch 15/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1857 - accuracy: 0.0450 - val_loss: 0.2483 - val_accuracy: 0.0324\n",
      "Epoch 16/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1856 - accuracy: 0.0258 - val_loss: 0.2484 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1856 - accuracy: 0.0264 - val_loss: 0.2490 - val_accuracy: 0.0594\n",
      "Epoch 18/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1855 - accuracy: 0.0456 - val_loss: 0.2495 - val_accuracy: 0.0342\n",
      "Epoch 19/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1854 - accuracy: 0.0625 - val_loss: 0.2483 - val_accuracy: 0.0288\n",
      "Epoch 20/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1854 - accuracy: 0.0432 - val_loss: 0.2488 - val_accuracy: 0.0306\n",
      "Epoch 21/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1853 - accuracy: 0.0312 - val_loss: 0.2483 - val_accuracy: 0.0414\n",
      "Epoch 22/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1852 - accuracy: 0.0529 - val_loss: 0.2483 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1852 - accuracy: 0.0156 - val_loss: 0.2484 - val_accuracy: 0.0594\n",
      "Epoch 24/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1851 - accuracy: 0.0372 - val_loss: 0.2479 - val_accuracy: 0.0432\n",
      "Epoch 25/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1850 - accuracy: 0.0486 - val_loss: 0.2475 - val_accuracy: 0.0324\n",
      "Epoch 26/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1850 - accuracy: 0.0360 - val_loss: 0.2470 - val_accuracy: 0.0288\n",
      "Epoch 27/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1849 - accuracy: 0.0372 - val_loss: 0.2474 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1848 - accuracy: 0.0192 - val_loss: 0.2476 - val_accuracy: 0.0576\n",
      "Epoch 29/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1848 - accuracy: 0.0577 - val_loss: 0.2476 - val_accuracy: 0.0360\n",
      "Epoch 30/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1847 - accuracy: 0.0492 - val_loss: 0.2468 - val_accuracy: 0.0018\n",
      "Epoch 31/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1846 - accuracy: 0.0354 - val_loss: 0.2466 - val_accuracy: 0.0306\n",
      "Epoch 32/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1845 - accuracy: 0.0414 - val_loss: 0.2465 - val_accuracy: 0.0072\n",
      "Epoch 33/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1845 - accuracy: 0.0414 - val_loss: 0.2473 - val_accuracy: 0.0306\n",
      "Epoch 34/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1844 - accuracy: 0.0432 - val_loss: 0.2465 - val_accuracy: 0.0252\n",
      "Epoch 35/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1843 - accuracy: 0.0474 - val_loss: 0.2461 - val_accuracy: 0.0522\n",
      "Epoch 36/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1843 - accuracy: 0.0577 - val_loss: 0.2468 - val_accuracy: 0.0432\n",
      "Epoch 37/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1842 - accuracy: 0.0577 - val_loss: 0.2461 - val_accuracy: 0.0504\n",
      "Epoch 38/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1841 - accuracy: 0.0390 - val_loss: 0.2462 - val_accuracy: 0.0360\n",
      "Epoch 39/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1840 - accuracy: 0.0565 - val_loss: 0.2459 - val_accuracy: 0.0450\n",
      "Epoch 40/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1839 - accuracy: 0.0456 - val_loss: 0.2452 - val_accuracy: 0.0054\n",
      "Epoch 41/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1839 - accuracy: 0.0444 - val_loss: 0.2459 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1838 - accuracy: 0.0210 - val_loss: 0.2463 - val_accuracy: 0.0342\n",
      "Epoch 43/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1837 - accuracy: 0.0541 - val_loss: 0.2464 - val_accuracy: 0.0396\n",
      "Epoch 44/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1836 - accuracy: 0.0511 - val_loss: 0.2456 - val_accuracy: 0.0594\n",
      "Epoch 45/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1835 - accuracy: 0.0667 - val_loss: 0.2456 - val_accuracy: 0.0468\n",
      "Epoch 46/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1835 - accuracy: 0.0691 - val_loss: 0.2453 - val_accuracy: 0.0360\n",
      "Epoch 47/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1834 - accuracy: 0.0607 - val_loss: 0.2455 - val_accuracy: 0.0504\n",
      "Epoch 48/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1833 - accuracy: 0.0547 - val_loss: 0.2454 - val_accuracy: 0.0971\n",
      "Epoch 49/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1831 - accuracy: 0.0721 - val_loss: 0.2449 - val_accuracy: 0.0432\n",
      "Epoch 50/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1830 - accuracy: 0.0625 - val_loss: 0.2449 - val_accuracy: 0.0576\n",
      "Epoch 51/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1830 - accuracy: 0.0625 - val_loss: 0.2443 - val_accuracy: 0.0432\n",
      "Epoch 52/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1828 - accuracy: 0.0679 - val_loss: 0.2445 - val_accuracy: 0.0845\n",
      "Epoch 53/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1828 - accuracy: 0.0901 - val_loss: 0.2444 - val_accuracy: 0.0486\n",
      "Epoch 54/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1826 - accuracy: 0.0649 - val_loss: 0.2443 - val_accuracy: 0.0647\n",
      "Epoch 55/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1825 - accuracy: 0.0913 - val_loss: 0.2438 - val_accuracy: 0.0629\n",
      "Epoch 56/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1824 - accuracy: 0.0517 - val_loss: 0.2437 - val_accuracy: 0.0540\n",
      "Epoch 57/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1823 - accuracy: 0.0673 - val_loss: 0.2436 - val_accuracy: 0.1151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1822 - accuracy: 0.0961 - val_loss: 0.2430 - val_accuracy: 0.0827\n",
      "Epoch 59/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1821 - accuracy: 0.0955 - val_loss: 0.2425 - val_accuracy: 0.0827\n",
      "Epoch 60/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1819 - accuracy: 0.0925 - val_loss: 0.2429 - val_accuracy: 0.0036\n",
      "Epoch 61/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1818 - accuracy: 0.0312 - val_loss: 0.2422 - val_accuracy: 0.0234\n",
      "Epoch 62/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1816 - accuracy: 0.0631 - val_loss: 0.2420 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1815 - accuracy: 0.0619 - val_loss: 0.2420 - val_accuracy: 0.0791\n",
      "Epoch 64/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1813 - accuracy: 0.0739 - val_loss: 0.2418 - val_accuracy: 0.0576\n",
      "Epoch 65/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1811 - accuracy: 0.0775 - val_loss: 0.2419 - val_accuracy: 0.0773\n",
      "Epoch 66/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1810 - accuracy: 0.0775 - val_loss: 0.2420 - val_accuracy: 0.1205\n",
      "Epoch 67/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1808 - accuracy: 0.1015 - val_loss: 0.2412 - val_accuracy: 0.1223\n",
      "Epoch 68/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1807 - accuracy: 0.0985 - val_loss: 0.2413 - val_accuracy: 0.1061\n",
      "Epoch 69/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1805 - accuracy: 0.1123 - val_loss: 0.2402 - val_accuracy: 0.0935\n",
      "Epoch 70/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1803 - accuracy: 0.0967 - val_loss: 0.2403 - val_accuracy: 0.0809\n",
      "Epoch 71/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1801 - accuracy: 0.0973 - val_loss: 0.2398 - val_accuracy: 0.1079\n",
      "Epoch 72/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1799 - accuracy: 0.0805 - val_loss: 0.2402 - val_accuracy: 0.0378\n",
      "Epoch 73/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1798 - accuracy: 0.0823 - val_loss: 0.2396 - val_accuracy: 0.1223\n",
      "Epoch 74/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1795 - accuracy: 0.0979 - val_loss: 0.2393 - val_accuracy: 0.1241\n",
      "Epoch 75/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1792 - accuracy: 0.1141 - val_loss: 0.2389 - val_accuracy: 0.0935\n",
      "Epoch 76/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1791 - accuracy: 0.1147 - val_loss: 0.2380 - val_accuracy: 0.0917\n",
      "Epoch 77/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1788 - accuracy: 0.1129 - val_loss: 0.2381 - val_accuracy: 0.0827\n",
      "Epoch 78/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1786 - accuracy: 0.1063 - val_loss: 0.2379 - val_accuracy: 0.1871\n",
      "Epoch 79/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1783 - accuracy: 0.1339 - val_loss: 0.2373 - val_accuracy: 0.1295\n",
      "Epoch 80/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1780 - accuracy: 0.1099 - val_loss: 0.2371 - val_accuracy: 0.1133\n",
      "Epoch 81/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1777 - accuracy: 0.1033 - val_loss: 0.2365 - val_accuracy: 0.0773\n",
      "Epoch 82/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1774 - accuracy: 0.0949 - val_loss: 0.2351 - val_accuracy: 0.1241\n",
      "Epoch 83/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1772 - accuracy: 0.1237 - val_loss: 0.2358 - val_accuracy: 0.0935\n",
      "Epoch 84/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1768 - accuracy: 0.1111 - val_loss: 0.2349 - val_accuracy: 0.0899\n",
      "Epoch 85/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1765 - accuracy: 0.1207 - val_loss: 0.2350 - val_accuracy: 0.1367\n",
      "Epoch 86/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1761 - accuracy: 0.1081 - val_loss: 0.2341 - val_accuracy: 0.1169\n",
      "Epoch 87/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1758 - accuracy: 0.1201 - val_loss: 0.2333 - val_accuracy: 0.1295\n",
      "Epoch 88/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1755 - accuracy: 0.1255 - val_loss: 0.2327 - val_accuracy: 0.1205\n",
      "Epoch 89/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1751 - accuracy: 0.1309 - val_loss: 0.2318 - val_accuracy: 0.1349\n",
      "Epoch 90/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1747 - accuracy: 0.1243 - val_loss: 0.2314 - val_accuracy: 0.1187\n",
      "Epoch 91/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1743 - accuracy: 0.1057 - val_loss: 0.2308 - val_accuracy: 0.1205\n",
      "Epoch 92/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1739 - accuracy: 0.1405 - val_loss: 0.2305 - val_accuracy: 0.0989\n",
      "Epoch 93/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1735 - accuracy: 0.1009 - val_loss: 0.2294 - val_accuracy: 0.1241\n",
      "Epoch 94/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1730 - accuracy: 0.1411 - val_loss: 0.2292 - val_accuracy: 0.1313\n",
      "Epoch 95/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1726 - accuracy: 0.1369 - val_loss: 0.2284 - val_accuracy: 0.1331\n",
      "Epoch 96/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1722 - accuracy: 0.1309 - val_loss: 0.2280 - val_accuracy: 0.1133\n",
      "Epoch 97/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1717 - accuracy: 0.1279 - val_loss: 0.2274 - val_accuracy: 0.1547\n",
      "Epoch 98/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1712 - accuracy: 0.1345 - val_loss: 0.2256 - val_accuracy: 0.1403\n",
      "Epoch 99/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1707 - accuracy: 0.1405 - val_loss: 0.2262 - val_accuracy: 0.0827\n",
      "Epoch 100/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1704 - accuracy: 0.1357 - val_loss: 0.2255 - val_accuracy: 0.1565\n",
      "Epoch 101/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1697 - accuracy: 0.1520 - val_loss: 0.2240 - val_accuracy: 0.1403\n",
      "Epoch 102/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1692 - accuracy: 0.1526 - val_loss: 0.2235 - val_accuracy: 0.1277\n",
      "Epoch 103/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1686 - accuracy: 0.1453 - val_loss: 0.2221 - val_accuracy: 0.1871\n",
      "Epoch 104/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1680 - accuracy: 0.1580 - val_loss: 0.2219 - val_accuracy: 0.1835\n",
      "Epoch 105/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1674 - accuracy: 0.1465 - val_loss: 0.2214 - val_accuracy: 0.1924\n",
      "Epoch 106/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1669 - accuracy: 0.1586 - val_loss: 0.2201 - val_accuracy: 0.2032\n",
      "Epoch 107/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1664 - accuracy: 0.1640 - val_loss: 0.2194 - val_accuracy: 0.1619\n",
      "Epoch 108/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1657 - accuracy: 0.1604 - val_loss: 0.2187 - val_accuracy: 0.2068\n",
      "Epoch 109/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1651 - accuracy: 0.1742 - val_loss: 0.2171 - val_accuracy: 0.1978\n",
      "Epoch 110/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1645 - accuracy: 0.1616 - val_loss: 0.2166 - val_accuracy: 0.1475\n",
      "Epoch 111/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1639 - accuracy: 0.1688 - val_loss: 0.2156 - val_accuracy: 0.1978\n",
      "Epoch 112/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1632 - accuracy: 0.1802 - val_loss: 0.2158 - val_accuracy: 0.1547\n",
      "Epoch 113/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1627 - accuracy: 0.1628 - val_loss: 0.2139 - val_accuracy: 0.2284\n",
      "Epoch 114/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1620 - accuracy: 0.1622 - val_loss: 0.2130 - val_accuracy: 0.2194\n",
      "Epoch 115/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1613 - accuracy: 0.1856 - val_loss: 0.2135 - val_accuracy: 0.1996\n",
      "Epoch 116/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1607 - accuracy: 0.1784 - val_loss: 0.2108 - val_accuracy: 0.2050\n",
      "Epoch 117/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1600 - accuracy: 0.1706 - val_loss: 0.2106 - val_accuracy: 0.1888\n",
      "Epoch 118/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1593 - accuracy: 0.1742 - val_loss: 0.2090 - val_accuracy: 0.2248\n",
      "Epoch 119/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1586 - accuracy: 0.1844 - val_loss: 0.2088 - val_accuracy: 0.1691\n",
      "Epoch 120/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1580 - accuracy: 0.1778 - val_loss: 0.2069 - val_accuracy: 0.1960\n",
      "Epoch 121/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1572 - accuracy: 0.1730 - val_loss: 0.2066 - val_accuracy: 0.1655\n",
      "Epoch 122/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1567 - accuracy: 0.1778 - val_loss: 0.2054 - val_accuracy: 0.1691\n",
      "Epoch 123/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1560 - accuracy: 0.1724 - val_loss: 0.2046 - val_accuracy: 0.1655\n",
      "Epoch 124/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1554 - accuracy: 0.1718 - val_loss: 0.2041 - val_accuracy: 0.1906\n",
      "Epoch 125/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1548 - accuracy: 0.1904 - val_loss: 0.2040 - val_accuracy: 0.1529\n",
      "Epoch 126/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1542 - accuracy: 0.1676 - val_loss: 0.2025 - val_accuracy: 0.1996\n",
      "Epoch 127/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1534 - accuracy: 0.1910 - val_loss: 0.2012 - val_accuracy: 0.1745\n",
      "Epoch 128/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1528 - accuracy: 0.1742 - val_loss: 0.2006 - val_accuracy: 0.2248\n",
      "Epoch 129/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1523 - accuracy: 0.1964 - val_loss: 0.2003 - val_accuracy: 0.1978\n",
      "Epoch 130/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1516 - accuracy: 0.1940 - val_loss: 0.1992 - val_accuracy: 0.1655\n",
      "Epoch 131/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1510 - accuracy: 0.1820 - val_loss: 0.1978 - val_accuracy: 0.1835\n",
      "Epoch 132/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1504 - accuracy: 0.1826 - val_loss: 0.1971 - val_accuracy: 0.1853\n",
      "Epoch 133/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1497 - accuracy: 0.1874 - val_loss: 0.1966 - val_accuracy: 0.2122\n",
      "Epoch 134/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1492 - accuracy: 0.1958 - val_loss: 0.1961 - val_accuracy: 0.2104\n",
      "Epoch 135/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1485 - accuracy: 0.1922 - val_loss: 0.1990 - val_accuracy: 0.2014\n",
      "Epoch 136/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1482 - accuracy: 0.1952 - val_loss: 0.1939 - val_accuracy: 0.1871\n",
      "Epoch 137/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1474 - accuracy: 0.1958 - val_loss: 0.1931 - val_accuracy: 0.2230\n",
      "Epoch 138/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1469 - accuracy: 0.2000 - val_loss: 0.1915 - val_accuracy: 0.1637\n",
      "Epoch 139/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1463 - accuracy: 0.1940 - val_loss: 0.1907 - val_accuracy: 0.1781\n",
      "Epoch 140/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1458 - accuracy: 0.1970 - val_loss: 0.1910 - val_accuracy: 0.2140\n",
      "Epoch 141/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1452 - accuracy: 0.1952 - val_loss: 0.1894 - val_accuracy: 0.2230\n",
      "Epoch 142/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1448 - accuracy: 0.1976 - val_loss: 0.1885 - val_accuracy: 0.2140\n",
      "Epoch 143/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1442 - accuracy: 0.2060 - val_loss: 0.1880 - val_accuracy: 0.1942\n",
      "Epoch 144/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1437 - accuracy: 0.1934 - val_loss: 0.1871 - val_accuracy: 0.1996\n",
      "Epoch 145/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1433 - accuracy: 0.2000 - val_loss: 0.1870 - val_accuracy: 0.2086\n",
      "Epoch 146/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1427 - accuracy: 0.2060 - val_loss: 0.1862 - val_accuracy: 0.1781\n",
      "Epoch 147/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1422 - accuracy: 0.1910 - val_loss: 0.1857 - val_accuracy: 0.2086\n",
      "Epoch 148/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1417 - accuracy: 0.1934 - val_loss: 0.1850 - val_accuracy: 0.1996\n",
      "Epoch 149/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1413 - accuracy: 0.2054 - val_loss: 0.1841 - val_accuracy: 0.1763\n",
      "Epoch 150/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1408 - accuracy: 0.2060 - val_loss: 0.1839 - val_accuracy: 0.1924\n",
      "Epoch 151/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1403 - accuracy: 0.2072 - val_loss: 0.1843 - val_accuracy: 0.2248\n",
      "Epoch 152/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1399 - accuracy: 0.2066 - val_loss: 0.1829 - val_accuracy: 0.2248\n",
      "Epoch 153/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1394 - accuracy: 0.2096 - val_loss: 0.1837 - val_accuracy: 0.1691\n",
      "Epoch 154/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1390 - accuracy: 0.1982 - val_loss: 0.1816 - val_accuracy: 0.2176\n",
      "Epoch 155/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1385 - accuracy: 0.2114 - val_loss: 0.1819 - val_accuracy: 0.2086\n",
      "Epoch 156/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1382 - accuracy: 0.2042 - val_loss: 0.1801 - val_accuracy: 0.2104\n",
      "Epoch 157/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1377 - accuracy: 0.2090 - val_loss: 0.1819 - val_accuracy: 0.2212\n",
      "Epoch 158/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1373 - accuracy: 0.2072 - val_loss: 0.1805 - val_accuracy: 0.2086\n",
      "Epoch 159/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1368 - accuracy: 0.2090 - val_loss: 0.1798 - val_accuracy: 0.2032\n",
      "Epoch 160/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1366 - accuracy: 0.1958 - val_loss: 0.1800 - val_accuracy: 0.2122\n",
      "Epoch 161/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1361 - accuracy: 0.2186 - val_loss: 0.1796 - val_accuracy: 0.2032\n",
      "Epoch 162/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1358 - accuracy: 0.2126 - val_loss: 0.1787 - val_accuracy: 0.2302\n",
      "Epoch 163/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1353 - accuracy: 0.2078 - val_loss: 0.1821 - val_accuracy: 0.1978\n",
      "Epoch 164/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1353 - accuracy: 0.2078 - val_loss: 0.1776 - val_accuracy: 0.2500\n",
      "Epoch 165/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1346 - accuracy: 0.2144 - val_loss: 0.1769 - val_accuracy: 0.2230\n",
      "Epoch 166/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1342 - accuracy: 0.2108 - val_loss: 0.1756 - val_accuracy: 0.2194\n",
      "Epoch 167/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1338 - accuracy: 0.2096 - val_loss: 0.1756 - val_accuracy: 0.2356\n",
      "Epoch 168/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1333 - accuracy: 0.2258 - val_loss: 0.1792 - val_accuracy: 0.1942\n",
      "Epoch 169/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1333 - accuracy: 0.2066 - val_loss: 0.1765 - val_accuracy: 0.2644\n",
      "Epoch 170/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1328 - accuracy: 0.2270 - val_loss: 0.1734 - val_accuracy: 0.2176\n",
      "Epoch 171/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1324 - accuracy: 0.2168 - val_loss: 0.1747 - val_accuracy: 0.2158\n",
      "Epoch 172/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1320 - accuracy: 0.2198 - val_loss: 0.1765 - val_accuracy: 0.2266\n",
      "Epoch 173/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1317 - accuracy: 0.2252 - val_loss: 0.1735 - val_accuracy: 0.2608\n",
      "Epoch 174/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1312 - accuracy: 0.2234 - val_loss: 0.1766 - val_accuracy: 0.2464\n",
      "Epoch 175/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1310 - accuracy: 0.2222 - val_loss: 0.1724 - val_accuracy: 0.2320\n",
      "Epoch 176/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1304 - accuracy: 0.2300 - val_loss: 0.1719 - val_accuracy: 0.2284\n",
      "Epoch 177/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1301 - accuracy: 0.2168 - val_loss: 0.1711 - val_accuracy: 0.2554\n",
      "Epoch 178/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1297 - accuracy: 0.2360 - val_loss: 0.1708 - val_accuracy: 0.2428\n",
      "Epoch 179/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1294 - accuracy: 0.2294 - val_loss: 0.1715 - val_accuracy: 0.2104\n",
      "Epoch 180/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1291 - accuracy: 0.2228 - val_loss: 0.1713 - val_accuracy: 0.2338\n",
      "Epoch 181/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1287 - accuracy: 0.2300 - val_loss: 0.1702 - val_accuracy: 0.2590\n",
      "Epoch 182/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1282 - accuracy: 0.2288 - val_loss: 0.1701 - val_accuracy: 0.2824\n",
      "Epoch 183/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1280 - accuracy: 0.2354 - val_loss: 0.1704 - val_accuracy: 0.2356\n",
      "Epoch 184/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1276 - accuracy: 0.2468 - val_loss: 0.1685 - val_accuracy: 0.2374\n",
      "Epoch 185/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1271 - accuracy: 0.2462 - val_loss: 0.1681 - val_accuracy: 0.2572\n",
      "Epoch 186/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1267 - accuracy: 0.2360 - val_loss: 0.1684 - val_accuracy: 0.2572\n",
      "Epoch 187/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1264 - accuracy: 0.2480 - val_loss: 0.1702 - val_accuracy: 0.2284\n",
      "Epoch 188/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1260 - accuracy: 0.2462 - val_loss: 0.1677 - val_accuracy: 0.2626\n",
      "Epoch 189/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1256 - accuracy: 0.2390 - val_loss: 0.1683 - val_accuracy: 0.2698\n",
      "Epoch 190/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1252 - accuracy: 0.2468 - val_loss: 0.1680 - val_accuracy: 0.2572\n",
      "Epoch 191/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1249 - accuracy: 0.2456 - val_loss: 0.1652 - val_accuracy: 0.2716\n",
      "Epoch 192/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1245 - accuracy: 0.2511 - val_loss: 0.1713 - val_accuracy: 0.2248\n",
      "Epoch 193/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1244 - accuracy: 0.2336 - val_loss: 0.1650 - val_accuracy: 0.2842\n",
      "Epoch 194/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1237 - accuracy: 0.2601 - val_loss: 0.1639 - val_accuracy: 0.2626\n",
      "Epoch 195/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1233 - accuracy: 0.2462 - val_loss: 0.1645 - val_accuracy: 0.2590\n",
      "Epoch 196/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1230 - accuracy: 0.2511 - val_loss: 0.1632 - val_accuracy: 0.2428\n",
      "Epoch 197/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1226 - accuracy: 0.2523 - val_loss: 0.1636 - val_accuracy: 0.2428\n",
      "Epoch 198/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1222 - accuracy: 0.2523 - val_loss: 0.1630 - val_accuracy: 0.2968\n",
      "Epoch 199/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1219 - accuracy: 0.2571 - val_loss: 0.1614 - val_accuracy: 0.2212\n",
      "Epoch 200/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1216 - accuracy: 0.2372 - val_loss: 0.1627 - val_accuracy: 0.2788\n",
      "Epoch 201/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1211 - accuracy: 0.2529 - val_loss: 0.1610 - val_accuracy: 0.2266\n",
      "Epoch 202/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1208 - accuracy: 0.2547 - val_loss: 0.1619 - val_accuracy: 0.2284\n",
      "Epoch 203/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1206 - accuracy: 0.2523 - val_loss: 0.1616 - val_accuracy: 0.2716\n",
      "Epoch 204/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1202 - accuracy: 0.2553 - val_loss: 0.1612 - val_accuracy: 0.2356\n",
      "Epoch 205/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1199 - accuracy: 0.2535 - val_loss: 0.1615 - val_accuracy: 0.2518\n",
      "Epoch 206/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1196 - accuracy: 0.2541 - val_loss: 0.1601 - val_accuracy: 0.2734\n",
      "Epoch 207/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1192 - accuracy: 0.2462 - val_loss: 0.1607 - val_accuracy: 0.2968\n",
      "Epoch 208/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1190 - accuracy: 0.2655 - val_loss: 0.1600 - val_accuracy: 0.2680\n",
      "Epoch 209/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1186 - accuracy: 0.2571 - val_loss: 0.1661 - val_accuracy: 0.2014\n",
      "Epoch 210/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1186 - accuracy: 0.2541 - val_loss: 0.1577 - val_accuracy: 0.2698\n",
      "Epoch 211/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1181 - accuracy: 0.2547 - val_loss: 0.1592 - val_accuracy: 0.2896\n",
      "Epoch 212/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1178 - accuracy: 0.2535 - val_loss: 0.1588 - val_accuracy: 0.2734\n",
      "Epoch 213/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1175 - accuracy: 0.2511 - val_loss: 0.1579 - val_accuracy: 0.2320\n",
      "Epoch 214/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1173 - accuracy: 0.2523 - val_loss: 0.1573 - val_accuracy: 0.2716\n",
      "Epoch 215/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1169 - accuracy: 0.2553 - val_loss: 0.1579 - val_accuracy: 0.2932\n",
      "Epoch 216/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1166 - accuracy: 0.2625 - val_loss: 0.1573 - val_accuracy: 0.2662\n",
      "Epoch 217/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1165 - accuracy: 0.2661 - val_loss: 0.1577 - val_accuracy: 0.2554\n",
      "Epoch 218/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1160 - accuracy: 0.2541 - val_loss: 0.1570 - val_accuracy: 0.2698\n",
      "Epoch 219/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1159 - accuracy: 0.2619 - val_loss: 0.1585 - val_accuracy: 0.2572\n",
      "Epoch 220/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1158 - accuracy: 0.2637 - val_loss: 0.1561 - val_accuracy: 0.2968\n",
      "Epoch 221/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1153 - accuracy: 0.2703 - val_loss: 0.1565 - val_accuracy: 0.2878\n",
      "Epoch 222/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1151 - accuracy: 0.2589 - val_loss: 0.1620 - val_accuracy: 0.2680\n",
      "Epoch 223/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1152 - accuracy: 0.2661 - val_loss: 0.1556 - val_accuracy: 0.2824\n",
      "Epoch 224/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1146 - accuracy: 0.2691 - val_loss: 0.1560 - val_accuracy: 0.2914\n",
      "Epoch 225/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1144 - accuracy: 0.2685 - val_loss: 0.1548 - val_accuracy: 0.2716\n",
      "Epoch 226/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1141 - accuracy: 0.2673 - val_loss: 0.1538 - val_accuracy: 0.2788\n",
      "Epoch 227/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1139 - accuracy: 0.2643 - val_loss: 0.1546 - val_accuracy: 0.2806\n",
      "Epoch 228/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1137 - accuracy: 0.2649 - val_loss: 0.1545 - val_accuracy: 0.2608\n",
      "Epoch 229/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1134 - accuracy: 0.2691 - val_loss: 0.1534 - val_accuracy: 0.2752\n",
      "Epoch 230/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1133 - accuracy: 0.2607 - val_loss: 0.1586 - val_accuracy: 0.1978\n",
      "Epoch 231/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1133 - accuracy: 0.2661 - val_loss: 0.1539 - val_accuracy: 0.2788\n",
      "Epoch 232/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1127 - accuracy: 0.2613 - val_loss: 0.1549 - val_accuracy: 0.2644\n",
      "Epoch 233/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1128 - accuracy: 0.2637 - val_loss: 0.1531 - val_accuracy: 0.2572\n",
      "Epoch 234/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1124 - accuracy: 0.2619 - val_loss: 0.1531 - val_accuracy: 0.2788\n",
      "Epoch 235/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1121 - accuracy: 0.2721 - val_loss: 0.1537 - val_accuracy: 0.2914\n",
      "Epoch 236/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1119 - accuracy: 0.2745 - val_loss: 0.1576 - val_accuracy: 0.3129\n",
      "Epoch 237/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1120 - accuracy: 0.2721 - val_loss: 0.1528 - val_accuracy: 0.2842\n",
      "Epoch 238/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1115 - accuracy: 0.2673 - val_loss: 0.1534 - val_accuracy: 0.2842\n",
      "Epoch 239/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1113 - accuracy: 0.2667 - val_loss: 0.1533 - val_accuracy: 0.2806\n",
      "Epoch 240/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1112 - accuracy: 0.2577 - val_loss: 0.1537 - val_accuracy: 0.3022\n",
      "Epoch 241/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1109 - accuracy: 0.2709 - val_loss: 0.1534 - val_accuracy: 0.2698\n",
      "Epoch 242/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1107 - accuracy: 0.2637 - val_loss: 0.1520 - val_accuracy: 0.2680\n",
      "Epoch 243/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1104 - accuracy: 0.2595 - val_loss: 0.1518 - val_accuracy: 0.2824\n",
      "Epoch 244/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1103 - accuracy: 0.2643 - val_loss: 0.1518 - val_accuracy: 0.2878\n",
      "Epoch 245/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1102 - accuracy: 0.2655 - val_loss: 0.1518 - val_accuracy: 0.3094\n",
      "Epoch 246/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1099 - accuracy: 0.2625 - val_loss: 0.1499 - val_accuracy: 0.2572\n",
      "Epoch 247/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1096 - accuracy: 0.2679 - val_loss: 0.1512 - val_accuracy: 0.2896\n",
      "Epoch 248/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1095 - accuracy: 0.2637 - val_loss: 0.1515 - val_accuracy: 0.2950\n",
      "Epoch 249/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1093 - accuracy: 0.2691 - val_loss: 0.1498 - val_accuracy: 0.2824\n",
      "Epoch 250/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1092 - accuracy: 0.2673 - val_loss: 0.1504 - val_accuracy: 0.2878\n",
      "Epoch 251/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1089 - accuracy: 0.2631 - val_loss: 0.1518 - val_accuracy: 0.2806\n",
      "Epoch 252/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1087 - accuracy: 0.2631 - val_loss: 0.1507 - val_accuracy: 0.2374\n",
      "Epoch 253/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1085 - accuracy: 0.2697 - val_loss: 0.1498 - val_accuracy: 0.2788\n",
      "Epoch 254/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1082 - accuracy: 0.2727 - val_loss: 0.1498 - val_accuracy: 0.3004\n",
      "Epoch 255/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1081 - accuracy: 0.2595 - val_loss: 0.1513 - val_accuracy: 0.3094\n",
      "Epoch 256/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1081 - accuracy: 0.2715 - val_loss: 0.1506 - val_accuracy: 0.2914\n",
      "Epoch 257/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1077 - accuracy: 0.2685 - val_loss: 0.1513 - val_accuracy: 0.2770\n",
      "Epoch 258/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1077 - accuracy: 0.2619 - val_loss: 0.1492 - val_accuracy: 0.2644\n",
      "Epoch 259/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1072 - accuracy: 0.2661 - val_loss: 0.1502 - val_accuracy: 0.2986\n",
      "Epoch 260/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1072 - accuracy: 0.2733 - val_loss: 0.1485 - val_accuracy: 0.3058\n",
      "Epoch 261/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1068 - accuracy: 0.2721 - val_loss: 0.1500 - val_accuracy: 0.2536\n",
      "Epoch 262/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1069 - accuracy: 0.2607 - val_loss: 0.1487 - val_accuracy: 0.3022\n",
      "Epoch 263/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1066 - accuracy: 0.2661 - val_loss: 0.1530 - val_accuracy: 0.3165\n",
      "Epoch 264/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1067 - accuracy: 0.2727 - val_loss: 0.1489 - val_accuracy: 0.2968\n",
      "Epoch 265/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1062 - accuracy: 0.2739 - val_loss: 0.1481 - val_accuracy: 0.2932\n",
      "Epoch 266/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1061 - accuracy: 0.2655 - val_loss: 0.1474 - val_accuracy: 0.2698\n",
      "Epoch 267/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1059 - accuracy: 0.2679 - val_loss: 0.1497 - val_accuracy: 0.2824\n",
      "Epoch 268/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1057 - accuracy: 0.2769 - val_loss: 0.1486 - val_accuracy: 0.3022\n",
      "Epoch 269/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1055 - accuracy: 0.2709 - val_loss: 0.1516 - val_accuracy: 0.2338\n",
      "Epoch 270/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1055 - accuracy: 0.2715 - val_loss: 0.1554 - val_accuracy: 0.2662\n",
      "Epoch 271/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1057 - accuracy: 0.2715 - val_loss: 0.1462 - val_accuracy: 0.2896\n",
      "Epoch 272/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1050 - accuracy: 0.2679 - val_loss: 0.1480 - val_accuracy: 0.3165\n",
      "Epoch 273/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1048 - accuracy: 0.2781 - val_loss: 0.1484 - val_accuracy: 0.2806\n",
      "Epoch 274/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1047 - accuracy: 0.2703 - val_loss: 0.1479 - val_accuracy: 0.2788\n",
      "Epoch 275/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1045 - accuracy: 0.2775 - val_loss: 0.1479 - val_accuracy: 0.2878\n",
      "Epoch 276/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1043 - accuracy: 0.2739 - val_loss: 0.1465 - val_accuracy: 0.3147\n",
      "Epoch 277/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1039 - accuracy: 0.2709 - val_loss: 0.1474 - val_accuracy: 0.3058\n",
      "Epoch 278/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1038 - accuracy: 0.2757 - val_loss: 0.1467 - val_accuracy: 0.3022\n",
      "Epoch 279/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1037 - accuracy: 0.2727 - val_loss: 0.1465 - val_accuracy: 0.2662\n",
      "Epoch 280/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1035 - accuracy: 0.2697 - val_loss: 0.1473 - val_accuracy: 0.2788\n",
      "Epoch 281/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1034 - accuracy: 0.2751 - val_loss: 0.1459 - val_accuracy: 0.2932\n",
      "Epoch 282/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1032 - accuracy: 0.2727 - val_loss: 0.1456 - val_accuracy: 0.2446\n",
      "Epoch 283/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1031 - accuracy: 0.2691 - val_loss: 0.1481 - val_accuracy: 0.2932\n",
      "Epoch 284/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1029 - accuracy: 0.2733 - val_loss: 0.1460 - val_accuracy: 0.3022\n",
      "Epoch 285/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1027 - accuracy: 0.2685 - val_loss: 0.1450 - val_accuracy: 0.2788\n",
      "Epoch 286/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1025 - accuracy: 0.2781 - val_loss: 0.1455 - val_accuracy: 0.3040\n",
      "Epoch 287/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1023 - accuracy: 0.2751 - val_loss: 0.1443 - val_accuracy: 0.2644\n",
      "Epoch 288/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1021 - accuracy: 0.2679 - val_loss: 0.1471 - val_accuracy: 0.2806\n",
      "Epoch 289/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1021 - accuracy: 0.2715 - val_loss: 0.1463 - val_accuracy: 0.2770\n",
      "Epoch 290/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1018 - accuracy: 0.2709 - val_loss: 0.1461 - val_accuracy: 0.3076\n",
      "Epoch 291/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1016 - accuracy: 0.2775 - val_loss: 0.1457 - val_accuracy: 0.2788\n",
      "Epoch 292/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1014 - accuracy: 0.2763 - val_loss: 0.1455 - val_accuracy: 0.2824\n",
      "Epoch 293/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1013 - accuracy: 0.2817 - val_loss: 0.1444 - val_accuracy: 0.2950\n",
      "Epoch 294/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1011 - accuracy: 0.2787 - val_loss: 0.1442 - val_accuracy: 0.2788\n",
      "Epoch 295/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1009 - accuracy: 0.2799 - val_loss: 0.1534 - val_accuracy: 0.2374\n",
      "Epoch 296/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1014 - accuracy: 0.2733 - val_loss: 0.1442 - val_accuracy: 0.2734\n",
      "Epoch 297/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1006 - accuracy: 0.2793 - val_loss: 0.1436 - val_accuracy: 0.2950\n",
      "Epoch 298/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1003 - accuracy: 0.2805 - val_loss: 0.1445 - val_accuracy: 0.2554\n",
      "Epoch 299/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1004 - accuracy: 0.2715 - val_loss: 0.1426 - val_accuracy: 0.2788\n",
      "Epoch 300/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1001 - accuracy: 0.2811 - val_loss: 0.1440 - val_accuracy: 0.2770\n",
      "Epoch 301/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0999 - accuracy: 0.2835 - val_loss: 0.1433 - val_accuracy: 0.2896\n",
      "Epoch 302/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0998 - accuracy: 0.2859 - val_loss: 0.1439 - val_accuracy: 0.2824\n",
      "Epoch 303/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0997 - accuracy: 0.2769 - val_loss: 0.1442 - val_accuracy: 0.3040\n",
      "Epoch 304/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0995 - accuracy: 0.2841 - val_loss: 0.1452 - val_accuracy: 0.2878\n",
      "Epoch 305/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0996 - accuracy: 0.2841 - val_loss: 0.1449 - val_accuracy: 0.2626\n",
      "Epoch 306/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0992 - accuracy: 0.2775 - val_loss: 0.1436 - val_accuracy: 0.3255\n",
      "Epoch 307/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0990 - accuracy: 0.2799 - val_loss: 0.1431 - val_accuracy: 0.2824\n",
      "Epoch 308/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0989 - accuracy: 0.2913 - val_loss: 0.1444 - val_accuracy: 0.2842\n",
      "Epoch 309/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0988 - accuracy: 0.2925 - val_loss: 0.1424 - val_accuracy: 0.2968\n",
      "Epoch 310/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0986 - accuracy: 0.2775 - val_loss: 0.1430 - val_accuracy: 0.2842\n",
      "Epoch 311/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0985 - accuracy: 0.2847 - val_loss: 0.1419 - val_accuracy: 0.2914\n",
      "Epoch 312/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0982 - accuracy: 0.2829 - val_loss: 0.1438 - val_accuracy: 0.3004\n",
      "Epoch 313/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0981 - accuracy: 0.2937 - val_loss: 0.1427 - val_accuracy: 0.2824\n",
      "Epoch 314/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0979 - accuracy: 0.2907 - val_loss: 0.1446 - val_accuracy: 0.3165\n",
      "Epoch 315/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0979 - accuracy: 0.2889 - val_loss: 0.1424 - val_accuracy: 0.2806\n",
      "Epoch 316/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0976 - accuracy: 0.2793 - val_loss: 0.1418 - val_accuracy: 0.3022\n",
      "Epoch 317/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0976 - accuracy: 0.2883 - val_loss: 0.1432 - val_accuracy: 0.3022\n",
      "Epoch 318/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0973 - accuracy: 0.2901 - val_loss: 0.1406 - val_accuracy: 0.2932\n",
      "Epoch 319/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0970 - accuracy: 0.2847 - val_loss: 0.1419 - val_accuracy: 0.3076\n",
      "Epoch 320/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0970 - accuracy: 0.2901 - val_loss: 0.1419 - val_accuracy: 0.3112\n",
      "Epoch 321/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0969 - accuracy: 0.2901 - val_loss: 0.1408 - val_accuracy: 0.3112\n",
      "Epoch 322/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0968 - accuracy: 0.2913 - val_loss: 0.1410 - val_accuracy: 0.3112\n",
      "Epoch 323/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0966 - accuracy: 0.2877 - val_loss: 0.1410 - val_accuracy: 0.3022\n",
      "Epoch 324/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0963 - accuracy: 0.2931 - val_loss: 0.1418 - val_accuracy: 0.3058\n",
      "Epoch 325/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0962 - accuracy: 0.2913 - val_loss: 0.1413 - val_accuracy: 0.3004\n",
      "Epoch 326/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0962 - accuracy: 0.2877 - val_loss: 0.1403 - val_accuracy: 0.2914\n",
      "Epoch 327/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0961 - accuracy: 0.2877 - val_loss: 0.1432 - val_accuracy: 0.3201\n",
      "Epoch 328/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0959 - accuracy: 0.3045 - val_loss: 0.1401 - val_accuracy: 0.2770\n",
      "Epoch 329/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0958 - accuracy: 0.2907 - val_loss: 0.1408 - val_accuracy: 0.3183\n",
      "Epoch 330/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0957 - accuracy: 0.2907 - val_loss: 0.1406 - val_accuracy: 0.3040\n",
      "Epoch 331/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0954 - accuracy: 0.2979 - val_loss: 0.1409 - val_accuracy: 0.3040\n",
      "Epoch 332/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0954 - accuracy: 0.2925 - val_loss: 0.1414 - val_accuracy: 0.3076\n",
      "Epoch 333/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0951 - accuracy: 0.2967 - val_loss: 0.1548 - val_accuracy: 0.2374\n",
      "Epoch 334/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0958 - accuracy: 0.2889 - val_loss: 0.1406 - val_accuracy: 0.3255\n",
      "Epoch 335/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0950 - accuracy: 0.3003 - val_loss: 0.1425 - val_accuracy: 0.3219\n",
      "Epoch 336/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0951 - accuracy: 0.2943 - val_loss: 0.1414 - val_accuracy: 0.3076\n",
      "Epoch 337/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0948 - accuracy: 0.2937 - val_loss: 0.1394 - val_accuracy: 0.3094\n",
      "Epoch 338/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0946 - accuracy: 0.2967 - val_loss: 0.1396 - val_accuracy: 0.2986\n",
      "Epoch 339/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0945 - accuracy: 0.2979 - val_loss: 0.1398 - val_accuracy: 0.2878\n",
      "Epoch 340/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0943 - accuracy: 0.2985 - val_loss: 0.1393 - val_accuracy: 0.3112\n",
      "Epoch 341/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0942 - accuracy: 0.3003 - val_loss: 0.1397 - val_accuracy: 0.3129\n",
      "Epoch 342/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0943 - accuracy: 0.2979 - val_loss: 0.1407 - val_accuracy: 0.3022\n",
      "Epoch 343/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0940 - accuracy: 0.3039 - val_loss: 0.1379 - val_accuracy: 0.3112\n",
      "Epoch 344/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0937 - accuracy: 0.3081 - val_loss: 0.1405 - val_accuracy: 0.2896\n",
      "Epoch 345/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0937 - accuracy: 0.3009 - val_loss: 0.1381 - val_accuracy: 0.3040\n",
      "Epoch 346/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0935 - accuracy: 0.3003 - val_loss: 0.1374 - val_accuracy: 0.2986\n",
      "Epoch 347/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0934 - accuracy: 0.2985 - val_loss: 0.1386 - val_accuracy: 0.3022\n",
      "Epoch 348/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0934 - accuracy: 0.2931 - val_loss: 0.1403 - val_accuracy: 0.3040\n",
      "Epoch 349/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0933 - accuracy: 0.2961 - val_loss: 0.1382 - val_accuracy: 0.3129\n",
      "Epoch 350/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0932 - accuracy: 0.3039 - val_loss: 0.1378 - val_accuracy: 0.3165\n",
      "Epoch 351/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0930 - accuracy: 0.3045 - val_loss: 0.1397 - val_accuracy: 0.3219\n",
      "Epoch 352/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0929 - accuracy: 0.2997 - val_loss: 0.1383 - val_accuracy: 0.3129\n",
      "Epoch 353/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0926 - accuracy: 0.3039 - val_loss: 0.1378 - val_accuracy: 0.3076\n",
      "Epoch 354/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0925 - accuracy: 0.2913 - val_loss: 0.1384 - val_accuracy: 0.3004\n",
      "Epoch 355/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0926 - accuracy: 0.3033 - val_loss: 0.1422 - val_accuracy: 0.3004\n",
      "Epoch 356/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0924 - accuracy: 0.3021 - val_loss: 0.1377 - val_accuracy: 0.3309\n",
      "Epoch 357/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0923 - accuracy: 0.3069 - val_loss: 0.1386 - val_accuracy: 0.2986\n",
      "Epoch 358/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0923 - accuracy: 0.2985 - val_loss: 0.1394 - val_accuracy: 0.2680\n",
      "Epoch 359/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0922 - accuracy: 0.3033 - val_loss: 0.1395 - val_accuracy: 0.3112\n",
      "Epoch 360/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0921 - accuracy: 0.3051 - val_loss: 0.1383 - val_accuracy: 0.3040\n",
      "Epoch 361/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0918 - accuracy: 0.3063 - val_loss: 0.1376 - val_accuracy: 0.3076\n",
      "Epoch 362/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0917 - accuracy: 0.3051 - val_loss: 0.1414 - val_accuracy: 0.2770\n",
      "Epoch 363/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0919 - accuracy: 0.3069 - val_loss: 0.1399 - val_accuracy: 0.2806\n",
      "Epoch 364/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0916 - accuracy: 0.2979 - val_loss: 0.1382 - val_accuracy: 0.2896\n",
      "Epoch 365/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0915 - accuracy: 0.3069 - val_loss: 0.1375 - val_accuracy: 0.3076\n",
      "Epoch 366/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0912 - accuracy: 0.3051 - val_loss: 0.1362 - val_accuracy: 0.3076\n",
      "Epoch 367/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0913 - accuracy: 0.3045 - val_loss: 0.1390 - val_accuracy: 0.3237\n",
      "Epoch 368/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0911 - accuracy: 0.3063 - val_loss: 0.1387 - val_accuracy: 0.3022\n",
      "Epoch 369/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0910 - accuracy: 0.3051 - val_loss: 0.1379 - val_accuracy: 0.2932\n",
      "Epoch 370/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0909 - accuracy: 0.3069 - val_loss: 0.1377 - val_accuracy: 0.2896\n",
      "Epoch 371/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0907 - accuracy: 0.3021 - val_loss: 0.1359 - val_accuracy: 0.3040\n",
      "Epoch 372/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0907 - accuracy: 0.2961 - val_loss: 0.1368 - val_accuracy: 0.3022\n",
      "Epoch 373/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0905 - accuracy: 0.3045 - val_loss: 0.1390 - val_accuracy: 0.3076\n",
      "Epoch 374/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0904 - accuracy: 0.3147 - val_loss: 0.1380 - val_accuracy: 0.3201\n",
      "Epoch 375/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0902 - accuracy: 0.3135 - val_loss: 0.1363 - val_accuracy: 0.2950\n",
      "Epoch 376/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0903 - accuracy: 0.3075 - val_loss: 0.1377 - val_accuracy: 0.3291\n",
      "Epoch 377/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0902 - accuracy: 0.3135 - val_loss: 0.1396 - val_accuracy: 0.3129\n",
      "Epoch 378/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0902 - accuracy: 0.3099 - val_loss: 0.1414 - val_accuracy: 0.2680\n",
      "Epoch 379/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0902 - accuracy: 0.2979 - val_loss: 0.1365 - val_accuracy: 0.3273\n",
      "Epoch 380/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0898 - accuracy: 0.3099 - val_loss: 0.1366 - val_accuracy: 0.3129\n",
      "Epoch 381/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0897 - accuracy: 0.3093 - val_loss: 0.1358 - val_accuracy: 0.3129\n",
      "Epoch 382/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0896 - accuracy: 0.3093 - val_loss: 0.1372 - val_accuracy: 0.2986\n",
      "Epoch 383/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0896 - accuracy: 0.3069 - val_loss: 0.1358 - val_accuracy: 0.3147\n",
      "Epoch 384/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0897 - accuracy: 0.3117 - val_loss: 0.1365 - val_accuracy: 0.3129\n",
      "Epoch 385/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0892 - accuracy: 0.3081 - val_loss: 0.1360 - val_accuracy: 0.3022\n",
      "Epoch 386/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0892 - accuracy: 0.3165 - val_loss: 0.1375 - val_accuracy: 0.3327\n",
      "Epoch 387/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0892 - accuracy: 0.3105 - val_loss: 0.1357 - val_accuracy: 0.3094\n",
      "Epoch 388/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0891 - accuracy: 0.3039 - val_loss: 0.1462 - val_accuracy: 0.3183\n",
      "Epoch 389/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0893 - accuracy: 0.3141 - val_loss: 0.1356 - val_accuracy: 0.3112\n",
      "Epoch 390/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0888 - accuracy: 0.3069 - val_loss: 0.1356 - val_accuracy: 0.3022\n",
      "Epoch 391/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0886 - accuracy: 0.3099 - val_loss: 0.1372 - val_accuracy: 0.2878\n",
      "Epoch 392/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0888 - accuracy: 0.3093 - val_loss: 0.1368 - val_accuracy: 0.2986\n",
      "Epoch 393/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0886 - accuracy: 0.3111 - val_loss: 0.1347 - val_accuracy: 0.3076\n",
      "Epoch 394/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0885 - accuracy: 0.3123 - val_loss: 0.1355 - val_accuracy: 0.3004\n",
      "Epoch 395/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0884 - accuracy: 0.3183 - val_loss: 0.1349 - val_accuracy: 0.3165\n",
      "Epoch 396/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0882 - accuracy: 0.3153 - val_loss: 0.1351 - val_accuracy: 0.2950\n",
      "Epoch 397/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0882 - accuracy: 0.3057 - val_loss: 0.1367 - val_accuracy: 0.3345\n",
      "Epoch 398/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0881 - accuracy: 0.3129 - val_loss: 0.1358 - val_accuracy: 0.2950\n",
      "Epoch 399/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0880 - accuracy: 0.3117 - val_loss: 0.1349 - val_accuracy: 0.2914\n",
      "Epoch 400/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0879 - accuracy: 0.3033 - val_loss: 0.1366 - val_accuracy: 0.3255\n",
      "Epoch 401/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0879 - accuracy: 0.3075 - val_loss: 0.1366 - val_accuracy: 0.3129\n",
      "Epoch 402/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0877 - accuracy: 0.3123 - val_loss: 0.1362 - val_accuracy: 0.3219\n",
      "Epoch 403/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0877 - accuracy: 0.3087 - val_loss: 0.1369 - val_accuracy: 0.3147\n",
      "Epoch 404/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0875 - accuracy: 0.3099 - val_loss: 0.1351 - val_accuracy: 0.3255\n",
      "Epoch 405/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0875 - accuracy: 0.3141 - val_loss: 0.1355 - val_accuracy: 0.2770\n",
      "Epoch 406/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0874 - accuracy: 0.3153 - val_loss: 0.1358 - val_accuracy: 0.3112\n",
      "Epoch 407/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0873 - accuracy: 0.3183 - val_loss: 0.1398 - val_accuracy: 0.3040\n",
      "Epoch 408/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0875 - accuracy: 0.3105 - val_loss: 0.1352 - val_accuracy: 0.2788\n",
      "Epoch 409/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0872 - accuracy: 0.3105 - val_loss: 0.1356 - val_accuracy: 0.3147\n",
      "Epoch 410/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0870 - accuracy: 0.3153 - val_loss: 0.1350 - val_accuracy: 0.2824\n",
      "Epoch 411/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0870 - accuracy: 0.3117 - val_loss: 0.1356 - val_accuracy: 0.2914\n",
      "Epoch 412/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0870 - accuracy: 0.3093 - val_loss: 0.1430 - val_accuracy: 0.2806\n",
      "Epoch 413/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0874 - accuracy: 0.3063 - val_loss: 0.1404 - val_accuracy: 0.3633\n",
      "Epoch 414/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0870 - accuracy: 0.3171 - val_loss: 0.1357 - val_accuracy: 0.3237\n",
      "Epoch 415/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0866 - accuracy: 0.3099 - val_loss: 0.1339 - val_accuracy: 0.3058\n",
      "Epoch 416/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0865 - accuracy: 0.3129 - val_loss: 0.1359 - val_accuracy: 0.3255\n",
      "Epoch 417/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0864 - accuracy: 0.3183 - val_loss: 0.1383 - val_accuracy: 0.2950\n",
      "Epoch 418/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0869 - accuracy: 0.3093 - val_loss: 0.1354 - val_accuracy: 0.3219\n",
      "Epoch 419/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0864 - accuracy: 0.3117 - val_loss: 0.1367 - val_accuracy: 0.2932\n",
      "Epoch 420/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0862 - accuracy: 0.3099 - val_loss: 0.1343 - val_accuracy: 0.3183\n",
      "Epoch 421/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0860 - accuracy: 0.3153 - val_loss: 0.1334 - val_accuracy: 0.3076\n",
      "Epoch 422/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0859 - accuracy: 0.3141 - val_loss: 0.1405 - val_accuracy: 0.3399\n",
      "Epoch 423/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0864 - accuracy: 0.3159 - val_loss: 0.1372 - val_accuracy: 0.2932\n",
      "Epoch 424/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0860 - accuracy: 0.3189 - val_loss: 0.1334 - val_accuracy: 0.3165\n",
      "Epoch 425/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0859 - accuracy: 0.3099 - val_loss: 0.1325 - val_accuracy: 0.3255\n",
      "Epoch 426/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0856 - accuracy: 0.3117 - val_loss: 0.1383 - val_accuracy: 0.3022\n",
      "Epoch 427/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0861 - accuracy: 0.3081 - val_loss: 0.1332 - val_accuracy: 0.3183\n",
      "Epoch 428/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0853 - accuracy: 0.3129 - val_loss: 0.1461 - val_accuracy: 0.3058\n",
      "Epoch 429/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0857 - accuracy: 0.3177 - val_loss: 0.1330 - val_accuracy: 0.3129\n",
      "Epoch 430/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0854 - accuracy: 0.3057 - val_loss: 0.1337 - val_accuracy: 0.3165\n",
      "Epoch 431/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0855 - accuracy: 0.3141 - val_loss: 0.1351 - val_accuracy: 0.3022\n",
      "Epoch 432/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0852 - accuracy: 0.3093 - val_loss: 0.1345 - val_accuracy: 0.3201\n",
      "Epoch 433/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0850 - accuracy: 0.3225 - val_loss: 0.1335 - val_accuracy: 0.3291\n",
      "Epoch 434/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0850 - accuracy: 0.3123 - val_loss: 0.1348 - val_accuracy: 0.3165\n",
      "Epoch 435/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0849 - accuracy: 0.3135 - val_loss: 0.1331 - val_accuracy: 0.3058\n",
      "Epoch 436/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0849 - accuracy: 0.3183 - val_loss: 0.1349 - val_accuracy: 0.3201\n",
      "Epoch 437/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0847 - accuracy: 0.3183 - val_loss: 0.1328 - val_accuracy: 0.3094\n",
      "Epoch 438/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0844 - accuracy: 0.3141 - val_loss: 0.1335 - val_accuracy: 0.3129\n",
      "Epoch 439/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0845 - accuracy: 0.3219 - val_loss: 0.1337 - val_accuracy: 0.2968\n",
      "Epoch 440/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0845 - accuracy: 0.3183 - val_loss: 0.1335 - val_accuracy: 0.3255\n",
      "Epoch 441/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0844 - accuracy: 0.3147 - val_loss: 0.1329 - val_accuracy: 0.3004\n",
      "Epoch 442/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0843 - accuracy: 0.3129 - val_loss: 0.1355 - val_accuracy: 0.3219\n",
      "Epoch 443/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0842 - accuracy: 0.3183 - val_loss: 0.1342 - val_accuracy: 0.3022\n",
      "Epoch 444/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0843 - accuracy: 0.3141 - val_loss: 0.1325 - val_accuracy: 0.3183\n",
      "Epoch 445/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0839 - accuracy: 0.3147 - val_loss: 0.1349 - val_accuracy: 0.3094\n",
      "Epoch 446/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0841 - accuracy: 0.3135 - val_loss: 0.1351 - val_accuracy: 0.3273\n",
      "Epoch 447/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0839 - accuracy: 0.3165 - val_loss: 0.1319 - val_accuracy: 0.3058\n",
      "Epoch 448/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0837 - accuracy: 0.3123 - val_loss: 0.1324 - val_accuracy: 0.3183\n",
      "Epoch 449/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0836 - accuracy: 0.3099 - val_loss: 0.1325 - val_accuracy: 0.3129\n",
      "Epoch 450/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0835 - accuracy: 0.3171 - val_loss: 0.1334 - val_accuracy: 0.2986\n",
      "Epoch 451/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0836 - accuracy: 0.3141 - val_loss: 0.1321 - val_accuracy: 0.2914\n",
      "Epoch 452/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0837 - accuracy: 0.3141 - val_loss: 0.1350 - val_accuracy: 0.3183\n",
      "Epoch 453/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0835 - accuracy: 0.3189 - val_loss: 0.1310 - val_accuracy: 0.2932\n",
      "Epoch 454/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0834 - accuracy: 0.3159 - val_loss: 0.1327 - val_accuracy: 0.3004\n",
      "Epoch 455/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0831 - accuracy: 0.3153 - val_loss: 0.1305 - val_accuracy: 0.3147\n",
      "Epoch 456/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0832 - accuracy: 0.3219 - val_loss: 0.1380 - val_accuracy: 0.3201\n",
      "Epoch 457/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0834 - accuracy: 0.3285 - val_loss: 0.1319 - val_accuracy: 0.2968\n",
      "Epoch 458/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0830 - accuracy: 0.3225 - val_loss: 0.1312 - val_accuracy: 0.3112\n",
      "Epoch 459/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0829 - accuracy: 0.3159 - val_loss: 0.1286 - val_accuracy: 0.3040\n",
      "Epoch 460/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0828 - accuracy: 0.3159 - val_loss: 0.1335 - val_accuracy: 0.3076\n",
      "Epoch 461/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0828 - accuracy: 0.3153 - val_loss: 0.1310 - val_accuracy: 0.3219\n",
      "Epoch 462/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0827 - accuracy: 0.3183 - val_loss: 0.1306 - val_accuracy: 0.2878\n",
      "Epoch 463/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0829 - accuracy: 0.3147 - val_loss: 0.1355 - val_accuracy: 0.3022\n",
      "Epoch 464/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0830 - accuracy: 0.3159 - val_loss: 0.1348 - val_accuracy: 0.3237\n",
      "Epoch 465/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0829 - accuracy: 0.3267 - val_loss: 0.1314 - val_accuracy: 0.2986\n",
      "Epoch 466/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0824 - accuracy: 0.3159 - val_loss: 0.1314 - val_accuracy: 0.3183\n",
      "Epoch 467/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0823 - accuracy: 0.3225 - val_loss: 0.1333 - val_accuracy: 0.3094\n",
      "Epoch 468/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0822 - accuracy: 0.3201 - val_loss: 0.1326 - val_accuracy: 0.3381\n",
      "Epoch 469/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0822 - accuracy: 0.3207 - val_loss: 0.1331 - val_accuracy: 0.3076\n",
      "Epoch 470/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0820 - accuracy: 0.3201 - val_loss: 0.1331 - val_accuracy: 0.3147\n",
      "Epoch 471/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0821 - accuracy: 0.3189 - val_loss: 0.1315 - val_accuracy: 0.3183\n",
      "Epoch 472/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0819 - accuracy: 0.3225 - val_loss: 0.1328 - val_accuracy: 0.3183\n",
      "Epoch 473/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0819 - accuracy: 0.3309 - val_loss: 0.1323 - val_accuracy: 0.3183\n",
      "Epoch 474/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0818 - accuracy: 0.3153 - val_loss: 0.1335 - val_accuracy: 0.2824\n",
      "Epoch 475/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0820 - accuracy: 0.3141 - val_loss: 0.1352 - val_accuracy: 0.2932\n",
      "Epoch 476/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0820 - accuracy: 0.3201 - val_loss: 0.1363 - val_accuracy: 0.2914\n",
      "Epoch 477/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0820 - accuracy: 0.3231 - val_loss: 0.1299 - val_accuracy: 0.3219\n",
      "Epoch 478/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0815 - accuracy: 0.3279 - val_loss: 0.1321 - val_accuracy: 0.3058\n",
      "Epoch 479/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0814 - accuracy: 0.3225 - val_loss: 0.1322 - val_accuracy: 0.3183\n",
      "Epoch 480/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0814 - accuracy: 0.3219 - val_loss: 0.1326 - val_accuracy: 0.3129\n",
      "Epoch 481/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0814 - accuracy: 0.3219 - val_loss: 0.1323 - val_accuracy: 0.3112\n",
      "Epoch 482/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0812 - accuracy: 0.3219 - val_loss: 0.1440 - val_accuracy: 0.3273\n",
      "Epoch 483/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0822 - accuracy: 0.3201 - val_loss: 0.1304 - val_accuracy: 0.3112\n",
      "Epoch 484/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0811 - accuracy: 0.3147 - val_loss: 0.1328 - val_accuracy: 0.2860\n",
      "Epoch 485/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0811 - accuracy: 0.3123 - val_loss: 0.1297 - val_accuracy: 0.2896\n",
      "Epoch 486/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0811 - accuracy: 0.3201 - val_loss: 0.1328 - val_accuracy: 0.2824\n",
      "Epoch 487/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0810 - accuracy: 0.3147 - val_loss: 0.1306 - val_accuracy: 0.3094\n",
      "Epoch 488/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0807 - accuracy: 0.3177 - val_loss: 0.1324 - val_accuracy: 0.3022\n",
      "Epoch 489/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0808 - accuracy: 0.3201 - val_loss: 0.1316 - val_accuracy: 0.3201\n",
      "Epoch 490/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0805 - accuracy: 0.3189 - val_loss: 0.1314 - val_accuracy: 0.3058\n",
      "Epoch 491/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0807 - accuracy: 0.3237 - val_loss: 0.1307 - val_accuracy: 0.3237\n",
      "Epoch 492/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0804 - accuracy: 0.3321 - val_loss: 0.1309 - val_accuracy: 0.3076\n",
      "Epoch 493/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0805 - accuracy: 0.3177 - val_loss: 0.1348 - val_accuracy: 0.3273\n",
      "Epoch 494/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0807 - accuracy: 0.3189 - val_loss: 0.1425 - val_accuracy: 0.2410\n",
      "Epoch 495/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0812 - accuracy: 0.3291 - val_loss: 0.1345 - val_accuracy: 0.3183\n",
      "Epoch 496/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0805 - accuracy: 0.3267 - val_loss: 0.1300 - val_accuracy: 0.3309\n",
      "Epoch 497/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0802 - accuracy: 0.3237 - val_loss: 0.1317 - val_accuracy: 0.2824\n",
      "Epoch 498/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0804 - accuracy: 0.3225 - val_loss: 0.1332 - val_accuracy: 0.3219\n",
      "Epoch 499/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0802 - accuracy: 0.3153 - val_loss: 0.1339 - val_accuracy: 0.3147\n",
      "Epoch 500/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0799 - accuracy: 0.3345 - val_loss: 0.1300 - val_accuracy: 0.3219\n",
      "Epoch 501/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0799 - accuracy: 0.3339 - val_loss: 0.1317 - val_accuracy: 0.3363\n",
      "Epoch 502/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0798 - accuracy: 0.3177 - val_loss: 0.1342 - val_accuracy: 0.3327\n",
      "Epoch 503/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0801 - accuracy: 0.3255 - val_loss: 0.1283 - val_accuracy: 0.3076\n",
      "Epoch 504/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0797 - accuracy: 0.3213 - val_loss: 0.1287 - val_accuracy: 0.3076\n",
      "Epoch 505/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0796 - accuracy: 0.3267 - val_loss: 0.1299 - val_accuracy: 0.3291\n",
      "Epoch 506/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0796 - accuracy: 0.3213 - val_loss: 0.1317 - val_accuracy: 0.3291\n",
      "Epoch 507/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0796 - accuracy: 0.3243 - val_loss: 0.1353 - val_accuracy: 0.3219\n",
      "Epoch 508/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0799 - accuracy: 0.3327 - val_loss: 0.1323 - val_accuracy: 0.3076\n",
      "Epoch 509/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0793 - accuracy: 0.3267 - val_loss: 0.1373 - val_accuracy: 0.3705\n",
      "Epoch 510/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0796 - accuracy: 0.3273 - val_loss: 0.1322 - val_accuracy: 0.3381\n",
      "Epoch 511/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0794 - accuracy: 0.3279 - val_loss: 0.1429 - val_accuracy: 0.3165\n",
      "Epoch 512/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0799 - accuracy: 0.3243 - val_loss: 0.1377 - val_accuracy: 0.2914\n",
      "Epoch 513/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0796 - accuracy: 0.3207 - val_loss: 0.1314 - val_accuracy: 0.3094\n",
      "Epoch 514/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0791 - accuracy: 0.3243 - val_loss: 0.1309 - val_accuracy: 0.3219\n",
      "Epoch 515/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0789 - accuracy: 0.3267 - val_loss: 0.1324 - val_accuracy: 0.3058\n",
      "Epoch 516/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0789 - accuracy: 0.3297 - val_loss: 0.1311 - val_accuracy: 0.3112\n",
      "Epoch 517/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0789 - accuracy: 0.3219 - val_loss: 0.1304 - val_accuracy: 0.3273\n",
      "Epoch 518/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0788 - accuracy: 0.3243 - val_loss: 0.1333 - val_accuracy: 0.3040\n",
      "Epoch 519/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0789 - accuracy: 0.3129 - val_loss: 0.1310 - val_accuracy: 0.3273\n",
      "Epoch 520/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0786 - accuracy: 0.3279 - val_loss: 0.1307 - val_accuracy: 0.3417\n",
      "Epoch 521/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0785 - accuracy: 0.3303 - val_loss: 0.1297 - val_accuracy: 0.3165\n",
      "Epoch 522/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0784 - accuracy: 0.3213 - val_loss: 0.1313 - val_accuracy: 0.3022\n",
      "Epoch 523/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0784 - accuracy: 0.3237 - val_loss: 0.1322 - val_accuracy: 0.3309\n",
      "Epoch 524/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0785 - accuracy: 0.3243 - val_loss: 0.1310 - val_accuracy: 0.3004\n",
      "Epoch 525/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0783 - accuracy: 0.3225 - val_loss: 0.1316 - val_accuracy: 0.3165\n",
      "Epoch 526/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0782 - accuracy: 0.3297 - val_loss: 0.1291 - val_accuracy: 0.3345\n",
      "Epoch 527/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0781 - accuracy: 0.3255 - val_loss: 0.1303 - val_accuracy: 0.3004\n",
      "Epoch 528/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0781 - accuracy: 0.3327 - val_loss: 0.1286 - val_accuracy: 0.3255\n",
      "Epoch 529/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0780 - accuracy: 0.3183 - val_loss: 0.1298 - val_accuracy: 0.3129\n",
      "Epoch 530/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0779 - accuracy: 0.3255 - val_loss: 0.1321 - val_accuracy: 0.3147\n",
      "Epoch 531/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0779 - accuracy: 0.3243 - val_loss: 0.1309 - val_accuracy: 0.3255\n",
      "Epoch 532/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0778 - accuracy: 0.3237 - val_loss: 0.1292 - val_accuracy: 0.3129\n",
      "Epoch 533/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0780 - accuracy: 0.3285 - val_loss: 0.1300 - val_accuracy: 0.3147\n",
      "Epoch 534/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0778 - accuracy: 0.3255 - val_loss: 0.1290 - val_accuracy: 0.2788\n",
      "Epoch 535/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0778 - accuracy: 0.3231 - val_loss: 0.1304 - val_accuracy: 0.3201\n",
      "Epoch 536/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0775 - accuracy: 0.3249 - val_loss: 0.1281 - val_accuracy: 0.3381\n",
      "Epoch 537/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0775 - accuracy: 0.3309 - val_loss: 0.1297 - val_accuracy: 0.3363\n",
      "Epoch 538/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0775 - accuracy: 0.3219 - val_loss: 0.1297 - val_accuracy: 0.3237\n",
      "Epoch 539/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0773 - accuracy: 0.3213 - val_loss: 0.1295 - val_accuracy: 0.3237\n",
      "Epoch 540/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0773 - accuracy: 0.3237 - val_loss: 0.1297 - val_accuracy: 0.3165\n",
      "Epoch 541/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0773 - accuracy: 0.3273 - val_loss: 0.1298 - val_accuracy: 0.3201\n",
      "Epoch 542/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0773 - accuracy: 0.3279 - val_loss: 0.1279 - val_accuracy: 0.3219\n",
      "Epoch 543/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0771 - accuracy: 0.3201 - val_loss: 0.1279 - val_accuracy: 0.3129\n",
      "Epoch 544/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0769 - accuracy: 0.3315 - val_loss: 0.1294 - val_accuracy: 0.3435\n",
      "Epoch 545/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0771 - accuracy: 0.3321 - val_loss: 0.1497 - val_accuracy: 0.3453\n",
      "Epoch 546/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0782 - accuracy: 0.3249 - val_loss: 0.1300 - val_accuracy: 0.3058\n",
      "Epoch 547/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0769 - accuracy: 0.3237 - val_loss: 0.1347 - val_accuracy: 0.2878\n",
      "Epoch 548/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0772 - accuracy: 0.3225 - val_loss: 0.1303 - val_accuracy: 0.2770\n",
      "Epoch 549/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0770 - accuracy: 0.3237 - val_loss: 0.1290 - val_accuracy: 0.3309\n",
      "Epoch 550/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0768 - accuracy: 0.3231 - val_loss: 0.1397 - val_accuracy: 0.3795\n",
      "Epoch 551/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0771 - accuracy: 0.3189 - val_loss: 0.1306 - val_accuracy: 0.3309\n",
      "Epoch 552/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0766 - accuracy: 0.3303 - val_loss: 0.1277 - val_accuracy: 0.3094\n",
      "Epoch 553/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0766 - accuracy: 0.3207 - val_loss: 0.1293 - val_accuracy: 0.3183\n",
      "Epoch 554/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0764 - accuracy: 0.3297 - val_loss: 0.1317 - val_accuracy: 0.3237\n",
      "Epoch 555/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0763 - accuracy: 0.3279 - val_loss: 0.1303 - val_accuracy: 0.3255\n",
      "Epoch 556/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0764 - accuracy: 0.3231 - val_loss: 0.1270 - val_accuracy: 0.3165\n",
      "Epoch 557/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0764 - accuracy: 0.3267 - val_loss: 0.1259 - val_accuracy: 0.3094\n",
      "Epoch 558/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0761 - accuracy: 0.3255 - val_loss: 0.1269 - val_accuracy: 0.3165\n",
      "Epoch 559/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0761 - accuracy: 0.3267 - val_loss: 0.1285 - val_accuracy: 0.3112\n",
      "Epoch 560/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0762 - accuracy: 0.3327 - val_loss: 0.1252 - val_accuracy: 0.3004\n",
      "Epoch 561/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0762 - accuracy: 0.3207 - val_loss: 0.1275 - val_accuracy: 0.3165\n",
      "Epoch 562/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0762 - accuracy: 0.3231 - val_loss: 0.1297 - val_accuracy: 0.3273\n",
      "Epoch 563/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0758 - accuracy: 0.3273 - val_loss: 0.1262 - val_accuracy: 0.3165\n",
      "Epoch 564/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0760 - accuracy: 0.3279 - val_loss: 0.1460 - val_accuracy: 0.2644\n",
      "Epoch 565/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0772 - accuracy: 0.3213 - val_loss: 0.1309 - val_accuracy: 0.2698\n",
      "Epoch 566/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0760 - accuracy: 0.3267 - val_loss: 0.1317 - val_accuracy: 0.3094\n",
      "Epoch 567/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0757 - accuracy: 0.3279 - val_loss: 0.1274 - val_accuracy: 0.3112\n",
      "Epoch 568/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0759 - accuracy: 0.3279 - val_loss: 0.1514 - val_accuracy: 0.2914\n",
      "Epoch 569/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0767 - accuracy: 0.3315 - val_loss: 0.1293 - val_accuracy: 0.3345\n",
      "Epoch 570/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0757 - accuracy: 0.3255 - val_loss: 0.1292 - val_accuracy: 0.3417\n",
      "Epoch 571/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0755 - accuracy: 0.3303 - val_loss: 0.1286 - val_accuracy: 0.3363\n",
      "Epoch 572/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0754 - accuracy: 0.3327 - val_loss: 0.1279 - val_accuracy: 0.3129\n",
      "Epoch 573/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0754 - accuracy: 0.3267 - val_loss: 0.1284 - val_accuracy: 0.2968\n",
      "Epoch 574/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0753 - accuracy: 0.3219 - val_loss: 0.1315 - val_accuracy: 0.3291\n",
      "Epoch 575/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0760 - accuracy: 0.3255 - val_loss: 0.1264 - val_accuracy: 0.3219\n",
      "Epoch 576/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0750 - accuracy: 0.3333 - val_loss: 0.1286 - val_accuracy: 0.3183\n",
      "Epoch 577/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0754 - accuracy: 0.3255 - val_loss: 0.1265 - val_accuracy: 0.3291\n",
      "Epoch 578/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0750 - accuracy: 0.3261 - val_loss: 0.1310 - val_accuracy: 0.3507\n",
      "Epoch 579/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0751 - accuracy: 0.3267 - val_loss: 0.1273 - val_accuracy: 0.3219\n",
      "Epoch 580/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0748 - accuracy: 0.3285 - val_loss: 0.1289 - val_accuracy: 0.2860\n",
      "Epoch 581/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0749 - accuracy: 0.3213 - val_loss: 0.1305 - val_accuracy: 0.3183\n",
      "Epoch 582/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0752 - accuracy: 0.3237 - val_loss: 0.1289 - val_accuracy: 0.3507\n",
      "Epoch 583/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0748 - accuracy: 0.3369 - val_loss: 0.1270 - val_accuracy: 0.3112\n",
      "Epoch 584/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0747 - accuracy: 0.3357 - val_loss: 0.1500 - val_accuracy: 0.3291\n",
      "Epoch 585/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0762 - accuracy: 0.3201 - val_loss: 0.1275 - val_accuracy: 0.3147\n",
      "Epoch 586/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0745 - accuracy: 0.3261 - val_loss: 0.1288 - val_accuracy: 0.3040\n",
      "Epoch 587/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0747 - accuracy: 0.3309 - val_loss: 0.1276 - val_accuracy: 0.3309\n",
      "Epoch 588/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0745 - accuracy: 0.3249 - val_loss: 0.1273 - val_accuracy: 0.3381\n",
      "Epoch 589/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0745 - accuracy: 0.3369 - val_loss: 0.1277 - val_accuracy: 0.3004\n",
      "Epoch 590/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0745 - accuracy: 0.3237 - val_loss: 0.1261 - val_accuracy: 0.2932\n",
      "Epoch 591/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0744 - accuracy: 0.3177 - val_loss: 0.1258 - val_accuracy: 0.2914\n",
      "Epoch 592/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0746 - accuracy: 0.3225 - val_loss: 0.1290 - val_accuracy: 0.3381\n",
      "Epoch 593/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0741 - accuracy: 0.3297 - val_loss: 0.1293 - val_accuracy: 0.2968\n",
      "Epoch 594/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0745 - accuracy: 0.3213 - val_loss: 0.1282 - val_accuracy: 0.3183\n",
      "Epoch 595/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0741 - accuracy: 0.3285 - val_loss: 0.1391 - val_accuracy: 0.2932\n",
      "Epoch 596/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0747 - accuracy: 0.3267 - val_loss: 0.1273 - val_accuracy: 0.3219\n",
      "Epoch 597/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0741 - accuracy: 0.3243 - val_loss: 0.1414 - val_accuracy: 0.2446\n",
      "Epoch 598/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0750 - accuracy: 0.3243 - val_loss: 0.1292 - val_accuracy: 0.2896\n",
      "Epoch 599/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0742 - accuracy: 0.3369 - val_loss: 0.1273 - val_accuracy: 0.3076\n",
      "Epoch 600/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0742 - accuracy: 0.3195 - val_loss: 0.1257 - val_accuracy: 0.3273\n",
      "Epoch 601/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0739 - accuracy: 0.3273 - val_loss: 0.1247 - val_accuracy: 0.3273\n",
      "Epoch 602/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0737 - accuracy: 0.3309 - val_loss: 0.1263 - val_accuracy: 0.3076\n",
      "Epoch 603/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0737 - accuracy: 0.3261 - val_loss: 0.1289 - val_accuracy: 0.3273\n",
      "Epoch 604/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0735 - accuracy: 0.3237 - val_loss: 0.1262 - val_accuracy: 0.3219\n",
      "Epoch 605/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0735 - accuracy: 0.3291 - val_loss: 0.1251 - val_accuracy: 0.3040\n",
      "Epoch 606/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0734 - accuracy: 0.3303 - val_loss: 0.1266 - val_accuracy: 0.2914\n",
      "Epoch 607/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0737 - accuracy: 0.3213 - val_loss: 0.1278 - val_accuracy: 0.3237\n",
      "Epoch 608/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0735 - accuracy: 0.3207 - val_loss: 0.1401 - val_accuracy: 0.2536\n",
      "Epoch 609/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0744 - accuracy: 0.3183 - val_loss: 0.1286 - val_accuracy: 0.3273\n",
      "Epoch 610/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0735 - accuracy: 0.3261 - val_loss: 0.1282 - val_accuracy: 0.3165\n",
      "Epoch 611/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0735 - accuracy: 0.3321 - val_loss: 0.1342 - val_accuracy: 0.3363\n",
      "Epoch 612/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0738 - accuracy: 0.3297 - val_loss: 0.1233 - val_accuracy: 0.3022\n",
      "Epoch 613/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0730 - accuracy: 0.3225 - val_loss: 0.1297 - val_accuracy: 0.2896\n",
      "Epoch 614/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0733 - accuracy: 0.3267 - val_loss: 0.1268 - val_accuracy: 0.3201\n",
      "Epoch 615/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0729 - accuracy: 0.3273 - val_loss: 0.1248 - val_accuracy: 0.3094\n",
      "Epoch 616/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0730 - accuracy: 0.3285 - val_loss: 0.1265 - val_accuracy: 0.3058\n",
      "Epoch 617/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0730 - accuracy: 0.3237 - val_loss: 0.1281 - val_accuracy: 0.3201\n",
      "Epoch 618/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0730 - accuracy: 0.3321 - val_loss: 0.1280 - val_accuracy: 0.3291\n",
      "Epoch 619/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0730 - accuracy: 0.3351 - val_loss: 0.1334 - val_accuracy: 0.2788\n",
      "Epoch 620/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0733 - accuracy: 0.3243 - val_loss: 0.1281 - val_accuracy: 0.3381\n",
      "Epoch 621/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0729 - accuracy: 0.3315 - val_loss: 0.1349 - val_accuracy: 0.2806\n",
      "Epoch 622/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0730 - accuracy: 0.3189 - val_loss: 0.1247 - val_accuracy: 0.3165\n",
      "Epoch 623/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0727 - accuracy: 0.3309 - val_loss: 0.1253 - val_accuracy: 0.3237\n",
      "Epoch 624/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0725 - accuracy: 0.3213 - val_loss: 0.1254 - val_accuracy: 0.3129\n",
      "Epoch 625/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0725 - accuracy: 0.3225 - val_loss: 0.1259 - val_accuracy: 0.3129\n",
      "Epoch 626/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0724 - accuracy: 0.3213 - val_loss: 0.1263 - val_accuracy: 0.2788\n",
      "Epoch 627/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0726 - accuracy: 0.3225 - val_loss: 0.1268 - val_accuracy: 0.3147\n",
      "Epoch 628/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0723 - accuracy: 0.3297 - val_loss: 0.1259 - val_accuracy: 0.3291\n",
      "Epoch 629/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0723 - accuracy: 0.3321 - val_loss: 0.1274 - val_accuracy: 0.3399\n",
      "Epoch 630/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0724 - accuracy: 0.3309 - val_loss: 0.1291 - val_accuracy: 0.3471\n",
      "Epoch 631/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0721 - accuracy: 0.3285 - val_loss: 0.1248 - val_accuracy: 0.3076\n",
      "Epoch 632/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0721 - accuracy: 0.3213 - val_loss: 0.1349 - val_accuracy: 0.3453\n",
      "Epoch 633/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0725 - accuracy: 0.3345 - val_loss: 0.1283 - val_accuracy: 0.3255\n",
      "Epoch 634/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0719 - accuracy: 0.3243 - val_loss: 0.1269 - val_accuracy: 0.3363\n",
      "Epoch 635/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0718 - accuracy: 0.3267 - val_loss: 0.1259 - val_accuracy: 0.3076\n",
      "Epoch 636/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0722 - accuracy: 0.3219 - val_loss: 0.1261 - val_accuracy: 0.3345\n",
      "Epoch 637/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0720 - accuracy: 0.3387 - val_loss: 0.1253 - val_accuracy: 0.3022\n",
      "Epoch 638/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0718 - accuracy: 0.3279 - val_loss: 0.1242 - val_accuracy: 0.2770\n",
      "Epoch 639/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0718 - accuracy: 0.3249 - val_loss: 0.1258 - val_accuracy: 0.3112\n",
      "Epoch 640/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0715 - accuracy: 0.3351 - val_loss: 0.1261 - val_accuracy: 0.3022\n",
      "Epoch 641/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0718 - accuracy: 0.3279 - val_loss: 0.1303 - val_accuracy: 0.3129\n",
      "Epoch 642/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0717 - accuracy: 0.3297 - val_loss: 0.1249 - val_accuracy: 0.3094\n",
      "Epoch 643/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0714 - accuracy: 0.3267 - val_loss: 0.1282 - val_accuracy: 0.3291\n",
      "Epoch 644/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0715 - accuracy: 0.3333 - val_loss: 0.1250 - val_accuracy: 0.2878\n",
      "Epoch 645/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0717 - accuracy: 0.3333 - val_loss: 0.1290 - val_accuracy: 0.3309\n",
      "Epoch 646/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0716 - accuracy: 0.3231 - val_loss: 0.1244 - val_accuracy: 0.3129\n",
      "Epoch 647/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0714 - accuracy: 0.3357 - val_loss: 0.1288 - val_accuracy: 0.3291\n",
      "Epoch 648/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0713 - accuracy: 0.3357 - val_loss: 0.1239 - val_accuracy: 0.3094\n",
      "Epoch 649/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0711 - accuracy: 0.3399 - val_loss: 0.1253 - val_accuracy: 0.3363\n",
      "Epoch 650/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0711 - accuracy: 0.3327 - val_loss: 0.1233 - val_accuracy: 0.3129\n",
      "Epoch 651/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0711 - accuracy: 0.3207 - val_loss: 0.1287 - val_accuracy: 0.3219\n",
      "Epoch 652/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0710 - accuracy: 0.3315 - val_loss: 0.1245 - val_accuracy: 0.3237\n",
      "Epoch 653/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0709 - accuracy: 0.3351 - val_loss: 0.1260 - val_accuracy: 0.2968\n",
      "Epoch 654/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0710 - accuracy: 0.3303 - val_loss: 0.1345 - val_accuracy: 0.3273\n",
      "Epoch 655/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0712 - accuracy: 0.3285 - val_loss: 0.1301 - val_accuracy: 0.3345\n",
      "Epoch 656/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0712 - accuracy: 0.3207 - val_loss: 0.1243 - val_accuracy: 0.3237\n",
      "Epoch 657/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0709 - accuracy: 0.3213 - val_loss: 0.1280 - val_accuracy: 0.3219\n",
      "Epoch 658/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0710 - accuracy: 0.3345 - val_loss: 0.1300 - val_accuracy: 0.3399\n",
      "Epoch 659/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0709 - accuracy: 0.3267 - val_loss: 0.1233 - val_accuracy: 0.3219\n",
      "Epoch 660/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0706 - accuracy: 0.3333 - val_loss: 0.1243 - val_accuracy: 0.3291\n",
      "Epoch 661/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0706 - accuracy: 0.3315 - val_loss: 0.1442 - val_accuracy: 0.2518\n",
      "Epoch 662/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0717 - accuracy: 0.3297 - val_loss: 0.1255 - val_accuracy: 0.3237\n",
      "Epoch 663/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0704 - accuracy: 0.3303 - val_loss: 0.1251 - val_accuracy: 0.3219\n",
      "Epoch 664/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0703 - accuracy: 0.3315 - val_loss: 0.1249 - val_accuracy: 0.2788\n",
      "Epoch 665/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0707 - accuracy: 0.3297 - val_loss: 0.1233 - val_accuracy: 0.3076\n",
      "Epoch 666/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0703 - accuracy: 0.3255 - val_loss: 0.1238 - val_accuracy: 0.3129\n",
      "Epoch 667/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0702 - accuracy: 0.3315 - val_loss: 0.1255 - val_accuracy: 0.3040\n",
      "Epoch 668/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0704 - accuracy: 0.3261 - val_loss: 0.1285 - val_accuracy: 0.3381\n",
      "Epoch 669/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0703 - accuracy: 0.3315 - val_loss: 0.1252 - val_accuracy: 0.3076\n",
      "Epoch 670/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0701 - accuracy: 0.3351 - val_loss: 0.1264 - val_accuracy: 0.3327\n",
      "Epoch 671/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0701 - accuracy: 0.3243 - val_loss: 0.1248 - val_accuracy: 0.2878\n",
      "Epoch 672/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0702 - accuracy: 0.3231 - val_loss: 0.1239 - val_accuracy: 0.3165\n",
      "Epoch 673/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0699 - accuracy: 0.3303 - val_loss: 0.1287 - val_accuracy: 0.3363\n",
      "Epoch 674/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0701 - accuracy: 0.3225 - val_loss: 0.1319 - val_accuracy: 0.2662\n",
      "Epoch 675/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0706 - accuracy: 0.3207 - val_loss: 0.1414 - val_accuracy: 0.2410\n",
      "Epoch 676/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0712 - accuracy: 0.3201 - val_loss: 0.1247 - val_accuracy: 0.3435\n",
      "Epoch 677/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0697 - accuracy: 0.3237 - val_loss: 0.1260 - val_accuracy: 0.3471\n",
      "Epoch 678/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0699 - accuracy: 0.3207 - val_loss: 0.1238 - val_accuracy: 0.3291\n",
      "Epoch 679/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0697 - accuracy: 0.3267 - val_loss: 0.1264 - val_accuracy: 0.3094\n",
      "Epoch 680/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0699 - accuracy: 0.3213 - val_loss: 0.1237 - val_accuracy: 0.2842\n",
      "Epoch 681/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0696 - accuracy: 0.3225 - val_loss: 0.1244 - val_accuracy: 0.3327\n",
      "Epoch 682/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0696 - accuracy: 0.3315 - val_loss: 0.1235 - val_accuracy: 0.3273\n",
      "Epoch 683/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0694 - accuracy: 0.3189 - val_loss: 0.1256 - val_accuracy: 0.3363\n",
      "Epoch 684/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0694 - accuracy: 0.3327 - val_loss: 0.1283 - val_accuracy: 0.3147\n",
      "Epoch 685/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0697 - accuracy: 0.3279 - val_loss: 0.1255 - val_accuracy: 0.3255\n",
      "Epoch 686/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0694 - accuracy: 0.3291 - val_loss: 0.1274 - val_accuracy: 0.3381\n",
      "Epoch 687/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0694 - accuracy: 0.3327 - val_loss: 0.1239 - val_accuracy: 0.3219\n",
      "Epoch 688/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0691 - accuracy: 0.3285 - val_loss: 0.1234 - val_accuracy: 0.3345\n",
      "Epoch 689/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0691 - accuracy: 0.3351 - val_loss: 0.1245 - val_accuracy: 0.3435\n",
      "Epoch 690/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0692 - accuracy: 0.3315 - val_loss: 0.1234 - val_accuracy: 0.3237\n",
      "Epoch 691/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0691 - accuracy: 0.3267 - val_loss: 0.1252 - val_accuracy: 0.3219\n",
      "Epoch 692/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0690 - accuracy: 0.3267 - val_loss: 0.1238 - val_accuracy: 0.3255\n",
      "Epoch 693/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0692 - accuracy: 0.3255 - val_loss: 0.1457 - val_accuracy: 0.2356\n",
      "Epoch 694/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0704 - accuracy: 0.3291 - val_loss: 0.1261 - val_accuracy: 0.3076\n",
      "Epoch 695/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0690 - accuracy: 0.3333 - val_loss: 0.1236 - val_accuracy: 0.3183\n",
      "Epoch 696/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0688 - accuracy: 0.3273 - val_loss: 0.1243 - val_accuracy: 0.3291\n",
      "Epoch 697/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0687 - accuracy: 0.3303 - val_loss: 0.1239 - val_accuracy: 0.3129\n",
      "Epoch 698/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0687 - accuracy: 0.3387 - val_loss: 0.1254 - val_accuracy: 0.3363\n",
      "Epoch 699/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0687 - accuracy: 0.3351 - val_loss: 0.1260 - val_accuracy: 0.2878\n",
      "Epoch 700/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0689 - accuracy: 0.3231 - val_loss: 0.1274 - val_accuracy: 0.3219\n",
      "Epoch 701/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0688 - accuracy: 0.3291 - val_loss: 0.1257 - val_accuracy: 0.3471\n",
      "Epoch 702/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0685 - accuracy: 0.3279 - val_loss: 0.1237 - val_accuracy: 0.3147\n",
      "Epoch 703/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0684 - accuracy: 0.3309 - val_loss: 0.1253 - val_accuracy: 0.3129\n",
      "Epoch 704/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0686 - accuracy: 0.3333 - val_loss: 0.1249 - val_accuracy: 0.3076\n",
      "Epoch 705/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0685 - accuracy: 0.3255 - val_loss: 0.1262 - val_accuracy: 0.3076\n",
      "Epoch 706/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0685 - accuracy: 0.3231 - val_loss: 0.1231 - val_accuracy: 0.3147\n",
      "Epoch 707/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0684 - accuracy: 0.3303 - val_loss: 0.1219 - val_accuracy: 0.3273\n",
      "Epoch 708/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0682 - accuracy: 0.3285 - val_loss: 0.1310 - val_accuracy: 0.3345\n",
      "Epoch 709/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0688 - accuracy: 0.3321 - val_loss: 0.1254 - val_accuracy: 0.3309\n",
      "Epoch 710/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0684 - accuracy: 0.3297 - val_loss: 0.1226 - val_accuracy: 0.3201\n",
      "Epoch 711/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0680 - accuracy: 0.3297 - val_loss: 0.1429 - val_accuracy: 0.3399\n",
      "Epoch 712/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0691 - accuracy: 0.3285 - val_loss: 0.1257 - val_accuracy: 0.3183\n",
      "Epoch 713/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0684 - accuracy: 0.3195 - val_loss: 0.1286 - val_accuracy: 0.3076\n",
      "Epoch 714/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0684 - accuracy: 0.3405 - val_loss: 0.1225 - val_accuracy: 0.3183\n",
      "Epoch 715/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0678 - accuracy: 0.3303 - val_loss: 0.1281 - val_accuracy: 0.3381\n",
      "Epoch 716/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0683 - accuracy: 0.3357 - val_loss: 0.1230 - val_accuracy: 0.3291\n",
      "Epoch 717/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0678 - accuracy: 0.3303 - val_loss: 0.1264 - val_accuracy: 0.3255\n",
      "Epoch 718/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0679 - accuracy: 0.3291 - val_loss: 0.1267 - val_accuracy: 0.3004\n",
      "Epoch 719/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0683 - accuracy: 0.3273 - val_loss: 0.1255 - val_accuracy: 0.3363\n",
      "Epoch 720/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0676 - accuracy: 0.3333 - val_loss: 0.1247 - val_accuracy: 0.3489\n",
      "Epoch 721/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0679 - accuracy: 0.3357 - val_loss: 0.1229 - val_accuracy: 0.3004\n",
      "Epoch 722/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0680 - accuracy: 0.3333 - val_loss: 0.1215 - val_accuracy: 0.3363\n",
      "Epoch 723/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0676 - accuracy: 0.3303 - val_loss: 0.1246 - val_accuracy: 0.3219\n",
      "Epoch 724/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0676 - accuracy: 0.3321 - val_loss: 0.1219 - val_accuracy: 0.3094\n",
      "Epoch 725/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0676 - accuracy: 0.3279 - val_loss: 0.1238 - val_accuracy: 0.3201\n",
      "Epoch 726/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0675 - accuracy: 0.3285 - val_loss: 0.1220 - val_accuracy: 0.3022\n",
      "Epoch 727/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0676 - accuracy: 0.3273 - val_loss: 0.1227 - val_accuracy: 0.3237\n",
      "Epoch 728/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0674 - accuracy: 0.3297 - val_loss: 0.1253 - val_accuracy: 0.3309\n",
      "Epoch 729/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0675 - accuracy: 0.3279 - val_loss: 0.1233 - val_accuracy: 0.3273\n",
      "Epoch 730/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0672 - accuracy: 0.3363 - val_loss: 0.1289 - val_accuracy: 0.3273\n",
      "Epoch 731/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0676 - accuracy: 0.3333 - val_loss: 0.1330 - val_accuracy: 0.3309\n",
      "Epoch 732/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0680 - accuracy: 0.3405 - val_loss: 0.1279 - val_accuracy: 0.2968\n",
      "Epoch 733/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0675 - accuracy: 0.3255 - val_loss: 0.1230 - val_accuracy: 0.3273\n",
      "Epoch 734/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0670 - accuracy: 0.3333 - val_loss: 0.1465 - val_accuracy: 0.3004\n",
      "Epoch 735/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0680 - accuracy: 0.3279 - val_loss: 0.1216 - val_accuracy: 0.3273\n",
      "Epoch 736/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0671 - accuracy: 0.3339 - val_loss: 0.1233 - val_accuracy: 0.3255\n",
      "Epoch 737/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0668 - accuracy: 0.3315 - val_loss: 0.1244 - val_accuracy: 0.3327\n",
      "Epoch 738/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0670 - accuracy: 0.3303 - val_loss: 0.1249 - val_accuracy: 0.3291\n",
      "Epoch 739/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0668 - accuracy: 0.3303 - val_loss: 0.1241 - val_accuracy: 0.3309\n",
      "Epoch 740/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0669 - accuracy: 0.3351 - val_loss: 0.1418 - val_accuracy: 0.3489\n",
      "Epoch 741/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0678 - accuracy: 0.3321 - val_loss: 0.1269 - val_accuracy: 0.3237\n",
      "Epoch 742/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0670 - accuracy: 0.3333 - val_loss: 0.1233 - val_accuracy: 0.3381\n",
      "Epoch 743/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0666 - accuracy: 0.3261 - val_loss: 0.1235 - val_accuracy: 0.3255\n",
      "Epoch 744/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0666 - accuracy: 0.3273 - val_loss: 0.1216 - val_accuracy: 0.3094\n",
      "Epoch 745/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0667 - accuracy: 0.3357 - val_loss: 0.1255 - val_accuracy: 0.3309\n",
      "Epoch 746/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0667 - accuracy: 0.3345 - val_loss: 0.1264 - val_accuracy: 0.3040\n",
      "Epoch 747/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0667 - accuracy: 0.3327 - val_loss: 0.1261 - val_accuracy: 0.3022\n",
      "Epoch 748/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0666 - accuracy: 0.3423 - val_loss: 0.1231 - val_accuracy: 0.3112\n",
      "Epoch 749/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0668 - accuracy: 0.3345 - val_loss: 0.1538 - val_accuracy: 0.2716\n",
      "Epoch 750/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0683 - accuracy: 0.3363 - val_loss: 0.1253 - val_accuracy: 0.3345\n",
      "Epoch 751/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0665 - accuracy: 0.3327 - val_loss: 0.1204 - val_accuracy: 0.3219\n",
      "Epoch 752/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0663 - accuracy: 0.3273 - val_loss: 0.1225 - val_accuracy: 0.3309\n",
      "Epoch 753/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0662 - accuracy: 0.3345 - val_loss: 0.1220 - val_accuracy: 0.3363\n",
      "Epoch 754/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0663 - accuracy: 0.3363 - val_loss: 0.1231 - val_accuracy: 0.3237\n",
      "Epoch 755/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0661 - accuracy: 0.3375 - val_loss: 0.1451 - val_accuracy: 0.2734\n",
      "Epoch 756/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0675 - accuracy: 0.3273 - val_loss: 0.1236 - val_accuracy: 0.3219\n",
      "Epoch 757/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0663 - accuracy: 0.3303 - val_loss: 0.1269 - val_accuracy: 0.3543\n",
      "Epoch 758/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0661 - accuracy: 0.3357 - val_loss: 0.1234 - val_accuracy: 0.3076\n",
      "Epoch 759/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0661 - accuracy: 0.3303 - val_loss: 0.1220 - val_accuracy: 0.3417\n",
      "Epoch 760/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0661 - accuracy: 0.3315 - val_loss: 0.1212 - val_accuracy: 0.3165\n",
      "Epoch 761/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0658 - accuracy: 0.3363 - val_loss: 0.1226 - val_accuracy: 0.3417\n",
      "Epoch 762/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0657 - accuracy: 0.3255 - val_loss: 0.1208 - val_accuracy: 0.3201\n",
      "Epoch 763/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0657 - accuracy: 0.3375 - val_loss: 0.1235 - val_accuracy: 0.3219\n",
      "Epoch 764/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0659 - accuracy: 0.3249 - val_loss: 0.1327 - val_accuracy: 0.3183\n",
      "Epoch 765/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0664 - accuracy: 0.3243 - val_loss: 0.1224 - val_accuracy: 0.3129\n",
      "Epoch 766/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0657 - accuracy: 0.3309 - val_loss: 0.1210 - val_accuracy: 0.3237\n",
      "Epoch 767/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0657 - accuracy: 0.3249 - val_loss: 0.1233 - val_accuracy: 0.3112\n",
      "Epoch 768/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0655 - accuracy: 0.3303 - val_loss: 0.1233 - val_accuracy: 0.3561\n",
      "Epoch 769/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0656 - accuracy: 0.3381 - val_loss: 0.1227 - val_accuracy: 0.3381\n",
      "Epoch 770/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0656 - accuracy: 0.3351 - val_loss: 0.1243 - val_accuracy: 0.3183\n",
      "Epoch 771/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0654 - accuracy: 0.3279 - val_loss: 0.1254 - val_accuracy: 0.3363\n",
      "Epoch 772/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0655 - accuracy: 0.3345 - val_loss: 0.1229 - val_accuracy: 0.3094\n",
      "Epoch 773/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0659 - accuracy: 0.3363 - val_loss: 0.1288 - val_accuracy: 0.3237\n",
      "Epoch 774/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0660 - accuracy: 0.3351 - val_loss: 0.1258 - val_accuracy: 0.3183\n",
      "Epoch 775/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0657 - accuracy: 0.3309 - val_loss: 0.1248 - val_accuracy: 0.3345\n",
      "Epoch 776/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0654 - accuracy: 0.3261 - val_loss: 0.1243 - val_accuracy: 0.3363\n",
      "Epoch 777/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0653 - accuracy: 0.3303 - val_loss: 0.1208 - val_accuracy: 0.3237\n",
      "Epoch 778/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0651 - accuracy: 0.3291 - val_loss: 0.1216 - val_accuracy: 0.3219\n",
      "Epoch 779/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0652 - accuracy: 0.3327 - val_loss: 0.1232 - val_accuracy: 0.3381\n",
      "Epoch 780/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0651 - accuracy: 0.3351 - val_loss: 0.1239 - val_accuracy: 0.3273\n",
      "Epoch 781/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0651 - accuracy: 0.3351 - val_loss: 0.1221 - val_accuracy: 0.3255\n",
      "Epoch 782/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0651 - accuracy: 0.3321 - val_loss: 0.1213 - val_accuracy: 0.3147\n",
      "Epoch 783/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0648 - accuracy: 0.3363 - val_loss: 0.1224 - val_accuracy: 0.3291\n",
      "Epoch 784/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0648 - accuracy: 0.3297 - val_loss: 0.1279 - val_accuracy: 0.3183\n",
      "Epoch 785/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0653 - accuracy: 0.3363 - val_loss: 0.1265 - val_accuracy: 0.3453\n",
      "Epoch 786/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0650 - accuracy: 0.3327 - val_loss: 0.1232 - val_accuracy: 0.3237\n",
      "Epoch 787/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0648 - accuracy: 0.3381 - val_loss: 0.1218 - val_accuracy: 0.3327\n",
      "Epoch 788/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0648 - accuracy: 0.3339 - val_loss: 0.1221 - val_accuracy: 0.3363\n",
      "Epoch 789/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0648 - accuracy: 0.3327 - val_loss: 0.1268 - val_accuracy: 0.3345\n",
      "Epoch 790/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0647 - accuracy: 0.3243 - val_loss: 0.1275 - val_accuracy: 0.3507\n",
      "Epoch 791/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0647 - accuracy: 0.3369 - val_loss: 0.1233 - val_accuracy: 0.3219\n",
      "Epoch 792/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0646 - accuracy: 0.3297 - val_loss: 0.1242 - val_accuracy: 0.3129\n",
      "Epoch 793/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0646 - accuracy: 0.3297 - val_loss: 0.1236 - val_accuracy: 0.3399\n",
      "Epoch 794/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0646 - accuracy: 0.3321 - val_loss: 0.1253 - val_accuracy: 0.3399\n",
      "Epoch 795/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.0646 - accuracy: 0.3291 - val_loss: 0.1266 - val_accuracy: 0.3183\n",
      "Epoch 796/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0647 - accuracy: 0.3303 - val_loss: 0.1231 - val_accuracy: 0.3273\n",
      "Epoch 797/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0644 - accuracy: 0.3327 - val_loss: 0.1256 - val_accuracy: 0.3309\n",
      "Epoch 798/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0643 - accuracy: 0.3303 - val_loss: 0.1213 - val_accuracy: 0.3255\n",
      "Epoch 799/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0642 - accuracy: 0.3249 - val_loss: 0.1224 - val_accuracy: 0.3183\n",
      "Epoch 800/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0643 - accuracy: 0.3285 - val_loss: 0.1218 - val_accuracy: 0.3255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-5f8a8e11cd4a>:128: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "  tprs.append(interp(mean_fpr, fpr, tpr))\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Epoch 1/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.6570 - accuracy: 0.0054 - val_loss: 0.6139 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.4327 - accuracy: 0.0060 - val_loss: 0.2712 - val_accuracy: 0.0072\n",
      "Epoch 3/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1952 - accuracy: 0.0246 - val_loss: 0.2531 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1886 - accuracy: 0.0186 - val_loss: 0.2519 - val_accuracy: 0.0450\n",
      "Epoch 5/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1877 - accuracy: 0.0402 - val_loss: 0.2522 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1874 - accuracy: 0.0138 - val_loss: 0.2520 - val_accuracy: 0.0324\n",
      "Epoch 7/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1872 - accuracy: 0.0300 - val_loss: 0.2517 - val_accuracy: 0.0450\n",
      "Epoch 8/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1871 - accuracy: 0.0414 - val_loss: 0.2511 - val_accuracy: 0.0468\n",
      "Epoch 9/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1869 - accuracy: 0.0492 - val_loss: 0.2513 - val_accuracy: 0.0306\n",
      "Epoch 10/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1868 - accuracy: 0.0438 - val_loss: 0.2513 - val_accuracy: 0.0468\n",
      "Epoch 11/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1867 - accuracy: 0.0450 - val_loss: 0.2511 - val_accuracy: 0.0468\n",
      "Epoch 12/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1866 - accuracy: 0.0498 - val_loss: 0.2504 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1865 - accuracy: 0.0174 - val_loss: 0.2509 - val_accuracy: 0.0288\n",
      "Epoch 14/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1864 - accuracy: 0.0432 - val_loss: 0.2508 - val_accuracy: 0.0270\n",
      "Epoch 15/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1863 - accuracy: 0.0234 - val_loss: 0.2507 - val_accuracy: 0.0288\n",
      "Epoch 16/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1863 - accuracy: 0.0306 - val_loss: 0.2503 - val_accuracy: 0.0629\n",
      "Epoch 17/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1861 - accuracy: 0.0529 - val_loss: 0.2497 - val_accuracy: 0.0558\n",
      "Epoch 18/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1860 - accuracy: 0.0547 - val_loss: 0.2496 - val_accuracy: 0.0719\n",
      "Epoch 19/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1860 - accuracy: 0.0408 - val_loss: 0.2496 - val_accuracy: 0.0342\n",
      "Epoch 20/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1858 - accuracy: 0.0372 - val_loss: 0.2494 - val_accuracy: 0.0701\n",
      "Epoch 21/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1857 - accuracy: 0.0583 - val_loss: 0.2495 - val_accuracy: 0.0234\n",
      "Epoch 22/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1857 - accuracy: 0.0192 - val_loss: 0.2495 - val_accuracy: 0.0450\n",
      "Epoch 23/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1856 - accuracy: 0.0360 - val_loss: 0.2489 - val_accuracy: 0.0647\n",
      "Epoch 24/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1854 - accuracy: 0.0444 - val_loss: 0.2490 - val_accuracy: 0.0540\n",
      "Epoch 25/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1854 - accuracy: 0.0505 - val_loss: 0.2490 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1853 - accuracy: 0.0174 - val_loss: 0.2492 - val_accuracy: 0.0719\n",
      "Epoch 27/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1852 - accuracy: 0.0535 - val_loss: 0.2488 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1852 - accuracy: 0.0174 - val_loss: 0.2490 - val_accuracy: 0.0234\n",
      "Epoch 29/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1851 - accuracy: 0.0216 - val_loss: 0.2486 - val_accuracy: 0.0522\n",
      "Epoch 30/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1850 - accuracy: 0.0420 - val_loss: 0.2487 - val_accuracy: 0.0378\n",
      "Epoch 31/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1849 - accuracy: 0.0492 - val_loss: 0.2484 - val_accuracy: 0.0504\n",
      "Epoch 32/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1848 - accuracy: 0.0613 - val_loss: 0.2485 - val_accuracy: 0.0468\n",
      "Epoch 33/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1847 - accuracy: 0.0312 - val_loss: 0.2478 - val_accuracy: 0.0144\n",
      "Epoch 34/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1846 - accuracy: 0.0480 - val_loss: 0.2471 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1845 - accuracy: 0.0258 - val_loss: 0.2469 - val_accuracy: 0.0522\n",
      "Epoch 36/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1845 - accuracy: 0.0541 - val_loss: 0.2472 - val_accuracy: 0.0198\n",
      "Epoch 37/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1843 - accuracy: 0.0565 - val_loss: 0.2475 - val_accuracy: 0.0594\n",
      "Epoch 38/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1842 - accuracy: 0.0583 - val_loss: 0.2472 - val_accuracy: 0.0450\n",
      "Epoch 39/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1841 - accuracy: 0.0456 - val_loss: 0.2470 - val_accuracy: 0.0827\n",
      "Epoch 40/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1840 - accuracy: 0.0691 - val_loss: 0.2470 - val_accuracy: 0.0378\n",
      "Epoch 41/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1839 - accuracy: 0.0667 - val_loss: 0.2465 - val_accuracy: 0.0288\n",
      "Epoch 42/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1838 - accuracy: 0.0625 - val_loss: 0.2461 - val_accuracy: 0.0270\n",
      "Epoch 43/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1837 - accuracy: 0.0282 - val_loss: 0.2463 - val_accuracy: 0.0647\n",
      "Epoch 44/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1836 - accuracy: 0.0727 - val_loss: 0.2462 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1835 - accuracy: 0.0396 - val_loss: 0.2463 - val_accuracy: 0.0090\n",
      "Epoch 46/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1833 - accuracy: 0.0456 - val_loss: 0.2457 - val_accuracy: 0.0018\n",
      "Epoch 47/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1832 - accuracy: 0.0547 - val_loss: 0.2456 - val_accuracy: 0.0414\n",
      "Epoch 48/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1831 - accuracy: 0.0444 - val_loss: 0.2452 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1830 - accuracy: 0.0342 - val_loss: 0.2448 - val_accuracy: 0.0198\n",
      "Epoch 50/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1829 - accuracy: 0.0372 - val_loss: 0.2448 - val_accuracy: 0.0396\n",
      "Epoch 51/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1828 - accuracy: 0.0571 - val_loss: 0.2443 - val_accuracy: 0.0576\n",
      "Epoch 52/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1826 - accuracy: 0.0781 - val_loss: 0.2448 - val_accuracy: 0.0450\n",
      "Epoch 53/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1824 - accuracy: 0.0763 - val_loss: 0.2443 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1823 - accuracy: 0.0288 - val_loss: 0.2443 - val_accuracy: 0.0306\n",
      "Epoch 55/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1821 - accuracy: 0.0631 - val_loss: 0.2440 - val_accuracy: 0.0324\n",
      "Epoch 56/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1820 - accuracy: 0.0541 - val_loss: 0.2442 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1818 - accuracy: 0.0216 - val_loss: 0.2436 - val_accuracy: 0.0791\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1816 - accuracy: 0.0733 - val_loss: 0.2429 - val_accuracy: 0.0809\n",
      "Epoch 59/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1815 - accuracy: 0.0793 - val_loss: 0.2424 - val_accuracy: 0.0881\n",
      "Epoch 60/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1813 - accuracy: 0.0823 - val_loss: 0.2429 - val_accuracy: 0.0737\n",
      "Epoch 61/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1811 - accuracy: 0.0739 - val_loss: 0.2421 - val_accuracy: 0.0234\n",
      "Epoch 62/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1809 - accuracy: 0.0805 - val_loss: 0.2420 - val_accuracy: 0.0234\n",
      "Epoch 63/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1807 - accuracy: 0.0751 - val_loss: 0.2416 - val_accuracy: 0.0252\n",
      "Epoch 64/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1805 - accuracy: 0.0517 - val_loss: 0.2411 - val_accuracy: 0.1133\n",
      "Epoch 65/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1803 - accuracy: 0.1201 - val_loss: 0.2410 - val_accuracy: 0.0863\n",
      "Epoch 66/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1802 - accuracy: 0.0943 - val_loss: 0.2405 - val_accuracy: 0.0665\n",
      "Epoch 67/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1799 - accuracy: 0.1009 - val_loss: 0.2408 - val_accuracy: 0.0809\n",
      "Epoch 68/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1796 - accuracy: 0.0889 - val_loss: 0.2401 - val_accuracy: 0.0558\n",
      "Epoch 69/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1794 - accuracy: 0.0931 - val_loss: 0.2394 - val_accuracy: 0.1169\n",
      "Epoch 70/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1791 - accuracy: 0.1057 - val_loss: 0.2391 - val_accuracy: 0.0845\n",
      "Epoch 71/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1789 - accuracy: 0.0931 - val_loss: 0.2385 - val_accuracy: 0.0288\n",
      "Epoch 72/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1786 - accuracy: 0.0793 - val_loss: 0.2386 - val_accuracy: 0.0468\n",
      "Epoch 73/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1783 - accuracy: 0.0793 - val_loss: 0.2384 - val_accuracy: 0.0917\n",
      "Epoch 74/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1780 - accuracy: 0.0865 - val_loss: 0.2377 - val_accuracy: 0.0719\n",
      "Epoch 75/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1777 - accuracy: 0.1117 - val_loss: 0.2367 - val_accuracy: 0.0935\n",
      "Epoch 76/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1774 - accuracy: 0.1171 - val_loss: 0.2363 - val_accuracy: 0.0522\n",
      "Epoch 77/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1770 - accuracy: 0.0925 - val_loss: 0.2361 - val_accuracy: 0.1025\n",
      "Epoch 78/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1767 - accuracy: 0.1087 - val_loss: 0.2357 - val_accuracy: 0.1241\n",
      "Epoch 79/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1763 - accuracy: 0.1183 - val_loss: 0.2348 - val_accuracy: 0.1169\n",
      "Epoch 80/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1760 - accuracy: 0.1363 - val_loss: 0.2346 - val_accuracy: 0.1331\n",
      "Epoch 81/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1756 - accuracy: 0.1249 - val_loss: 0.2340 - val_accuracy: 0.0773\n",
      "Epoch 82/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1752 - accuracy: 0.1159 - val_loss: 0.2334 - val_accuracy: 0.0450\n",
      "Epoch 83/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1749 - accuracy: 0.1087 - val_loss: 0.2332 - val_accuracy: 0.1169\n",
      "Epoch 84/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1745 - accuracy: 0.1069 - val_loss: 0.2325 - val_accuracy: 0.1025\n",
      "Epoch 85/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1741 - accuracy: 0.1219 - val_loss: 0.2317 - val_accuracy: 0.0665\n",
      "Epoch 86/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1736 - accuracy: 0.0955 - val_loss: 0.2313 - val_accuracy: 0.0881\n",
      "Epoch 87/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1732 - accuracy: 0.1135 - val_loss: 0.2303 - val_accuracy: 0.0773\n",
      "Epoch 88/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1727 - accuracy: 0.1339 - val_loss: 0.2304 - val_accuracy: 0.1223\n",
      "Epoch 89/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1723 - accuracy: 0.1321 - val_loss: 0.2288 - val_accuracy: 0.0540\n",
      "Epoch 90/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1718 - accuracy: 0.1231 - val_loss: 0.2287 - val_accuracy: 0.0342\n",
      "Epoch 91/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1714 - accuracy: 0.1057 - val_loss: 0.2282 - val_accuracy: 0.1277\n",
      "Epoch 92/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1709 - accuracy: 0.1327 - val_loss: 0.2270 - val_accuracy: 0.0809\n",
      "Epoch 93/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1704 - accuracy: 0.1225 - val_loss: 0.2268 - val_accuracy: 0.1151\n",
      "Epoch 94/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1699 - accuracy: 0.1249 - val_loss: 0.2261 - val_accuracy: 0.1223\n",
      "Epoch 95/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1694 - accuracy: 0.1267 - val_loss: 0.2242 - val_accuracy: 0.1061\n",
      "Epoch 96/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1689 - accuracy: 0.1417 - val_loss: 0.2238 - val_accuracy: 0.1187\n",
      "Epoch 97/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1684 - accuracy: 0.1345 - val_loss: 0.2240 - val_accuracy: 0.1403\n",
      "Epoch 98/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1679 - accuracy: 0.1405 - val_loss: 0.2228 - val_accuracy: 0.1439\n",
      "Epoch 99/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1674 - accuracy: 0.1339 - val_loss: 0.2226 - val_accuracy: 0.0971\n",
      "Epoch 100/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1669 - accuracy: 0.1183 - val_loss: 0.2208 - val_accuracy: 0.0953\n",
      "Epoch 101/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1664 - accuracy: 0.1321 - val_loss: 0.2205 - val_accuracy: 0.1403\n",
      "Epoch 102/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1658 - accuracy: 0.1526 - val_loss: 0.2205 - val_accuracy: 0.1187\n",
      "Epoch 103/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1653 - accuracy: 0.1285 - val_loss: 0.2200 - val_accuracy: 0.1043\n",
      "Epoch 104/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1647 - accuracy: 0.1303 - val_loss: 0.2187 - val_accuracy: 0.1529\n",
      "Epoch 105/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1642 - accuracy: 0.1465 - val_loss: 0.2182 - val_accuracy: 0.1007\n",
      "Epoch 106/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1637 - accuracy: 0.1261 - val_loss: 0.2173 - val_accuracy: 0.0971\n",
      "Epoch 107/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1631 - accuracy: 0.1381 - val_loss: 0.2163 - val_accuracy: 0.1385\n",
      "Epoch 108/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1626 - accuracy: 0.1357 - val_loss: 0.2151 - val_accuracy: 0.1457\n",
      "Epoch 109/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1620 - accuracy: 0.1526 - val_loss: 0.2153 - val_accuracy: 0.1259\n",
      "Epoch 110/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1614 - accuracy: 0.1453 - val_loss: 0.2137 - val_accuracy: 0.1547\n",
      "Epoch 111/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1608 - accuracy: 0.1604 - val_loss: 0.2133 - val_accuracy: 0.1493\n",
      "Epoch 112/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1603 - accuracy: 0.1514 - val_loss: 0.2120 - val_accuracy: 0.1511\n",
      "Epoch 113/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1597 - accuracy: 0.1417 - val_loss: 0.2110 - val_accuracy: 0.1169\n",
      "Epoch 114/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1591 - accuracy: 0.1489 - val_loss: 0.2106 - val_accuracy: 0.1475\n",
      "Epoch 115/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1585 - accuracy: 0.1483 - val_loss: 0.2097 - val_accuracy: 0.1475\n",
      "Epoch 116/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1579 - accuracy: 0.1423 - val_loss: 0.2079 - val_accuracy: 0.1493\n",
      "Epoch 117/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1573 - accuracy: 0.1465 - val_loss: 0.2092 - val_accuracy: 0.1493\n",
      "Epoch 118/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1569 - accuracy: 0.1381 - val_loss: 0.2083 - val_accuracy: 0.1403\n",
      "Epoch 119/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1561 - accuracy: 0.1477 - val_loss: 0.2070 - val_accuracy: 0.1583\n",
      "Epoch 120/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1556 - accuracy: 0.1544 - val_loss: 0.2059 - val_accuracy: 0.1835\n",
      "Epoch 121/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1550 - accuracy: 0.1495 - val_loss: 0.2053 - val_accuracy: 0.1871\n",
      "Epoch 122/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1545 - accuracy: 0.1610 - val_loss: 0.2043 - val_accuracy: 0.1421\n",
      "Epoch 123/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1539 - accuracy: 0.1502 - val_loss: 0.2054 - val_accuracy: 0.1133\n",
      "Epoch 124/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1535 - accuracy: 0.1489 - val_loss: 0.2027 - val_accuracy: 0.1349\n",
      "Epoch 125/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1529 - accuracy: 0.1502 - val_loss: 0.2035 - val_accuracy: 0.1295\n",
      "Epoch 126/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1524 - accuracy: 0.1604 - val_loss: 0.2017 - val_accuracy: 0.1457\n",
      "Epoch 127/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1517 - accuracy: 0.1622 - val_loss: 0.2013 - val_accuracy: 0.1727\n",
      "Epoch 128/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1513 - accuracy: 0.1658 - val_loss: 0.1999 - val_accuracy: 0.1565\n",
      "Epoch 129/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1508 - accuracy: 0.1598 - val_loss: 0.2004 - val_accuracy: 0.1655\n",
      "Epoch 130/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1503 - accuracy: 0.1652 - val_loss: 0.1989 - val_accuracy: 0.1637\n",
      "Epoch 131/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1498 - accuracy: 0.1634 - val_loss: 0.1996 - val_accuracy: 0.1763\n",
      "Epoch 132/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1493 - accuracy: 0.1670 - val_loss: 0.1975 - val_accuracy: 0.1799\n",
      "Epoch 133/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1488 - accuracy: 0.1652 - val_loss: 0.1979 - val_accuracy: 0.1871\n",
      "Epoch 134/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1484 - accuracy: 0.1784 - val_loss: 0.1973 - val_accuracy: 0.1906\n",
      "Epoch 135/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1478 - accuracy: 0.1706 - val_loss: 0.1965 - val_accuracy: 0.1637\n",
      "Epoch 136/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1475 - accuracy: 0.1676 - val_loss: 0.1964 - val_accuracy: 0.1924\n",
      "Epoch 137/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1470 - accuracy: 0.1730 - val_loss: 0.1960 - val_accuracy: 0.1745\n",
      "Epoch 138/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1466 - accuracy: 0.1598 - val_loss: 0.1950 - val_accuracy: 0.1888\n",
      "Epoch 139/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1461 - accuracy: 0.1748 - val_loss: 0.1951 - val_accuracy: 0.1960\n",
      "Epoch 140/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1458 - accuracy: 0.1820 - val_loss: 0.1943 - val_accuracy: 0.2032\n",
      "Epoch 141/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1453 - accuracy: 0.1784 - val_loss: 0.1945 - val_accuracy: 0.1709\n",
      "Epoch 142/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1450 - accuracy: 0.1730 - val_loss: 0.1936 - val_accuracy: 0.1978\n",
      "Epoch 143/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1446 - accuracy: 0.1778 - val_loss: 0.1929 - val_accuracy: 0.1978\n",
      "Epoch 144/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1442 - accuracy: 0.1784 - val_loss: 0.1925 - val_accuracy: 0.1942\n",
      "Epoch 145/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1438 - accuracy: 0.1796 - val_loss: 0.1918 - val_accuracy: 0.1619\n",
      "Epoch 146/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1435 - accuracy: 0.1820 - val_loss: 0.1914 - val_accuracy: 0.2122\n",
      "Epoch 147/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1431 - accuracy: 0.1862 - val_loss: 0.1914 - val_accuracy: 0.2158\n",
      "Epoch 148/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1428 - accuracy: 0.1826 - val_loss: 0.1906 - val_accuracy: 0.1799\n",
      "Epoch 149/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1424 - accuracy: 0.1880 - val_loss: 0.1908 - val_accuracy: 0.2302\n",
      "Epoch 150/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1421 - accuracy: 0.1910 - val_loss: 0.1910 - val_accuracy: 0.1709\n",
      "Epoch 151/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1419 - accuracy: 0.1820 - val_loss: 0.1898 - val_accuracy: 0.2230\n",
      "Epoch 152/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1414 - accuracy: 0.1910 - val_loss: 0.1887 - val_accuracy: 0.1996\n",
      "Epoch 153/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1410 - accuracy: 0.1898 - val_loss: 0.1889 - val_accuracy: 0.1996\n",
      "Epoch 154/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1408 - accuracy: 0.1922 - val_loss: 0.1888 - val_accuracy: 0.2302\n",
      "Epoch 155/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1404 - accuracy: 0.2024 - val_loss: 0.1916 - val_accuracy: 0.1906\n",
      "Epoch 156/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1403 - accuracy: 0.1838 - val_loss: 0.1878 - val_accuracy: 0.2104\n",
      "Epoch 157/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1398 - accuracy: 0.1946 - val_loss: 0.1881 - val_accuracy: 0.2356\n",
      "Epoch 158/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1395 - accuracy: 0.2048 - val_loss: 0.1868 - val_accuracy: 0.2140\n",
      "Epoch 159/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1392 - accuracy: 0.1982 - val_loss: 0.1875 - val_accuracy: 0.1978\n",
      "Epoch 160/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1389 - accuracy: 0.1982 - val_loss: 0.1858 - val_accuracy: 0.1978\n",
      "Epoch 161/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1386 - accuracy: 0.1928 - val_loss: 0.1858 - val_accuracy: 0.2194\n",
      "Epoch 162/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1382 - accuracy: 0.1898 - val_loss: 0.1868 - val_accuracy: 0.1978\n",
      "Epoch 163/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1379 - accuracy: 0.1910 - val_loss: 0.1870 - val_accuracy: 0.2176\n",
      "Epoch 164/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1376 - accuracy: 0.2012 - val_loss: 0.1849 - val_accuracy: 0.2158\n",
      "Epoch 165/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1373 - accuracy: 0.1964 - val_loss: 0.1859 - val_accuracy: 0.2050\n",
      "Epoch 166/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1370 - accuracy: 0.1940 - val_loss: 0.1850 - val_accuracy: 0.2302\n",
      "Epoch 167/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1368 - accuracy: 0.1970 - val_loss: 0.1841 - val_accuracy: 0.2428\n",
      "Epoch 168/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1366 - accuracy: 0.2024 - val_loss: 0.1846 - val_accuracy: 0.2212\n",
      "Epoch 169/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1362 - accuracy: 0.1970 - val_loss: 0.1841 - val_accuracy: 0.2266\n",
      "Epoch 170/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1359 - accuracy: 0.2042 - val_loss: 0.1835 - val_accuracy: 0.2122\n",
      "Epoch 171/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1355 - accuracy: 0.2030 - val_loss: 0.1847 - val_accuracy: 0.2374\n",
      "Epoch 172/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1354 - accuracy: 0.2078 - val_loss: 0.1830 - val_accuracy: 0.2320\n",
      "Epoch 173/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1349 - accuracy: 0.2036 - val_loss: 0.1821 - val_accuracy: 0.2176\n",
      "Epoch 174/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1346 - accuracy: 0.2048 - val_loss: 0.1854 - val_accuracy: 0.2302\n",
      "Epoch 175/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1344 - accuracy: 0.2168 - val_loss: 0.1816 - val_accuracy: 0.2392\n",
      "Epoch 176/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1341 - accuracy: 0.2114 - val_loss: 0.1823 - val_accuracy: 0.2266\n",
      "Epoch 177/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1337 - accuracy: 0.2162 - val_loss: 0.1811 - val_accuracy: 0.2266\n",
      "Epoch 178/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1335 - accuracy: 0.2120 - val_loss: 0.1801 - val_accuracy: 0.2284\n",
      "Epoch 179/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1331 - accuracy: 0.2120 - val_loss: 0.1795 - val_accuracy: 0.2320\n",
      "Epoch 180/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1327 - accuracy: 0.2144 - val_loss: 0.1798 - val_accuracy: 0.2248\n",
      "Epoch 181/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1325 - accuracy: 0.2138 - val_loss: 0.1799 - val_accuracy: 0.2536\n",
      "Epoch 182/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1322 - accuracy: 0.2180 - val_loss: 0.1801 - val_accuracy: 0.2626\n",
      "Epoch 183/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1319 - accuracy: 0.2162 - val_loss: 0.1798 - val_accuracy: 0.2032\n",
      "Epoch 184/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1317 - accuracy: 0.2204 - val_loss: 0.1779 - val_accuracy: 0.2194\n",
      "Epoch 185/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1313 - accuracy: 0.2174 - val_loss: 0.1783 - val_accuracy: 0.2194\n",
      "Epoch 186/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1310 - accuracy: 0.2156 - val_loss: 0.1807 - val_accuracy: 0.2536\n",
      "Epoch 187/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1310 - accuracy: 0.2198 - val_loss: 0.1765 - val_accuracy: 0.2140\n",
      "Epoch 188/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1304 - accuracy: 0.2234 - val_loss: 0.1768 - val_accuracy: 0.2374\n",
      "Epoch 189/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1302 - accuracy: 0.2204 - val_loss: 0.1800 - val_accuracy: 0.1960\n",
      "Epoch 190/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1301 - accuracy: 0.2180 - val_loss: 0.1833 - val_accuracy: 0.1906\n",
      "Epoch 191/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1299 - accuracy: 0.2198 - val_loss: 0.1792 - val_accuracy: 0.2410\n",
      "Epoch 192/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1294 - accuracy: 0.2210 - val_loss: 0.1750 - val_accuracy: 0.2410\n",
      "Epoch 193/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1290 - accuracy: 0.2216 - val_loss: 0.1750 - val_accuracy: 0.2410\n",
      "Epoch 194/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1286 - accuracy: 0.2204 - val_loss: 0.1748 - val_accuracy: 0.2392\n",
      "Epoch 195/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1284 - accuracy: 0.2282 - val_loss: 0.1758 - val_accuracy: 0.2356\n",
      "Epoch 196/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1282 - accuracy: 0.2246 - val_loss: 0.1777 - val_accuracy: 0.2590\n",
      "Epoch 197/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1280 - accuracy: 0.2222 - val_loss: 0.1761 - val_accuracy: 0.2374\n",
      "Epoch 198/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1276 - accuracy: 0.2270 - val_loss: 0.1736 - val_accuracy: 0.2320\n",
      "Epoch 199/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1272 - accuracy: 0.2372 - val_loss: 0.1740 - val_accuracy: 0.2608\n",
      "Epoch 200/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1270 - accuracy: 0.2324 - val_loss: 0.1739 - val_accuracy: 0.2446\n",
      "Epoch 201/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1268 - accuracy: 0.2324 - val_loss: 0.1740 - val_accuracy: 0.2500\n",
      "Epoch 202/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1265 - accuracy: 0.2336 - val_loss: 0.1725 - val_accuracy: 0.2536\n",
      "Epoch 203/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1261 - accuracy: 0.2330 - val_loss: 0.1728 - val_accuracy: 0.2410\n",
      "Epoch 204/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1259 - accuracy: 0.2324 - val_loss: 0.1721 - val_accuracy: 0.2500\n",
      "Epoch 205/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1256 - accuracy: 0.2306 - val_loss: 0.1719 - val_accuracy: 0.2302\n",
      "Epoch 206/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1254 - accuracy: 0.2324 - val_loss: 0.1711 - val_accuracy: 0.2752\n",
      "Epoch 207/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1250 - accuracy: 0.2372 - val_loss: 0.1738 - val_accuracy: 0.2536\n",
      "Epoch 208/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1248 - accuracy: 0.2294 - val_loss: 0.1718 - val_accuracy: 0.2428\n",
      "Epoch 209/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1246 - accuracy: 0.2336 - val_loss: 0.1698 - val_accuracy: 0.2302\n",
      "Epoch 210/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1242 - accuracy: 0.2336 - val_loss: 0.1706 - val_accuracy: 0.2356\n",
      "Epoch 211/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1239 - accuracy: 0.2444 - val_loss: 0.1706 - val_accuracy: 0.2716\n",
      "Epoch 212/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1238 - accuracy: 0.2348 - val_loss: 0.1696 - val_accuracy: 0.2464\n",
      "Epoch 213/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1233 - accuracy: 0.2378 - val_loss: 0.1698 - val_accuracy: 0.2644\n",
      "Epoch 214/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1231 - accuracy: 0.2378 - val_loss: 0.1682 - val_accuracy: 0.2626\n",
      "Epoch 215/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1228 - accuracy: 0.2426 - val_loss: 0.1693 - val_accuracy: 0.2572\n",
      "Epoch 216/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1225 - accuracy: 0.2505 - val_loss: 0.1707 - val_accuracy: 0.2266\n",
      "Epoch 217/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1223 - accuracy: 0.2444 - val_loss: 0.1666 - val_accuracy: 0.2518\n",
      "Epoch 218/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1219 - accuracy: 0.2426 - val_loss: 0.1664 - val_accuracy: 0.2572\n",
      "Epoch 219/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1216 - accuracy: 0.2438 - val_loss: 0.1676 - val_accuracy: 0.2680\n",
      "Epoch 220/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1214 - accuracy: 0.2432 - val_loss: 0.1658 - val_accuracy: 0.2734\n",
      "Epoch 221/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1210 - accuracy: 0.2408 - val_loss: 0.1704 - val_accuracy: 0.2050\n",
      "Epoch 222/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1210 - accuracy: 0.2498 - val_loss: 0.1651 - val_accuracy: 0.2752\n",
      "Epoch 223/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1205 - accuracy: 0.2505 - val_loss: 0.1695 - val_accuracy: 0.2428\n",
      "Epoch 224/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1205 - accuracy: 0.2408 - val_loss: 0.1641 - val_accuracy: 0.2410\n",
      "Epoch 225/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1200 - accuracy: 0.2462 - val_loss: 0.1653 - val_accuracy: 0.2770\n",
      "Epoch 226/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1198 - accuracy: 0.2498 - val_loss: 0.1630 - val_accuracy: 0.2698\n",
      "Epoch 227/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1194 - accuracy: 0.2432 - val_loss: 0.1657 - val_accuracy: 0.2554\n",
      "Epoch 228/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1190 - accuracy: 0.2589 - val_loss: 0.1640 - val_accuracy: 0.2500\n",
      "Epoch 229/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1188 - accuracy: 0.2547 - val_loss: 0.1634 - val_accuracy: 0.2734\n",
      "Epoch 230/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1185 - accuracy: 0.2480 - val_loss: 0.1623 - val_accuracy: 0.2698\n",
      "Epoch 231/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1182 - accuracy: 0.2571 - val_loss: 0.1632 - val_accuracy: 0.2554\n",
      "Epoch 232/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1180 - accuracy: 0.2649 - val_loss: 0.1634 - val_accuracy: 0.2860\n",
      "Epoch 233/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1177 - accuracy: 0.2498 - val_loss: 0.1612 - val_accuracy: 0.2536\n",
      "Epoch 234/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1174 - accuracy: 0.2517 - val_loss: 0.1627 - val_accuracy: 0.2500\n",
      "Epoch 235/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1171 - accuracy: 0.2541 - val_loss: 0.1611 - val_accuracy: 0.2824\n",
      "Epoch 236/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1168 - accuracy: 0.2583 - val_loss: 0.1606 - val_accuracy: 0.2662\n",
      "Epoch 237/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1165 - accuracy: 0.2595 - val_loss: 0.1597 - val_accuracy: 0.2842\n",
      "Epoch 238/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1162 - accuracy: 0.2583 - val_loss: 0.1600 - val_accuracy: 0.2824\n",
      "Epoch 239/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1160 - accuracy: 0.2601 - val_loss: 0.1598 - val_accuracy: 0.2752\n",
      "Epoch 240/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1157 - accuracy: 0.2643 - val_loss: 0.1599 - val_accuracy: 0.2932\n",
      "Epoch 241/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1155 - accuracy: 0.2553 - val_loss: 0.1586 - val_accuracy: 0.2878\n",
      "Epoch 242/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1151 - accuracy: 0.2637 - val_loss: 0.1596 - val_accuracy: 0.2842\n",
      "Epoch 243/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1151 - accuracy: 0.2601 - val_loss: 0.1580 - val_accuracy: 0.2752\n",
      "Epoch 244/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1147 - accuracy: 0.2721 - val_loss: 0.1575 - val_accuracy: 0.2860\n",
      "Epoch 245/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1145 - accuracy: 0.2631 - val_loss: 0.1576 - val_accuracy: 0.2788\n",
      "Epoch 246/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1142 - accuracy: 0.2679 - val_loss: 0.1575 - val_accuracy: 0.2698\n",
      "Epoch 247/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1142 - accuracy: 0.2625 - val_loss: 0.1575 - val_accuracy: 0.2824\n",
      "Epoch 248/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1138 - accuracy: 0.2691 - val_loss: 0.1571 - val_accuracy: 0.2878\n",
      "Epoch 249/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1135 - accuracy: 0.2733 - val_loss: 0.1564 - val_accuracy: 0.2788\n",
      "Epoch 250/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1133 - accuracy: 0.2595 - val_loss: 0.1587 - val_accuracy: 0.2572\n",
      "Epoch 251/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1132 - accuracy: 0.2679 - val_loss: 0.1554 - val_accuracy: 0.2842\n",
      "Epoch 252/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1129 - accuracy: 0.2685 - val_loss: 0.1558 - val_accuracy: 0.3129\n",
      "Epoch 253/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1127 - accuracy: 0.2625 - val_loss: 0.1553 - val_accuracy: 0.3058\n",
      "Epoch 254/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1123 - accuracy: 0.2655 - val_loss: 0.1546 - val_accuracy: 0.2860\n",
      "Epoch 255/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1121 - accuracy: 0.2679 - val_loss: 0.1546 - val_accuracy: 0.2932\n",
      "Epoch 256/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1119 - accuracy: 0.2667 - val_loss: 0.1542 - val_accuracy: 0.2950\n",
      "Epoch 257/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1116 - accuracy: 0.2685 - val_loss: 0.1545 - val_accuracy: 0.2932\n",
      "Epoch 258/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1115 - accuracy: 0.2661 - val_loss: 0.1536 - val_accuracy: 0.2842\n",
      "Epoch 259/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1114 - accuracy: 0.2709 - val_loss: 0.1541 - val_accuracy: 0.2878\n",
      "Epoch 260/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1110 - accuracy: 0.2703 - val_loss: 0.1535 - val_accuracy: 0.2806\n",
      "Epoch 261/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1109 - accuracy: 0.2721 - val_loss: 0.1529 - val_accuracy: 0.2932\n",
      "Epoch 262/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1106 - accuracy: 0.2631 - val_loss: 0.1525 - val_accuracy: 0.2950\n",
      "Epoch 263/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1105 - accuracy: 0.2715 - val_loss: 0.1528 - val_accuracy: 0.2770\n",
      "Epoch 264/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1103 - accuracy: 0.2655 - val_loss: 0.1529 - val_accuracy: 0.2968\n",
      "Epoch 265/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1099 - accuracy: 0.2715 - val_loss: 0.1578 - val_accuracy: 0.2482\n",
      "Epoch 266/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1100 - accuracy: 0.2709 - val_loss: 0.1526 - val_accuracy: 0.3201\n",
      "Epoch 267/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1096 - accuracy: 0.2697 - val_loss: 0.1526 - val_accuracy: 0.2824\n",
      "Epoch 268/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1094 - accuracy: 0.2679 - val_loss: 0.1534 - val_accuracy: 0.2986\n",
      "Epoch 269/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1091 - accuracy: 0.2673 - val_loss: 0.1520 - val_accuracy: 0.3094\n",
      "Epoch 270/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1089 - accuracy: 0.2673 - val_loss: 0.1519 - val_accuracy: 0.3022\n",
      "Epoch 271/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1087 - accuracy: 0.2709 - val_loss: 0.1509 - val_accuracy: 0.2878\n",
      "Epoch 272/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1085 - accuracy: 0.2703 - val_loss: 0.1512 - val_accuracy: 0.3058\n",
      "Epoch 273/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1083 - accuracy: 0.2715 - val_loss: 0.1513 - val_accuracy: 0.2986\n",
      "Epoch 274/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1080 - accuracy: 0.2661 - val_loss: 0.1495 - val_accuracy: 0.2878\n",
      "Epoch 275/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1077 - accuracy: 0.2727 - val_loss: 0.1511 - val_accuracy: 0.2860\n",
      "Epoch 276/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1076 - accuracy: 0.2667 - val_loss: 0.1500 - val_accuracy: 0.3165\n",
      "Epoch 277/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1072 - accuracy: 0.2727 - val_loss: 0.1494 - val_accuracy: 0.2950\n",
      "Epoch 278/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1071 - accuracy: 0.2715 - val_loss: 0.1494 - val_accuracy: 0.3201\n",
      "Epoch 279/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1068 - accuracy: 0.2793 - val_loss: 0.1511 - val_accuracy: 0.2824\n",
      "Epoch 280/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1069 - accuracy: 0.2661 - val_loss: 0.1492 - val_accuracy: 0.3273\n",
      "Epoch 281/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1064 - accuracy: 0.2721 - val_loss: 0.1489 - val_accuracy: 0.3147\n",
      "Epoch 282/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1063 - accuracy: 0.2691 - val_loss: 0.1488 - val_accuracy: 0.2968\n",
      "Epoch 283/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1061 - accuracy: 0.2805 - val_loss: 0.1484 - val_accuracy: 0.3129\n",
      "Epoch 284/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1058 - accuracy: 0.2781 - val_loss: 0.1479 - val_accuracy: 0.3147\n",
      "Epoch 285/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1055 - accuracy: 0.2691 - val_loss: 0.1477 - val_accuracy: 0.3058\n",
      "Epoch 286/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1053 - accuracy: 0.2781 - val_loss: 0.1463 - val_accuracy: 0.3094\n",
      "Epoch 287/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1051 - accuracy: 0.2733 - val_loss: 0.1475 - val_accuracy: 0.2842\n",
      "Epoch 288/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1049 - accuracy: 0.2793 - val_loss: 0.1488 - val_accuracy: 0.2824\n",
      "Epoch 289/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1048 - accuracy: 0.2799 - val_loss: 0.1478 - val_accuracy: 0.3094\n",
      "Epoch 290/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1045 - accuracy: 0.2817 - val_loss: 0.1477 - val_accuracy: 0.2932\n",
      "Epoch 291/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1043 - accuracy: 0.2757 - val_loss: 0.1457 - val_accuracy: 0.2932\n",
      "Epoch 292/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1039 - accuracy: 0.2715 - val_loss: 0.1510 - val_accuracy: 0.2230\n",
      "Epoch 293/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1042 - accuracy: 0.2733 - val_loss: 0.1456 - val_accuracy: 0.3129\n",
      "Epoch 294/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1037 - accuracy: 0.2769 - val_loss: 0.1447 - val_accuracy: 0.3129\n",
      "Epoch 295/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1034 - accuracy: 0.2757 - val_loss: 0.1454 - val_accuracy: 0.3129\n",
      "Epoch 296/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1031 - accuracy: 0.2835 - val_loss: 0.1439 - val_accuracy: 0.2896\n",
      "Epoch 297/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1029 - accuracy: 0.2709 - val_loss: 0.1465 - val_accuracy: 0.2878\n",
      "Epoch 298/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1029 - accuracy: 0.2781 - val_loss: 0.1463 - val_accuracy: 0.2770\n",
      "Epoch 299/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1027 - accuracy: 0.2817 - val_loss: 0.1445 - val_accuracy: 0.3022\n",
      "Epoch 300/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1023 - accuracy: 0.2763 - val_loss: 0.1448 - val_accuracy: 0.2806\n",
      "Epoch 301/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1020 - accuracy: 0.2787 - val_loss: 0.1445 - val_accuracy: 0.2842\n",
      "Epoch 302/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1020 - accuracy: 0.2733 - val_loss: 0.1430 - val_accuracy: 0.2950\n",
      "Epoch 303/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1016 - accuracy: 0.2769 - val_loss: 0.1437 - val_accuracy: 0.2770\n",
      "Epoch 304/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1014 - accuracy: 0.2775 - val_loss: 0.1440 - val_accuracy: 0.3004\n",
      "Epoch 305/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1011 - accuracy: 0.2799 - val_loss: 0.1460 - val_accuracy: 0.2896\n",
      "Epoch 306/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1012 - accuracy: 0.2763 - val_loss: 0.1423 - val_accuracy: 0.2968\n",
      "Epoch 307/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1007 - accuracy: 0.2757 - val_loss: 0.1430 - val_accuracy: 0.2716\n",
      "Epoch 308/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1007 - accuracy: 0.2787 - val_loss: 0.1427 - val_accuracy: 0.2986\n",
      "Epoch 309/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1003 - accuracy: 0.2811 - val_loss: 0.1423 - val_accuracy: 0.2842\n",
      "Epoch 310/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1003 - accuracy: 0.2925 - val_loss: 0.1421 - val_accuracy: 0.3058\n",
      "Epoch 311/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0999 - accuracy: 0.2811 - val_loss: 0.1431 - val_accuracy: 0.3022\n",
      "Epoch 312/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0999 - accuracy: 0.2835 - val_loss: 0.1430 - val_accuracy: 0.2842\n",
      "Epoch 313/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0996 - accuracy: 0.2847 - val_loss: 0.1437 - val_accuracy: 0.2734\n",
      "Epoch 314/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0997 - accuracy: 0.2799 - val_loss: 0.1439 - val_accuracy: 0.3022\n",
      "Epoch 315/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0993 - accuracy: 0.2865 - val_loss: 0.1445 - val_accuracy: 0.2644\n",
      "Epoch 316/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0992 - accuracy: 0.2829 - val_loss: 0.1431 - val_accuracy: 0.2752\n",
      "Epoch 317/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0990 - accuracy: 0.2919 - val_loss: 0.1407 - val_accuracy: 0.2878\n",
      "Epoch 318/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0986 - accuracy: 0.2835 - val_loss: 0.1433 - val_accuracy: 0.2518\n",
      "Epoch 319/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0988 - accuracy: 0.2775 - val_loss: 0.1403 - val_accuracy: 0.2788\n",
      "Epoch 320/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0982 - accuracy: 0.2799 - val_loss: 0.1458 - val_accuracy: 0.3345\n",
      "Epoch 321/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0984 - accuracy: 0.2847 - val_loss: 0.1416 - val_accuracy: 0.2824\n",
      "Epoch 322/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0979 - accuracy: 0.2835 - val_loss: 0.1415 - val_accuracy: 0.3094\n",
      "Epoch 323/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0978 - accuracy: 0.2835 - val_loss: 0.1393 - val_accuracy: 0.2914\n",
      "Epoch 324/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0975 - accuracy: 0.2871 - val_loss: 0.1394 - val_accuracy: 0.2914\n",
      "Epoch 325/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0972 - accuracy: 0.2859 - val_loss: 0.1388 - val_accuracy: 0.2626\n",
      "Epoch 326/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0973 - accuracy: 0.2883 - val_loss: 0.1398 - val_accuracy: 0.3219\n",
      "Epoch 327/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0970 - accuracy: 0.2877 - val_loss: 0.1391 - val_accuracy: 0.2644\n",
      "Epoch 328/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0968 - accuracy: 0.2865 - val_loss: 0.1385 - val_accuracy: 0.2734\n",
      "Epoch 329/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0966 - accuracy: 0.2817 - val_loss: 0.1382 - val_accuracy: 0.2716\n",
      "Epoch 330/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0965 - accuracy: 0.2889 - val_loss: 0.1412 - val_accuracy: 0.2644\n",
      "Epoch 331/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0965 - accuracy: 0.2925 - val_loss: 0.1391 - val_accuracy: 0.2824\n",
      "Epoch 332/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0962 - accuracy: 0.2847 - val_loss: 0.1506 - val_accuracy: 0.2806\n",
      "Epoch 333/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0965 - accuracy: 0.2895 - val_loss: 0.1374 - val_accuracy: 0.2878\n",
      "Epoch 334/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0959 - accuracy: 0.2859 - val_loss: 0.1393 - val_accuracy: 0.3201\n",
      "Epoch 335/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0957 - accuracy: 0.2859 - val_loss: 0.1377 - val_accuracy: 0.2806\n",
      "Epoch 336/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0954 - accuracy: 0.2865 - val_loss: 0.1384 - val_accuracy: 0.2644\n",
      "Epoch 337/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0953 - accuracy: 0.2895 - val_loss: 0.1383 - val_accuracy: 0.2878\n",
      "Epoch 338/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0953 - accuracy: 0.2865 - val_loss: 0.1379 - val_accuracy: 0.2896\n",
      "Epoch 339/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0950 - accuracy: 0.2901 - val_loss: 0.1391 - val_accuracy: 0.3129\n",
      "Epoch 340/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0949 - accuracy: 0.2931 - val_loss: 0.1377 - val_accuracy: 0.2770\n",
      "Epoch 341/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0947 - accuracy: 0.2919 - val_loss: 0.1364 - val_accuracy: 0.2914\n",
      "Epoch 342/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0946 - accuracy: 0.2907 - val_loss: 0.1371 - val_accuracy: 0.2788\n",
      "Epoch 343/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0943 - accuracy: 0.2883 - val_loss: 0.1372 - val_accuracy: 0.2950\n",
      "Epoch 344/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0942 - accuracy: 0.2847 - val_loss: 0.1387 - val_accuracy: 0.2770\n",
      "Epoch 345/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0942 - accuracy: 0.2967 - val_loss: 0.1345 - val_accuracy: 0.2626\n",
      "Epoch 346/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0939 - accuracy: 0.2919 - val_loss: 0.1350 - val_accuracy: 0.2482\n",
      "Epoch 347/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0938 - accuracy: 0.2949 - val_loss: 0.1370 - val_accuracy: 0.2968\n",
      "Epoch 348/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0935 - accuracy: 0.2961 - val_loss: 0.1358 - val_accuracy: 0.2842\n",
      "Epoch 349/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0934 - accuracy: 0.2937 - val_loss: 0.1364 - val_accuracy: 0.3147\n",
      "Epoch 350/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0933 - accuracy: 0.2961 - val_loss: 0.1354 - val_accuracy: 0.2770\n",
      "Epoch 351/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0930 - accuracy: 0.2985 - val_loss: 0.1437 - val_accuracy: 0.3004\n",
      "Epoch 352/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0934 - accuracy: 0.2979 - val_loss: 0.1355 - val_accuracy: 0.2860\n",
      "Epoch 353/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0929 - accuracy: 0.2913 - val_loss: 0.1354 - val_accuracy: 0.2770\n",
      "Epoch 354/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0925 - accuracy: 0.2949 - val_loss: 0.1353 - val_accuracy: 0.2986\n",
      "Epoch 355/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0924 - accuracy: 0.2979 - val_loss: 0.1348 - val_accuracy: 0.2842\n",
      "Epoch 356/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0923 - accuracy: 0.2991 - val_loss: 0.1372 - val_accuracy: 0.2986\n",
      "Epoch 357/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0923 - accuracy: 0.2985 - val_loss: 0.1340 - val_accuracy: 0.2644\n",
      "Epoch 358/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0920 - accuracy: 0.2949 - val_loss: 0.1340 - val_accuracy: 0.2950\n",
      "Epoch 359/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0919 - accuracy: 0.2997 - val_loss: 0.1393 - val_accuracy: 0.3147\n",
      "Epoch 360/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0919 - accuracy: 0.2955 - val_loss: 0.1358 - val_accuracy: 0.2806\n",
      "Epoch 361/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0915 - accuracy: 0.2943 - val_loss: 0.1475 - val_accuracy: 0.2194\n",
      "Epoch 362/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0924 - accuracy: 0.2997 - val_loss: 0.1355 - val_accuracy: 0.2896\n",
      "Epoch 363/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0913 - accuracy: 0.3033 - val_loss: 0.1341 - val_accuracy: 0.2914\n",
      "Epoch 364/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0910 - accuracy: 0.3003 - val_loss: 0.1335 - val_accuracy: 0.2824\n",
      "Epoch 365/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0910 - accuracy: 0.2997 - val_loss: 0.1361 - val_accuracy: 0.3147\n",
      "Epoch 366/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0911 - accuracy: 0.2973 - val_loss: 0.1338 - val_accuracy: 0.2896\n",
      "Epoch 367/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0906 - accuracy: 0.2955 - val_loss: 0.1337 - val_accuracy: 0.2698\n",
      "Epoch 368/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0905 - accuracy: 0.2997 - val_loss: 0.1355 - val_accuracy: 0.3058\n",
      "Epoch 369/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0904 - accuracy: 0.3033 - val_loss: 0.1332 - val_accuracy: 0.2716\n",
      "Epoch 370/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0903 - accuracy: 0.2997 - val_loss: 0.1316 - val_accuracy: 0.2986\n",
      "Epoch 371/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0900 - accuracy: 0.3027 - val_loss: 0.1337 - val_accuracy: 0.3040\n",
      "Epoch 372/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0900 - accuracy: 0.3015 - val_loss: 0.1345 - val_accuracy: 0.2914\n",
      "Epoch 373/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0898 - accuracy: 0.3087 - val_loss: 0.1312 - val_accuracy: 0.2878\n",
      "Epoch 374/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0895 - accuracy: 0.3039 - val_loss: 0.1334 - val_accuracy: 0.2860\n",
      "Epoch 375/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0894 - accuracy: 0.2967 - val_loss: 0.1318 - val_accuracy: 0.2932\n",
      "Epoch 376/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0893 - accuracy: 0.2985 - val_loss: 0.1413 - val_accuracy: 0.3040\n",
      "Epoch 377/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0896 - accuracy: 0.3189 - val_loss: 0.1318 - val_accuracy: 0.3004\n",
      "Epoch 378/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0890 - accuracy: 0.2979 - val_loss: 0.1321 - val_accuracy: 0.2752\n",
      "Epoch 379/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0889 - accuracy: 0.3051 - val_loss: 0.1378 - val_accuracy: 0.3399\n",
      "Epoch 380/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0892 - accuracy: 0.3015 - val_loss: 0.1528 - val_accuracy: 0.3004\n",
      "Epoch 381/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0896 - accuracy: 0.3075 - val_loss: 0.1332 - val_accuracy: 0.3165\n",
      "Epoch 382/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0885 - accuracy: 0.3081 - val_loss: 0.1315 - val_accuracy: 0.2734\n",
      "Epoch 383/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0884 - accuracy: 0.2973 - val_loss: 0.1310 - val_accuracy: 0.2896\n",
      "Epoch 384/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0880 - accuracy: 0.3015 - val_loss: 0.1307 - val_accuracy: 0.2914\n",
      "Epoch 385/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0880 - accuracy: 0.3051 - val_loss: 0.1314 - val_accuracy: 0.2968\n",
      "Epoch 386/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0879 - accuracy: 0.3123 - val_loss: 0.1309 - val_accuracy: 0.2950\n",
      "Epoch 387/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0878 - accuracy: 0.3123 - val_loss: 0.1319 - val_accuracy: 0.2770\n",
      "Epoch 388/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0878 - accuracy: 0.3015 - val_loss: 0.1320 - val_accuracy: 0.2572\n",
      "Epoch 389/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0876 - accuracy: 0.3075 - val_loss: 0.1321 - val_accuracy: 0.2824\n",
      "Epoch 390/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0873 - accuracy: 0.2991 - val_loss: 0.1368 - val_accuracy: 0.2392\n",
      "Epoch 391/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0876 - accuracy: 0.3087 - val_loss: 0.1485 - val_accuracy: 0.2878\n",
      "Epoch 392/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0879 - accuracy: 0.3147 - val_loss: 0.1295 - val_accuracy: 0.2986\n",
      "Epoch 393/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0869 - accuracy: 0.3153 - val_loss: 0.1317 - val_accuracy: 0.3112\n",
      "Epoch 394/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0869 - accuracy: 0.3105 - val_loss: 0.1308 - val_accuracy: 0.2716\n",
      "Epoch 395/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0868 - accuracy: 0.3105 - val_loss: 0.1285 - val_accuracy: 0.2932\n",
      "Epoch 396/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0865 - accuracy: 0.3135 - val_loss: 0.1321 - val_accuracy: 0.3076\n",
      "Epoch 397/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0864 - accuracy: 0.3123 - val_loss: 0.1287 - val_accuracy: 0.2770\n",
      "Epoch 398/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0862 - accuracy: 0.3111 - val_loss: 0.1316 - val_accuracy: 0.3004\n",
      "Epoch 399/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0862 - accuracy: 0.3165 - val_loss: 0.1314 - val_accuracy: 0.2896\n",
      "Epoch 400/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0861 - accuracy: 0.3045 - val_loss: 0.1290 - val_accuracy: 0.3040\n",
      "Epoch 401/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0856 - accuracy: 0.3135 - val_loss: 0.1408 - val_accuracy: 0.2770\n",
      "Epoch 402/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0864 - accuracy: 0.3165 - val_loss: 0.1416 - val_accuracy: 0.3147\n",
      "Epoch 403/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0861 - accuracy: 0.3195 - val_loss: 0.1302 - val_accuracy: 0.2536\n",
      "Epoch 404/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0857 - accuracy: 0.3045 - val_loss: 0.1294 - val_accuracy: 0.3058\n",
      "Epoch 405/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0851 - accuracy: 0.3207 - val_loss: 0.1287 - val_accuracy: 0.2896\n",
      "Epoch 406/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0853 - accuracy: 0.3075 - val_loss: 0.1277 - val_accuracy: 0.3040\n",
      "Epoch 407/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0850 - accuracy: 0.3243 - val_loss: 0.1282 - val_accuracy: 0.2986\n",
      "Epoch 408/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0850 - accuracy: 0.3117 - val_loss: 0.1333 - val_accuracy: 0.2374\n",
      "Epoch 409/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0851 - accuracy: 0.3105 - val_loss: 0.1268 - val_accuracy: 0.2878\n",
      "Epoch 410/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0847 - accuracy: 0.3123 - val_loss: 0.1282 - val_accuracy: 0.2986\n",
      "Epoch 411/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0845 - accuracy: 0.3087 - val_loss: 0.1275 - val_accuracy: 0.3004\n",
      "Epoch 412/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0843 - accuracy: 0.3165 - val_loss: 0.1271 - val_accuracy: 0.3183\n",
      "Epoch 413/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0842 - accuracy: 0.3183 - val_loss: 0.1279 - val_accuracy: 0.3076\n",
      "Epoch 414/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0842 - accuracy: 0.3105 - val_loss: 0.1295 - val_accuracy: 0.3273\n",
      "Epoch 415/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0841 - accuracy: 0.3207 - val_loss: 0.1289 - val_accuracy: 0.2824\n",
      "Epoch 416/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0840 - accuracy: 0.3207 - val_loss: 0.1270 - val_accuracy: 0.3147\n",
      "Epoch 417/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0837 - accuracy: 0.3195 - val_loss: 0.1270 - val_accuracy: 0.2950\n",
      "Epoch 418/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0836 - accuracy: 0.3141 - val_loss: 0.1272 - val_accuracy: 0.2914\n",
      "Epoch 419/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0837 - accuracy: 0.3141 - val_loss: 0.1274 - val_accuracy: 0.3040\n",
      "Epoch 420/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0834 - accuracy: 0.3261 - val_loss: 0.1267 - val_accuracy: 0.3004\n",
      "Epoch 421/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0833 - accuracy: 0.3147 - val_loss: 0.1263 - val_accuracy: 0.2842\n",
      "Epoch 422/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0833 - accuracy: 0.3189 - val_loss: 0.1259 - val_accuracy: 0.3076\n",
      "Epoch 423/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0831 - accuracy: 0.3153 - val_loss: 0.1267 - val_accuracy: 0.2896\n",
      "Epoch 424/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0831 - accuracy: 0.3123 - val_loss: 0.1271 - val_accuracy: 0.3040\n",
      "Epoch 425/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0829 - accuracy: 0.3129 - val_loss: 0.1267 - val_accuracy: 0.2914\n",
      "Epoch 426/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0830 - accuracy: 0.3087 - val_loss: 0.1293 - val_accuracy: 0.3273\n",
      "Epoch 427/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0830 - accuracy: 0.3201 - val_loss: 0.1301 - val_accuracy: 0.2968\n",
      "Epoch 428/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0831 - accuracy: 0.3015 - val_loss: 0.1262 - val_accuracy: 0.2842\n",
      "Epoch 429/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0826 - accuracy: 0.3117 - val_loss: 0.1258 - val_accuracy: 0.2932\n",
      "Epoch 430/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0822 - accuracy: 0.3093 - val_loss: 0.1311 - val_accuracy: 0.3147\n",
      "Epoch 431/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0825 - accuracy: 0.3249 - val_loss: 0.1299 - val_accuracy: 0.3040\n",
      "Epoch 432/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0822 - accuracy: 0.3189 - val_loss: 0.1265 - val_accuracy: 0.3058\n",
      "Epoch 433/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0822 - accuracy: 0.3153 - val_loss: 0.1257 - val_accuracy: 0.2968\n",
      "Epoch 434/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0821 - accuracy: 0.3171 - val_loss: 0.1268 - val_accuracy: 0.3094\n",
      "Epoch 435/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0820 - accuracy: 0.3117 - val_loss: 0.1256 - val_accuracy: 0.3040\n",
      "Epoch 436/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0819 - accuracy: 0.3123 - val_loss: 0.1261 - val_accuracy: 0.2968\n",
      "Epoch 437/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0818 - accuracy: 0.3165 - val_loss: 0.1278 - val_accuracy: 0.3004\n",
      "Epoch 438/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0816 - accuracy: 0.3147 - val_loss: 0.1267 - val_accuracy: 0.3165\n",
      "Epoch 439/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0814 - accuracy: 0.3141 - val_loss: 0.1252 - val_accuracy: 0.3183\n",
      "Epoch 440/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0813 - accuracy: 0.3165 - val_loss: 0.1288 - val_accuracy: 0.2914\n",
      "Epoch 441/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0814 - accuracy: 0.3171 - val_loss: 0.1248 - val_accuracy: 0.2914\n",
      "Epoch 442/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0813 - accuracy: 0.3081 - val_loss: 0.1250 - val_accuracy: 0.3183\n",
      "Epoch 443/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0810 - accuracy: 0.3159 - val_loss: 0.1234 - val_accuracy: 0.2842\n",
      "Epoch 444/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0809 - accuracy: 0.3105 - val_loss: 0.1247 - val_accuracy: 0.3022\n",
      "Epoch 445/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0809 - accuracy: 0.3171 - val_loss: 0.1316 - val_accuracy: 0.2392\n",
      "Epoch 446/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0813 - accuracy: 0.3057 - val_loss: 0.1469 - val_accuracy: 0.2482\n",
      "Epoch 447/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0822 - accuracy: 0.3105 - val_loss: 0.1366 - val_accuracy: 0.2716\n",
      "Epoch 448/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0813 - accuracy: 0.3189 - val_loss: 0.1240 - val_accuracy: 0.3076\n",
      "Epoch 449/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0806 - accuracy: 0.3039 - val_loss: 0.1234 - val_accuracy: 0.2968\n",
      "Epoch 450/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0805 - accuracy: 0.3021 - val_loss: 0.1234 - val_accuracy: 0.3040\n",
      "Epoch 451/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0803 - accuracy: 0.3219 - val_loss: 0.1278 - val_accuracy: 0.3399\n",
      "Epoch 452/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0804 - accuracy: 0.3225 - val_loss: 0.1239 - val_accuracy: 0.3147\n",
      "Epoch 453/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0800 - accuracy: 0.3183 - val_loss: 0.1516 - val_accuracy: 0.2626\n",
      "Epoch 454/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0814 - accuracy: 0.3117 - val_loss: 0.1238 - val_accuracy: 0.3040\n",
      "Epoch 455/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0800 - accuracy: 0.3177 - val_loss: 0.1235 - val_accuracy: 0.2968\n",
      "Epoch 456/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0800 - accuracy: 0.3141 - val_loss: 0.1240 - val_accuracy: 0.3112\n",
      "Epoch 457/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0798 - accuracy: 0.3153 - val_loss: 0.1291 - val_accuracy: 0.3058\n",
      "Epoch 458/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0801 - accuracy: 0.3009 - val_loss: 0.1230 - val_accuracy: 0.3112\n",
      "Epoch 459/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0797 - accuracy: 0.3129 - val_loss: 0.1225 - val_accuracy: 0.3058\n",
      "Epoch 460/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0795 - accuracy: 0.3117 - val_loss: 0.1259 - val_accuracy: 0.3129\n",
      "Epoch 461/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0796 - accuracy: 0.3123 - val_loss: 0.1210 - val_accuracy: 0.2968\n",
      "Epoch 462/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0795 - accuracy: 0.3201 - val_loss: 0.1277 - val_accuracy: 0.3004\n",
      "Epoch 463/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0796 - accuracy: 0.3237 - val_loss: 0.1238 - val_accuracy: 0.2968\n",
      "Epoch 464/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0792 - accuracy: 0.3237 - val_loss: 0.1228 - val_accuracy: 0.2986\n",
      "Epoch 465/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0790 - accuracy: 0.3069 - val_loss: 0.1236 - val_accuracy: 0.3040\n",
      "Epoch 466/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0789 - accuracy: 0.3159 - val_loss: 0.1238 - val_accuracy: 0.3183\n",
      "Epoch 467/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0789 - accuracy: 0.3135 - val_loss: 0.1228 - val_accuracy: 0.2932\n",
      "Epoch 468/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0790 - accuracy: 0.3189 - val_loss: 0.1310 - val_accuracy: 0.1960\n",
      "Epoch 469/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0797 - accuracy: 0.3105 - val_loss: 0.1245 - val_accuracy: 0.2806\n",
      "Epoch 470/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0792 - accuracy: 0.3087 - val_loss: 0.1310 - val_accuracy: 0.3471\n",
      "Epoch 471/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0791 - accuracy: 0.3171 - val_loss: 0.1225 - val_accuracy: 0.3129\n",
      "Epoch 472/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0786 - accuracy: 0.3243 - val_loss: 0.1233 - val_accuracy: 0.3040\n",
      "Epoch 473/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0785 - accuracy: 0.3213 - val_loss: 0.1215 - val_accuracy: 0.3201\n",
      "Epoch 474/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0783 - accuracy: 0.3099 - val_loss: 0.1292 - val_accuracy: 0.3112\n",
      "Epoch 475/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0790 - accuracy: 0.3141 - val_loss: 0.1230 - val_accuracy: 0.3255\n",
      "Epoch 476/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0782 - accuracy: 0.3225 - val_loss: 0.1218 - val_accuracy: 0.3165\n",
      "Epoch 477/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0781 - accuracy: 0.3201 - val_loss: 0.1224 - val_accuracy: 0.2842\n",
      "Epoch 478/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0783 - accuracy: 0.3189 - val_loss: 0.1234 - val_accuracy: 0.3147\n",
      "Epoch 479/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0780 - accuracy: 0.3147 - val_loss: 0.1277 - val_accuracy: 0.3094\n",
      "Epoch 480/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0780 - accuracy: 0.3195 - val_loss: 0.1209 - val_accuracy: 0.2788\n",
      "Epoch 481/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0777 - accuracy: 0.3165 - val_loss: 0.1207 - val_accuracy: 0.3094\n",
      "Epoch 482/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0778 - accuracy: 0.3111 - val_loss: 0.1214 - val_accuracy: 0.3201\n",
      "Epoch 483/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0775 - accuracy: 0.3063 - val_loss: 0.1211 - val_accuracy: 0.3237\n",
      "Epoch 484/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0775 - accuracy: 0.3243 - val_loss: 0.1220 - val_accuracy: 0.3129\n",
      "Epoch 485/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0774 - accuracy: 0.3171 - val_loss: 0.1215 - val_accuracy: 0.2914\n",
      "Epoch 486/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0774 - accuracy: 0.3213 - val_loss: 0.1204 - val_accuracy: 0.3237\n",
      "Epoch 487/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0772 - accuracy: 0.3135 - val_loss: 0.1334 - val_accuracy: 0.3381\n",
      "Epoch 488/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0780 - accuracy: 0.3171 - val_loss: 0.1220 - val_accuracy: 0.2914\n",
      "Epoch 489/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0773 - accuracy: 0.3135 - val_loss: 0.1218 - val_accuracy: 0.3112\n",
      "Epoch 490/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0770 - accuracy: 0.3147 - val_loss: 0.1216 - val_accuracy: 0.3004\n",
      "Epoch 491/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0771 - accuracy: 0.3069 - val_loss: 0.1242 - val_accuracy: 0.3094\n",
      "Epoch 492/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0770 - accuracy: 0.3093 - val_loss: 0.1327 - val_accuracy: 0.3867\n",
      "Epoch 493/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0775 - accuracy: 0.3321 - val_loss: 0.1217 - val_accuracy: 0.3022\n",
      "Epoch 494/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0767 - accuracy: 0.3087 - val_loss: 0.1224 - val_accuracy: 0.3112\n",
      "Epoch 495/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0768 - accuracy: 0.3165 - val_loss: 0.1212 - val_accuracy: 0.3022\n",
      "Epoch 496/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0768 - accuracy: 0.3105 - val_loss: 0.1222 - val_accuracy: 0.2824\n",
      "Epoch 497/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0769 - accuracy: 0.3237 - val_loss: 0.1337 - val_accuracy: 0.3399\n",
      "Epoch 498/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0773 - accuracy: 0.3147 - val_loss: 0.1212 - val_accuracy: 0.3255\n",
      "Epoch 499/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0763 - accuracy: 0.3207 - val_loss: 0.1198 - val_accuracy: 0.3040\n",
      "Epoch 500/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0765 - accuracy: 0.3081 - val_loss: 0.1209 - val_accuracy: 0.3183\n",
      "Epoch 501/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0762 - accuracy: 0.3213 - val_loss: 0.1218 - val_accuracy: 0.3201\n",
      "Epoch 502/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0762 - accuracy: 0.3171 - val_loss: 0.1200 - val_accuracy: 0.3004\n",
      "Epoch 503/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0762 - accuracy: 0.3159 - val_loss: 0.1231 - val_accuracy: 0.3345\n",
      "Epoch 504/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0762 - accuracy: 0.3207 - val_loss: 0.1193 - val_accuracy: 0.3129\n",
      "Epoch 505/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0760 - accuracy: 0.3189 - val_loss: 0.1213 - val_accuracy: 0.2914\n",
      "Epoch 506/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0760 - accuracy: 0.3141 - val_loss: 0.1212 - val_accuracy: 0.3129\n",
      "Epoch 507/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0757 - accuracy: 0.3123 - val_loss: 0.1186 - val_accuracy: 0.2914\n",
      "Epoch 508/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0757 - accuracy: 0.3249 - val_loss: 0.1198 - val_accuracy: 0.3183\n",
      "Epoch 509/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0756 - accuracy: 0.3135 - val_loss: 0.1202 - val_accuracy: 0.3183\n",
      "Epoch 510/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0756 - accuracy: 0.3189 - val_loss: 0.1199 - val_accuracy: 0.3129\n",
      "Epoch 511/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0753 - accuracy: 0.3075 - val_loss: 0.1192 - val_accuracy: 0.3058\n",
      "Epoch 512/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0755 - accuracy: 0.3153 - val_loss: 0.1189 - val_accuracy: 0.3219\n",
      "Epoch 513/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0754 - accuracy: 0.3159 - val_loss: 0.1194 - val_accuracy: 0.3219\n",
      "Epoch 514/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0753 - accuracy: 0.3183 - val_loss: 0.1193 - val_accuracy: 0.2950\n",
      "Epoch 515/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0754 - accuracy: 0.3123 - val_loss: 0.1214 - val_accuracy: 0.3237\n",
      "Epoch 516/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0751 - accuracy: 0.3183 - val_loss: 0.1247 - val_accuracy: 0.3165\n",
      "Epoch 517/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0754 - accuracy: 0.3207 - val_loss: 0.1186 - val_accuracy: 0.3129\n",
      "Epoch 518/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0751 - accuracy: 0.3165 - val_loss: 0.1409 - val_accuracy: 0.2158\n",
      "Epoch 519/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0765 - accuracy: 0.3183 - val_loss: 0.1231 - val_accuracy: 0.3147\n",
      "Epoch 520/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0751 - accuracy: 0.3153 - val_loss: 0.1210 - val_accuracy: 0.3237\n",
      "Epoch 521/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0748 - accuracy: 0.3249 - val_loss: 0.1193 - val_accuracy: 0.3094\n",
      "Epoch 522/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0751 - accuracy: 0.3171 - val_loss: 0.1205 - val_accuracy: 0.3058\n",
      "Epoch 523/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0748 - accuracy: 0.3075 - val_loss: 0.1198 - val_accuracy: 0.3273\n",
      "Epoch 524/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0747 - accuracy: 0.3261 - val_loss: 0.1192 - val_accuracy: 0.3040\n",
      "Epoch 525/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0747 - accuracy: 0.3177 - val_loss: 0.1185 - val_accuracy: 0.2896\n",
      "Epoch 526/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0744 - accuracy: 0.3153 - val_loss: 0.1204 - val_accuracy: 0.2950\n",
      "Epoch 527/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0745 - accuracy: 0.3183 - val_loss: 0.1429 - val_accuracy: 0.2266\n",
      "Epoch 528/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0760 - accuracy: 0.3219 - val_loss: 0.1199 - val_accuracy: 0.3129\n",
      "Epoch 529/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0742 - accuracy: 0.3243 - val_loss: 0.1245 - val_accuracy: 0.3381\n",
      "Epoch 530/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0745 - accuracy: 0.3315 - val_loss: 0.1187 - val_accuracy: 0.3147\n",
      "Epoch 531/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0741 - accuracy: 0.3207 - val_loss: 0.1191 - val_accuracy: 0.2968\n",
      "Epoch 532/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0740 - accuracy: 0.3255 - val_loss: 0.1195 - val_accuracy: 0.3076\n",
      "Epoch 533/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0742 - accuracy: 0.3207 - val_loss: 0.1186 - val_accuracy: 0.3219\n",
      "Epoch 534/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0739 - accuracy: 0.3231 - val_loss: 0.1185 - val_accuracy: 0.3040\n",
      "Epoch 535/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0739 - accuracy: 0.3249 - val_loss: 0.1228 - val_accuracy: 0.2896\n",
      "Epoch 536/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0741 - accuracy: 0.3105 - val_loss: 0.1183 - val_accuracy: 0.3201\n",
      "Epoch 537/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0737 - accuracy: 0.3189 - val_loss: 0.1201 - val_accuracy: 0.3147\n",
      "Epoch 538/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0738 - accuracy: 0.3195 - val_loss: 0.1180 - val_accuracy: 0.3112\n",
      "Epoch 539/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0736 - accuracy: 0.3135 - val_loss: 0.1168 - val_accuracy: 0.3058\n",
      "Epoch 540/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0736 - accuracy: 0.3195 - val_loss: 0.1446 - val_accuracy: 0.3201\n",
      "Epoch 541/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0750 - accuracy: 0.3147 - val_loss: 0.1185 - val_accuracy: 0.3022\n",
      "Epoch 542/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0735 - accuracy: 0.3213 - val_loss: 0.1191 - val_accuracy: 0.2986\n",
      "Epoch 543/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0734 - accuracy: 0.3135 - val_loss: 0.1356 - val_accuracy: 0.2770\n",
      "Epoch 544/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0743 - accuracy: 0.3237 - val_loss: 0.1203 - val_accuracy: 0.3058\n",
      "Epoch 545/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0734 - accuracy: 0.3165 - val_loss: 0.1176 - val_accuracy: 0.3022\n",
      "Epoch 546/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0732 - accuracy: 0.3177 - val_loss: 0.1181 - val_accuracy: 0.3076\n",
      "Epoch 547/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0731 - accuracy: 0.3183 - val_loss: 0.1265 - val_accuracy: 0.2392\n",
      "Epoch 548/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0737 - accuracy: 0.3249 - val_loss: 0.1494 - val_accuracy: 0.2122\n",
      "Epoch 549/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0747 - accuracy: 0.3141 - val_loss: 0.1191 - val_accuracy: 0.3129\n",
      "Epoch 550/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0731 - accuracy: 0.3189 - val_loss: 0.1177 - val_accuracy: 0.3147\n",
      "Epoch 551/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0727 - accuracy: 0.3159 - val_loss: 0.1188 - val_accuracy: 0.3112\n",
      "Epoch 552/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0729 - accuracy: 0.3177 - val_loss: 0.1176 - val_accuracy: 0.3165\n",
      "Epoch 553/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0727 - accuracy: 0.3207 - val_loss: 0.1249 - val_accuracy: 0.3183\n",
      "Epoch 554/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0732 - accuracy: 0.3111 - val_loss: 0.1202 - val_accuracy: 0.3022\n",
      "Epoch 555/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0728 - accuracy: 0.3165 - val_loss: 0.1153 - val_accuracy: 0.2914\n",
      "Epoch 556/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0728 - accuracy: 0.3231 - val_loss: 0.1169 - val_accuracy: 0.2896\n",
      "Epoch 557/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0724 - accuracy: 0.3189 - val_loss: 0.1197 - val_accuracy: 0.3291\n",
      "Epoch 558/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0725 - accuracy: 0.3255 - val_loss: 0.1248 - val_accuracy: 0.2806\n",
      "Epoch 559/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0728 - accuracy: 0.3177 - val_loss: 0.1186 - val_accuracy: 0.3399\n",
      "Epoch 560/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0728 - accuracy: 0.3165 - val_loss: 0.1162 - val_accuracy: 0.2986\n",
      "Epoch 561/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0724 - accuracy: 0.3201 - val_loss: 0.1154 - val_accuracy: 0.3058\n",
      "Epoch 562/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0722 - accuracy: 0.3147 - val_loss: 0.1188 - val_accuracy: 0.3255\n",
      "Epoch 563/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0725 - accuracy: 0.3249 - val_loss: 0.1158 - val_accuracy: 0.3112\n",
      "Epoch 564/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0721 - accuracy: 0.3099 - val_loss: 0.1171 - val_accuracy: 0.3022\n",
      "Epoch 565/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0722 - accuracy: 0.3261 - val_loss: 0.1169 - val_accuracy: 0.3147\n",
      "Epoch 566/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0720 - accuracy: 0.3261 - val_loss: 0.1167 - val_accuracy: 0.3183\n",
      "Epoch 567/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0718 - accuracy: 0.3147 - val_loss: 0.1177 - val_accuracy: 0.3094\n",
      "Epoch 568/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0721 - accuracy: 0.3297 - val_loss: 0.1235 - val_accuracy: 0.2878\n",
      "Epoch 569/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0721 - accuracy: 0.3171 - val_loss: 0.1185 - val_accuracy: 0.3219\n",
      "Epoch 570/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0717 - accuracy: 0.3177 - val_loss: 0.1343 - val_accuracy: 0.3363\n",
      "Epoch 571/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0729 - accuracy: 0.3249 - val_loss: 0.1170 - val_accuracy: 0.3076\n",
      "Epoch 572/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0717 - accuracy: 0.3207 - val_loss: 0.1178 - val_accuracy: 0.3525\n",
      "Epoch 573/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0718 - accuracy: 0.3189 - val_loss: 0.1182 - val_accuracy: 0.3255\n",
      "Epoch 574/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0715 - accuracy: 0.3141 - val_loss: 0.1185 - val_accuracy: 0.3183\n",
      "Epoch 575/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0716 - accuracy: 0.3213 - val_loss: 0.1244 - val_accuracy: 0.3345\n",
      "Epoch 576/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0718 - accuracy: 0.3231 - val_loss: 0.1163 - val_accuracy: 0.3201\n",
      "Epoch 577/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0714 - accuracy: 0.3237 - val_loss: 0.1227 - val_accuracy: 0.3165\n",
      "Epoch 578/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0718 - accuracy: 0.3237 - val_loss: 0.1252 - val_accuracy: 0.3525\n",
      "Epoch 579/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0718 - accuracy: 0.3261 - val_loss: 0.1172 - val_accuracy: 0.3004\n",
      "Epoch 580/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0712 - accuracy: 0.3267 - val_loss: 0.1198 - val_accuracy: 0.3022\n",
      "Epoch 581/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0715 - accuracy: 0.3279 - val_loss: 0.1167 - val_accuracy: 0.3183\n",
      "Epoch 582/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0710 - accuracy: 0.3255 - val_loss: 0.1168 - val_accuracy: 0.3435\n",
      "Epoch 583/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0712 - accuracy: 0.3261 - val_loss: 0.1168 - val_accuracy: 0.3147\n",
      "Epoch 584/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0709 - accuracy: 0.3207 - val_loss: 0.1160 - val_accuracy: 0.3237\n",
      "Epoch 585/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0709 - accuracy: 0.3309 - val_loss: 0.1156 - val_accuracy: 0.3129\n",
      "Epoch 586/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0711 - accuracy: 0.3249 - val_loss: 0.1417 - val_accuracy: 0.3201\n",
      "Epoch 587/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0722 - accuracy: 0.3237 - val_loss: 0.1149 - val_accuracy: 0.3040\n",
      "Epoch 588/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0708 - accuracy: 0.3261 - val_loss: 0.1175 - val_accuracy: 0.3255\n",
      "Epoch 589/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0708 - accuracy: 0.3303 - val_loss: 0.1172 - val_accuracy: 0.3112\n",
      "Epoch 590/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0706 - accuracy: 0.3255 - val_loss: 0.1261 - val_accuracy: 0.3058\n",
      "Epoch 591/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0713 - accuracy: 0.3219 - val_loss: 0.1183 - val_accuracy: 0.3345\n",
      "Epoch 592/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0707 - accuracy: 0.3225 - val_loss: 0.1196 - val_accuracy: 0.3219\n",
      "Epoch 593/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0707 - accuracy: 0.3255 - val_loss: 0.1140 - val_accuracy: 0.3004\n",
      "Epoch 594/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0705 - accuracy: 0.3243 - val_loss: 0.1170 - val_accuracy: 0.3255\n",
      "Epoch 595/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0704 - accuracy: 0.3225 - val_loss: 0.1189 - val_accuracy: 0.2572\n",
      "Epoch 596/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0709 - accuracy: 0.3237 - val_loss: 0.1316 - val_accuracy: 0.3489\n",
      "Epoch 597/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0713 - accuracy: 0.3321 - val_loss: 0.1189 - val_accuracy: 0.3273\n",
      "Epoch 598/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0704 - accuracy: 0.3261 - val_loss: 0.1157 - val_accuracy: 0.3112\n",
      "Epoch 599/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0703 - accuracy: 0.3213 - val_loss: 0.1173 - val_accuracy: 0.2608\n",
      "Epoch 600/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0708 - accuracy: 0.3189 - val_loss: 0.1219 - val_accuracy: 0.3435\n",
      "Epoch 601/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0702 - accuracy: 0.3273 - val_loss: 0.1294 - val_accuracy: 0.2824\n",
      "Epoch 602/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0709 - accuracy: 0.3153 - val_loss: 0.1156 - val_accuracy: 0.3094\n",
      "Epoch 603/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0700 - accuracy: 0.3201 - val_loss: 0.1165 - val_accuracy: 0.3112\n",
      "Epoch 604/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0699 - accuracy: 0.3279 - val_loss: 0.1169 - val_accuracy: 0.3129\n",
      "Epoch 605/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0699 - accuracy: 0.3201 - val_loss: 0.1160 - val_accuracy: 0.3094\n",
      "Epoch 606/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0696 - accuracy: 0.3231 - val_loss: 0.1162 - val_accuracy: 0.3219\n",
      "Epoch 607/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0697 - accuracy: 0.3273 - val_loss: 0.1243 - val_accuracy: 0.3094\n",
      "Epoch 608/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0702 - accuracy: 0.3225 - val_loss: 0.1153 - val_accuracy: 0.3147\n",
      "Epoch 609/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0697 - accuracy: 0.3213 - val_loss: 0.1141 - val_accuracy: 0.3112\n",
      "Epoch 610/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0696 - accuracy: 0.3285 - val_loss: 0.1144 - val_accuracy: 0.3183\n",
      "Epoch 611/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0695 - accuracy: 0.3267 - val_loss: 0.1155 - val_accuracy: 0.3399\n",
      "Epoch 612/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0695 - accuracy: 0.3273 - val_loss: 0.1172 - val_accuracy: 0.3112\n",
      "Epoch 613/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0695 - accuracy: 0.3195 - val_loss: 0.1153 - val_accuracy: 0.2986\n",
      "Epoch 614/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0695 - accuracy: 0.3255 - val_loss: 0.1144 - val_accuracy: 0.3201\n",
      "Epoch 615/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0692 - accuracy: 0.3231 - val_loss: 0.1136 - val_accuracy: 0.3129\n",
      "Epoch 616/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0692 - accuracy: 0.3237 - val_loss: 0.1155 - val_accuracy: 0.3219\n",
      "Epoch 617/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0692 - accuracy: 0.3297 - val_loss: 0.1149 - val_accuracy: 0.3183\n",
      "Epoch 618/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0693 - accuracy: 0.3225 - val_loss: 0.1152 - val_accuracy: 0.3237\n",
      "Epoch 619/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0690 - accuracy: 0.3297 - val_loss: 0.1140 - val_accuracy: 0.3058\n",
      "Epoch 620/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0690 - accuracy: 0.3231 - val_loss: 0.1160 - val_accuracy: 0.3112\n",
      "Epoch 621/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0690 - accuracy: 0.3207 - val_loss: 0.1170 - val_accuracy: 0.3309\n",
      "Epoch 622/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0688 - accuracy: 0.3297 - val_loss: 0.1151 - val_accuracy: 0.3112\n",
      "Epoch 623/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0690 - accuracy: 0.3207 - val_loss: 0.1167 - val_accuracy: 0.3183\n",
      "Epoch 624/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0689 - accuracy: 0.3225 - val_loss: 0.1163 - val_accuracy: 0.3291\n",
      "Epoch 625/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0687 - accuracy: 0.3303 - val_loss: 0.1154 - val_accuracy: 0.2932\n",
      "Epoch 626/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0688 - accuracy: 0.3189 - val_loss: 0.1186 - val_accuracy: 0.3183\n",
      "Epoch 627/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0689 - accuracy: 0.3207 - val_loss: 0.1234 - val_accuracy: 0.3022\n",
      "Epoch 628/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0692 - accuracy: 0.3213 - val_loss: 0.1161 - val_accuracy: 0.3417\n",
      "Epoch 629/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0687 - accuracy: 0.3351 - val_loss: 0.1153 - val_accuracy: 0.3094\n",
      "Epoch 630/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0685 - accuracy: 0.3255 - val_loss: 0.1167 - val_accuracy: 0.3273\n",
      "Epoch 631/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0684 - accuracy: 0.3207 - val_loss: 0.1247 - val_accuracy: 0.2950\n",
      "Epoch 632/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0690 - accuracy: 0.3285 - val_loss: 0.1326 - val_accuracy: 0.2194\n",
      "Epoch 633/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0696 - accuracy: 0.3045 - val_loss: 0.1149 - val_accuracy: 0.3004\n",
      "Epoch 634/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0684 - accuracy: 0.3219 - val_loss: 0.1168 - val_accuracy: 0.3381\n",
      "Epoch 635/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0683 - accuracy: 0.3219 - val_loss: 0.1149 - val_accuracy: 0.3417\n",
      "Epoch 636/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0683 - accuracy: 0.3285 - val_loss: 0.1159 - val_accuracy: 0.3201\n",
      "Epoch 637/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0684 - accuracy: 0.3303 - val_loss: 0.1148 - val_accuracy: 0.2878\n",
      "Epoch 638/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0683 - accuracy: 0.3225 - val_loss: 0.1175 - val_accuracy: 0.3219\n",
      "Epoch 639/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0685 - accuracy: 0.3285 - val_loss: 0.1187 - val_accuracy: 0.3237\n",
      "Epoch 640/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0683 - accuracy: 0.3279 - val_loss: 0.1137 - val_accuracy: 0.3201\n",
      "Epoch 641/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0681 - accuracy: 0.3249 - val_loss: 0.1382 - val_accuracy: 0.2788\n",
      "Epoch 642/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0690 - accuracy: 0.3201 - val_loss: 0.1158 - val_accuracy: 0.3255\n",
      "Epoch 643/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0681 - accuracy: 0.3195 - val_loss: 0.1180 - val_accuracy: 0.2968\n",
      "Epoch 644/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0686 - accuracy: 0.3231 - val_loss: 0.1166 - val_accuracy: 0.2788\n",
      "Epoch 645/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0682 - accuracy: 0.3183 - val_loss: 0.1145 - val_accuracy: 0.3273\n",
      "Epoch 646/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0676 - accuracy: 0.3309 - val_loss: 0.1311 - val_accuracy: 0.3363\n",
      "Epoch 647/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0689 - accuracy: 0.3129 - val_loss: 0.1163 - val_accuracy: 0.2878\n",
      "Epoch 648/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0679 - accuracy: 0.3225 - val_loss: 0.1140 - val_accuracy: 0.3040\n",
      "Epoch 649/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0677 - accuracy: 0.3255 - val_loss: 0.1120 - val_accuracy: 0.3076\n",
      "Epoch 650/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0674 - accuracy: 0.3243 - val_loss: 0.1133 - val_accuracy: 0.3219\n",
      "Epoch 651/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0675 - accuracy: 0.3237 - val_loss: 0.1161 - val_accuracy: 0.3237\n",
      "Epoch 652/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0676 - accuracy: 0.3231 - val_loss: 0.1218 - val_accuracy: 0.3112\n",
      "Epoch 653/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0680 - accuracy: 0.3291 - val_loss: 0.1141 - val_accuracy: 0.3129\n",
      "Epoch 654/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0673 - accuracy: 0.3249 - val_loss: 0.1147 - val_accuracy: 0.3147\n",
      "Epoch 655/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0672 - accuracy: 0.3327 - val_loss: 0.1210 - val_accuracy: 0.3471\n",
      "Epoch 656/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0678 - accuracy: 0.3327 - val_loss: 0.1152 - val_accuracy: 0.3094\n",
      "Epoch 657/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0672 - accuracy: 0.3309 - val_loss: 0.1164 - val_accuracy: 0.3165\n",
      "Epoch 658/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0672 - accuracy: 0.3273 - val_loss: 0.1184 - val_accuracy: 0.2500\n",
      "Epoch 659/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0676 - accuracy: 0.3225 - val_loss: 0.1132 - val_accuracy: 0.3363\n",
      "Epoch 660/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0671 - accuracy: 0.3339 - val_loss: 0.1132 - val_accuracy: 0.3004\n",
      "Epoch 661/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0672 - accuracy: 0.3285 - val_loss: 0.1143 - val_accuracy: 0.3004\n",
      "Epoch 662/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0669 - accuracy: 0.3345 - val_loss: 0.1116 - val_accuracy: 0.3094\n",
      "Epoch 663/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0668 - accuracy: 0.3261 - val_loss: 0.1141 - val_accuracy: 0.3040\n",
      "Epoch 664/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0670 - accuracy: 0.3267 - val_loss: 0.1130 - val_accuracy: 0.3076\n",
      "Epoch 665/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0668 - accuracy: 0.3279 - val_loss: 0.1330 - val_accuracy: 0.3795\n",
      "Epoch 666/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0676 - accuracy: 0.3291 - val_loss: 0.1143 - val_accuracy: 0.2914\n",
      "Epoch 667/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0669 - accuracy: 0.3267 - val_loss: 0.1140 - val_accuracy: 0.3147\n",
      "Epoch 668/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0667 - accuracy: 0.3207 - val_loss: 0.1136 - val_accuracy: 0.3309\n",
      "Epoch 669/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0664 - accuracy: 0.3279 - val_loss: 0.1127 - val_accuracy: 0.3291\n",
      "Epoch 670/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0665 - accuracy: 0.3327 - val_loss: 0.1141 - val_accuracy: 0.3183\n",
      "Epoch 671/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0666 - accuracy: 0.3327 - val_loss: 0.1343 - val_accuracy: 0.2212\n",
      "Epoch 672/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0675 - accuracy: 0.3195 - val_loss: 0.1155 - val_accuracy: 0.3022\n",
      "Epoch 673/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0667 - accuracy: 0.3267 - val_loss: 0.1103 - val_accuracy: 0.3076\n",
      "Epoch 674/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0665 - accuracy: 0.3339 - val_loss: 0.1124 - val_accuracy: 0.3165\n",
      "Epoch 675/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0665 - accuracy: 0.3207 - val_loss: 0.1121 - val_accuracy: 0.3255\n",
      "Epoch 676/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0663 - accuracy: 0.3297 - val_loss: 0.1143 - val_accuracy: 0.3147\n",
      "Epoch 677/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0663 - accuracy: 0.3303 - val_loss: 0.1111 - val_accuracy: 0.3129\n",
      "Epoch 678/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0660 - accuracy: 0.3279 - val_loss: 0.1197 - val_accuracy: 0.3129\n",
      "Epoch 679/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0666 - accuracy: 0.3357 - val_loss: 0.1135 - val_accuracy: 0.3327\n",
      "Epoch 680/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0661 - accuracy: 0.3321 - val_loss: 0.1117 - val_accuracy: 0.3183\n",
      "Epoch 681/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0659 - accuracy: 0.3273 - val_loss: 0.1137 - val_accuracy: 0.3255\n",
      "Epoch 682/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0659 - accuracy: 0.3273 - val_loss: 0.1144 - val_accuracy: 0.3327\n",
      "Epoch 683/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0659 - accuracy: 0.3267 - val_loss: 0.1157 - val_accuracy: 0.3291\n",
      "Epoch 684/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0659 - accuracy: 0.3291 - val_loss: 0.1145 - val_accuracy: 0.3237\n",
      "Epoch 685/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0658 - accuracy: 0.3267 - val_loss: 0.1163 - val_accuracy: 0.3507\n",
      "Epoch 686/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0660 - accuracy: 0.3387 - val_loss: 0.1119 - val_accuracy: 0.3022\n",
      "Epoch 687/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0658 - accuracy: 0.3351 - val_loss: 0.1129 - val_accuracy: 0.3327\n",
      "Epoch 688/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0656 - accuracy: 0.3303 - val_loss: 0.1126 - val_accuracy: 0.3183\n",
      "Epoch 689/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0655 - accuracy: 0.3273 - val_loss: 0.1151 - val_accuracy: 0.2914\n",
      "Epoch 690/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0656 - accuracy: 0.3225 - val_loss: 0.1169 - val_accuracy: 0.3201\n",
      "Epoch 691/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0659 - accuracy: 0.3243 - val_loss: 0.1132 - val_accuracy: 0.3165\n",
      "Epoch 692/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0655 - accuracy: 0.3267 - val_loss: 0.1128 - val_accuracy: 0.3237\n",
      "Epoch 693/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0653 - accuracy: 0.3285 - val_loss: 0.1156 - val_accuracy: 0.3201\n",
      "Epoch 694/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0655 - accuracy: 0.3213 - val_loss: 0.1141 - val_accuracy: 0.3291\n",
      "Epoch 695/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0652 - accuracy: 0.3237 - val_loss: 0.1176 - val_accuracy: 0.2914\n",
      "Epoch 696/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0656 - accuracy: 0.3195 - val_loss: 0.1151 - val_accuracy: 0.3363\n",
      "Epoch 697/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0655 - accuracy: 0.3327 - val_loss: 0.1200 - val_accuracy: 0.3309\n",
      "Epoch 698/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0655 - accuracy: 0.3459 - val_loss: 0.1104 - val_accuracy: 0.3183\n",
      "Epoch 699/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0651 - accuracy: 0.3321 - val_loss: 0.1124 - val_accuracy: 0.3345\n",
      "Epoch 700/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0650 - accuracy: 0.3363 - val_loss: 0.1148 - val_accuracy: 0.3381\n",
      "Epoch 701/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0650 - accuracy: 0.3381 - val_loss: 0.1342 - val_accuracy: 0.3165\n",
      "Epoch 702/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0665 - accuracy: 0.3231 - val_loss: 0.1160 - val_accuracy: 0.3381\n",
      "Epoch 703/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0651 - accuracy: 0.3279 - val_loss: 0.1129 - val_accuracy: 0.3165\n",
      "Epoch 704/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0651 - accuracy: 0.3255 - val_loss: 0.1182 - val_accuracy: 0.3273\n",
      "Epoch 705/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0652 - accuracy: 0.3351 - val_loss: 0.1146 - val_accuracy: 0.3381\n",
      "Epoch 706/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0650 - accuracy: 0.3339 - val_loss: 0.1132 - val_accuracy: 0.3201\n",
      "Epoch 707/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0648 - accuracy: 0.3333 - val_loss: 0.1136 - val_accuracy: 0.3219\n",
      "Epoch 708/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0649 - accuracy: 0.3333 - val_loss: 0.1117 - val_accuracy: 0.3345\n",
      "Epoch 709/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0647 - accuracy: 0.3285 - val_loss: 0.1191 - val_accuracy: 0.2680\n",
      "Epoch 710/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0653 - accuracy: 0.3249 - val_loss: 0.1491 - val_accuracy: 0.3363\n",
      "Epoch 711/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0660 - accuracy: 0.3291 - val_loss: 0.1141 - val_accuracy: 0.3273\n",
      "Epoch 712/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0646 - accuracy: 0.3285 - val_loss: 0.1145 - val_accuracy: 0.3255\n",
      "Epoch 713/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0647 - accuracy: 0.3357 - val_loss: 0.1110 - val_accuracy: 0.3273\n",
      "Epoch 714/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0646 - accuracy: 0.3297 - val_loss: 0.1118 - val_accuracy: 0.3219\n",
      "Epoch 715/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0643 - accuracy: 0.3315 - val_loss: 0.1128 - val_accuracy: 0.2896\n",
      "Epoch 716/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0647 - accuracy: 0.3225 - val_loss: 0.1138 - val_accuracy: 0.3058\n",
      "Epoch 717/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0644 - accuracy: 0.3351 - val_loss: 0.1281 - val_accuracy: 0.4137\n",
      "Epoch 718/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0653 - accuracy: 0.3417 - val_loss: 0.1147 - val_accuracy: 0.3094\n",
      "Epoch 719/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0647 - accuracy: 0.3261 - val_loss: 0.1105 - val_accuracy: 0.3471\n",
      "Epoch 720/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0643 - accuracy: 0.3249 - val_loss: 0.1110 - val_accuracy: 0.3183\n",
      "Epoch 721/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0642 - accuracy: 0.3267 - val_loss: 0.1151 - val_accuracy: 0.3345\n",
      "Epoch 722/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0644 - accuracy: 0.3261 - val_loss: 0.1128 - val_accuracy: 0.3381\n",
      "Epoch 723/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0639 - accuracy: 0.3297 - val_loss: 0.1094 - val_accuracy: 0.3183\n",
      "Epoch 724/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0638 - accuracy: 0.3237 - val_loss: 0.1124 - val_accuracy: 0.3363\n",
      "Epoch 725/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0638 - accuracy: 0.3231 - val_loss: 0.1115 - val_accuracy: 0.3345\n",
      "Epoch 726/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0638 - accuracy: 0.3285 - val_loss: 0.1199 - val_accuracy: 0.3237\n",
      "Epoch 727/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0644 - accuracy: 0.3381 - val_loss: 0.1111 - val_accuracy: 0.3201\n",
      "Epoch 728/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0637 - accuracy: 0.3219 - val_loss: 0.1132 - val_accuracy: 0.3183\n",
      "Epoch 729/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0639 - accuracy: 0.3321 - val_loss: 0.1129 - val_accuracy: 0.3291\n",
      "Epoch 730/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0639 - accuracy: 0.3291 - val_loss: 0.1181 - val_accuracy: 0.3237\n",
      "Epoch 731/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0640 - accuracy: 0.3393 - val_loss: 0.1132 - val_accuracy: 0.3219\n",
      "Epoch 732/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0638 - accuracy: 0.3297 - val_loss: 0.1285 - val_accuracy: 0.3004\n",
      "Epoch 733/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0647 - accuracy: 0.3285 - val_loss: 0.1147 - val_accuracy: 0.3309\n",
      "Epoch 734/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0636 - accuracy: 0.3303 - val_loss: 0.1101 - val_accuracy: 0.3147\n",
      "Epoch 735/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0635 - accuracy: 0.3243 - val_loss: 0.1147 - val_accuracy: 0.3345\n",
      "Epoch 736/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0634 - accuracy: 0.3321 - val_loss: 0.1115 - val_accuracy: 0.3201\n",
      "Epoch 737/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0634 - accuracy: 0.3297 - val_loss: 0.1106 - val_accuracy: 0.3147\n",
      "Epoch 738/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0634 - accuracy: 0.3231 - val_loss: 0.1118 - val_accuracy: 0.3219\n",
      "Epoch 739/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0634 - accuracy: 0.3387 - val_loss: 0.1155 - val_accuracy: 0.3435\n",
      "Epoch 740/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0634 - accuracy: 0.3309 - val_loss: 0.1174 - val_accuracy: 0.2986\n",
      "Epoch 741/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0635 - accuracy: 0.3285 - val_loss: 0.1227 - val_accuracy: 0.3363\n",
      "Epoch 742/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0638 - accuracy: 0.3135 - val_loss: 0.1106 - val_accuracy: 0.3183\n",
      "Epoch 743/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0630 - accuracy: 0.3315 - val_loss: 0.1116 - val_accuracy: 0.3381\n",
      "Epoch 744/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0631 - accuracy: 0.3267 - val_loss: 0.1126 - val_accuracy: 0.3291\n",
      "Epoch 745/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0631 - accuracy: 0.3339 - val_loss: 0.1243 - val_accuracy: 0.3381\n",
      "Epoch 746/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0639 - accuracy: 0.3321 - val_loss: 0.1296 - val_accuracy: 0.3525\n",
      "Epoch 747/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0644 - accuracy: 0.3291 - val_loss: 0.1147 - val_accuracy: 0.3183\n",
      "Epoch 748/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0631 - accuracy: 0.3219 - val_loss: 0.1108 - val_accuracy: 0.3273\n",
      "Epoch 749/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0629 - accuracy: 0.3225 - val_loss: 0.1128 - val_accuracy: 0.3525\n",
      "Epoch 750/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0630 - accuracy: 0.3327 - val_loss: 0.1116 - val_accuracy: 0.3147\n",
      "Epoch 751/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0630 - accuracy: 0.3267 - val_loss: 0.1199 - val_accuracy: 0.2752\n",
      "Epoch 752/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0632 - accuracy: 0.3327 - val_loss: 0.1135 - val_accuracy: 0.3291\n",
      "Epoch 753/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0628 - accuracy: 0.3249 - val_loss: 0.1155 - val_accuracy: 0.3201\n",
      "Epoch 754/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0632 - accuracy: 0.3255 - val_loss: 0.1154 - val_accuracy: 0.3489\n",
      "Epoch 755/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0628 - accuracy: 0.3231 - val_loss: 0.1080 - val_accuracy: 0.3237\n",
      "Epoch 756/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0628 - accuracy: 0.3315 - val_loss: 0.1095 - val_accuracy: 0.3309\n",
      "Epoch 757/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0625 - accuracy: 0.3285 - val_loss: 0.1112 - val_accuracy: 0.3273\n",
      "Epoch 758/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0626 - accuracy: 0.3297 - val_loss: 0.1119 - val_accuracy: 0.3183\n",
      "Epoch 759/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0628 - accuracy: 0.3303 - val_loss: 0.1116 - val_accuracy: 0.3237\n",
      "Epoch 760/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0626 - accuracy: 0.3225 - val_loss: 0.1110 - val_accuracy: 0.3327\n",
      "Epoch 761/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0624 - accuracy: 0.3321 - val_loss: 0.1113 - val_accuracy: 0.3237\n",
      "Epoch 762/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0623 - accuracy: 0.3279 - val_loss: 0.1114 - val_accuracy: 0.3417\n",
      "Epoch 763/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0623 - accuracy: 0.3339 - val_loss: 0.1144 - val_accuracy: 0.3507\n",
      "Epoch 764/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0625 - accuracy: 0.3333 - val_loss: 0.1183 - val_accuracy: 0.2896\n",
      "Epoch 765/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0631 - accuracy: 0.3219 - val_loss: 0.1104 - val_accuracy: 0.3309\n",
      "Epoch 766/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0622 - accuracy: 0.3321 - val_loss: 0.1107 - val_accuracy: 0.3273\n",
      "Epoch 767/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0620 - accuracy: 0.3303 - val_loss: 0.1109 - val_accuracy: 0.3129\n",
      "Epoch 768/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0622 - accuracy: 0.3237 - val_loss: 0.1119 - val_accuracy: 0.3453\n",
      "Epoch 769/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0620 - accuracy: 0.3381 - val_loss: 0.1109 - val_accuracy: 0.3201\n",
      "Epoch 770/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0622 - accuracy: 0.3309 - val_loss: 0.1118 - val_accuracy: 0.3309\n",
      "Epoch 771/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0619 - accuracy: 0.3321 - val_loss: 0.1093 - val_accuracy: 0.3417\n",
      "Epoch 772/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0621 - accuracy: 0.3261 - val_loss: 0.1128 - val_accuracy: 0.3255\n",
      "Epoch 773/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0623 - accuracy: 0.3255 - val_loss: 0.1119 - val_accuracy: 0.3237\n",
      "Epoch 774/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0619 - accuracy: 0.3207 - val_loss: 0.1104 - val_accuracy: 0.3147\n",
      "Epoch 775/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0621 - accuracy: 0.3237 - val_loss: 0.1114 - val_accuracy: 0.3219\n",
      "Epoch 776/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0617 - accuracy: 0.3273 - val_loss: 0.1169 - val_accuracy: 0.3291\n",
      "Epoch 777/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0620 - accuracy: 0.3219 - val_loss: 0.1128 - val_accuracy: 0.3219\n",
      "Epoch 778/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0618 - accuracy: 0.3333 - val_loss: 0.1123 - val_accuracy: 0.3309\n",
      "Epoch 779/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0617 - accuracy: 0.3291 - val_loss: 0.1130 - val_accuracy: 0.3381\n",
      "Epoch 780/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0620 - accuracy: 0.3189 - val_loss: 0.1109 - val_accuracy: 0.3201\n",
      "Epoch 781/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0615 - accuracy: 0.3213 - val_loss: 0.1090 - val_accuracy: 0.3255\n",
      "Epoch 782/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0615 - accuracy: 0.3291 - val_loss: 0.1168 - val_accuracy: 0.3345\n",
      "Epoch 783/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0617 - accuracy: 0.3381 - val_loss: 0.1115 - val_accuracy: 0.3489\n",
      "Epoch 784/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0617 - accuracy: 0.3249 - val_loss: 0.1124 - val_accuracy: 0.3309\n",
      "Epoch 785/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0615 - accuracy: 0.3147 - val_loss: 0.1094 - val_accuracy: 0.3417\n",
      "Epoch 786/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0612 - accuracy: 0.3213 - val_loss: 0.1108 - val_accuracy: 0.3291\n",
      "Epoch 787/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0612 - accuracy: 0.3309 - val_loss: 0.1114 - val_accuracy: 0.3417\n",
      "Epoch 788/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0616 - accuracy: 0.3291 - val_loss: 0.1131 - val_accuracy: 0.3363\n",
      "Epoch 789/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0613 - accuracy: 0.3303 - val_loss: 0.1120 - val_accuracy: 0.3255\n",
      "Epoch 790/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0613 - accuracy: 0.3279 - val_loss: 0.1151 - val_accuracy: 0.3004\n",
      "Epoch 791/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0613 - accuracy: 0.3351 - val_loss: 0.1109 - val_accuracy: 0.3525\n",
      "Epoch 792/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0611 - accuracy: 0.3243 - val_loss: 0.1097 - val_accuracy: 0.3399\n",
      "Epoch 793/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0609 - accuracy: 0.3339 - val_loss: 0.1114 - val_accuracy: 0.3327\n",
      "Epoch 794/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0610 - accuracy: 0.3243 - val_loss: 0.1124 - val_accuracy: 0.3399\n",
      "Epoch 795/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0609 - accuracy: 0.3315 - val_loss: 0.1117 - val_accuracy: 0.3327\n",
      "Epoch 796/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0610 - accuracy: 0.3273 - val_loss: 0.1095 - val_accuracy: 0.3417\n",
      "Epoch 797/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0610 - accuracy: 0.3309 - val_loss: 0.1086 - val_accuracy: 0.3237\n",
      "Epoch 798/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0609 - accuracy: 0.3321 - val_loss: 0.1099 - val_accuracy: 0.3345\n",
      "Epoch 799/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0609 - accuracy: 0.3201 - val_loss: 0.1130 - val_accuracy: 0.3417\n",
      "Epoch 800/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0607 - accuracy: 0.3291 - val_loss: 0.1098 - val_accuracy: 0.3291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-5f8a8e11cd4a>:128: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "  tprs.append(interp(mean_fpr, fpr, tpr))\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Epoch 1/800\n",
      "53/53 [==============================] - 1s 3ms/step - loss: 0.6689 - accuracy: 0.0192 - val_loss: 0.6359 - val_accuracy: 0.0252\n",
      "Epoch 2/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.5076 - accuracy: 0.0132 - val_loss: 0.3305 - val_accuracy: 0.0360\n",
      "Epoch 3/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.2098 - accuracy: 0.0288 - val_loss: 0.2547 - val_accuracy: 0.0432\n",
      "Epoch 4/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1892 - accuracy: 0.0438 - val_loss: 0.2516 - val_accuracy: 0.0737\n",
      "Epoch 5/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1876 - accuracy: 0.0535 - val_loss: 0.2507 - val_accuracy: 0.0432\n",
      "Epoch 6/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1872 - accuracy: 0.0505 - val_loss: 0.2500 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1870 - accuracy: 0.0378 - val_loss: 0.2503 - val_accuracy: 0.0683\n",
      "Epoch 8/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1869 - accuracy: 0.0444 - val_loss: 0.2499 - val_accuracy: 0.0450\n",
      "Epoch 9/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1868 - accuracy: 0.0378 - val_loss: 0.2499 - val_accuracy: 0.0234\n",
      "Epoch 10/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1868 - accuracy: 0.0432 - val_loss: 0.2494 - val_accuracy: 0.0450\n",
      "Epoch 11/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1867 - accuracy: 0.0402 - val_loss: 0.2498 - val_accuracy: 0.0414\n",
      "Epoch 12/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1866 - accuracy: 0.0444 - val_loss: 0.2496 - val_accuracy: 0.0360\n",
      "Epoch 13/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1865 - accuracy: 0.0402 - val_loss: 0.2492 - val_accuracy: 0.0360\n",
      "Epoch 14/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1864 - accuracy: 0.0312 - val_loss: 0.2490 - val_accuracy: 0.0504\n",
      "Epoch 15/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1864 - accuracy: 0.0456 - val_loss: 0.2490 - val_accuracy: 0.0468\n",
      "Epoch 16/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1864 - accuracy: 0.0565 - val_loss: 0.2493 - val_accuracy: 0.0540\n",
      "Epoch 17/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1862 - accuracy: 0.0685 - val_loss: 0.2495 - val_accuracy: 0.0360\n",
      "Epoch 18/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1862 - accuracy: 0.0523 - val_loss: 0.2491 - val_accuracy: 0.0252\n",
      "Epoch 19/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1861 - accuracy: 0.0583 - val_loss: 0.2494 - val_accuracy: 0.0432\n",
      "Epoch 20/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1861 - accuracy: 0.0583 - val_loss: 0.2495 - val_accuracy: 0.0432\n",
      "Epoch 21/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1860 - accuracy: 0.0559 - val_loss: 0.2492 - val_accuracy: 0.0432\n",
      "Epoch 22/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1859 - accuracy: 0.0565 - val_loss: 0.2485 - val_accuracy: 0.0360\n",
      "Epoch 23/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1858 - accuracy: 0.0571 - val_loss: 0.2487 - val_accuracy: 0.0486\n",
      "Epoch 24/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1857 - accuracy: 0.0631 - val_loss: 0.2489 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1856 - accuracy: 0.0336 - val_loss: 0.2484 - val_accuracy: 0.0486\n",
      "Epoch 26/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1856 - accuracy: 0.0523 - val_loss: 0.2480 - val_accuracy: 0.0018\n",
      "Epoch 27/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1855 - accuracy: 0.0517 - val_loss: 0.2482 - val_accuracy: 0.0036\n",
      "Epoch 28/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1855 - accuracy: 0.0432 - val_loss: 0.2481 - val_accuracy: 0.0054\n",
      "Epoch 29/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1854 - accuracy: 0.0330 - val_loss: 0.2477 - val_accuracy: 0.0360\n",
      "Epoch 30/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1853 - accuracy: 0.0637 - val_loss: 0.2480 - val_accuracy: 0.0126\n",
      "Epoch 31/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1852 - accuracy: 0.0384 - val_loss: 0.2474 - val_accuracy: 0.0773\n",
      "Epoch 32/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1852 - accuracy: 0.0727 - val_loss: 0.2472 - val_accuracy: 0.0540\n",
      "Epoch 33/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1851 - accuracy: 0.0631 - val_loss: 0.2475 - val_accuracy: 0.0737\n",
      "Epoch 34/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1849 - accuracy: 0.0715 - val_loss: 0.2470 - val_accuracy: 0.0360\n",
      "Epoch 35/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1849 - accuracy: 0.0631 - val_loss: 0.2471 - val_accuracy: 0.0342\n",
      "Epoch 36/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1848 - accuracy: 0.0517 - val_loss: 0.2467 - val_accuracy: 0.0054\n",
      "Epoch 37/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1848 - accuracy: 0.0763 - val_loss: 0.2471 - val_accuracy: 0.0629\n",
      "Epoch 38/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1847 - accuracy: 0.0787 - val_loss: 0.2468 - val_accuracy: 0.0018\n",
      "Epoch 39/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1846 - accuracy: 0.0607 - val_loss: 0.2468 - val_accuracy: 0.0108\n",
      "Epoch 40/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1845 - accuracy: 0.0414 - val_loss: 0.2467 - val_accuracy: 0.0558\n",
      "Epoch 41/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1844 - accuracy: 0.1051 - val_loss: 0.2461 - val_accuracy: 0.0647\n",
      "Epoch 42/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1843 - accuracy: 0.0793 - val_loss: 0.2464 - val_accuracy: 0.0486\n",
      "Epoch 43/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1843 - accuracy: 0.0673 - val_loss: 0.2459 - val_accuracy: 0.0594\n",
      "Epoch 44/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1841 - accuracy: 0.0895 - val_loss: 0.2462 - val_accuracy: 0.0360\n",
      "Epoch 45/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1841 - accuracy: 0.0775 - val_loss: 0.2461 - val_accuracy: 0.0144\n",
      "Epoch 46/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1840 - accuracy: 0.0679 - val_loss: 0.2462 - val_accuracy: 0.0108\n",
      "Epoch 47/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1839 - accuracy: 0.0571 - val_loss: 0.2463 - val_accuracy: 0.0306\n",
      "Epoch 48/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1838 - accuracy: 0.0643 - val_loss: 0.2457 - val_accuracy: 0.0612\n",
      "Epoch 49/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1836 - accuracy: 0.1093 - val_loss: 0.2459 - val_accuracy: 0.0881\n",
      "Epoch 50/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1835 - accuracy: 0.1117 - val_loss: 0.2452 - val_accuracy: 0.0576\n",
      "Epoch 51/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1834 - accuracy: 0.0823 - val_loss: 0.2450 - val_accuracy: 0.0342\n",
      "Epoch 52/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1833 - accuracy: 0.0931 - val_loss: 0.2448 - val_accuracy: 0.0647\n",
      "Epoch 53/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1832 - accuracy: 0.0907 - val_loss: 0.2446 - val_accuracy: 0.1493\n",
      "Epoch 54/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1831 - accuracy: 0.1165 - val_loss: 0.2446 - val_accuracy: 0.1133\n",
      "Epoch 55/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1829 - accuracy: 0.1147 - val_loss: 0.2444 - val_accuracy: 0.0989\n",
      "Epoch 56/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1828 - accuracy: 0.1129 - val_loss: 0.2441 - val_accuracy: 0.0845\n",
      "Epoch 57/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1827 - accuracy: 0.1201 - val_loss: 0.2440 - val_accuracy: 0.0288\n",
      "Epoch 58/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1825 - accuracy: 0.0823 - val_loss: 0.2438 - val_accuracy: 0.0827\n",
      "Epoch 59/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1824 - accuracy: 0.1027 - val_loss: 0.2433 - val_accuracy: 0.0899\n",
      "Epoch 60/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1822 - accuracy: 0.1099 - val_loss: 0.2436 - val_accuracy: 0.0234\n",
      "Epoch 61/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1821 - accuracy: 0.1009 - val_loss: 0.2434 - val_accuracy: 0.0863\n",
      "Epoch 62/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1819 - accuracy: 0.1075 - val_loss: 0.2433 - val_accuracy: 0.0468\n",
      "Epoch 63/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1817 - accuracy: 0.0823 - val_loss: 0.2427 - val_accuracy: 0.1115\n",
      "Epoch 64/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1816 - accuracy: 0.1195 - val_loss: 0.2425 - val_accuracy: 0.0701\n",
      "Epoch 65/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1814 - accuracy: 0.0895 - val_loss: 0.2428 - val_accuracy: 0.1007\n",
      "Epoch 66/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1812 - accuracy: 0.1165 - val_loss: 0.2419 - val_accuracy: 0.1385\n",
      "Epoch 67/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1811 - accuracy: 0.1315 - val_loss: 0.2416 - val_accuracy: 0.0342\n",
      "Epoch 68/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1809 - accuracy: 0.0907 - val_loss: 0.2415 - val_accuracy: 0.0935\n",
      "Epoch 69/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1807 - accuracy: 0.1033 - val_loss: 0.2412 - val_accuracy: 0.0719\n",
      "Epoch 70/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1805 - accuracy: 0.1153 - val_loss: 0.2407 - val_accuracy: 0.0971\n",
      "Epoch 71/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1803 - accuracy: 0.1093 - val_loss: 0.2411 - val_accuracy: 0.1223\n",
      "Epoch 72/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1801 - accuracy: 0.1297 - val_loss: 0.2406 - val_accuracy: 0.0504\n",
      "Epoch 73/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1798 - accuracy: 0.1015 - val_loss: 0.2407 - val_accuracy: 0.1133\n",
      "Epoch 74/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1796 - accuracy: 0.1267 - val_loss: 0.2397 - val_accuracy: 0.1043\n",
      "Epoch 75/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1794 - accuracy: 0.1387 - val_loss: 0.2397 - val_accuracy: 0.1061\n",
      "Epoch 76/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1791 - accuracy: 0.1333 - val_loss: 0.2399 - val_accuracy: 0.0989\n",
      "Epoch 77/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1789 - accuracy: 0.1279 - val_loss: 0.2389 - val_accuracy: 0.0486\n",
      "Epoch 78/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1786 - accuracy: 0.1207 - val_loss: 0.2384 - val_accuracy: 0.0899\n",
      "Epoch 79/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1784 - accuracy: 0.1165 - val_loss: 0.2378 - val_accuracy: 0.1529\n",
      "Epoch 80/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1780 - accuracy: 0.1423 - val_loss: 0.2382 - val_accuracy: 0.1097\n",
      "Epoch 81/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1778 - accuracy: 0.1393 - val_loss: 0.2373 - val_accuracy: 0.1529\n",
      "Epoch 82/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1775 - accuracy: 0.1622 - val_loss: 0.2366 - val_accuracy: 0.1601\n",
      "Epoch 83/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1771 - accuracy: 0.1417 - val_loss: 0.2362 - val_accuracy: 0.1025\n",
      "Epoch 84/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1768 - accuracy: 0.1291 - val_loss: 0.2350 - val_accuracy: 0.1223\n",
      "Epoch 85/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1765 - accuracy: 0.1465 - val_loss: 0.2353 - val_accuracy: 0.1727\n",
      "Epoch 86/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1760 - accuracy: 0.1598 - val_loss: 0.2352 - val_accuracy: 0.1583\n",
      "Epoch 87/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1757 - accuracy: 0.1568 - val_loss: 0.2346 - val_accuracy: 0.1115\n",
      "Epoch 88/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1754 - accuracy: 0.1502 - val_loss: 0.2338 - val_accuracy: 0.1529\n",
      "Epoch 89/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1749 - accuracy: 0.1634 - val_loss: 0.2334 - val_accuracy: 0.1439\n",
      "Epoch 90/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1745 - accuracy: 0.1574 - val_loss: 0.2331 - val_accuracy: 0.1619\n",
      "Epoch 91/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1741 - accuracy: 0.1652 - val_loss: 0.2322 - val_accuracy: 0.1511\n",
      "Epoch 92/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1736 - accuracy: 0.1670 - val_loss: 0.2321 - val_accuracy: 0.1385\n",
      "Epoch 93/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1731 - accuracy: 0.1508 - val_loss: 0.2309 - val_accuracy: 0.1439\n",
      "Epoch 94/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1727 - accuracy: 0.1574 - val_loss: 0.2302 - val_accuracy: 0.1277\n",
      "Epoch 95/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1722 - accuracy: 0.1538 - val_loss: 0.2296 - val_accuracy: 0.1547\n",
      "Epoch 96/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1717 - accuracy: 0.1628 - val_loss: 0.2288 - val_accuracy: 0.1835\n",
      "Epoch 97/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1712 - accuracy: 0.1700 - val_loss: 0.2284 - val_accuracy: 0.1709\n",
      "Epoch 98/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1707 - accuracy: 0.1724 - val_loss: 0.2270 - val_accuracy: 0.1745\n",
      "Epoch 99/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1701 - accuracy: 0.1754 - val_loss: 0.2269 - val_accuracy: 0.1547\n",
      "Epoch 100/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1695 - accuracy: 0.1790 - val_loss: 0.2263 - val_accuracy: 0.1619\n",
      "Epoch 101/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1689 - accuracy: 0.1850 - val_loss: 0.2251 - val_accuracy: 0.1996\n",
      "Epoch 102/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1683 - accuracy: 0.1784 - val_loss: 0.2242 - val_accuracy: 0.1691\n",
      "Epoch 103/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1677 - accuracy: 0.1700 - val_loss: 0.2238 - val_accuracy: 0.1763\n",
      "Epoch 104/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1671 - accuracy: 0.1940 - val_loss: 0.2231 - val_accuracy: 0.1817\n",
      "Epoch 105/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1665 - accuracy: 0.1952 - val_loss: 0.2222 - val_accuracy: 0.1763\n",
      "Epoch 106/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1659 - accuracy: 0.1874 - val_loss: 0.2208 - val_accuracy: 0.2194\n",
      "Epoch 107/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1653 - accuracy: 0.2078 - val_loss: 0.2203 - val_accuracy: 0.1888\n",
      "Epoch 108/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1646 - accuracy: 0.1982 - val_loss: 0.2194 - val_accuracy: 0.2212\n",
      "Epoch 109/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1639 - accuracy: 0.2132 - val_loss: 0.2184 - val_accuracy: 0.2086\n",
      "Epoch 110/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1633 - accuracy: 0.2060 - val_loss: 0.2178 - val_accuracy: 0.1799\n",
      "Epoch 111/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1626 - accuracy: 0.2036 - val_loss: 0.2165 - val_accuracy: 0.1960\n",
      "Epoch 112/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1619 - accuracy: 0.2210 - val_loss: 0.2158 - val_accuracy: 0.1601\n",
      "Epoch 113/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1613 - accuracy: 0.2078 - val_loss: 0.2152 - val_accuracy: 0.2230\n",
      "Epoch 114/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1605 - accuracy: 0.2204 - val_loss: 0.2143 - val_accuracy: 0.2176\n",
      "Epoch 115/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1600 - accuracy: 0.2192 - val_loss: 0.2134 - val_accuracy: 0.1799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1592 - accuracy: 0.2072 - val_loss: 0.2122 - val_accuracy: 0.2248\n",
      "Epoch 117/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1585 - accuracy: 0.2324 - val_loss: 0.2119 - val_accuracy: 0.1888\n",
      "Epoch 118/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1579 - accuracy: 0.2138 - val_loss: 0.2112 - val_accuracy: 0.2194\n",
      "Epoch 119/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1572 - accuracy: 0.2270 - val_loss: 0.2100 - val_accuracy: 0.2212\n",
      "Epoch 120/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1566 - accuracy: 0.2258 - val_loss: 0.2097 - val_accuracy: 0.2050\n",
      "Epoch 121/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1559 - accuracy: 0.2126 - val_loss: 0.2083 - val_accuracy: 0.2158\n",
      "Epoch 122/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1553 - accuracy: 0.2162 - val_loss: 0.2080 - val_accuracy: 0.2014\n",
      "Epoch 123/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1546 - accuracy: 0.2174 - val_loss: 0.2072 - val_accuracy: 0.2194\n",
      "Epoch 124/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1539 - accuracy: 0.2270 - val_loss: 0.2065 - val_accuracy: 0.2392\n",
      "Epoch 125/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1534 - accuracy: 0.2210 - val_loss: 0.2046 - val_accuracy: 0.2392\n",
      "Epoch 126/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1526 - accuracy: 0.2288 - val_loss: 0.2041 - val_accuracy: 0.2410\n",
      "Epoch 127/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1520 - accuracy: 0.2324 - val_loss: 0.2046 - val_accuracy: 0.2554\n",
      "Epoch 128/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1514 - accuracy: 0.2258 - val_loss: 0.2025 - val_accuracy: 0.2482\n",
      "Epoch 129/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1508 - accuracy: 0.2390 - val_loss: 0.2017 - val_accuracy: 0.2320\n",
      "Epoch 130/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1501 - accuracy: 0.2318 - val_loss: 0.2021 - val_accuracy: 0.2392\n",
      "Epoch 131/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1497 - accuracy: 0.2390 - val_loss: 0.2010 - val_accuracy: 0.2464\n",
      "Epoch 132/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1490 - accuracy: 0.2390 - val_loss: 0.2010 - val_accuracy: 0.1978\n",
      "Epoch 133/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1484 - accuracy: 0.2312 - val_loss: 0.1994 - val_accuracy: 0.2230\n",
      "Epoch 134/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1477 - accuracy: 0.2312 - val_loss: 0.1990 - val_accuracy: 0.1888\n",
      "Epoch 135/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1473 - accuracy: 0.2348 - val_loss: 0.1984 - val_accuracy: 0.2194\n",
      "Epoch 136/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1467 - accuracy: 0.2294 - val_loss: 0.1973 - val_accuracy: 0.2320\n",
      "Epoch 137/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1462 - accuracy: 0.2336 - val_loss: 0.1974 - val_accuracy: 0.2464\n",
      "Epoch 138/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1456 - accuracy: 0.2438 - val_loss: 0.1956 - val_accuracy: 0.2338\n",
      "Epoch 139/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1451 - accuracy: 0.2462 - val_loss: 0.1961 - val_accuracy: 0.2392\n",
      "Epoch 140/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1445 - accuracy: 0.2324 - val_loss: 0.2003 - val_accuracy: 0.2464\n",
      "Epoch 141/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1442 - accuracy: 0.2474 - val_loss: 0.1950 - val_accuracy: 0.1853\n",
      "Epoch 142/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1435 - accuracy: 0.2258 - val_loss: 0.1930 - val_accuracy: 0.2158\n",
      "Epoch 143/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1431 - accuracy: 0.2396 - val_loss: 0.1935 - val_accuracy: 0.2680\n",
      "Epoch 144/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1425 - accuracy: 0.2486 - val_loss: 0.1930 - val_accuracy: 0.2158\n",
      "Epoch 145/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1421 - accuracy: 0.2456 - val_loss: 0.1916 - val_accuracy: 0.2392\n",
      "Epoch 146/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1415 - accuracy: 0.2426 - val_loss: 0.1917 - val_accuracy: 0.2032\n",
      "Epoch 147/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1411 - accuracy: 0.2462 - val_loss: 0.1909 - val_accuracy: 0.2428\n",
      "Epoch 148/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1406 - accuracy: 0.2547 - val_loss: 0.1910 - val_accuracy: 0.2158\n",
      "Epoch 149/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1402 - accuracy: 0.2577 - val_loss: 0.1901 - val_accuracy: 0.2554\n",
      "Epoch 150/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1395 - accuracy: 0.2553 - val_loss: 0.1892 - val_accuracy: 0.2374\n",
      "Epoch 151/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1391 - accuracy: 0.2492 - val_loss: 0.1889 - val_accuracy: 0.2500\n",
      "Epoch 152/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1387 - accuracy: 0.2402 - val_loss: 0.1887 - val_accuracy: 0.2122\n",
      "Epoch 153/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1382 - accuracy: 0.2523 - val_loss: 0.1876 - val_accuracy: 0.2680\n",
      "Epoch 154/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1378 - accuracy: 0.2595 - val_loss: 0.1876 - val_accuracy: 0.2698\n",
      "Epoch 155/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1374 - accuracy: 0.2511 - val_loss: 0.1870 - val_accuracy: 0.2788\n",
      "Epoch 156/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1369 - accuracy: 0.2643 - val_loss: 0.1868 - val_accuracy: 0.2572\n",
      "Epoch 157/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1365 - accuracy: 0.2480 - val_loss: 0.1865 - val_accuracy: 0.2716\n",
      "Epoch 158/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1359 - accuracy: 0.2739 - val_loss: 0.1871 - val_accuracy: 0.2284\n",
      "Epoch 159/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1355 - accuracy: 0.2613 - val_loss: 0.1852 - val_accuracy: 0.2446\n",
      "Epoch 160/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1351 - accuracy: 0.2673 - val_loss: 0.1858 - val_accuracy: 0.2374\n",
      "Epoch 161/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1347 - accuracy: 0.2637 - val_loss: 0.1841 - val_accuracy: 0.2788\n",
      "Epoch 162/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1342 - accuracy: 0.2679 - val_loss: 0.1831 - val_accuracy: 0.2554\n",
      "Epoch 163/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1339 - accuracy: 0.2739 - val_loss: 0.1840 - val_accuracy: 0.2410\n",
      "Epoch 164/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1334 - accuracy: 0.2589 - val_loss: 0.1865 - val_accuracy: 0.2608\n",
      "Epoch 165/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1330 - accuracy: 0.2721 - val_loss: 0.1833 - val_accuracy: 0.2626\n",
      "Epoch 166/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1327 - accuracy: 0.2679 - val_loss: 0.1825 - val_accuracy: 0.2626\n",
      "Epoch 167/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1321 - accuracy: 0.2763 - val_loss: 0.1816 - val_accuracy: 0.2428\n",
      "Epoch 168/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1317 - accuracy: 0.2793 - val_loss: 0.1804 - val_accuracy: 0.2770\n",
      "Epoch 169/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1313 - accuracy: 0.2793 - val_loss: 0.1803 - val_accuracy: 0.2608\n",
      "Epoch 170/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1309 - accuracy: 0.2721 - val_loss: 0.1804 - val_accuracy: 0.2338\n",
      "Epoch 171/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1305 - accuracy: 0.2733 - val_loss: 0.1793 - val_accuracy: 0.2626\n",
      "Epoch 172/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1301 - accuracy: 0.2769 - val_loss: 0.1787 - val_accuracy: 0.2878\n",
      "Epoch 173/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1297 - accuracy: 0.2865 - val_loss: 0.1795 - val_accuracy: 0.3022\n",
      "Epoch 174/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1294 - accuracy: 0.2913 - val_loss: 0.1789 - val_accuracy: 0.2842\n",
      "Epoch 175/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1289 - accuracy: 0.2913 - val_loss: 0.1781 - val_accuracy: 0.2410\n",
      "Epoch 176/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1286 - accuracy: 0.2925 - val_loss: 0.1784 - val_accuracy: 0.2464\n",
      "Epoch 177/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1283 - accuracy: 0.2835 - val_loss: 0.1791 - val_accuracy: 0.2752\n",
      "Epoch 178/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1279 - accuracy: 0.2949 - val_loss: 0.1792 - val_accuracy: 0.2536\n",
      "Epoch 179/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1275 - accuracy: 0.2961 - val_loss: 0.1755 - val_accuracy: 0.2860\n",
      "Epoch 180/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1271 - accuracy: 0.2955 - val_loss: 0.1757 - val_accuracy: 0.2518\n",
      "Epoch 181/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1267 - accuracy: 0.2943 - val_loss: 0.1760 - val_accuracy: 0.2896\n",
      "Epoch 182/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1263 - accuracy: 0.2979 - val_loss: 0.1764 - val_accuracy: 0.2626\n",
      "Epoch 183/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1260 - accuracy: 0.3003 - val_loss: 0.1751 - val_accuracy: 0.2644\n",
      "Epoch 184/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1258 - accuracy: 0.2985 - val_loss: 0.1760 - val_accuracy: 0.3004\n",
      "Epoch 185/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1252 - accuracy: 0.2901 - val_loss: 0.1806 - val_accuracy: 0.2554\n",
      "Epoch 186/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1251 - accuracy: 0.2919 - val_loss: 0.1742 - val_accuracy: 0.2950\n",
      "Epoch 187/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1245 - accuracy: 0.3051 - val_loss: 0.1737 - val_accuracy: 0.2950\n",
      "Epoch 188/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1241 - accuracy: 0.3069 - val_loss: 0.1728 - val_accuracy: 0.2932\n",
      "Epoch 189/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1238 - accuracy: 0.3009 - val_loss: 0.1723 - val_accuracy: 0.3022\n",
      "Epoch 190/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1234 - accuracy: 0.2985 - val_loss: 0.1726 - val_accuracy: 0.2914\n",
      "Epoch 191/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1232 - accuracy: 0.3009 - val_loss: 0.1733 - val_accuracy: 0.2698\n",
      "Epoch 192/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1228 - accuracy: 0.3111 - val_loss: 0.1709 - val_accuracy: 0.3022\n",
      "Epoch 193/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1225 - accuracy: 0.2985 - val_loss: 0.1708 - val_accuracy: 0.2878\n",
      "Epoch 194/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1222 - accuracy: 0.3003 - val_loss: 0.1716 - val_accuracy: 0.2896\n",
      "Epoch 195/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1217 - accuracy: 0.3141 - val_loss: 0.1705 - val_accuracy: 0.3076\n",
      "Epoch 196/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1213 - accuracy: 0.3045 - val_loss: 0.1703 - val_accuracy: 0.3201\n",
      "Epoch 197/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1211 - accuracy: 0.3069 - val_loss: 0.1698 - val_accuracy: 0.3201\n",
      "Epoch 198/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1208 - accuracy: 0.3087 - val_loss: 0.1697 - val_accuracy: 0.3058\n",
      "Epoch 199/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1204 - accuracy: 0.3129 - val_loss: 0.1700 - val_accuracy: 0.3237\n",
      "Epoch 200/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1202 - accuracy: 0.3021 - val_loss: 0.1681 - val_accuracy: 0.3076\n",
      "Epoch 201/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1197 - accuracy: 0.3189 - val_loss: 0.1687 - val_accuracy: 0.3273\n",
      "Epoch 202/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1195 - accuracy: 0.3171 - val_loss: 0.1684 - val_accuracy: 0.2968\n",
      "Epoch 203/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1192 - accuracy: 0.3081 - val_loss: 0.1678 - val_accuracy: 0.2842\n",
      "Epoch 204/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1188 - accuracy: 0.3123 - val_loss: 0.1752 - val_accuracy: 0.3345\n",
      "Epoch 205/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1189 - accuracy: 0.3123 - val_loss: 0.1666 - val_accuracy: 0.2932\n",
      "Epoch 206/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1181 - accuracy: 0.3063 - val_loss: 0.1668 - val_accuracy: 0.3129\n",
      "Epoch 207/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1179 - accuracy: 0.3141 - val_loss: 0.1693 - val_accuracy: 0.3076\n",
      "Epoch 208/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1178 - accuracy: 0.3039 - val_loss: 0.1645 - val_accuracy: 0.3291\n",
      "Epoch 209/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1172 - accuracy: 0.3159 - val_loss: 0.1647 - val_accuracy: 0.3129\n",
      "Epoch 210/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1169 - accuracy: 0.3201 - val_loss: 0.1650 - val_accuracy: 0.2914\n",
      "Epoch 211/800\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.1168 - accuracy: 0.3123 - val_loss: 0.1643 - val_accuracy: 0.3327\n",
      "Epoch 212/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1164 - accuracy: 0.3177 - val_loss: 0.1649 - val_accuracy: 0.3309\n",
      "Epoch 213/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1162 - accuracy: 0.3183 - val_loss: 0.1652 - val_accuracy: 0.2986\n",
      "Epoch 214/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1158 - accuracy: 0.3285 - val_loss: 0.1644 - val_accuracy: 0.3291\n",
      "Epoch 215/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1157 - accuracy: 0.3141 - val_loss: 0.1649 - val_accuracy: 0.2770\n",
      "Epoch 216/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1154 - accuracy: 0.3183 - val_loss: 0.1620 - val_accuracy: 0.3201\n",
      "Epoch 217/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1151 - accuracy: 0.3159 - val_loss: 0.1631 - val_accuracy: 0.3219\n",
      "Epoch 218/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1149 - accuracy: 0.3135 - val_loss: 0.1632 - val_accuracy: 0.3076\n",
      "Epoch 219/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1146 - accuracy: 0.3177 - val_loss: 0.1617 - val_accuracy: 0.3058\n",
      "Epoch 220/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1145 - accuracy: 0.3183 - val_loss: 0.1627 - val_accuracy: 0.3291\n",
      "Epoch 221/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1142 - accuracy: 0.3261 - val_loss: 0.1605 - val_accuracy: 0.3112\n",
      "Epoch 222/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1139 - accuracy: 0.3159 - val_loss: 0.1620 - val_accuracy: 0.3112\n",
      "Epoch 223/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1136 - accuracy: 0.3069 - val_loss: 0.1617 - val_accuracy: 0.3273\n",
      "Epoch 224/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1134 - accuracy: 0.3183 - val_loss: 0.1612 - val_accuracy: 0.3399\n",
      "Epoch 225/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1131 - accuracy: 0.3183 - val_loss: 0.1609 - val_accuracy: 0.2914\n",
      "Epoch 226/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1131 - accuracy: 0.3117 - val_loss: 0.1612 - val_accuracy: 0.3489\n",
      "Epoch 227/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1129 - accuracy: 0.3261 - val_loss: 0.1612 - val_accuracy: 0.3219\n",
      "Epoch 228/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1126 - accuracy: 0.3165 - val_loss: 0.1602 - val_accuracy: 0.3363\n",
      "Epoch 229/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1123 - accuracy: 0.3225 - val_loss: 0.1588 - val_accuracy: 0.3291\n",
      "Epoch 230/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1121 - accuracy: 0.3219 - val_loss: 0.1594 - val_accuracy: 0.3417\n",
      "Epoch 231/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1119 - accuracy: 0.3291 - val_loss: 0.1646 - val_accuracy: 0.3147\n",
      "Epoch 232/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1121 - accuracy: 0.3177 - val_loss: 0.1588 - val_accuracy: 0.3255\n",
      "Epoch 233/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1116 - accuracy: 0.3189 - val_loss: 0.1587 - val_accuracy: 0.3507\n",
      "Epoch 234/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1114 - accuracy: 0.3135 - val_loss: 0.1593 - val_accuracy: 0.3129\n",
      "Epoch 235/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1113 - accuracy: 0.3255 - val_loss: 0.1580 - val_accuracy: 0.3327\n",
      "Epoch 236/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1110 - accuracy: 0.3231 - val_loss: 0.1589 - val_accuracy: 0.3058\n",
      "Epoch 237/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1109 - accuracy: 0.3207 - val_loss: 0.1593 - val_accuracy: 0.3363\n",
      "Epoch 238/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1106 - accuracy: 0.3171 - val_loss: 0.1583 - val_accuracy: 0.3327\n",
      "Epoch 239/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1104 - accuracy: 0.3183 - val_loss: 0.1568 - val_accuracy: 0.3327\n",
      "Epoch 240/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1102 - accuracy: 0.3153 - val_loss: 0.1573 - val_accuracy: 0.3237\n",
      "Epoch 241/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1102 - accuracy: 0.3273 - val_loss: 0.1590 - val_accuracy: 0.3309\n",
      "Epoch 242/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1100 - accuracy: 0.3183 - val_loss: 0.1566 - val_accuracy: 0.3435\n",
      "Epoch 243/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1096 - accuracy: 0.3207 - val_loss: 0.1579 - val_accuracy: 0.3435\n",
      "Epoch 244/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1097 - accuracy: 0.3171 - val_loss: 0.1573 - val_accuracy: 0.3381\n",
      "Epoch 245/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1094 - accuracy: 0.3147 - val_loss: 0.1625 - val_accuracy: 0.2662\n",
      "Epoch 246/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1095 - accuracy: 0.3189 - val_loss: 0.1555 - val_accuracy: 0.3291\n",
      "Epoch 247/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1091 - accuracy: 0.3147 - val_loss: 0.1556 - val_accuracy: 0.3435\n",
      "Epoch 248/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1088 - accuracy: 0.3207 - val_loss: 0.1573 - val_accuracy: 0.3201\n",
      "Epoch 249/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1087 - accuracy: 0.3225 - val_loss: 0.1560 - val_accuracy: 0.3183\n",
      "Epoch 250/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1085 - accuracy: 0.3219 - val_loss: 0.1557 - val_accuracy: 0.3381\n",
      "Epoch 251/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1085 - accuracy: 0.3225 - val_loss: 0.1560 - val_accuracy: 0.3381\n",
      "Epoch 252/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1082 - accuracy: 0.3141 - val_loss: 0.1549 - val_accuracy: 0.3309\n",
      "Epoch 253/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1080 - accuracy: 0.3177 - val_loss: 0.1558 - val_accuracy: 0.3399\n",
      "Epoch 254/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1078 - accuracy: 0.3201 - val_loss: 0.1560 - val_accuracy: 0.3183\n",
      "Epoch 255/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1076 - accuracy: 0.3165 - val_loss: 0.1567 - val_accuracy: 0.3183\n",
      "Epoch 256/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1076 - accuracy: 0.3135 - val_loss: 0.1549 - val_accuracy: 0.3669\n",
      "Epoch 257/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1074 - accuracy: 0.3177 - val_loss: 0.1552 - val_accuracy: 0.3309\n",
      "Epoch 258/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1072 - accuracy: 0.3183 - val_loss: 0.1555 - val_accuracy: 0.3291\n",
      "Epoch 259/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1070 - accuracy: 0.3147 - val_loss: 0.1543 - val_accuracy: 0.3237\n",
      "Epoch 260/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1069 - accuracy: 0.3195 - val_loss: 0.1550 - val_accuracy: 0.3291\n",
      "Epoch 261/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1068 - accuracy: 0.3225 - val_loss: 0.1536 - val_accuracy: 0.3309\n",
      "Epoch 262/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1066 - accuracy: 0.3129 - val_loss: 0.1546 - val_accuracy: 0.3255\n",
      "Epoch 263/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1064 - accuracy: 0.3219 - val_loss: 0.1539 - val_accuracy: 0.3381\n",
      "Epoch 264/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1063 - accuracy: 0.3267 - val_loss: 0.1531 - val_accuracy: 0.3273\n",
      "Epoch 265/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1060 - accuracy: 0.3189 - val_loss: 0.1538 - val_accuracy: 0.3327\n",
      "Epoch 266/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1058 - accuracy: 0.3195 - val_loss: 0.1544 - val_accuracy: 0.3237\n",
      "Epoch 267/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1058 - accuracy: 0.3219 - val_loss: 0.1574 - val_accuracy: 0.3094\n",
      "Epoch 268/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1059 - accuracy: 0.3183 - val_loss: 0.1521 - val_accuracy: 0.3435\n",
      "Epoch 269/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1054 - accuracy: 0.3123 - val_loss: 0.1551 - val_accuracy: 0.3327\n",
      "Epoch 270/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1054 - accuracy: 0.3207 - val_loss: 0.1527 - val_accuracy: 0.3507\n",
      "Epoch 271/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1051 - accuracy: 0.3135 - val_loss: 0.1528 - val_accuracy: 0.3183\n",
      "Epoch 272/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1051 - accuracy: 0.3141 - val_loss: 0.1539 - val_accuracy: 0.3147\n",
      "Epoch 273/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1049 - accuracy: 0.3117 - val_loss: 0.1507 - val_accuracy: 0.3435\n",
      "Epoch 274/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1046 - accuracy: 0.3195 - val_loss: 0.1525 - val_accuracy: 0.3507\n",
      "Epoch 275/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1046 - accuracy: 0.3153 - val_loss: 0.1526 - val_accuracy: 0.3255\n",
      "Epoch 276/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1043 - accuracy: 0.3165 - val_loss: 0.1523 - val_accuracy: 0.3237\n",
      "Epoch 277/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1043 - accuracy: 0.3069 - val_loss: 0.1519 - val_accuracy: 0.3327\n",
      "Epoch 278/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1041 - accuracy: 0.3171 - val_loss: 0.1595 - val_accuracy: 0.2806\n",
      "Epoch 279/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1042 - accuracy: 0.3135 - val_loss: 0.1534 - val_accuracy: 0.3219\n",
      "Epoch 280/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1038 - accuracy: 0.3177 - val_loss: 0.1517 - val_accuracy: 0.3291\n",
      "Epoch 281/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1036 - accuracy: 0.3195 - val_loss: 0.1520 - val_accuracy: 0.3309\n",
      "Epoch 282/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1032 - accuracy: 0.3165 - val_loss: 0.1508 - val_accuracy: 0.3453\n",
      "Epoch 283/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1032 - accuracy: 0.3219 - val_loss: 0.1587 - val_accuracy: 0.2842\n",
      "Epoch 284/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1035 - accuracy: 0.3075 - val_loss: 0.1501 - val_accuracy: 0.3237\n",
      "Epoch 285/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1028 - accuracy: 0.3069 - val_loss: 0.1494 - val_accuracy: 0.3291\n",
      "Epoch 286/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1028 - accuracy: 0.3105 - val_loss: 0.1507 - val_accuracy: 0.3273\n",
      "Epoch 287/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1026 - accuracy: 0.3105 - val_loss: 0.1503 - val_accuracy: 0.3381\n",
      "Epoch 288/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1024 - accuracy: 0.3237 - val_loss: 0.1514 - val_accuracy: 0.3219\n",
      "Epoch 289/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1023 - accuracy: 0.3195 - val_loss: 0.1507 - val_accuracy: 0.3255\n",
      "Epoch 290/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1022 - accuracy: 0.3129 - val_loss: 0.1490 - val_accuracy: 0.3363\n",
      "Epoch 291/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1018 - accuracy: 0.3213 - val_loss: 0.1499 - val_accuracy: 0.3327\n",
      "Epoch 292/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1018 - accuracy: 0.3207 - val_loss: 0.1503 - val_accuracy: 0.3471\n",
      "Epoch 293/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1016 - accuracy: 0.3225 - val_loss: 0.1599 - val_accuracy: 0.3471\n",
      "Epoch 294/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1019 - accuracy: 0.3165 - val_loss: 0.1492 - val_accuracy: 0.3417\n",
      "Epoch 295/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1013 - accuracy: 0.3123 - val_loss: 0.1495 - val_accuracy: 0.3327\n",
      "Epoch 296/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1011 - accuracy: 0.3093 - val_loss: 0.1570 - val_accuracy: 0.2806\n",
      "Epoch 297/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1013 - accuracy: 0.3099 - val_loss: 0.1492 - val_accuracy: 0.3489\n",
      "Epoch 298/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1008 - accuracy: 0.3093 - val_loss: 0.1493 - val_accuracy: 0.3471\n",
      "Epoch 299/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1005 - accuracy: 0.3189 - val_loss: 0.1480 - val_accuracy: 0.3417\n",
      "Epoch 300/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1004 - accuracy: 0.3219 - val_loss: 0.1479 - val_accuracy: 0.3597\n",
      "Epoch 301/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1004 - accuracy: 0.3123 - val_loss: 0.1499 - val_accuracy: 0.3435\n",
      "Epoch 302/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.1003 - accuracy: 0.3195 - val_loss: 0.1476 - val_accuracy: 0.3399\n",
      "Epoch 303/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0999 - accuracy: 0.3213 - val_loss: 0.1476 - val_accuracy: 0.3327\n",
      "Epoch 304/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0998 - accuracy: 0.3183 - val_loss: 0.1484 - val_accuracy: 0.3345\n",
      "Epoch 305/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0997 - accuracy: 0.3225 - val_loss: 0.1485 - val_accuracy: 0.3309\n",
      "Epoch 306/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0995 - accuracy: 0.3153 - val_loss: 0.1472 - val_accuracy: 0.3345\n",
      "Epoch 307/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0993 - accuracy: 0.3165 - val_loss: 0.1488 - val_accuracy: 0.3291\n",
      "Epoch 308/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0992 - accuracy: 0.3207 - val_loss: 0.1459 - val_accuracy: 0.3471\n",
      "Epoch 309/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0990 - accuracy: 0.3183 - val_loss: 0.1465 - val_accuracy: 0.3345\n",
      "Epoch 310/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0989 - accuracy: 0.3111 - val_loss: 0.1465 - val_accuracy: 0.3525\n",
      "Epoch 311/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0987 - accuracy: 0.3111 - val_loss: 0.1490 - val_accuracy: 0.3327\n",
      "Epoch 312/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0987 - accuracy: 0.3231 - val_loss: 0.1471 - val_accuracy: 0.3489\n",
      "Epoch 313/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0985 - accuracy: 0.3183 - val_loss: 0.1465 - val_accuracy: 0.3543\n",
      "Epoch 314/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0982 - accuracy: 0.3123 - val_loss: 0.1535 - val_accuracy: 0.2932\n",
      "Epoch 315/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0984 - accuracy: 0.3141 - val_loss: 0.1475 - val_accuracy: 0.3381\n",
      "Epoch 316/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0979 - accuracy: 0.3165 - val_loss: 0.1468 - val_accuracy: 0.3399\n",
      "Epoch 317/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0978 - accuracy: 0.3183 - val_loss: 0.1465 - val_accuracy: 0.3327\n",
      "Epoch 318/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0978 - accuracy: 0.3165 - val_loss: 0.1508 - val_accuracy: 0.2896\n",
      "Epoch 319/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0978 - accuracy: 0.3057 - val_loss: 0.1448 - val_accuracy: 0.3453\n",
      "Epoch 320/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0975 - accuracy: 0.3105 - val_loss: 0.1457 - val_accuracy: 0.3453\n",
      "Epoch 321/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0972 - accuracy: 0.3189 - val_loss: 0.1457 - val_accuracy: 0.3201\n",
      "Epoch 322/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0971 - accuracy: 0.3087 - val_loss: 0.1453 - val_accuracy: 0.3327\n",
      "Epoch 323/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0969 - accuracy: 0.3111 - val_loss: 0.1445 - val_accuracy: 0.3363\n",
      "Epoch 324/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0967 - accuracy: 0.3147 - val_loss: 0.1444 - val_accuracy: 0.3165\n",
      "Epoch 325/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0967 - accuracy: 0.3099 - val_loss: 0.1458 - val_accuracy: 0.3291\n",
      "Epoch 326/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0966 - accuracy: 0.3087 - val_loss: 0.1446 - val_accuracy: 0.3363\n",
      "Epoch 327/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0963 - accuracy: 0.3057 - val_loss: 0.1441 - val_accuracy: 0.3399\n",
      "Epoch 328/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0961 - accuracy: 0.3063 - val_loss: 0.1434 - val_accuracy: 0.3417\n",
      "Epoch 329/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0960 - accuracy: 0.3075 - val_loss: 0.1458 - val_accuracy: 0.3381\n",
      "Epoch 330/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0960 - accuracy: 0.3105 - val_loss: 0.1444 - val_accuracy: 0.3453\n",
      "Epoch 331/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0958 - accuracy: 0.3075 - val_loss: 0.1448 - val_accuracy: 0.3651\n",
      "Epoch 332/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0957 - accuracy: 0.3087 - val_loss: 0.1447 - val_accuracy: 0.3183\n",
      "Epoch 333/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0956 - accuracy: 0.3093 - val_loss: 0.1446 - val_accuracy: 0.3615\n",
      "Epoch 334/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0954 - accuracy: 0.3105 - val_loss: 0.1434 - val_accuracy: 0.3327\n",
      "Epoch 335/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0953 - accuracy: 0.3087 - val_loss: 0.1433 - val_accuracy: 0.3399\n",
      "Epoch 336/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0950 - accuracy: 0.3075 - val_loss: 0.1442 - val_accuracy: 0.3651\n",
      "Epoch 337/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0949 - accuracy: 0.3081 - val_loss: 0.1523 - val_accuracy: 0.2950\n",
      "Epoch 338/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0952 - accuracy: 0.3015 - val_loss: 0.1430 - val_accuracy: 0.3471\n",
      "Epoch 339/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0947 - accuracy: 0.3099 - val_loss: 0.1436 - val_accuracy: 0.3399\n",
      "Epoch 340/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0944 - accuracy: 0.3111 - val_loss: 0.1422 - val_accuracy: 0.3471\n",
      "Epoch 341/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0944 - accuracy: 0.3039 - val_loss: 0.1422 - val_accuracy: 0.3363\n",
      "Epoch 342/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0942 - accuracy: 0.3123 - val_loss: 0.1433 - val_accuracy: 0.3435\n",
      "Epoch 343/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0942 - accuracy: 0.3087 - val_loss: 0.1423 - val_accuracy: 0.3489\n",
      "Epoch 344/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0940 - accuracy: 0.3075 - val_loss: 0.1428 - val_accuracy: 0.3507\n",
      "Epoch 345/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0939 - accuracy: 0.3051 - val_loss: 0.1422 - val_accuracy: 0.3543\n",
      "Epoch 346/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0936 - accuracy: 0.3093 - val_loss: 0.1433 - val_accuracy: 0.3507\n",
      "Epoch 347/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0937 - accuracy: 0.2985 - val_loss: 0.1422 - val_accuracy: 0.3597\n",
      "Epoch 348/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0934 - accuracy: 0.3069 - val_loss: 0.1433 - val_accuracy: 0.3507\n",
      "Epoch 349/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0933 - accuracy: 0.3027 - val_loss: 0.1410 - val_accuracy: 0.3435\n",
      "Epoch 350/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0931 - accuracy: 0.3045 - val_loss: 0.1417 - val_accuracy: 0.3435\n",
      "Epoch 351/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0931 - accuracy: 0.3039 - val_loss: 0.1416 - val_accuracy: 0.3633\n",
      "Epoch 352/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0929 - accuracy: 0.3063 - val_loss: 0.1502 - val_accuracy: 0.3237\n",
      "Epoch 353/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0932 - accuracy: 0.3105 - val_loss: 0.1524 - val_accuracy: 0.2770\n",
      "Epoch 354/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0933 - accuracy: 0.3027 - val_loss: 0.1403 - val_accuracy: 0.3273\n",
      "Epoch 355/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0926 - accuracy: 0.3051 - val_loss: 0.1418 - val_accuracy: 0.3543\n",
      "Epoch 356/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0924 - accuracy: 0.3039 - val_loss: 0.1426 - val_accuracy: 0.3777\n",
      "Epoch 357/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0924 - accuracy: 0.3075 - val_loss: 0.1428 - val_accuracy: 0.3255\n",
      "Epoch 358/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0923 - accuracy: 0.3123 - val_loss: 0.1426 - val_accuracy: 0.3507\n",
      "Epoch 359/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0921 - accuracy: 0.3087 - val_loss: 0.1401 - val_accuracy: 0.3381\n",
      "Epoch 360/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0919 - accuracy: 0.2979 - val_loss: 0.1466 - val_accuracy: 0.3615\n",
      "Epoch 361/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0922 - accuracy: 0.3081 - val_loss: 0.1420 - val_accuracy: 0.2932\n",
      "Epoch 362/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0918 - accuracy: 0.2961 - val_loss: 0.1435 - val_accuracy: 0.3579\n",
      "Epoch 363/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0918 - accuracy: 0.3075 - val_loss: 0.1410 - val_accuracy: 0.3561\n",
      "Epoch 364/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0915 - accuracy: 0.3045 - val_loss: 0.1401 - val_accuracy: 0.3543\n",
      "Epoch 365/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0912 - accuracy: 0.3045 - val_loss: 0.1408 - val_accuracy: 0.3489\n",
      "Epoch 366/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0911 - accuracy: 0.3015 - val_loss: 0.1549 - val_accuracy: 0.2482\n",
      "Epoch 367/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0920 - accuracy: 0.2943 - val_loss: 0.1391 - val_accuracy: 0.3219\n",
      "Epoch 368/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0911 - accuracy: 0.3033 - val_loss: 0.1396 - val_accuracy: 0.3291\n",
      "Epoch 369/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0909 - accuracy: 0.3087 - val_loss: 0.1398 - val_accuracy: 0.3417\n",
      "Epoch 370/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0908 - accuracy: 0.3039 - val_loss: 0.1403 - val_accuracy: 0.3597\n",
      "Epoch 371/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0906 - accuracy: 0.3081 - val_loss: 0.1397 - val_accuracy: 0.3453\n",
      "Epoch 372/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0904 - accuracy: 0.3003 - val_loss: 0.1425 - val_accuracy: 0.3219\n",
      "Epoch 373/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0906 - accuracy: 0.3045 - val_loss: 0.1403 - val_accuracy: 0.3453\n",
      "Epoch 374/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0904 - accuracy: 0.3009 - val_loss: 0.1421 - val_accuracy: 0.3489\n",
      "Epoch 375/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0903 - accuracy: 0.3069 - val_loss: 0.1396 - val_accuracy: 0.3399\n",
      "Epoch 376/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0900 - accuracy: 0.3033 - val_loss: 0.1397 - val_accuracy: 0.3471\n",
      "Epoch 377/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0900 - accuracy: 0.3045 - val_loss: 0.1391 - val_accuracy: 0.3651\n",
      "Epoch 378/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0898 - accuracy: 0.3141 - val_loss: 0.1388 - val_accuracy: 0.3417\n",
      "Epoch 379/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0896 - accuracy: 0.3063 - val_loss: 0.1373 - val_accuracy: 0.3381\n",
      "Epoch 380/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0895 - accuracy: 0.2991 - val_loss: 0.1386 - val_accuracy: 0.3687\n",
      "Epoch 381/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0895 - accuracy: 0.3075 - val_loss: 0.1397 - val_accuracy: 0.3399\n",
      "Epoch 382/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0893 - accuracy: 0.3105 - val_loss: 0.1392 - val_accuracy: 0.3112\n",
      "Epoch 383/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0892 - accuracy: 0.3033 - val_loss: 0.1389 - val_accuracy: 0.3453\n",
      "Epoch 384/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0890 - accuracy: 0.3057 - val_loss: 0.1436 - val_accuracy: 0.3435\n",
      "Epoch 385/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0894 - accuracy: 0.3069 - val_loss: 0.1394 - val_accuracy: 0.3435\n",
      "Epoch 386/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0889 - accuracy: 0.3141 - val_loss: 0.1388 - val_accuracy: 0.3507\n",
      "Epoch 387/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0887 - accuracy: 0.3087 - val_loss: 0.1396 - val_accuracy: 0.3453\n",
      "Epoch 388/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0887 - accuracy: 0.3183 - val_loss: 0.1385 - val_accuracy: 0.3381\n",
      "Epoch 389/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0886 - accuracy: 0.3027 - val_loss: 0.1372 - val_accuracy: 0.3435\n",
      "Epoch 390/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0885 - accuracy: 0.3099 - val_loss: 0.1411 - val_accuracy: 0.3399\n",
      "Epoch 391/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0884 - accuracy: 0.3057 - val_loss: 0.1529 - val_accuracy: 0.2716\n",
      "Epoch 392/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0889 - accuracy: 0.3039 - val_loss: 0.1394 - val_accuracy: 0.3255\n",
      "Epoch 393/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0881 - accuracy: 0.3093 - val_loss: 0.1388 - val_accuracy: 0.3489\n",
      "Epoch 394/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0879 - accuracy: 0.3129 - val_loss: 0.1380 - val_accuracy: 0.3435\n",
      "Epoch 395/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0879 - accuracy: 0.3099 - val_loss: 0.1371 - val_accuracy: 0.3381\n",
      "Epoch 396/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0876 - accuracy: 0.3075 - val_loss: 0.1390 - val_accuracy: 0.3381\n",
      "Epoch 397/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0878 - accuracy: 0.3105 - val_loss: 0.1376 - val_accuracy: 0.3219\n",
      "Epoch 398/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0877 - accuracy: 0.3009 - val_loss: 0.1392 - val_accuracy: 0.3597\n",
      "Epoch 399/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0875 - accuracy: 0.3057 - val_loss: 0.1382 - val_accuracy: 0.3471\n",
      "Epoch 400/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0873 - accuracy: 0.3123 - val_loss: 0.1378 - val_accuracy: 0.3327\n",
      "Epoch 401/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0872 - accuracy: 0.3081 - val_loss: 0.1369 - val_accuracy: 0.3417\n",
      "Epoch 402/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0870 - accuracy: 0.3117 - val_loss: 0.1379 - val_accuracy: 0.3273\n",
      "Epoch 403/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0871 - accuracy: 0.3141 - val_loss: 0.1362 - val_accuracy: 0.3327\n",
      "Epoch 404/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0867 - accuracy: 0.3093 - val_loss: 0.1370 - val_accuracy: 0.3345\n",
      "Epoch 405/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0867 - accuracy: 0.3129 - val_loss: 0.1376 - val_accuracy: 0.3543\n",
      "Epoch 406/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0869 - accuracy: 0.3051 - val_loss: 0.1359 - val_accuracy: 0.3417\n",
      "Epoch 407/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0864 - accuracy: 0.3075 - val_loss: 0.1373 - val_accuracy: 0.3345\n",
      "Epoch 408/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0865 - accuracy: 0.3135 - val_loss: 0.1378 - val_accuracy: 0.3561\n",
      "Epoch 409/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0863 - accuracy: 0.3099 - val_loss: 0.1358 - val_accuracy: 0.3309\n",
      "Epoch 410/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0863 - accuracy: 0.3087 - val_loss: 0.1359 - val_accuracy: 0.3525\n",
      "Epoch 411/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0861 - accuracy: 0.3147 - val_loss: 0.1361 - val_accuracy: 0.3435\n",
      "Epoch 412/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0859 - accuracy: 0.3147 - val_loss: 0.1378 - val_accuracy: 0.3291\n",
      "Epoch 413/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0860 - accuracy: 0.3159 - val_loss: 0.1368 - val_accuracy: 0.3471\n",
      "Epoch 414/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0857 - accuracy: 0.3183 - val_loss: 0.1351 - val_accuracy: 0.3345\n",
      "Epoch 415/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0854 - accuracy: 0.3183 - val_loss: 0.1473 - val_accuracy: 0.2824\n",
      "Epoch 416/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0862 - accuracy: 0.3033 - val_loss: 0.1365 - val_accuracy: 0.3345\n",
      "Epoch 417/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0854 - accuracy: 0.3141 - val_loss: 0.1363 - val_accuracy: 0.3507\n",
      "Epoch 418/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0856 - accuracy: 0.3099 - val_loss: 0.1369 - val_accuracy: 0.3453\n",
      "Epoch 419/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0853 - accuracy: 0.3117 - val_loss: 0.1360 - val_accuracy: 0.3417\n",
      "Epoch 420/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0851 - accuracy: 0.3207 - val_loss: 0.1353 - val_accuracy: 0.3381\n",
      "Epoch 421/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0849 - accuracy: 0.3129 - val_loss: 0.1347 - val_accuracy: 0.3112\n",
      "Epoch 422/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0850 - accuracy: 0.3111 - val_loss: 0.1357 - val_accuracy: 0.3381\n",
      "Epoch 423/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0849 - accuracy: 0.3165 - val_loss: 0.1370 - val_accuracy: 0.3435\n",
      "Epoch 424/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0849 - accuracy: 0.3207 - val_loss: 0.1341 - val_accuracy: 0.3255\n",
      "Epoch 425/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0845 - accuracy: 0.3165 - val_loss: 0.1353 - val_accuracy: 0.3327\n",
      "Epoch 426/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0845 - accuracy: 0.3171 - val_loss: 0.1360 - val_accuracy: 0.3327\n",
      "Epoch 427/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0844 - accuracy: 0.3129 - val_loss: 0.1390 - val_accuracy: 0.3327\n",
      "Epoch 428/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0846 - accuracy: 0.3093 - val_loss: 0.1351 - val_accuracy: 0.3327\n",
      "Epoch 429/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0841 - accuracy: 0.3201 - val_loss: 0.1363 - val_accuracy: 0.3525\n",
      "Epoch 430/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0841 - accuracy: 0.3153 - val_loss: 0.1349 - val_accuracy: 0.3327\n",
      "Epoch 431/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0840 - accuracy: 0.3213 - val_loss: 0.1359 - val_accuracy: 0.3381\n",
      "Epoch 432/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0837 - accuracy: 0.3255 - val_loss: 0.1344 - val_accuracy: 0.3237\n",
      "Epoch 433/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0837 - accuracy: 0.3135 - val_loss: 0.1357 - val_accuracy: 0.3309\n",
      "Epoch 434/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0836 - accuracy: 0.3195 - val_loss: 0.1388 - val_accuracy: 0.3507\n",
      "Epoch 435/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0838 - accuracy: 0.3081 - val_loss: 0.1339 - val_accuracy: 0.3309\n",
      "Epoch 436/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0833 - accuracy: 0.3159 - val_loss: 0.1361 - val_accuracy: 0.3399\n",
      "Epoch 437/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0835 - accuracy: 0.3183 - val_loss: 0.1366 - val_accuracy: 0.3219\n",
      "Epoch 438/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0834 - accuracy: 0.3081 - val_loss: 0.1359 - val_accuracy: 0.3363\n",
      "Epoch 439/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0833 - accuracy: 0.3219 - val_loss: 0.1515 - val_accuracy: 0.2338\n",
      "Epoch 440/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0842 - accuracy: 0.3147 - val_loss: 0.1355 - val_accuracy: 0.3363\n",
      "Epoch 441/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0830 - accuracy: 0.3201 - val_loss: 0.1362 - val_accuracy: 0.3435\n",
      "Epoch 442/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0830 - accuracy: 0.3165 - val_loss: 0.1457 - val_accuracy: 0.3273\n",
      "Epoch 443/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0834 - accuracy: 0.3219 - val_loss: 0.1340 - val_accuracy: 0.3381\n",
      "Epoch 444/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0826 - accuracy: 0.3189 - val_loss: 0.1475 - val_accuracy: 0.3543\n",
      "Epoch 445/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0832 - accuracy: 0.3159 - val_loss: 0.1345 - val_accuracy: 0.3327\n",
      "Epoch 446/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0825 - accuracy: 0.3171 - val_loss: 0.1329 - val_accuracy: 0.3147\n",
      "Epoch 447/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0825 - accuracy: 0.3165 - val_loss: 0.1553 - val_accuracy: 0.3201\n",
      "Epoch 448/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0833 - accuracy: 0.3195 - val_loss: 0.1334 - val_accuracy: 0.3453\n",
      "Epoch 449/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0821 - accuracy: 0.3219 - val_loss: 0.1382 - val_accuracy: 0.3112\n",
      "Epoch 450/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0826 - accuracy: 0.3165 - val_loss: 0.1348 - val_accuracy: 0.3219\n",
      "Epoch 451/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0821 - accuracy: 0.3099 - val_loss: 0.1416 - val_accuracy: 0.3327\n",
      "Epoch 452/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0823 - accuracy: 0.3237 - val_loss: 0.1335 - val_accuracy: 0.3399\n",
      "Epoch 453/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0817 - accuracy: 0.3201 - val_loss: 0.1337 - val_accuracy: 0.3237\n",
      "Epoch 454/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0817 - accuracy: 0.3201 - val_loss: 0.1349 - val_accuracy: 0.3453\n",
      "Epoch 455/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0817 - accuracy: 0.3219 - val_loss: 0.1324 - val_accuracy: 0.3453\n",
      "Epoch 456/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0814 - accuracy: 0.3237 - val_loss: 0.1367 - val_accuracy: 0.3219\n",
      "Epoch 457/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0818 - accuracy: 0.3207 - val_loss: 0.1327 - val_accuracy: 0.3363\n",
      "Epoch 458/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0812 - accuracy: 0.3195 - val_loss: 0.1340 - val_accuracy: 0.3399\n",
      "Epoch 459/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0812 - accuracy: 0.3243 - val_loss: 0.1335 - val_accuracy: 0.3453\n",
      "Epoch 460/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0813 - accuracy: 0.3195 - val_loss: 0.1333 - val_accuracy: 0.3309\n",
      "Epoch 461/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0812 - accuracy: 0.3117 - val_loss: 0.1325 - val_accuracy: 0.3507\n",
      "Epoch 462/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0807 - accuracy: 0.3141 - val_loss: 0.1325 - val_accuracy: 0.3561\n",
      "Epoch 463/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0806 - accuracy: 0.3195 - val_loss: 0.1342 - val_accuracy: 0.3561\n",
      "Epoch 464/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0808 - accuracy: 0.3117 - val_loss: 0.1330 - val_accuracy: 0.3561\n",
      "Epoch 465/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0806 - accuracy: 0.3219 - val_loss: 0.1325 - val_accuracy: 0.3453\n",
      "Epoch 466/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0806 - accuracy: 0.3195 - val_loss: 0.1343 - val_accuracy: 0.3201\n",
      "Epoch 467/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0804 - accuracy: 0.3171 - val_loss: 0.1329 - val_accuracy: 0.3615\n",
      "Epoch 468/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0802 - accuracy: 0.3207 - val_loss: 0.1342 - val_accuracy: 0.3525\n",
      "Epoch 469/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0803 - accuracy: 0.3111 - val_loss: 0.1328 - val_accuracy: 0.3363\n",
      "Epoch 470/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0800 - accuracy: 0.3207 - val_loss: 0.1333 - val_accuracy: 0.3561\n",
      "Epoch 471/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0800 - accuracy: 0.3135 - val_loss: 0.1331 - val_accuracy: 0.3435\n",
      "Epoch 472/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0801 - accuracy: 0.3237 - val_loss: 0.1320 - val_accuracy: 0.3309\n",
      "Epoch 473/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0799 - accuracy: 0.3189 - val_loss: 0.1334 - val_accuracy: 0.3615\n",
      "Epoch 474/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0797 - accuracy: 0.3219 - val_loss: 0.1318 - val_accuracy: 0.3399\n",
      "Epoch 475/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0796 - accuracy: 0.3141 - val_loss: 0.1313 - val_accuracy: 0.3381\n",
      "Epoch 476/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0793 - accuracy: 0.3183 - val_loss: 0.1330 - val_accuracy: 0.3255\n",
      "Epoch 477/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0796 - accuracy: 0.3159 - val_loss: 0.1327 - val_accuracy: 0.3345\n",
      "Epoch 478/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0795 - accuracy: 0.3219 - val_loss: 0.1301 - val_accuracy: 0.3345\n",
      "Epoch 479/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0794 - accuracy: 0.3165 - val_loss: 0.1320 - val_accuracy: 0.3435\n",
      "Epoch 480/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0791 - accuracy: 0.3243 - val_loss: 0.1308 - val_accuracy: 0.3543\n",
      "Epoch 481/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0790 - accuracy: 0.3183 - val_loss: 0.1309 - val_accuracy: 0.3327\n",
      "Epoch 482/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0787 - accuracy: 0.3183 - val_loss: 0.1305 - val_accuracy: 0.3453\n",
      "Epoch 483/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0787 - accuracy: 0.3159 - val_loss: 0.1354 - val_accuracy: 0.3237\n",
      "Epoch 484/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0791 - accuracy: 0.3183 - val_loss: 0.1316 - val_accuracy: 0.3309\n",
      "Epoch 485/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0786 - accuracy: 0.3189 - val_loss: 0.1334 - val_accuracy: 0.3399\n",
      "Epoch 486/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0789 - accuracy: 0.3201 - val_loss: 0.1406 - val_accuracy: 0.2950\n",
      "Epoch 487/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0789 - accuracy: 0.3171 - val_loss: 0.1299 - val_accuracy: 0.3453\n",
      "Epoch 488/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0786 - accuracy: 0.3237 - val_loss: 0.1317 - val_accuracy: 0.3453\n",
      "Epoch 489/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0781 - accuracy: 0.3249 - val_loss: 0.1302 - val_accuracy: 0.3291\n",
      "Epoch 490/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0781 - accuracy: 0.3231 - val_loss: 0.1340 - val_accuracy: 0.2824\n",
      "Epoch 491/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0785 - accuracy: 0.3189 - val_loss: 0.1368 - val_accuracy: 0.2842\n",
      "Epoch 492/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0785 - accuracy: 0.3135 - val_loss: 0.1300 - val_accuracy: 0.3507\n",
      "Epoch 493/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0778 - accuracy: 0.3153 - val_loss: 0.1309 - val_accuracy: 0.3381\n",
      "Epoch 494/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0777 - accuracy: 0.3207 - val_loss: 0.1319 - val_accuracy: 0.3435\n",
      "Epoch 495/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0778 - accuracy: 0.3129 - val_loss: 0.1320 - val_accuracy: 0.3435\n",
      "Epoch 496/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0776 - accuracy: 0.3291 - val_loss: 0.1335 - val_accuracy: 0.3435\n",
      "Epoch 497/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0778 - accuracy: 0.3273 - val_loss: 0.1315 - val_accuracy: 0.3291\n",
      "Epoch 498/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0775 - accuracy: 0.3213 - val_loss: 0.1295 - val_accuracy: 0.3219\n",
      "Epoch 499/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0773 - accuracy: 0.3201 - val_loss: 0.1320 - val_accuracy: 0.3327\n",
      "Epoch 500/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0774 - accuracy: 0.3123 - val_loss: 0.1294 - val_accuracy: 0.3417\n",
      "Epoch 501/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0770 - accuracy: 0.3201 - val_loss: 0.1319 - val_accuracy: 0.3561\n",
      "Epoch 502/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0773 - accuracy: 0.3189 - val_loss: 0.1291 - val_accuracy: 0.3363\n",
      "Epoch 503/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0767 - accuracy: 0.3207 - val_loss: 0.1298 - val_accuracy: 0.3471\n",
      "Epoch 504/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0767 - accuracy: 0.3177 - val_loss: 0.1308 - val_accuracy: 0.3471\n",
      "Epoch 505/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0768 - accuracy: 0.3195 - val_loss: 0.1297 - val_accuracy: 0.3381\n",
      "Epoch 506/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0767 - accuracy: 0.3219 - val_loss: 0.1309 - val_accuracy: 0.3004\n",
      "Epoch 507/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0769 - accuracy: 0.3153 - val_loss: 0.1317 - val_accuracy: 0.3723\n",
      "Epoch 508/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0769 - accuracy: 0.3183 - val_loss: 0.1313 - val_accuracy: 0.3705\n",
      "Epoch 509/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0765 - accuracy: 0.3177 - val_loss: 0.1321 - val_accuracy: 0.3453\n",
      "Epoch 510/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0767 - accuracy: 0.3159 - val_loss: 0.1306 - val_accuracy: 0.3417\n",
      "Epoch 511/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0763 - accuracy: 0.3165 - val_loss: 0.1309 - val_accuracy: 0.3561\n",
      "Epoch 512/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0763 - accuracy: 0.3189 - val_loss: 0.1305 - val_accuracy: 0.3345\n",
      "Epoch 513/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0766 - accuracy: 0.3117 - val_loss: 0.1328 - val_accuracy: 0.3615\n",
      "Epoch 514/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0762 - accuracy: 0.3237 - val_loss: 0.1334 - val_accuracy: 0.3201\n",
      "Epoch 515/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0761 - accuracy: 0.3141 - val_loss: 0.1335 - val_accuracy: 0.3759\n",
      "Epoch 516/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0760 - accuracy: 0.3201 - val_loss: 0.1293 - val_accuracy: 0.3471\n",
      "Epoch 517/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0760 - accuracy: 0.3231 - val_loss: 0.1283 - val_accuracy: 0.3309\n",
      "Epoch 518/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0756 - accuracy: 0.3195 - val_loss: 0.1292 - val_accuracy: 0.3309\n",
      "Epoch 519/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0755 - accuracy: 0.3195 - val_loss: 0.1296 - val_accuracy: 0.3525\n",
      "Epoch 520/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0755 - accuracy: 0.3189 - val_loss: 0.1298 - val_accuracy: 0.3363\n",
      "Epoch 521/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0756 - accuracy: 0.3153 - val_loss: 0.1305 - val_accuracy: 0.3669\n",
      "Epoch 522/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0753 - accuracy: 0.3201 - val_loss: 0.1288 - val_accuracy: 0.3525\n",
      "Epoch 523/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0752 - accuracy: 0.3171 - val_loss: 0.1306 - val_accuracy: 0.3291\n",
      "Epoch 524/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0752 - accuracy: 0.3183 - val_loss: 0.1349 - val_accuracy: 0.3291\n",
      "Epoch 525/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0756 - accuracy: 0.3123 - val_loss: 0.1307 - val_accuracy: 0.3453\n",
      "Epoch 526/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0751 - accuracy: 0.3171 - val_loss: 0.1301 - val_accuracy: 0.3399\n",
      "Epoch 527/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0749 - accuracy: 0.3183 - val_loss: 0.1345 - val_accuracy: 0.3022\n",
      "Epoch 528/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0751 - accuracy: 0.3213 - val_loss: 0.1537 - val_accuracy: 0.3129\n",
      "Epoch 529/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0760 - accuracy: 0.3159 - val_loss: 0.1297 - val_accuracy: 0.3417\n",
      "Epoch 530/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0747 - accuracy: 0.3117 - val_loss: 0.1283 - val_accuracy: 0.3615\n",
      "Epoch 531/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0746 - accuracy: 0.3153 - val_loss: 0.1276 - val_accuracy: 0.3291\n",
      "Epoch 532/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0745 - accuracy: 0.3177 - val_loss: 0.1303 - val_accuracy: 0.3579\n",
      "Epoch 533/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0745 - accuracy: 0.3201 - val_loss: 0.1292 - val_accuracy: 0.3669\n",
      "Epoch 534/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0744 - accuracy: 0.3177 - val_loss: 0.1304 - val_accuracy: 0.3687\n",
      "Epoch 535/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0744 - accuracy: 0.3267 - val_loss: 0.1305 - val_accuracy: 0.3471\n",
      "Epoch 536/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0741 - accuracy: 0.3195 - val_loss: 0.1288 - val_accuracy: 0.3363\n",
      "Epoch 537/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0740 - accuracy: 0.3219 - val_loss: 0.1274 - val_accuracy: 0.3363\n",
      "Epoch 538/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0740 - accuracy: 0.3165 - val_loss: 0.1289 - val_accuracy: 0.3471\n",
      "Epoch 539/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0740 - accuracy: 0.3165 - val_loss: 0.1266 - val_accuracy: 0.3327\n",
      "Epoch 540/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0737 - accuracy: 0.3195 - val_loss: 0.1305 - val_accuracy: 0.3309\n",
      "Epoch 541/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0739 - accuracy: 0.3183 - val_loss: 0.1309 - val_accuracy: 0.3201\n",
      "Epoch 542/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0740 - accuracy: 0.3171 - val_loss: 0.1277 - val_accuracy: 0.3507\n",
      "Epoch 543/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0735 - accuracy: 0.3183 - val_loss: 0.1290 - val_accuracy: 0.3417\n",
      "Epoch 544/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0735 - accuracy: 0.3135 - val_loss: 0.1309 - val_accuracy: 0.3255\n",
      "Epoch 545/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0737 - accuracy: 0.3153 - val_loss: 0.1281 - val_accuracy: 0.3543\n",
      "Epoch 546/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0733 - accuracy: 0.3135 - val_loss: 0.1272 - val_accuracy: 0.3471\n",
      "Epoch 547/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0732 - accuracy: 0.3177 - val_loss: 0.1279 - val_accuracy: 0.3453\n",
      "Epoch 548/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0731 - accuracy: 0.3243 - val_loss: 0.1276 - val_accuracy: 0.3363\n",
      "Epoch 549/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0731 - accuracy: 0.3195 - val_loss: 0.1278 - val_accuracy: 0.3291\n",
      "Epoch 550/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0731 - accuracy: 0.3153 - val_loss: 0.1281 - val_accuracy: 0.3651\n",
      "Epoch 551/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0730 - accuracy: 0.3153 - val_loss: 0.1315 - val_accuracy: 0.3112\n",
      "Epoch 552/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0733 - accuracy: 0.3147 - val_loss: 0.1270 - val_accuracy: 0.3291\n",
      "Epoch 553/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0729 - accuracy: 0.3129 - val_loss: 0.1290 - val_accuracy: 0.3561\n",
      "Epoch 554/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0726 - accuracy: 0.3177 - val_loss: 0.1267 - val_accuracy: 0.3543\n",
      "Epoch 555/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0727 - accuracy: 0.3147 - val_loss: 0.1296 - val_accuracy: 0.3345\n",
      "Epoch 556/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0726 - accuracy: 0.3099 - val_loss: 0.1287 - val_accuracy: 0.3273\n",
      "Epoch 557/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0726 - accuracy: 0.3141 - val_loss: 0.1294 - val_accuracy: 0.3543\n",
      "Epoch 558/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0723 - accuracy: 0.3195 - val_loss: 0.1295 - val_accuracy: 0.3399\n",
      "Epoch 559/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0723 - accuracy: 0.3207 - val_loss: 0.1267 - val_accuracy: 0.3525\n",
      "Epoch 560/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0723 - accuracy: 0.3207 - val_loss: 0.1247 - val_accuracy: 0.3219\n",
      "Epoch 561/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0723 - accuracy: 0.3069 - val_loss: 0.1284 - val_accuracy: 0.3417\n",
      "Epoch 562/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0721 - accuracy: 0.3147 - val_loss: 0.1265 - val_accuracy: 0.3237\n",
      "Epoch 563/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0721 - accuracy: 0.3111 - val_loss: 0.1353 - val_accuracy: 0.3094\n",
      "Epoch 564/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0727 - accuracy: 0.3063 - val_loss: 0.1271 - val_accuracy: 0.3471\n",
      "Epoch 565/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0720 - accuracy: 0.3147 - val_loss: 0.1261 - val_accuracy: 0.3435\n",
      "Epoch 566/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0717 - accuracy: 0.3183 - val_loss: 0.1286 - val_accuracy: 0.3417\n",
      "Epoch 567/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0719 - accuracy: 0.3201 - val_loss: 0.1304 - val_accuracy: 0.3291\n",
      "Epoch 568/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0718 - accuracy: 0.3153 - val_loss: 0.1267 - val_accuracy: 0.3363\n",
      "Epoch 569/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0717 - accuracy: 0.3189 - val_loss: 0.1266 - val_accuracy: 0.3471\n",
      "Epoch 570/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0716 - accuracy: 0.3129 - val_loss: 0.1257 - val_accuracy: 0.3345\n",
      "Epoch 571/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0715 - accuracy: 0.3135 - val_loss: 0.1381 - val_accuracy: 0.4101\n",
      "Epoch 572/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0720 - accuracy: 0.3213 - val_loss: 0.1290 - val_accuracy: 0.3327\n",
      "Epoch 573/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0716 - accuracy: 0.3147 - val_loss: 0.1283 - val_accuracy: 0.3543\n",
      "Epoch 574/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0715 - accuracy: 0.3165 - val_loss: 0.1277 - val_accuracy: 0.3651\n",
      "Epoch 575/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0713 - accuracy: 0.3177 - val_loss: 0.1274 - val_accuracy: 0.3435\n",
      "Epoch 576/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0712 - accuracy: 0.3141 - val_loss: 0.1270 - val_accuracy: 0.3399\n",
      "Epoch 577/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0710 - accuracy: 0.3099 - val_loss: 0.1269 - val_accuracy: 0.3147\n",
      "Epoch 578/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0711 - accuracy: 0.3189 - val_loss: 0.1273 - val_accuracy: 0.3471\n",
      "Epoch 579/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0709 - accuracy: 0.3135 - val_loss: 0.1262 - val_accuracy: 0.3507\n",
      "Epoch 580/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0708 - accuracy: 0.3273 - val_loss: 0.1278 - val_accuracy: 0.3435\n",
      "Epoch 581/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0708 - accuracy: 0.3177 - val_loss: 0.1274 - val_accuracy: 0.3543\n",
      "Epoch 582/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0706 - accuracy: 0.3225 - val_loss: 0.1273 - val_accuracy: 0.3489\n",
      "Epoch 583/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0707 - accuracy: 0.3093 - val_loss: 0.1284 - val_accuracy: 0.3201\n",
      "Epoch 584/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0707 - accuracy: 0.3225 - val_loss: 0.1250 - val_accuracy: 0.3399\n",
      "Epoch 585/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0706 - accuracy: 0.3147 - val_loss: 0.1298 - val_accuracy: 0.3615\n",
      "Epoch 586/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0706 - accuracy: 0.3087 - val_loss: 0.1255 - val_accuracy: 0.3561\n",
      "Epoch 587/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0704 - accuracy: 0.3165 - val_loss: 0.1274 - val_accuracy: 0.3651\n",
      "Epoch 588/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0705 - accuracy: 0.3093 - val_loss: 0.1257 - val_accuracy: 0.3453\n",
      "Epoch 589/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0702 - accuracy: 0.3159 - val_loss: 0.1282 - val_accuracy: 0.3597\n",
      "Epoch 590/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0701 - accuracy: 0.3207 - val_loss: 0.1262 - val_accuracy: 0.3615\n",
      "Epoch 591/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0703 - accuracy: 0.3135 - val_loss: 0.1262 - val_accuracy: 0.3291\n",
      "Epoch 592/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0701 - accuracy: 0.3135 - val_loss: 0.1254 - val_accuracy: 0.3525\n",
      "Epoch 593/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0699 - accuracy: 0.3165 - val_loss: 0.1249 - val_accuracy: 0.3399\n",
      "Epoch 594/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0699 - accuracy: 0.3129 - val_loss: 0.1301 - val_accuracy: 0.3525\n",
      "Epoch 595/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0699 - accuracy: 0.3171 - val_loss: 0.1272 - val_accuracy: 0.3453\n",
      "Epoch 596/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0698 - accuracy: 0.3141 - val_loss: 0.1244 - val_accuracy: 0.3363\n",
      "Epoch 597/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0696 - accuracy: 0.3153 - val_loss: 0.1268 - val_accuracy: 0.3291\n",
      "Epoch 598/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0697 - accuracy: 0.3105 - val_loss: 0.1233 - val_accuracy: 0.3507\n",
      "Epoch 599/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0697 - accuracy: 0.3153 - val_loss: 0.1280 - val_accuracy: 0.3417\n",
      "Epoch 600/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0699 - accuracy: 0.3111 - val_loss: 0.1267 - val_accuracy: 0.3579\n",
      "Epoch 601/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0695 - accuracy: 0.3207 - val_loss: 0.1268 - val_accuracy: 0.3615\n",
      "Epoch 602/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0694 - accuracy: 0.3219 - val_loss: 0.1257 - val_accuracy: 0.3417\n",
      "Epoch 603/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0695 - accuracy: 0.3105 - val_loss: 0.1268 - val_accuracy: 0.3543\n",
      "Epoch 604/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0694 - accuracy: 0.3183 - val_loss: 0.1249 - val_accuracy: 0.3453\n",
      "Epoch 605/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0692 - accuracy: 0.3129 - val_loss: 0.1255 - val_accuracy: 0.3669\n",
      "Epoch 606/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0693 - accuracy: 0.3171 - val_loss: 0.1252 - val_accuracy: 0.3507\n",
      "Epoch 607/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0689 - accuracy: 0.3195 - val_loss: 0.1242 - val_accuracy: 0.3291\n",
      "Epoch 608/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0691 - accuracy: 0.3189 - val_loss: 0.1248 - val_accuracy: 0.3309\n",
      "Epoch 609/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0692 - accuracy: 0.3177 - val_loss: 0.1254 - val_accuracy: 0.3291\n",
      "Epoch 610/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0689 - accuracy: 0.3147 - val_loss: 0.1261 - val_accuracy: 0.3651\n",
      "Epoch 611/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0687 - accuracy: 0.3141 - val_loss: 0.1269 - val_accuracy: 0.3525\n",
      "Epoch 612/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0688 - accuracy: 0.3117 - val_loss: 0.1238 - val_accuracy: 0.3381\n",
      "Epoch 613/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0687 - accuracy: 0.3159 - val_loss: 0.1244 - val_accuracy: 0.3345\n",
      "Epoch 614/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0688 - accuracy: 0.3129 - val_loss: 0.1243 - val_accuracy: 0.3291\n",
      "Epoch 615/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0687 - accuracy: 0.3165 - val_loss: 0.1251 - val_accuracy: 0.3435\n",
      "Epoch 616/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0685 - accuracy: 0.3147 - val_loss: 0.1252 - val_accuracy: 0.3381\n",
      "Epoch 617/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0685 - accuracy: 0.3183 - val_loss: 0.1233 - val_accuracy: 0.3417\n",
      "Epoch 618/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0683 - accuracy: 0.3165 - val_loss: 0.1271 - val_accuracy: 0.3579\n",
      "Epoch 619/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0682 - accuracy: 0.3141 - val_loss: 0.1258 - val_accuracy: 0.3597\n",
      "Epoch 620/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0683 - accuracy: 0.3147 - val_loss: 0.1269 - val_accuracy: 0.3399\n",
      "Epoch 621/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0685 - accuracy: 0.3159 - val_loss: 0.1262 - val_accuracy: 0.3345\n",
      "Epoch 622/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0681 - accuracy: 0.3051 - val_loss: 0.1258 - val_accuracy: 0.3525\n",
      "Epoch 623/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0681 - accuracy: 0.3171 - val_loss: 0.1270 - val_accuracy: 0.3381\n",
      "Epoch 624/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0681 - accuracy: 0.3171 - val_loss: 0.1264 - val_accuracy: 0.3633\n",
      "Epoch 625/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0682 - accuracy: 0.3147 - val_loss: 0.1246 - val_accuracy: 0.3867\n",
      "Epoch 626/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0679 - accuracy: 0.3147 - val_loss: 0.1259 - val_accuracy: 0.3363\n",
      "Epoch 627/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0681 - accuracy: 0.3081 - val_loss: 0.1230 - val_accuracy: 0.3471\n",
      "Epoch 628/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0678 - accuracy: 0.3177 - val_loss: 0.1466 - val_accuracy: 0.2716\n",
      "Epoch 629/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0690 - accuracy: 0.3105 - val_loss: 0.1534 - val_accuracy: 0.2302\n",
      "Epoch 630/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0691 - accuracy: 0.3063 - val_loss: 0.1220 - val_accuracy: 0.3453\n",
      "Epoch 631/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0676 - accuracy: 0.3177 - val_loss: 0.1289 - val_accuracy: 0.3543\n",
      "Epoch 632/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0679 - accuracy: 0.3147 - val_loss: 0.1288 - val_accuracy: 0.3183\n",
      "Epoch 633/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0677 - accuracy: 0.3195 - val_loss: 0.1254 - val_accuracy: 0.3687\n",
      "Epoch 634/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0677 - accuracy: 0.3165 - val_loss: 0.1255 - val_accuracy: 0.3309\n",
      "Epoch 635/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0675 - accuracy: 0.3201 - val_loss: 0.1267 - val_accuracy: 0.3543\n",
      "Epoch 636/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0676 - accuracy: 0.3171 - val_loss: 0.1238 - val_accuracy: 0.3453\n",
      "Epoch 637/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0673 - accuracy: 0.3165 - val_loss: 0.1226 - val_accuracy: 0.3399\n",
      "Epoch 638/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0672 - accuracy: 0.3237 - val_loss: 0.1254 - val_accuracy: 0.3633\n",
      "Epoch 639/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0671 - accuracy: 0.3111 - val_loss: 0.1240 - val_accuracy: 0.3381\n",
      "Epoch 640/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0672 - accuracy: 0.3189 - val_loss: 0.1229 - val_accuracy: 0.3471\n",
      "Epoch 641/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0669 - accuracy: 0.3177 - val_loss: 0.1254 - val_accuracy: 0.3363\n",
      "Epoch 642/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0671 - accuracy: 0.3099 - val_loss: 0.1241 - val_accuracy: 0.3615\n",
      "Epoch 643/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0671 - accuracy: 0.3159 - val_loss: 0.1274 - val_accuracy: 0.3579\n",
      "Epoch 644/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0669 - accuracy: 0.3147 - val_loss: 0.1268 - val_accuracy: 0.3579\n",
      "Epoch 645/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0668 - accuracy: 0.3177 - val_loss: 0.1233 - val_accuracy: 0.3579\n",
      "Epoch 646/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0668 - accuracy: 0.3141 - val_loss: 0.1252 - val_accuracy: 0.3597\n",
      "Epoch 647/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0669 - accuracy: 0.3153 - val_loss: 0.1265 - val_accuracy: 0.3633\n",
      "Epoch 648/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0668 - accuracy: 0.3183 - val_loss: 0.1265 - val_accuracy: 0.3615\n",
      "Epoch 649/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0667 - accuracy: 0.3147 - val_loss: 0.1321 - val_accuracy: 0.3345\n",
      "Epoch 650/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0669 - accuracy: 0.3117 - val_loss: 0.1267 - val_accuracy: 0.3705\n",
      "Epoch 651/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0666 - accuracy: 0.3153 - val_loss: 0.1222 - val_accuracy: 0.3399\n",
      "Epoch 652/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0664 - accuracy: 0.3177 - val_loss: 0.1294 - val_accuracy: 0.3687\n",
      "Epoch 653/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0665 - accuracy: 0.3207 - val_loss: 0.1233 - val_accuracy: 0.3453\n",
      "Epoch 654/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0663 - accuracy: 0.3195 - val_loss: 0.1241 - val_accuracy: 0.3741\n",
      "Epoch 655/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0664 - accuracy: 0.3159 - val_loss: 0.1408 - val_accuracy: 0.2950\n",
      "Epoch 656/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0671 - accuracy: 0.3201 - val_loss: 0.1525 - val_accuracy: 0.2428\n",
      "Epoch 657/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0674 - accuracy: 0.3141 - val_loss: 0.1281 - val_accuracy: 0.3489\n",
      "Epoch 658/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0665 - accuracy: 0.3057 - val_loss: 0.1220 - val_accuracy: 0.3507\n",
      "Epoch 659/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0659 - accuracy: 0.3159 - val_loss: 0.1228 - val_accuracy: 0.3543\n",
      "Epoch 660/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0661 - accuracy: 0.3177 - val_loss: 0.1461 - val_accuracy: 0.2788\n",
      "Epoch 661/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0672 - accuracy: 0.3165 - val_loss: 0.1247 - val_accuracy: 0.3453\n",
      "Epoch 662/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0660 - accuracy: 0.3075 - val_loss: 0.1211 - val_accuracy: 0.3687\n",
      "Epoch 663/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0658 - accuracy: 0.3207 - val_loss: 0.1226 - val_accuracy: 0.3435\n",
      "Epoch 664/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0657 - accuracy: 0.3159 - val_loss: 0.1258 - val_accuracy: 0.3579\n",
      "Epoch 665/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0658 - accuracy: 0.3147 - val_loss: 0.1232 - val_accuracy: 0.3741\n",
      "Epoch 666/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0656 - accuracy: 0.3207 - val_loss: 0.1244 - val_accuracy: 0.3651\n",
      "Epoch 667/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0655 - accuracy: 0.3159 - val_loss: 0.1215 - val_accuracy: 0.3507\n",
      "Epoch 668/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0655 - accuracy: 0.3075 - val_loss: 0.1225 - val_accuracy: 0.3669\n",
      "Epoch 669/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0653 - accuracy: 0.3159 - val_loss: 0.1313 - val_accuracy: 0.3112\n",
      "Epoch 670/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0660 - accuracy: 0.3177 - val_loss: 0.1222 - val_accuracy: 0.3381\n",
      "Epoch 671/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0655 - accuracy: 0.3183 - val_loss: 0.1532 - val_accuracy: 0.2554\n",
      "Epoch 672/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0674 - accuracy: 0.3171 - val_loss: 0.1243 - val_accuracy: 0.3363\n",
      "Epoch 673/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0655 - accuracy: 0.3141 - val_loss: 0.1233 - val_accuracy: 0.3687\n",
      "Epoch 674/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0653 - accuracy: 0.3303 - val_loss: 0.1247 - val_accuracy: 0.3525\n",
      "Epoch 675/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0653 - accuracy: 0.3255 - val_loss: 0.1230 - val_accuracy: 0.3669\n",
      "Epoch 676/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0652 - accuracy: 0.3123 - val_loss: 0.1410 - val_accuracy: 0.3561\n",
      "Epoch 677/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0659 - accuracy: 0.3201 - val_loss: 0.1220 - val_accuracy: 0.3579\n",
      "Epoch 678/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0649 - accuracy: 0.3279 - val_loss: 0.1243 - val_accuracy: 0.3705\n",
      "Epoch 679/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0652 - accuracy: 0.3159 - val_loss: 0.1242 - val_accuracy: 0.3687\n",
      "Epoch 680/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0651 - accuracy: 0.3171 - val_loss: 0.1217 - val_accuracy: 0.3759\n",
      "Epoch 681/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0649 - accuracy: 0.3201 - val_loss: 0.1234 - val_accuracy: 0.3687\n",
      "Epoch 682/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0649 - accuracy: 0.3261 - val_loss: 0.1227 - val_accuracy: 0.3687\n",
      "Epoch 683/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0647 - accuracy: 0.3189 - val_loss: 0.1230 - val_accuracy: 0.3561\n",
      "Epoch 684/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0647 - accuracy: 0.3195 - val_loss: 0.1223 - val_accuracy: 0.3831\n",
      "Epoch 685/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0647 - accuracy: 0.3309 - val_loss: 0.1213 - val_accuracy: 0.3489\n",
      "Epoch 686/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0646 - accuracy: 0.3165 - val_loss: 0.1219 - val_accuracy: 0.3507\n",
      "Epoch 687/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0644 - accuracy: 0.3255 - val_loss: 0.1210 - val_accuracy: 0.3435\n",
      "Epoch 688/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0644 - accuracy: 0.3207 - val_loss: 0.1243 - val_accuracy: 0.3615\n",
      "Epoch 689/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0645 - accuracy: 0.3267 - val_loss: 0.1236 - val_accuracy: 0.3651\n",
      "Epoch 690/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0646 - accuracy: 0.3237 - val_loss: 0.1230 - val_accuracy: 0.3597\n",
      "Epoch 691/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0642 - accuracy: 0.3315 - val_loss: 0.1230 - val_accuracy: 0.3633\n",
      "Epoch 692/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0642 - accuracy: 0.3309 - val_loss: 0.1225 - val_accuracy: 0.3489\n",
      "Epoch 693/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0643 - accuracy: 0.3141 - val_loss: 0.1223 - val_accuracy: 0.3597\n",
      "Epoch 694/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0640 - accuracy: 0.3273 - val_loss: 0.1203 - val_accuracy: 0.3417\n",
      "Epoch 695/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0640 - accuracy: 0.3195 - val_loss: 0.1217 - val_accuracy: 0.3525\n",
      "Epoch 696/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0643 - accuracy: 0.3159 - val_loss: 0.1217 - val_accuracy: 0.3507\n",
      "Epoch 697/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0640 - accuracy: 0.3213 - val_loss: 0.1415 - val_accuracy: 0.3543\n",
      "Epoch 698/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0650 - accuracy: 0.3273 - val_loss: 0.1233 - val_accuracy: 0.3777\n",
      "Epoch 699/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0642 - accuracy: 0.3183 - val_loss: 0.1274 - val_accuracy: 0.3363\n",
      "Epoch 700/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0643 - accuracy: 0.3111 - val_loss: 0.1219 - val_accuracy: 0.3435\n",
      "Epoch 701/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0637 - accuracy: 0.3237 - val_loss: 0.1229 - val_accuracy: 0.3741\n",
      "Epoch 702/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0639 - accuracy: 0.3243 - val_loss: 0.1247 - val_accuracy: 0.3651\n",
      "Epoch 703/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0640 - accuracy: 0.3177 - val_loss: 0.1228 - val_accuracy: 0.3687\n",
      "Epoch 704/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0638 - accuracy: 0.3183 - val_loss: 0.1244 - val_accuracy: 0.3741\n",
      "Epoch 705/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0638 - accuracy: 0.3225 - val_loss: 0.1362 - val_accuracy: 0.2986\n",
      "Epoch 706/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0645 - accuracy: 0.3165 - val_loss: 0.1201 - val_accuracy: 0.3489\n",
      "Epoch 707/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0635 - accuracy: 0.3147 - val_loss: 0.1226 - val_accuracy: 0.3885\n",
      "Epoch 708/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0638 - accuracy: 0.3147 - val_loss: 0.1231 - val_accuracy: 0.3777\n",
      "Epoch 709/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0634 - accuracy: 0.3231 - val_loss: 0.1269 - val_accuracy: 0.3813\n",
      "Epoch 710/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0636 - accuracy: 0.3177 - val_loss: 0.1249 - val_accuracy: 0.3561\n",
      "Epoch 711/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0638 - accuracy: 0.3213 - val_loss: 0.1190 - val_accuracy: 0.3507\n",
      "Epoch 712/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0633 - accuracy: 0.3165 - val_loss: 0.1249 - val_accuracy: 0.3435\n",
      "Epoch 713/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0634 - accuracy: 0.3249 - val_loss: 0.1283 - val_accuracy: 0.3993\n",
      "Epoch 714/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0637 - accuracy: 0.3315 - val_loss: 0.1342 - val_accuracy: 0.3759\n",
      "Epoch 715/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0640 - accuracy: 0.3231 - val_loss: 0.1275 - val_accuracy: 0.3921\n",
      "Epoch 716/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0634 - accuracy: 0.3213 - val_loss: 0.1238 - val_accuracy: 0.3615\n",
      "Epoch 717/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0631 - accuracy: 0.3129 - val_loss: 0.1243 - val_accuracy: 0.3831\n",
      "Epoch 718/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0631 - accuracy: 0.3147 - val_loss: 0.1438 - val_accuracy: 0.2806\n",
      "Epoch 719/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0640 - accuracy: 0.3207 - val_loss: 0.1201 - val_accuracy: 0.3759\n",
      "Epoch 720/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0631 - accuracy: 0.3213 - val_loss: 0.1246 - val_accuracy: 0.3705\n",
      "Epoch 721/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0630 - accuracy: 0.3111 - val_loss: 0.1210 - val_accuracy: 0.3399\n",
      "Epoch 722/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0630 - accuracy: 0.3147 - val_loss: 0.1252 - val_accuracy: 0.3687\n",
      "Epoch 723/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0630 - accuracy: 0.3207 - val_loss: 0.1198 - val_accuracy: 0.3489\n",
      "Epoch 724/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0627 - accuracy: 0.3189 - val_loss: 0.1220 - val_accuracy: 0.3058\n",
      "Epoch 725/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0633 - accuracy: 0.3201 - val_loss: 0.1222 - val_accuracy: 0.3651\n",
      "Epoch 726/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0628 - accuracy: 0.3231 - val_loss: 0.1274 - val_accuracy: 0.3345\n",
      "Epoch 727/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0628 - accuracy: 0.3195 - val_loss: 0.1223 - val_accuracy: 0.3867\n",
      "Epoch 728/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0629 - accuracy: 0.3189 - val_loss: 0.1200 - val_accuracy: 0.3777\n",
      "Epoch 729/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0624 - accuracy: 0.3231 - val_loss: 0.1238 - val_accuracy: 0.3687\n",
      "Epoch 730/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0626 - accuracy: 0.3165 - val_loss: 0.1212 - val_accuracy: 0.3651\n",
      "Epoch 731/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0627 - accuracy: 0.3117 - val_loss: 0.1216 - val_accuracy: 0.2950\n",
      "Epoch 732/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0627 - accuracy: 0.3129 - val_loss: 0.1208 - val_accuracy: 0.3651\n",
      "Epoch 733/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0626 - accuracy: 0.3249 - val_loss: 0.1229 - val_accuracy: 0.3921\n",
      "Epoch 734/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0621 - accuracy: 0.3231 - val_loss: 0.1205 - val_accuracy: 0.3525\n",
      "Epoch 735/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0622 - accuracy: 0.3291 - val_loss: 0.1235 - val_accuracy: 0.3543\n",
      "Epoch 736/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0625 - accuracy: 0.3141 - val_loss: 0.1196 - val_accuracy: 0.3471\n",
      "Epoch 737/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0622 - accuracy: 0.3165 - val_loss: 0.1212 - val_accuracy: 0.4047\n",
      "Epoch 738/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0620 - accuracy: 0.3267 - val_loss: 0.1198 - val_accuracy: 0.3795\n",
      "Epoch 739/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0619 - accuracy: 0.3243 - val_loss: 0.1229 - val_accuracy: 0.3435\n",
      "Epoch 740/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0624 - accuracy: 0.3123 - val_loss: 0.1202 - val_accuracy: 0.3723\n",
      "Epoch 741/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0618 - accuracy: 0.3327 - val_loss: 0.1209 - val_accuracy: 0.3633\n",
      "Epoch 742/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0620 - accuracy: 0.3261 - val_loss: 0.1196 - val_accuracy: 0.3579\n",
      "Epoch 743/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0620 - accuracy: 0.3219 - val_loss: 0.1203 - val_accuracy: 0.3489\n",
      "Epoch 744/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0620 - accuracy: 0.3243 - val_loss: 0.1215 - val_accuracy: 0.3777\n",
      "Epoch 745/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0616 - accuracy: 0.3279 - val_loss: 0.1224 - val_accuracy: 0.3813\n",
      "Epoch 746/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0618 - accuracy: 0.3231 - val_loss: 0.1218 - val_accuracy: 0.3687\n",
      "Epoch 747/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0617 - accuracy: 0.3261 - val_loss: 0.1208 - val_accuracy: 0.3705\n",
      "Epoch 748/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0616 - accuracy: 0.3165 - val_loss: 0.1468 - val_accuracy: 0.2248\n",
      "Epoch 749/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0633 - accuracy: 0.3195 - val_loss: 0.1178 - val_accuracy: 0.3687\n",
      "Epoch 750/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0614 - accuracy: 0.3195 - val_loss: 0.1281 - val_accuracy: 0.3579\n",
      "Epoch 751/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0619 - accuracy: 0.3249 - val_loss: 0.1217 - val_accuracy: 0.3831\n",
      "Epoch 752/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0614 - accuracy: 0.3171 - val_loss: 0.1237 - val_accuracy: 0.3849\n",
      "Epoch 753/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0617 - accuracy: 0.3279 - val_loss: 0.1359 - val_accuracy: 0.3040\n",
      "Epoch 754/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0625 - accuracy: 0.3201 - val_loss: 0.1221 - val_accuracy: 0.3741\n",
      "Epoch 755/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0615 - accuracy: 0.3243 - val_loss: 0.1214 - val_accuracy: 0.3777\n",
      "Epoch 756/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0613 - accuracy: 0.3249 - val_loss: 0.1236 - val_accuracy: 0.3669\n",
      "Epoch 757/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0614 - accuracy: 0.3261 - val_loss: 0.1207 - val_accuracy: 0.3651\n",
      "Epoch 758/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0613 - accuracy: 0.3285 - val_loss: 0.1215 - val_accuracy: 0.3669\n",
      "Epoch 759/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0613 - accuracy: 0.3219 - val_loss: 0.1219 - val_accuracy: 0.3453\n",
      "Epoch 760/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0613 - accuracy: 0.3171 - val_loss: 0.1479 - val_accuracy: 0.2842\n",
      "Epoch 761/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0625 - accuracy: 0.3183 - val_loss: 0.1221 - val_accuracy: 0.3777\n",
      "Epoch 762/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0612 - accuracy: 0.3321 - val_loss: 0.1264 - val_accuracy: 0.3831\n",
      "Epoch 763/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0614 - accuracy: 0.3303 - val_loss: 0.1265 - val_accuracy: 0.3489\n",
      "Epoch 764/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0613 - accuracy: 0.3237 - val_loss: 0.1227 - val_accuracy: 0.3687\n",
      "Epoch 765/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0610 - accuracy: 0.3237 - val_loss: 0.1215 - val_accuracy: 0.3633\n",
      "Epoch 766/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0610 - accuracy: 0.3237 - val_loss: 0.1218 - val_accuracy: 0.3723\n",
      "Epoch 767/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0609 - accuracy: 0.3237 - val_loss: 0.1272 - val_accuracy: 0.3651\n",
      "Epoch 768/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0610 - accuracy: 0.3273 - val_loss: 0.1253 - val_accuracy: 0.3831\n",
      "Epoch 769/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0609 - accuracy: 0.3315 - val_loss: 0.1187 - val_accuracy: 0.3471\n",
      "Epoch 770/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0607 - accuracy: 0.3345 - val_loss: 0.1208 - val_accuracy: 0.3939\n",
      "Epoch 771/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0605 - accuracy: 0.3267 - val_loss: 0.1213 - val_accuracy: 0.3831\n",
      "Epoch 772/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0605 - accuracy: 0.3243 - val_loss: 0.1202 - val_accuracy: 0.3687\n",
      "Epoch 773/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0604 - accuracy: 0.3279 - val_loss: 0.1218 - val_accuracy: 0.3723\n",
      "Epoch 774/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0607 - accuracy: 0.3201 - val_loss: 0.1296 - val_accuracy: 0.3309\n",
      "Epoch 775/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0608 - accuracy: 0.3261 - val_loss: 0.1198 - val_accuracy: 0.3939\n",
      "Epoch 776/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0601 - accuracy: 0.3255 - val_loss: 0.1433 - val_accuracy: 0.4263\n",
      "Epoch 777/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0612 - accuracy: 0.3339 - val_loss: 0.1210 - val_accuracy: 0.4011\n",
      "Epoch 778/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0603 - accuracy: 0.3315 - val_loss: 0.1210 - val_accuracy: 0.3975\n",
      "Epoch 779/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0604 - accuracy: 0.3273 - val_loss: 0.1202 - val_accuracy: 0.3795\n",
      "Epoch 780/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0601 - accuracy: 0.3249 - val_loss: 0.1197 - val_accuracy: 0.3759\n",
      "Epoch 781/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0602 - accuracy: 0.3333 - val_loss: 0.1202 - val_accuracy: 0.3723\n",
      "Epoch 782/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0600 - accuracy: 0.3207 - val_loss: 0.1234 - val_accuracy: 0.3813\n",
      "Epoch 783/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0602 - accuracy: 0.3339 - val_loss: 0.1233 - val_accuracy: 0.3849\n",
      "Epoch 784/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0600 - accuracy: 0.3273 - val_loss: 0.1209 - val_accuracy: 0.3633\n",
      "Epoch 785/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0599 - accuracy: 0.3279 - val_loss: 0.1221 - val_accuracy: 0.3975\n",
      "Epoch 786/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0602 - accuracy: 0.3327 - val_loss: 0.1199 - val_accuracy: 0.3669\n",
      "Epoch 787/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0599 - accuracy: 0.3327 - val_loss: 0.1226 - val_accuracy: 0.3543\n",
      "Epoch 788/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0599 - accuracy: 0.3309 - val_loss: 0.1204 - val_accuracy: 0.3741\n",
      "Epoch 789/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0600 - accuracy: 0.3255 - val_loss: 0.1188 - val_accuracy: 0.3867\n",
      "Epoch 790/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0597 - accuracy: 0.3339 - val_loss: 0.1212 - val_accuracy: 0.3723\n",
      "Epoch 791/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0597 - accuracy: 0.3351 - val_loss: 0.1255 - val_accuracy: 0.3579\n",
      "Epoch 792/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0597 - accuracy: 0.3327 - val_loss: 0.1224 - val_accuracy: 0.3255\n",
      "Epoch 793/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0598 - accuracy: 0.3261 - val_loss: 0.1215 - val_accuracy: 0.3813\n",
      "Epoch 794/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0595 - accuracy: 0.3315 - val_loss: 0.1186 - val_accuracy: 0.3831\n",
      "Epoch 795/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0594 - accuracy: 0.3315 - val_loss: 0.1201 - val_accuracy: 0.3741\n",
      "Epoch 796/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0594 - accuracy: 0.3255 - val_loss: 0.1308 - val_accuracy: 0.3831\n",
      "Epoch 797/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0605 - accuracy: 0.3267 - val_loss: 0.1329 - val_accuracy: 0.3759\n",
      "Epoch 798/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0601 - accuracy: 0.3249 - val_loss: 0.1265 - val_accuracy: 0.3112\n",
      "Epoch 799/800\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0596 - accuracy: 0.3255 - val_loss: 0.1209 - val_accuracy: 0.3363\n",
      "Epoch 800/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/53 [..............................] - ETA: 0s - loss: 0.0571 - accuracy: 0.4062\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "53/53 [==============================] - 0s 1ms/step - loss: 0.0599 - accuracy: 0.3279 - val_loss: 0.1216 - val_accuracy: 0.3723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-5f8a8e11cd4a>:128: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "  tprs.append(interp(mean_fpr, fpr, tpr))\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[555, 0]\n",
      "[0, 0]\n",
      "both b and c are zero\n",
      "[553, 2]\n",
      "[0, 0]\n",
      "[512, 11]\n",
      "[29, 3]\n",
      "[515, 8]\n",
      "[5, 27]\n",
      "[531, 2]\n",
      "[17, 5]\n",
      "[524, 9]\n",
      "[5, 17]\n",
      "[541, 2]\n",
      "[9, 3]\n",
      "[528, 15]\n",
      "[2, 10]\n",
      "[543, 5]\n",
      "[4, 3]\n",
      "[533, 15]\n",
      "[2, 5]\n",
      "[491, 30]\n",
      "[18, 16]\n",
      "[507, 14]\n",
      "[10, 24]\n",
      "[496, 28]\n",
      "[23, 8]\n",
      "[510, 14]\n",
      "[6, 25]\n",
      "[533, 6]\n",
      "[10, 6]\n",
      "[526, 13]\n",
      "[3, 13]\n",
      "[537, 7]\n",
      "[8, 3]\n",
      "[521, 23]\n",
      "[4, 7]\n",
      "[528, 11]\n",
      "[12, 4]\n",
      "[519, 20]\n",
      "[8, 8]\n",
      "[518, 11]\n",
      "[7, 19]\n",
      "[515, 14]\n",
      "[7, 19]\n",
      "[519, 6]\n",
      "[13, 17]\n",
      "[515, 10]\n",
      "[9, 21]\n",
      "[523, 11]\n",
      "[15, 6]\n",
      "[520, 14]\n",
      "[3, 18]\n",
      "[529, 3]\n",
      "[16, 7]\n",
      "[519, 13]\n",
      "[9, 14]\n",
      "[534, 5]\n",
      "[13, 3]\n",
      "[529, 10]\n",
      "[9, 7]\n",
      "[495, 33]\n",
      "[19, 8]\n",
      "[512, 16]\n",
      "[6, 21]\n",
      "[503, 20]\n",
      "[19, 13]\n",
      "[513, 10]\n",
      "[6, 26]\n",
      "[529, 9]\n",
      "[13, 4]\n",
      "[522, 16]\n",
      "[0, 17]\n",
      "[537, 5]\n",
      "[8, 5]\n",
      "[517, 25]\n",
      "[7, 6]\n",
      "[514, 5]\n",
      "[27, 9]\n",
      "[501, 18]\n",
      "[30, 6]\n",
      "[517, 14]\n",
      "[5, 19]\n",
      "[512, 19]\n",
      "[5, 19]\n",
      "[519, 4]\n",
      "[10, 22]\n",
      "[514, 9]\n",
      "[6, 26]\n",
      "[540, 3]\n",
      "[7, 5]\n",
      "[525, 18]\n",
      "[5, 7]\n",
      "[539, 4]\n",
      "[8, 4]\n",
      "[522, 21]\n",
      "[9, 3]\n",
      "[543, 4]\n",
      "[4, 4]\n",
      "[527, 20]\n",
      "[3, 5]\n",
      "[531, 4]\n",
      "[13, 7]\n",
      "[525, 10]\n",
      "[4, 16]\n",
      "[497, 33]\n",
      "[15, 10]\n",
      "[518, 12]\n",
      "[7, 18]\n",
      "[517, 12]\n",
      "[10, 16]\n",
      "[511, 18]\n",
      "[6, 20]\n",
      "[512, 18]\n",
      "[14, 11]\n",
      "[515, 15]\n",
      "[5, 20]\n",
      "[517, 8]\n",
      "[6, 24]\n",
      "[517, 8]\n",
      "[6, 24]\n",
      "[535, 6]\n",
      "[8, 6]\n",
      "[524, 17]\n",
      "[5, 9]\n",
      "[541, 4]\n",
      "[8, 2]\n",
      "[522, 23]\n",
      "[1, 9]\n",
      "[530, 8]\n",
      "[15, 2]\n",
      "[525, 13]\n",
      "[8, 9]\n",
      "[541, 7]\n",
      "[3, 4]\n",
      "[524, 24]\n",
      "[3, 4]\n",
      "[535, 5]\n",
      "[9, 6]\n",
      "[522, 18]\n",
      "[6, 9]\n",
      "[535, 5]\n",
      "[12, 3]\n",
      "[523, 17]\n",
      "[6, 9]\n",
      "[540, 2]\n",
      "[12, 1]\n",
      "[532, 10]\n",
      "[9, 4]\n",
      "[539, 4]\n",
      "[11, 1]\n",
      "[519, 24]\n",
      "[4, 8]\n",
      "[529, 9]\n",
      "[14, 3]\n",
      "[520, 18]\n",
      "[8, 9]\n",
      "[538, 5]\n",
      "[5, 7]\n",
      "[533, 10]\n",
      "[5, 7]\n",
      "[544, 1]\n",
      "[5, 5]\n",
      "[537, 8]\n",
      "[2, 8]\n",
      "[531, 5]\n",
      "[16, 3]\n",
      "[525, 11]\n",
      "[5, 14]\n",
      "[540, 6]\n",
      "[6, 3]\n",
      "[527, 19]\n",
      "[0, 9]\n",
      "[527, 11]\n",
      "[15, 2]\n",
      "[522, 16]\n",
      "[5, 12]\n",
      "[530, 11]\n",
      "[11, 3]\n",
      "[520, 21]\n",
      "[8, 6]\n",
      "[542, 4]\n",
      "[6, 3]\n",
      "[527, 19]\n",
      "[3, 6]\n",
      "[523, 7]\n",
      "[21, 4]\n",
      "[524, 6]\n",
      "[4, 21]\n",
      "[499, 35]\n",
      "[13, 8]\n",
      "[514, 20]\n",
      "[4, 17]\n",
      "[520, 8]\n",
      "[3, 24]\n",
      "[513, 15]\n",
      "[3, 24]\n",
      "[494, 31]\n",
      "[21, 9]\n",
      "[507, 18]\n",
      "[7, 23]\n",
      "[525, 6]\n",
      "[2, 22]\n",
      "[520, 11]\n",
      "[1, 23]\n",
      "------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds of MWPM:\n",
      "> Accuracy: 0.9687733565135155 (+- 0.001283853178239937)\n",
      "> F1: 0.707948958038217(+- 0.011484866892512222)\n",
      "> Time: 13.9772894 (+- 0.5926796513860744)\n",
      "##############################################################################\n",
      "Average scores for all folds of PLUT:\n",
      "> Accuracy: 0.9486213763488379 (+- 0.00037344856596287386)\n",
      "> F1: 0.2333291545857569(+- 0.006515857524481468)\n",
      "> Time: 0.007106459999999999 (+- 0.0007175047110646727)\n",
      "##############################################################################\n",
      "Average scores for all folds of NN:\n",
      "> Accuracy: 0.9654745901212248 (+- 0.0003817689310152301)\n",
      "> F1: 0.6354933034357041(+- 0.0060442509982107755)\n",
      "> Time: 0.05690719999999999 (+- 0.002018826751358323)\n",
      "##############################################################################\n",
      "> AUC for class : 0.2707581227436823 (+- 0.0)\n",
      "X^2 for MWPM and NN: 0\n",
      "X^2 for PLUT and NN: 0.5\n",
      "> AUC for class X00: 0.7087582212820392 (+- 0.03238681980408706)\n",
      "X^2 for MWPM and NN: 9.025\n",
      "X^2 for PLUT and NN: 0.3076923076923077\n",
      "> AUC for class X01: 0.9347115534735144 (+- 0.015990849659736534)\n",
      "X^2 for MWPM and NN: 13.473684210526315\n",
      "X^2 for PLUT and NN: 0.6428571428571429\n",
      "> AUC for class X02: 0.9863907692400768 (+- 0.005162319534379088)\n",
      "X^2 for MWPM and NN: 5.818181818181818\n",
      "X^2 for PLUT and NN: 8.470588235294118\n",
      "> AUC for class X03: 0.985165844142086 (+- 0.009588413349015468)\n",
      "X^2 for MWPM and NN: 0.0\n",
      "X^2 for PLUT and NN: 8.470588235294118\n",
      "> AUC for class X04: 0.9403306190783457 (+- 0.009477613152370607)\n",
      "X^2 for MWPM and NN: 2.5208333333333335\n",
      "X^2 for PLUT and NN: 0.375\n",
      "> AUC for class X10: 0.934008913533533 (+- 0.011928221735052204)\n",
      "X^2 for MWPM and NN: 0.3137254901960784\n",
      "X^2 for PLUT and NN: 2.45\n",
      "> AUC for class X11: 0.9718017837606382 (+- 0.018713235298199217)\n",
      "X^2 for MWPM and NN: 1.5625\n",
      "X^2 for PLUT and NN: 5.0625\n",
      "> AUC for class X12: 0.984783952689477 (+- 0.0028197213935379916)\n",
      "X^2 for MWPM and NN: 0.26666666666666666\n",
      "X^2 for PLUT and NN: 12.0\n",
      "> AUC for class X13: 0.9825298859089635 (+- 0.006104782444697375)\n",
      "X^2 for MWPM and NN: 0.17391304347826086\n",
      "X^2 for PLUT and NN: 4.321428571428571\n",
      "> AUC for class X14: 0.9273679501063583 (+- 0.03425568999219603)\n",
      "X^2 for MWPM and NN: 0.5\n",
      "X^2 for PLUT and NN: 1.7142857142857142\n",
      "> AUC for class X20: 0.9314914922007089 (+- 0.020145223183776557)\n",
      "X^2 for MWPM and NN: 3.3684210526315788\n",
      "X^2 for PLUT and NN: 0.0\n",
      "> AUC for class X21: 0.9786239297213578 (+- 0.007304155754580289)\n",
      "X^2 for MWPM and NN: 0.9615384615384616\n",
      "X^2 for PLUT and NN: 5.882352941176471\n",
      "> AUC for class X22: 0.9814171327363749 (+- 0.010153742182836939)\n",
      "X^2 for MWPM and NN: 10.31578947368421\n",
      "X^2 for PLUT and NN: 0.4090909090909091\n",
      "> AUC for class X23: 0.9781409652514828 (+- 0.011718966261684243)\n",
      "X^2 for MWPM and NN: 4.5\n",
      "X^2 for PLUT and NN: 0.0\n",
      "> AUC for class X24: 0.9384627611076789 (+- 0.005415230045467545)\n",
      "X^2 for MWPM and NN: 3.25\n",
      "X^2 for PLUT and NN: 3.6818181818181817\n",
      "> AUC for class X30: 0.9417126349751811 (+- 0.015433796141176355)\n",
      "X^2 for MWPM and NN: 0.0\n",
      "X^2 for PLUT and NN: 0.5625\n",
      "> AUC for class X31: 0.9815289542321111 (+- 0.009759814047647544)\n",
      "X^2 for MWPM and NN: 1.1363636363636365\n",
      "X^2 for PLUT and NN: 14.0625\n",
      "> AUC for class X32: 0.985863818315949 (+- 0.009036861599995928)\n",
      "X^2 for MWPM and NN: 1.2307692307692308\n",
      "X^2 for PLUT and NN: 9.03125\n",
      "> AUC for class X33: 0.9798121804965897 (+- 0.004078651114677935)\n",
      "X^2 for MWPM and NN: 16.53125\n",
      "X^2 for PLUT and NN: 3.5208333333333335\n",
      "> AUC for class X34: 0.9359642288755342 (+- 0.01634960294170798)\n",
      "X^2 for MWPM and NN: 3.3684210526315788\n",
      "X^2 for PLUT and NN: 7.041666666666667\n",
      "> AUC for class X40: 0.9311160696620455 (+- 0.02709545677423297)\n",
      "X^2 for MWPM and NN: 3.5\n",
      "X^2 for PLUT and NN: 0.26666666666666666\n",
      "> AUC for class X41: 0.9874609572791133 (+- 0.010425212564634564)\n",
      "X^2 for MWPM and NN: 2.5\n",
      "X^2 for PLUT and NN: 6.260869565217392\n",
      "> AUC for class X42: 0.985175509753274 (+- 0.007577038134414004)\n",
      "X^2 for MWPM and NN: 2.0833333333333335\n",
      "X^2 for PLUT and NN: 4.033333333333333\n",
      "> AUC for class X43: 0.9872740396363178 (+- 0.005290747632189924)\n",
      "X^2 for MWPM and NN: 0.125\n",
      "X^2 for PLUT and NN: 11.130434782608695\n",
      "> AUC for class X44: 0.972720829419018 (+- 0.021327724523400367)\n",
      "X^2 for MWPM and NN: 5.882352941176471\n",
      "X^2 for PLUT and NN: 1.7857142857142858\n",
      "> AUC for class Z00: 0.9080284968511956 (+- 0.004640705516243338)\n",
      "X^2 for MWPM and NN: 6.020833333333333\n",
      "X^2 for PLUT and NN: 0.8421052631578947\n",
      "> AUC for class Z01: 0.9400486755960417 (+- 0.014244872918082218)\n",
      "X^2 for MWPM and NN: 0.045454545454545456\n",
      "X^2 for PLUT and NN: 5.041666666666667\n",
      "> AUC for class Z02: 0.9353961120573893 (+- 0.02267268792502485)\n",
      "X^2 for MWPM and NN: 0.28125\n",
      "X^2 for PLUT and NN: 4.05\n",
      "> AUC for class Z03: 0.9212607019588278 (+- 0.02623145003147706)\n",
      "X^2 for MWPM and NN: 0.07142857142857142\n",
      "X^2 for PLUT and NN: 0.07142857142857142\n",
      "> AUC for class Z04: 0.968398999857734 (+- 0.012089818267777943)\n",
      "X^2 for MWPM and NN: 0.6428571428571429\n",
      "X^2 for PLUT and NN: 5.5\n",
      "> AUC for class Z10: 0.9929217378496105 (+- 0.004574041843026559)\n",
      "X^2 for MWPM and NN: 2.0833333333333335\n",
      "X^2 for PLUT and NN: 18.375\n",
      "> AUC for class Z11: 0.9791056689529389 (+- 0.006429581312280342)\n",
      "X^2 for MWPM and NN: 2.782608695652174\n",
      "X^2 for PLUT and NN: 0.7619047619047619\n",
      "> AUC for class Z12: 0.9784653318850791 (+- 0.020693085373440997)\n",
      "X^2 for MWPM and NN: 0.9\n",
      "X^2 for PLUT and NN: 14.814814814814815\n",
      "> AUC for class Z13: 0.9779093569854587 (+- 0.007087253552136326)\n",
      "X^2 for MWPM and NN: 1.7857142857142858\n",
      "X^2 for PLUT and NN: 5.041666666666667\n",
      "> AUC for class Z14: 0.983567062117913 (+- 0.011235279197217892)\n",
      "X^2 for MWPM and NN: 3.764705882352941\n",
      "X^2 for PLUT and NN: 4.3478260869565215\n",
      "> AUC for class Z20: 0.9798158439333996 (+- 0.006785505914762061)\n",
      "X^2 for MWPM and NN: 8.642857142857142\n",
      "X^2 for PLUT and NN: 0.0\n",
      "> AUC for class Z21: 0.9779040511272485 (+- 0.01017009987988355)\n",
      "X^2 for MWPM and NN: 4.266666666666667\n",
      "X^2 for PLUT and NN: 12.892857142857142\n",
      "> AUC for class Z22: 0.9801752814860356 (+- 0.009143792798808094)\n",
      "X^2 for MWPM and NN: 1.565217391304348\n",
      "X^2 for PLUT and NN: 3.1153846153846154\n",
      "> AUC for class Z23: 0.9816871203440638 (+- 0.005146481750487481)\n",
      "X^2 for MWPM and NN: 0.1\n",
      "X^2 for PLUT and NN: 1.0666666666666667\n",
      "> AUC for class Z24: 0.9879632207434994 (+- 0.0040788682984227685)\n",
      "X^2 for MWPM and NN: 4.166666666666667\n",
      "X^2 for PLUT and NN: 2.5\n",
      "> AUC for class Z30: 0.9854473485939504 (+- 0.009208950168093814)\n",
      "X^2 for MWPM and NN: 6.857142857142857\n",
      "X^2 for PLUT and NN: 1.5625\n",
      "> AUC for class Z31: 0.9791911928405144 (+- 0.00823881763686298)\n",
      "X^2 for MWPM and NN: 0.08333333333333333\n",
      "X^2 for PLUT and NN: 17.05263157894737\n",
      "> AUC for class Z32: 0.9769777872237757 (+- 0.008089425763836021)\n",
      "X^2 for MWPM and NN: 0.9615384615384616\n",
      "X^2 for PLUT and NN: 4.761904761904762\n",
      "> AUC for class Z33: 0.9726604062230167 (+- 0.008050548656318482)\n",
      "X^2 for MWPM and NN: 0.045454545454545456\n",
      "X^2 for PLUT and NN: 4.9655172413793105\n",
      "> AUC for class Z34: 0.9720065444248032 (+- 0.01990948797900834)\n",
      "X^2 for MWPM and NN: 0.9\n",
      "X^2 for PLUT and NN: 10.227272727272727\n",
      "> AUC for class Z40: 0.9486385927553578 (+- 0.02325201352499929)\n",
      "X^2 for MWPM and NN: 8.035714285714286\n",
      "X^2 for PLUT and NN: 0.1\n",
      "> AUC for class Z41: 0.9372629291787558 (+- 0.021816967256710715)\n",
      "X^2 for MWPM and NN: 9.1875\n",
      "X^2 for PLUT and NN: 9.375\n",
      "> AUC for class Z42: 0.934930047731199 (+- 0.007161186596857463)\n",
      "X^2 for MWPM and NN: 1.4545454545454546\n",
      "X^2 for PLUT and NN: 6.722222222222222\n",
      "> AUC for class Z43: 0.9143947466279915 (+- 0.022569894548908488)\n",
      "X^2 for MWPM and NN: 1.5576923076923077\n",
      "X^2 for PLUT and NN: 4.0\n",
      "> AUC for class Z44: 0.927686003668914 (+- 0.01612231112843994)\n",
      "X^2 for MWPM and NN: 1.125\n",
      "X^2 for PLUT and NN: 6.75\n",
      "###################################################################################\n",
      "TOTAL F1 NN: [0.6404332129963899, 0.6366704161979754, 0.640863416449572, 0.635320251758608, 0.6241792197759752]\n",
      "TOTAL F1 PLUT: [0.22245762711864406, 0.23177766124803356, 0.24076029567053855, 0.23236074270557025, 0.2392894461859979]\n",
      "TOTAL F1 MWPM: [0.7012056044314109, 0.7082096933728981, 0.7290770757525636, 0.6951340615690169, 0.7061183550651955]\n",
      "TOTAL ACC NN: [0.9648751586965708, 0.9657657657657653, 0.9659070835541418, 0.9652004946122578, 0.9656244479773887]\n",
      "TOTAL ACC PLUT: [0.948229651572856, 0.9482423600070627, 0.9491962550786045, 0.948878290054757, 0.9485603250309095]\n",
      "TOTAL ACC MWPM: [0.9676611651854966, 0.9687334393216726, 0.971065182829887, 0.9674615792262832, 0.9689454160042374]\n",
      "TOTAL TIME NN: [0.0578994, 0.0568611, 0.0597402, 0.0564711, 0.0535642]\n",
      "TOTAL TIME PLUT: [0.0069816, 0.0069765, 0.0067835, 0.0063299, 0.0084608]\n",
      "TOTAL TIME MWPM: [15.0943841, 13.575014400000022, 13.509872699999999, 13.632751299999983, 14.0744245]\n",
      "------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "inputs = x_d5.copy()\n",
    "inputs = np.array(inputs)\n",
    "inputs = np.stack(inputs)\n",
    "targets = np.array(y_d5)\n",
    "#targets = np.stack(targets)\n",
    "targets = np.array([np.array(xi) for xi in targets])\n",
    "\n",
    "\n",
    "n_classes = 51\n",
    "\n",
    "acc_per_fold = []\n",
    "f1_per_fold = []\n",
    "time_per_fold = []\n",
    "\n",
    "acc_per_fold_plut = []\n",
    "f1_per_fold_plut = []\n",
    "time_per_fold_plut = []\n",
    "\n",
    "acc_per_fold_mwpm = []\n",
    "f1_per_fold_mwpm = []\n",
    "time_per_fold_mwpm = []\n",
    "\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "aucs_classes = {}\n",
    "for i in mlb.classes_:\n",
    "    aucs_classes[i] = []\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "for i, (train, test) in enumerate(kfold.split(inputs, targets)):\n",
    "    i_train = inputs[train].copy()\n",
    "    i_test = inputs[test].copy()\n",
    "    x_test_d5 = i_test[:,:2]\n",
    "    inputs_train = i_train[:,3:]\n",
    "    inputs_test = i_test[:,3:]\n",
    "    \n",
    "    ####################################################################################################\n",
    "    #test MWPM decoder for this fold\n",
    "    #labels = targets[train], features = inputs[train]\n",
    "    #x_test_d5 = translate_to_graph(testData_d5_MWPM, targets[test], mlb)\n",
    "    decoding_d5, time_mwpm = do_new_decoding(x_test_d5, 5, 0)\n",
    "    decoding_d5['combine'] = decoding_d5[[0, 1]].values.tolist()\n",
    "    decoding_d5['combine'].apply(lambda x: x[0].extend(x[1]))\n",
    "    decoding_d5 = np.array(decoding_d5[0])\n",
    "                                              \n",
    "    time_per_fold_mwpm.append(time_mwpm)\n",
    "                                              \n",
    "    pred_mwpm = mlb.transform(decoding_d5)\n",
    "    if fold_no < 5:\n",
    "        acc = partial_accuracy(targets[test], pred_mwpm)\n",
    "    else:\n",
    "        acc, contingency_mwpm = partial_accuracy_and_contingency(targets[test], pred_mwpm, mlb)\n",
    "        \n",
    "    acc_per_fold_mwpm.append(acc)\n",
    "    f1_per_fold_mwpm.append(f1_score(targets[test], pred_mwpm, average='micro'))\n",
    "    \n",
    "    #####################################################################################################\n",
    "    #test the plut decoder for this fold\n",
    "    \n",
    "    lookup_d5 = lookup_decoder(5)\n",
    "    \n",
    "    lookup_d5 = train_plut(lookup_d5, inputs_train, targets[train])\n",
    "    \n",
    "    start = time.time_ns()\n",
    "    pred_plut_d5 = test_plut(lookup_d5, inputs_test)\n",
    "    end = time.time_ns() \n",
    "    time_per_fold_plut.append((end - start)/ (10 ** 9))\n",
    "    \n",
    "    if fold_no < 5:\n",
    "        acc = partial_accuracy(targets[test], pred_plut_d5)\n",
    "    else:\n",
    "        acc, contingency_plut = partial_accuracy_and_contingency(targets[test], pred_plut_d5, mlb)\n",
    "        \n",
    "    acc_per_fold_plut.append(acc)\n",
    "    f1_per_fold_plut.append(f1_score(targets[test], pred_plut_d5, average='micro'))\n",
    "\n",
    "    #####################################################################################################\n",
    "    #Test the NN decoder for this fold\n",
    "    \n",
    "    model_d5 = compile_FFNN_model_DepthFive(5)\n",
    "    # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    inputs_train = np.asarray(inputs_train).astype('int32')\n",
    "    inputs_test = np.asarray(inputs_test).astype('int32')\n",
    "    \n",
    "    # Fit data to model\n",
    "    \n",
    "    history = model_d5.fit(\n",
    "    inputs_train,\n",
    "    targets[train],\n",
    "    validation_split=.25,\n",
    "    epochs = 800\n",
    ")\n",
    "   # Generate generalization metrics\n",
    "    scores = model_d5.evaluate(inputs_test, targets[test], verbose=0)\n",
    "    \n",
    "    start = time.time_ns()\n",
    "    predictions_d5 = model_d5.predict(inputs_test)\n",
    "    end = time.time_ns() \n",
    "    time_per_fold.append((end - start)/ (10 ** 9))\n",
    "    \n",
    "    #threshold based on previous tests with train_test_split in hyperparameter tuning\n",
    "    pred=predictions_d5.copy() #change here\n",
    "    pred[pred>=.4]=1 \n",
    "    pred[pred<.4]=0\n",
    "    \n",
    "    if fold_no < 5:\n",
    "        acc = scores[1]\n",
    "    else:\n",
    "        acc, contingency_nn = partial_accuracy_and_contingency(targets[test], pred, mlb)\n",
    "\n",
    "    acc_per_fold.append(acc)\n",
    "    f1_per_fold.append(f1_score(targets[test], pred, average='micro'))\n",
    "\n",
    "    \n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(targets[test].ravel(), predictions_d5.ravel())\n",
    "    tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "    tprs[-1][0] = 0.0\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "    plt.plot(fpr, tpr, lw=1, alpha=0.3, label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
    "    \n",
    "    #get the AUCs of each class, used to get average AUC of each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(targets[test][:, i], predictions_d5[:, i]) \n",
    "        aucs_classes[mlb.classes_[i]].append(auc(fpr[i], tpr[i]))\n",
    "        \n",
    "        \n",
    "#########################################################################################################\n",
    "#compute McNemar's statistic on results of last fold\n",
    "\n",
    "mcnemar_results_mwpm = {}\n",
    "mcnemar_results_plut = {}\n",
    "\n",
    "for class_ in mlb.classes_:\n",
    "    \n",
    "    #compute the x^2 for NN and MWPM\n",
    "    mcnemar_results_mwpm[class_]=contingency_table_and_t(contingency_nn[class_], contingency_mwpm[class_])[1]    \n",
    "    #comput the x^2 for NN and PLUT\n",
    "    mcnemar_results_plut[class_] = contingency_table_and_t(contingency_nn[class_], contingency_plut[class_])[1]\n",
    "        \n",
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds of MWPM:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold_mwpm)} (+- {np.std(acc_per_fold_mwpm)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold_mwpm)}(+- {np.std(f1_per_fold_mwpm)})')\n",
    "print(f'> Time: {np.mean(time_per_fold_mwpm)} (+- {np.std(time_per_fold_mwpm)})')\n",
    "print(\"##############################################################################\")\n",
    "print('Average scores for all folds of PLUT:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold_plut)} (+- {np.std(acc_per_fold_plut)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold_plut)}(+- {np.std(f1_per_fold_plut)})')\n",
    "print(f'> Time: {np.mean(time_per_fold_plut)} (+- {np.std(time_per_fold_plut)})')\n",
    "print(\"##############################################################################\")\n",
    "print('Average scores for all folds of NN:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold)}(+- {np.std(f1_per_fold)})')\n",
    "print(f'> Time: {np.mean(time_per_fold)} (+- {np.std(time_per_fold)})')\n",
    "print(\"##############################################################################\")\n",
    "for key in aucs_classes:\n",
    "    cleanedList = [x for x in aucs_classes[key] if str(x) != 'nan']\n",
    "    print(f'> AUC for class {key}: {np.mean(cleanedList)} (+- {np.std(cleanedList)})') #this has to be a for loop\n",
    "    print(\"X^2 for MWPM and NN: \" + str(mcnemar_results_mwpm[key]))  \n",
    "    print(\"X^2 for PLUT and NN: \" + str(mcnemar_results_plut[key]))\n",
    "    \n",
    "print(\"###################################################################################\")\n",
    "print(\"TOTAL F1 NN: \" + str(f1_per_fold))\n",
    "print(\"TOTAL F1 PLUT: \" + str(f1_per_fold_plut))\n",
    "print(\"TOTAL F1 MWPM: \" + str(f1_per_fold_mwpm))\n",
    "print(\"TOTAL ACC NN: \" + str(acc_per_fold))\n",
    "print(\"TOTAL ACC PLUT: \" + str(acc_per_fold_plut))\n",
    "print(\"TOTAL ACC MWPM: \" + str(acc_per_fold_mwpm))\n",
    "print(\"TOTAL TIME NN: \" + str(time_per_fold))\n",
    "print(\"TOTAL TIME PLUT: \" + str(time_per_fold_plut))\n",
    "print(\"TOTAL TIME MWPM: \" + str(time_per_fold_mwpm))\n",
    "print('------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "         label='Chance', alpha=.8)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "         label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "         lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                 label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "plt.xlim([-0.01, 1.01])\n",
    "plt.ylim([-0.01, 1.01])\n",
    "plt.xlabel('False Positive Rate',fontsize=18)\n",
    "plt.ylabel('True Positive Rate',fontsize=18)\n",
    "plt.title('Cross-Validation ROC of Depth 5 NN',fontsize=18)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Epoch 1/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.1626 - accuracy: 0.0508 - val_loss: 0.1871 - val_accuracy: 0.1021\n",
      "Epoch 2/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.1424 - accuracy: 0.1243 - val_loss: 0.1547 - val_accuracy: 0.2459\n",
      "Epoch 3/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.1144 - accuracy: 0.1863 - val_loss: 0.1231 - val_accuracy: 0.3035\n",
      "Epoch 4/150\n",
      "7941/7941 [==============================] - 19s 2ms/step - loss: 0.0943 - accuracy: 0.2082 - val_loss: 0.1068 - val_accuracy: 0.3038\n",
      "Epoch 5/150\n",
      "7941/7941 [==============================] - 17s 2ms/step - loss: 0.0823 - accuracy: 0.2231 - val_loss: 0.0981 - val_accuracy: 0.3182\n",
      "Epoch 6/150\n",
      "7941/7941 [==============================] - 17s 2ms/step - loss: 0.0749 - accuracy: 0.2269 - val_loss: 0.0919 - val_accuracy: 0.3407\n",
      "Epoch 7/150\n",
      "7941/7941 [==============================] - 17s 2ms/step - loss: 0.0693 - accuracy: 0.2284 - val_loss: 0.0865 - val_accuracy: 0.3411\n",
      "Epoch 8/150\n",
      "7941/7941 [==============================] - 16s 2ms/step - loss: 0.0647 - accuracy: 0.2303 - val_loss: 0.0819 - val_accuracy: 0.3386\n",
      "Epoch 9/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0613 - accuracy: 0.2307 - val_loss: 0.0780 - val_accuracy: 0.3475\n",
      "Epoch 10/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0586 - accuracy: 0.2316 - val_loss: 0.0757 - val_accuracy: 0.3339\n",
      "Epoch 11/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0564 - accuracy: 0.2317 - val_loss: 0.0732 - val_accuracy: 0.3444\n",
      "Epoch 12/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0545 - accuracy: 0.2323 - val_loss: 0.0710 - val_accuracy: 0.3417\n",
      "Epoch 13/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0529 - accuracy: 0.2328 - val_loss: 0.0705 - val_accuracy: 0.3314\n",
      "Epoch 14/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0516 - accuracy: 0.2323 - val_loss: 0.0679 - val_accuracy: 0.3361\n",
      "Epoch 15/150\n",
      "7941/7941 [==============================] - 16s 2ms/step - loss: 0.0505 - accuracy: 0.2325 - val_loss: 0.0660 - val_accuracy: 0.3445\n",
      "Epoch 16/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0495 - accuracy: 0.2323 - val_loss: 0.0656 - val_accuracy: 0.3343\n",
      "Epoch 17/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0486 - accuracy: 0.2326 - val_loss: 0.0644 - val_accuracy: 0.3285\n",
      "Epoch 18/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0478 - accuracy: 0.2322 - val_loss: 0.0630 - val_accuracy: 0.3418\n",
      "Epoch 19/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0471 - accuracy: 0.2325 - val_loss: 0.0627 - val_accuracy: 0.3449\n",
      "Epoch 20/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0464 - accuracy: 0.2326 - val_loss: 0.0616 - val_accuracy: 0.3226\n",
      "Epoch 21/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0458 - accuracy: 0.2332 - val_loss: 0.0604 - val_accuracy: 0.3357\n",
      "Epoch 22/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0452 - accuracy: 0.2331 - val_loss: 0.0597 - val_accuracy: 0.3327\n",
      "Epoch 23/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0447 - accuracy: 0.2344 - val_loss: 0.0590 - val_accuracy: 0.3296\n",
      "Epoch 24/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0442 - accuracy: 0.2330 - val_loss: 0.0586 - val_accuracy: 0.3356\n",
      "Epoch 25/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0438 - accuracy: 0.2338 - val_loss: 0.0578 - val_accuracy: 0.3233\n",
      "Epoch 26/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0433 - accuracy: 0.2341 - val_loss: 0.0575 - val_accuracy: 0.3239\n",
      "Epoch 27/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0429 - accuracy: 0.2346 - val_loss: 0.0567 - val_accuracy: 0.3387\n",
      "Epoch 28/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0426 - accuracy: 0.2340 - val_loss: 0.0562 - val_accuracy: 0.3231\n",
      "Epoch 29/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0422 - accuracy: 0.2343 - val_loss: 0.0562 - val_accuracy: 0.3301\n",
      "Epoch 30/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0419 - accuracy: 0.2347 - val_loss: 0.0558 - val_accuracy: 0.3272\n",
      "Epoch 31/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0416 - accuracy: 0.2342 - val_loss: 0.0550 - val_accuracy: 0.3219\n",
      "Epoch 32/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0412 - accuracy: 0.2347 - val_loss: 0.0546 - val_accuracy: 0.3136\n",
      "Epoch 33/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0410 - accuracy: 0.2352 - val_loss: 0.0544 - val_accuracy: 0.3168\n",
      "Epoch 34/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0407 - accuracy: 0.2350 - val_loss: 0.0540 - val_accuracy: 0.3203\n",
      "Epoch 35/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0404 - accuracy: 0.2345 - val_loss: 0.0541 - val_accuracy: 0.3273\n",
      "Epoch 36/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0401 - accuracy: 0.2339 - val_loss: 0.0539 - val_accuracy: 0.3284\n",
      "Epoch 37/150\n",
      "7941/7941 [==============================] - 16s 2ms/step - loss: 0.0399 - accuracy: 0.2341 - val_loss: 0.0539 - val_accuracy: 0.3297\n",
      "Epoch 38/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0396 - accuracy: 0.2337 - val_loss: 0.0531 - val_accuracy: 0.3186\n",
      "Epoch 39/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0394 - accuracy: 0.2344 - val_loss: 0.0526 - val_accuracy: 0.3210\n",
      "Epoch 40/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0392 - accuracy: 0.2344 - val_loss: 0.0525 - val_accuracy: 0.3140\n",
      "Epoch 41/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0390 - accuracy: 0.2335 - val_loss: 0.0524 - val_accuracy: 0.3279\n",
      "Epoch 42/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0388 - accuracy: 0.2334 - val_loss: 0.0519 - val_accuracy: 0.3044\n",
      "Epoch 43/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0386 - accuracy: 0.2333 - val_loss: 0.0515 - val_accuracy: 0.3284\n",
      "Epoch 44/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0384 - accuracy: 0.2331 - val_loss: 0.0517 - val_accuracy: 0.3174\n",
      "Epoch 45/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0382 - accuracy: 0.2321 - val_loss: 0.0510 - val_accuracy: 0.3063\n",
      "Epoch 46/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0380 - accuracy: 0.2328 - val_loss: 0.0509 - val_accuracy: 0.3183\n",
      "Epoch 47/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0378 - accuracy: 0.2328 - val_loss: 0.0507 - val_accuracy: 0.3198\n",
      "Epoch 48/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0376 - accuracy: 0.2323 - val_loss: 0.0504 - val_accuracy: 0.3099\n",
      "Epoch 49/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0375 - accuracy: 0.2322 - val_loss: 0.0502 - val_accuracy: 0.3061\n",
      "Epoch 50/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0373 - accuracy: 0.2316 - val_loss: 0.0502 - val_accuracy: 0.3068\n",
      "Epoch 51/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0372 - accuracy: 0.2311 - val_loss: 0.0500 - val_accuracy: 0.3140\n",
      "Epoch 52/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0370 - accuracy: 0.2317 - val_loss: 0.0498 - val_accuracy: 0.3143\n",
      "Epoch 53/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0368 - accuracy: 0.2311 - val_loss: 0.0493 - val_accuracy: 0.3132\n",
      "Epoch 54/150\n",
      "7941/7941 [==============================] - 16s 2ms/step - loss: 0.0367 - accuracy: 0.2306 - val_loss: 0.0494 - val_accuracy: 0.3033\n",
      "Epoch 55/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0366 - accuracy: 0.2301 - val_loss: 0.0498 - val_accuracy: 0.3183\n",
      "Epoch 56/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0364 - accuracy: 0.2296 - val_loss: 0.0490 - val_accuracy: 0.2993\n",
      "Epoch 57/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0363 - accuracy: 0.2295 - val_loss: 0.0491 - val_accuracy: 0.3153\n",
      "Epoch 58/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0362 - accuracy: 0.2293 - val_loss: 0.0492 - val_accuracy: 0.3075\n",
      "Epoch 59/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0360 - accuracy: 0.2293 - val_loss: 0.0483 - val_accuracy: 0.2988\n",
      "Epoch 60/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0359 - accuracy: 0.2293 - val_loss: 0.0484 - val_accuracy: 0.3233\n",
      "Epoch 61/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0358 - accuracy: 0.2286 - val_loss: 0.0487 - val_accuracy: 0.3154\n",
      "Epoch 62/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0357 - accuracy: 0.2285 - val_loss: 0.0479 - val_accuracy: 0.3197\n",
      "Epoch 63/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0356 - accuracy: 0.2286 - val_loss: 0.0480 - val_accuracy: 0.3186\n",
      "Epoch 64/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0354 - accuracy: 0.2284 - val_loss: 0.0473 - val_accuracy: 0.3288\n",
      "Epoch 65/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0353 - accuracy: 0.2284 - val_loss: 0.0482 - val_accuracy: 0.3132\n",
      "Epoch 66/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0352 - accuracy: 0.2274 - val_loss: 0.0475 - val_accuracy: 0.3042\n",
      "Epoch 67/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0351 - accuracy: 0.2268 - val_loss: 0.0482 - val_accuracy: 0.3150\n",
      "Epoch 68/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0350 - accuracy: 0.2265 - val_loss: 0.0479 - val_accuracy: 0.3109\n",
      "Epoch 69/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0349 - accuracy: 0.2271 - val_loss: 0.0476 - val_accuracy: 0.3054\n",
      "Epoch 70/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0348 - accuracy: 0.2262 - val_loss: 0.0477 - val_accuracy: 0.3093\n",
      "Epoch 71/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0347 - accuracy: 0.2261 - val_loss: 0.0477 - val_accuracy: 0.2918\n",
      "Epoch 72/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0346 - accuracy: 0.2264 - val_loss: 0.0477 - val_accuracy: 0.3026\n",
      "Epoch 73/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0345 - accuracy: 0.2262 - val_loss: 0.0479 - val_accuracy: 0.3102\n",
      "Epoch 74/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0344 - accuracy: 0.2257 - val_loss: 0.0470 - val_accuracy: 0.3029\n",
      "Epoch 75/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0343 - accuracy: 0.2257 - val_loss: 0.0467 - val_accuracy: 0.3085\n",
      "Epoch 76/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0342 - accuracy: 0.2249 - val_loss: 0.0471 - val_accuracy: 0.3159\n",
      "Epoch 77/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0341 - accuracy: 0.2249 - val_loss: 0.0460 - val_accuracy: 0.2929\n",
      "Epoch 78/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0340 - accuracy: 0.2249 - val_loss: 0.0463 - val_accuracy: 0.2973\n",
      "Epoch 79/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0339 - accuracy: 0.2245 - val_loss: 0.0474 - val_accuracy: 0.3211\n",
      "Epoch 80/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0339 - accuracy: 0.2243 - val_loss: 0.0467 - val_accuracy: 0.3209\n",
      "Epoch 81/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0338 - accuracy: 0.2241 - val_loss: 0.0457 - val_accuracy: 0.3018\n",
      "Epoch 82/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0337 - accuracy: 0.2248 - val_loss: 0.0466 - val_accuracy: 0.3002\n",
      "Epoch 83/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0336 - accuracy: 0.2248 - val_loss: 0.0461 - val_accuracy: 0.3272\n",
      "Epoch 84/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0335 - accuracy: 0.2250 - val_loss: 0.0462 - val_accuracy: 0.3240\n",
      "Epoch 85/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0335 - accuracy: 0.2249 - val_loss: 0.0458 - val_accuracy: 0.2994\n",
      "Epoch 86/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0334 - accuracy: 0.2245 - val_loss: 0.0461 - val_accuracy: 0.3141\n",
      "Epoch 87/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0333 - accuracy: 0.2250 - val_loss: 0.0460 - val_accuracy: 0.3116\n",
      "Epoch 88/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0332 - accuracy: 0.2249 - val_loss: 0.0459 - val_accuracy: 0.3034\n",
      "Epoch 89/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0332 - accuracy: 0.2239 - val_loss: 0.0454 - val_accuracy: 0.3042\n",
      "Epoch 90/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0331 - accuracy: 0.2239 - val_loss: 0.0457 - val_accuracy: 0.3159\n",
      "Epoch 91/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0330 - accuracy: 0.2230 - val_loss: 0.0455 - val_accuracy: 0.3018\n",
      "Epoch 92/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0329 - accuracy: 0.2236 - val_loss: 0.0457 - val_accuracy: 0.3027\n",
      "Epoch 93/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0329 - accuracy: 0.2243 - val_loss: 0.0456 - val_accuracy: 0.3099\n",
      "Epoch 94/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0328 - accuracy: 0.2240 - val_loss: 0.0454 - val_accuracy: 0.2850\n",
      "Epoch 95/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0327 - accuracy: 0.2235 - val_loss: 0.0454 - val_accuracy: 0.3063\n",
      "Epoch 96/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0327 - accuracy: 0.2236 - val_loss: 0.0451 - val_accuracy: 0.3024\n",
      "Epoch 97/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0326 - accuracy: 0.2242 - val_loss: 0.0450 - val_accuracy: 0.2979\n",
      "Epoch 98/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0325 - accuracy: 0.2238 - val_loss: 0.0451 - val_accuracy: 0.3125\n",
      "Epoch 99/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0325 - accuracy: 0.2244 - val_loss: 0.0451 - val_accuracy: 0.3189\n",
      "Epoch 100/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0324 - accuracy: 0.2244 - val_loss: 0.0450 - val_accuracy: 0.3049\n",
      "Epoch 101/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0323 - accuracy: 0.2234 - val_loss: 0.0449 - val_accuracy: 0.3026\n",
      "Epoch 102/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0323 - accuracy: 0.2238 - val_loss: 0.0447 - val_accuracy: 0.3240\n",
      "Epoch 103/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0322 - accuracy: 0.2242 - val_loss: 0.0447 - val_accuracy: 0.3048\n",
      "Epoch 104/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0322 - accuracy: 0.2235 - val_loss: 0.0448 - val_accuracy: 0.3126\n",
      "Epoch 105/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0321 - accuracy: 0.2241 - val_loss: 0.0445 - val_accuracy: 0.2976\n",
      "Epoch 106/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0320 - accuracy: 0.2238 - val_loss: 0.0444 - val_accuracy: 0.3019\n",
      "Epoch 107/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0320 - accuracy: 0.2231 - val_loss: 0.0454 - val_accuracy: 0.3174\n",
      "Epoch 108/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0319 - accuracy: 0.2240 - val_loss: 0.0446 - val_accuracy: 0.3039\n",
      "Epoch 109/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0319 - accuracy: 0.2240 - val_loss: 0.0443 - val_accuracy: 0.3069\n",
      "Epoch 110/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0318 - accuracy: 0.2249 - val_loss: 0.0447 - val_accuracy: 0.2905\n",
      "Epoch 111/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0318 - accuracy: 0.2236 - val_loss: 0.0443 - val_accuracy: 0.2925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0317 - accuracy: 0.2235 - val_loss: 0.0442 - val_accuracy: 0.3151\n",
      "Epoch 113/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0317 - accuracy: 0.2233 - val_loss: 0.0448 - val_accuracy: 0.3027\n",
      "Epoch 114/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0316 - accuracy: 0.2247 - val_loss: 0.0444 - val_accuracy: 0.2966\n",
      "Epoch 115/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0316 - accuracy: 0.2240 - val_loss: 0.0447 - val_accuracy: 0.3085\n",
      "Epoch 116/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0315 - accuracy: 0.2241 - val_loss: 0.0446 - val_accuracy: 0.3025\n",
      "Epoch 117/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0314 - accuracy: 0.2247 - val_loss: 0.0441 - val_accuracy: 0.3124\n",
      "Epoch 118/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0314 - accuracy: 0.2241 - val_loss: 0.0449 - val_accuracy: 0.3167\n",
      "Epoch 119/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0313 - accuracy: 0.2254 - val_loss: 0.0442 - val_accuracy: 0.3058\n",
      "Epoch 120/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0313 - accuracy: 0.2246 - val_loss: 0.0443 - val_accuracy: 0.3173\n",
      "Epoch 121/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0312 - accuracy: 0.2254 - val_loss: 0.0444 - val_accuracy: 0.3197\n",
      "Epoch 122/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0312 - accuracy: 0.2252 - val_loss: 0.0450 - val_accuracy: 0.3061\n",
      "Epoch 123/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0312 - accuracy: 0.2243 - val_loss: 0.0446 - val_accuracy: 0.2907\n",
      "Epoch 124/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0311 - accuracy: 0.2252 - val_loss: 0.0438 - val_accuracy: 0.2965\n",
      "Epoch 125/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0311 - accuracy: 0.2248 - val_loss: 0.0440 - val_accuracy: 0.3223\n",
      "Epoch 126/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0310 - accuracy: 0.2246 - val_loss: 0.0439 - val_accuracy: 0.3044\n",
      "Epoch 127/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0310 - accuracy: 0.2257 - val_loss: 0.0442 - val_accuracy: 0.3197\n",
      "Epoch 128/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0309 - accuracy: 0.2248 - val_loss: 0.0436 - val_accuracy: 0.3145\n",
      "Epoch 129/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0309 - accuracy: 0.2253 - val_loss: 0.0437 - val_accuracy: 0.3136\n",
      "Epoch 130/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0308 - accuracy: 0.2249 - val_loss: 0.0438 - val_accuracy: 0.3122\n",
      "Epoch 131/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0308 - accuracy: 0.2246 - val_loss: 0.0432 - val_accuracy: 0.3065\n",
      "Epoch 132/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0307 - accuracy: 0.2251 - val_loss: 0.0446 - val_accuracy: 0.3119\n",
      "Epoch 133/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0307 - accuracy: 0.2266 - val_loss: 0.0435 - val_accuracy: 0.3074\n",
      "Epoch 134/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0306 - accuracy: 0.2265 - val_loss: 0.0435 - val_accuracy: 0.3121\n",
      "Epoch 135/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0306 - accuracy: 0.2254 - val_loss: 0.0436 - val_accuracy: 0.3096\n",
      "Epoch 136/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0306 - accuracy: 0.2264 - val_loss: 0.0437 - val_accuracy: 0.3123\n",
      "Epoch 137/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0305 - accuracy: 0.2253 - val_loss: 0.0439 - val_accuracy: 0.3195\n",
      "Epoch 138/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0305 - accuracy: 0.2267 - val_loss: 0.0434 - val_accuracy: 0.3158\n",
      "Epoch 139/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0304 - accuracy: 0.2262 - val_loss: 0.0429 - val_accuracy: 0.3197\n",
      "Epoch 140/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0304 - accuracy: 0.2262 - val_loss: 0.0432 - val_accuracy: 0.3162\n",
      "Epoch 141/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0304 - accuracy: 0.2267 - val_loss: 0.0437 - val_accuracy: 0.3140\n",
      "Epoch 142/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0303 - accuracy: 0.2262 - val_loss: 0.0433 - val_accuracy: 0.3140\n",
      "Epoch 143/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0303 - accuracy: 0.2270 - val_loss: 0.0432 - val_accuracy: 0.3266\n",
      "Epoch 144/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0302 - accuracy: 0.2267 - val_loss: 0.0440 - val_accuracy: 0.3244\n",
      "Epoch 145/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0302 - accuracy: 0.2271 - val_loss: 0.0433 - val_accuracy: 0.3124\n",
      "Epoch 146/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0302 - accuracy: 0.2279 - val_loss: 0.0430 - val_accuracy: 0.3171\n",
      "Epoch 147/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0301 - accuracy: 0.2277 - val_loss: 0.0426 - val_accuracy: 0.3225\n",
      "Epoch 148/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0301 - accuracy: 0.2273 - val_loss: 0.0434 - val_accuracy: 0.3101\n",
      "Epoch 149/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0300 - accuracy: 0.2277 - val_loss: 0.0429 - val_accuracy: 0.3165\n",
      "Epoch 150/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0300 - accuracy: 0.2272 - val_loss: 0.0428 - val_accuracy: 0.3126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-1a1788afa7e3>:140: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "  tprs.append(interp(mean_fpr, fpr, tpr))\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Epoch 1/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.1626 - accuracy: 0.0573 - val_loss: 0.1865 - val_accuracy: 0.1414\n",
      "Epoch 2/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.1406 - accuracy: 0.1760 - val_loss: 0.1485 - val_accuracy: 0.2735\n",
      "Epoch 3/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.1101 - accuracy: 0.2145 - val_loss: 0.1221 - val_accuracy: 0.3055\n",
      "Epoch 4/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0940 - accuracy: 0.2131 - val_loss: 0.1086 - val_accuracy: 0.3089\n",
      "Epoch 5/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0833 - accuracy: 0.2137 - val_loss: 0.0990 - val_accuracy: 0.3115\n",
      "Epoch 6/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0752 - accuracy: 0.2199 - val_loss: 0.0914 - val_accuracy: 0.3023\n",
      "Epoch 7/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0692 - accuracy: 0.2223 - val_loss: 0.0854 - val_accuracy: 0.2756\n",
      "Epoch 8/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0648 - accuracy: 0.2189 - val_loss: 0.0815 - val_accuracy: 0.2890\n",
      "Epoch 9/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0616 - accuracy: 0.2184 - val_loss: 0.0787 - val_accuracy: 0.2861\n",
      "Epoch 10/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0591 - accuracy: 0.2184 - val_loss: 0.0759 - val_accuracy: 0.3057\n",
      "Epoch 11/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0568 - accuracy: 0.2200 - val_loss: 0.0733 - val_accuracy: 0.3075\n",
      "Epoch 12/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0549 - accuracy: 0.2220 - val_loss: 0.0715 - val_accuracy: 0.3118\n",
      "Epoch 13/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0531 - accuracy: 0.2245 - val_loss: 0.0698 - val_accuracy: 0.3439\n",
      "Epoch 14/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0517 - accuracy: 0.2254 - val_loss: 0.0678 - val_accuracy: 0.3288\n",
      "Epoch 15/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0505 - accuracy: 0.2273 - val_loss: 0.0667 - val_accuracy: 0.3130\n",
      "Epoch 16/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0494 - accuracy: 0.2289 - val_loss: 0.0648 - val_accuracy: 0.3287\n",
      "Epoch 17/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0485 - accuracy: 0.2289 - val_loss: 0.0640 - val_accuracy: 0.3236\n",
      "Epoch 18/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0477 - accuracy: 0.2289 - val_loss: 0.0631 - val_accuracy: 0.3218\n",
      "Epoch 19/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0469 - accuracy: 0.2300 - val_loss: 0.0628 - val_accuracy: 0.3283\n",
      "Epoch 20/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0463 - accuracy: 0.2297 - val_loss: 0.0610 - val_accuracy: 0.3082\n",
      "Epoch 21/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0457 - accuracy: 0.2296 - val_loss: 0.0601 - val_accuracy: 0.3289\n",
      "Epoch 22/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0451 - accuracy: 0.2292 - val_loss: 0.0594 - val_accuracy: 0.3209\n",
      "Epoch 23/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0446 - accuracy: 0.2290 - val_loss: 0.0591 - val_accuracy: 0.3125\n",
      "Epoch 24/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0441 - accuracy: 0.2290 - val_loss: 0.0582 - val_accuracy: 0.3086\n",
      "Epoch 25/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0437 - accuracy: 0.2283 - val_loss: 0.0573 - val_accuracy: 0.3141\n",
      "Epoch 26/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0432 - accuracy: 0.2281 - val_loss: 0.0568 - val_accuracy: 0.3102\n",
      "Epoch 27/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0429 - accuracy: 0.2283 - val_loss: 0.0564 - val_accuracy: 0.3324\n",
      "Epoch 28/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0425 - accuracy: 0.2284 - val_loss: 0.0564 - val_accuracy: 0.3426\n",
      "Epoch 29/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0421 - accuracy: 0.2273 - val_loss: 0.0556 - val_accuracy: 0.3225\n",
      "Epoch 30/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0418 - accuracy: 0.2273 - val_loss: 0.0548 - val_accuracy: 0.3006\n",
      "Epoch 31/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0414 - accuracy: 0.2273 - val_loss: 0.0546 - val_accuracy: 0.3176\n",
      "Epoch 32/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0411 - accuracy: 0.2267 - val_loss: 0.0545 - val_accuracy: 0.3148\n",
      "Epoch 33/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0408 - accuracy: 0.2281 - val_loss: 0.0550 - val_accuracy: 0.3076\n",
      "Epoch 34/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0405 - accuracy: 0.2272 - val_loss: 0.0539 - val_accuracy: 0.3063\n",
      "Epoch 35/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0403 - accuracy: 0.2281 - val_loss: 0.0534 - val_accuracy: 0.2839\n",
      "Epoch 36/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0400 - accuracy: 0.2276 - val_loss: 0.0530 - val_accuracy: 0.3086\n",
      "Epoch 37/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0397 - accuracy: 0.2278 - val_loss: 0.0525 - val_accuracy: 0.3087\n",
      "Epoch 38/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0395 - accuracy: 0.2269 - val_loss: 0.0523 - val_accuracy: 0.3027\n",
      "Epoch 39/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0393 - accuracy: 0.2279 - val_loss: 0.0523 - val_accuracy: 0.3159\n",
      "Epoch 40/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0390 - accuracy: 0.2270 - val_loss: 0.0528 - val_accuracy: 0.3176\n",
      "Epoch 41/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0388 - accuracy: 0.2277 - val_loss: 0.0518 - val_accuracy: 0.3081\n",
      "Epoch 42/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0386 - accuracy: 0.2273 - val_loss: 0.0518 - val_accuracy: 0.3073\n",
      "Epoch 43/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0384 - accuracy: 0.2272 - val_loss: 0.0510 - val_accuracy: 0.3093\n",
      "Epoch 44/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0382 - accuracy: 0.2272 - val_loss: 0.0513 - val_accuracy: 0.3064\n",
      "Epoch 45/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0380 - accuracy: 0.2268 - val_loss: 0.0504 - val_accuracy: 0.3191\n",
      "Epoch 46/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0378 - accuracy: 0.2267 - val_loss: 0.0504 - val_accuracy: 0.3038\n",
      "Epoch 47/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0376 - accuracy: 0.2273 - val_loss: 0.0500 - val_accuracy: 0.3012\n",
      "Epoch 48/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0375 - accuracy: 0.2255 - val_loss: 0.0504 - val_accuracy: 0.3266\n",
      "Epoch 49/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0373 - accuracy: 0.2256 - val_loss: 0.0496 - val_accuracy: 0.3071\n",
      "Epoch 50/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0371 - accuracy: 0.2268 - val_loss: 0.0497 - val_accuracy: 0.3033\n",
      "Epoch 51/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0370 - accuracy: 0.2261 - val_loss: 0.0493 - val_accuracy: 0.3197\n",
      "Epoch 52/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0368 - accuracy: 0.2255 - val_loss: 0.0496 - val_accuracy: 0.2970\n",
      "Epoch 53/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0367 - accuracy: 0.2257 - val_loss: 0.0495 - val_accuracy: 0.2972\n",
      "Epoch 54/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0365 - accuracy: 0.2258 - val_loss: 0.0492 - val_accuracy: 0.3237\n",
      "Epoch 55/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0364 - accuracy: 0.2261 - val_loss: 0.0493 - val_accuracy: 0.3041\n",
      "Epoch 56/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0363 - accuracy: 0.2249 - val_loss: 0.0488 - val_accuracy: 0.3108\n",
      "Epoch 57/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0361 - accuracy: 0.2251 - val_loss: 0.0481 - val_accuracy: 0.3015\n",
      "Epoch 58/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0360 - accuracy: 0.2243 - val_loss: 0.0484 - val_accuracy: 0.3011\n",
      "Epoch 59/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0359 - accuracy: 0.2247 - val_loss: 0.0477 - val_accuracy: 0.3033\n",
      "Epoch 60/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0358 - accuracy: 0.2242 - val_loss: 0.0487 - val_accuracy: 0.3072\n",
      "Epoch 61/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0356 - accuracy: 0.2250 - val_loss: 0.0476 - val_accuracy: 0.3120\n",
      "Epoch 62/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0355 - accuracy: 0.2248 - val_loss: 0.0478 - val_accuracy: 0.2949\n",
      "Epoch 63/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0354 - accuracy: 0.2242 - val_loss: 0.0480 - val_accuracy: 0.3226\n",
      "Epoch 64/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0353 - accuracy: 0.2234 - val_loss: 0.0480 - val_accuracy: 0.3160\n",
      "Epoch 65/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0352 - accuracy: 0.2239 - val_loss: 0.0475 - val_accuracy: 0.3138\n",
      "Epoch 66/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0351 - accuracy: 0.2244 - val_loss: 0.0480 - val_accuracy: 0.3144\n",
      "Epoch 67/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0350 - accuracy: 0.2241 - val_loss: 0.0472 - val_accuracy: 0.2959\n",
      "Epoch 68/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0349 - accuracy: 0.2240 - val_loss: 0.0475 - val_accuracy: 0.3103\n",
      "Epoch 69/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0348 - accuracy: 0.2234 - val_loss: 0.0479 - val_accuracy: 0.3150\n",
      "Epoch 70/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0347 - accuracy: 0.2238 - val_loss: 0.0472 - val_accuracy: 0.3133\n",
      "Epoch 71/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0346 - accuracy: 0.2239 - val_loss: 0.0477 - val_accuracy: 0.2900\n",
      "Epoch 72/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0345 - accuracy: 0.2229 - val_loss: 0.0477 - val_accuracy: 0.3043\n",
      "Epoch 73/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0344 - accuracy: 0.2222 - val_loss: 0.0475 - val_accuracy: 0.3084\n",
      "Epoch 74/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0343 - accuracy: 0.2232 - val_loss: 0.0471 - val_accuracy: 0.3178\n",
      "Epoch 75/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0342 - accuracy: 0.2236 - val_loss: 0.0469 - val_accuracy: 0.3086\n",
      "Epoch 76/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0341 - accuracy: 0.2229 - val_loss: 0.0471 - val_accuracy: 0.3251\n",
      "Epoch 77/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0340 - accuracy: 0.2236 - val_loss: 0.0466 - val_accuracy: 0.3200\n",
      "Epoch 78/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0339 - accuracy: 0.2228 - val_loss: 0.0468 - val_accuracy: 0.2841\n",
      "Epoch 79/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0339 - accuracy: 0.2230 - val_loss: 0.0464 - val_accuracy: 0.3167\n",
      "Epoch 80/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0338 - accuracy: 0.2225 - val_loss: 0.0464 - val_accuracy: 0.3215\n",
      "Epoch 81/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0337 - accuracy: 0.2225 - val_loss: 0.0457 - val_accuracy: 0.3121\n",
      "Epoch 82/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0336 - accuracy: 0.2228 - val_loss: 0.0464 - val_accuracy: 0.3091\n",
      "Epoch 83/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0335 - accuracy: 0.2223 - val_loss: 0.0458 - val_accuracy: 0.3197\n",
      "Epoch 84/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0335 - accuracy: 0.2223 - val_loss: 0.0454 - val_accuracy: 0.3003\n",
      "Epoch 85/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0334 - accuracy: 0.2227 - val_loss: 0.0455 - val_accuracy: 0.2893\n",
      "Epoch 86/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0333 - accuracy: 0.2225 - val_loss: 0.0465 - val_accuracy: 0.3074\n",
      "Epoch 87/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0332 - accuracy: 0.2218 - val_loss: 0.0465 - val_accuracy: 0.3190\n",
      "Epoch 88/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0332 - accuracy: 0.2228 - val_loss: 0.0458 - val_accuracy: 0.3053\n",
      "Epoch 89/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0331 - accuracy: 0.2223 - val_loss: 0.0457 - val_accuracy: 0.3034\n",
      "Epoch 90/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0330 - accuracy: 0.2214 - val_loss: 0.0455 - val_accuracy: 0.3005\n",
      "Epoch 91/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0329 - accuracy: 0.2220 - val_loss: 0.0451 - val_accuracy: 0.2998\n",
      "Epoch 92/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0329 - accuracy: 0.2230 - val_loss: 0.0455 - val_accuracy: 0.3082\n",
      "Epoch 93/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0328 - accuracy: 0.2219 - val_loss: 0.0455 - val_accuracy: 0.3036\n",
      "Epoch 94/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0327 - accuracy: 0.2219 - val_loss: 0.0455 - val_accuracy: 0.2977\n",
      "Epoch 95/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0327 - accuracy: 0.2229 - val_loss: 0.0453 - val_accuracy: 0.2822\n",
      "Epoch 96/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0326 - accuracy: 0.2237 - val_loss: 0.0453 - val_accuracy: 0.2765\n",
      "Epoch 97/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0325 - accuracy: 0.2216 - val_loss: 0.0449 - val_accuracy: 0.3013\n",
      "Epoch 98/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0325 - accuracy: 0.2216 - val_loss: 0.0450 - val_accuracy: 0.2986\n",
      "Epoch 99/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0324 - accuracy: 0.2220 - val_loss: 0.0449 - val_accuracy: 0.2921\n",
      "Epoch 100/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0324 - accuracy: 0.2217 - val_loss: 0.0450 - val_accuracy: 0.3045\n",
      "Epoch 101/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0323 - accuracy: 0.2213 - val_loss: 0.0454 - val_accuracy: 0.3143\n",
      "Epoch 102/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0322 - accuracy: 0.2222 - val_loss: 0.0450 - val_accuracy: 0.3124\n",
      "Epoch 103/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0322 - accuracy: 0.2219 - val_loss: 0.0451 - val_accuracy: 0.3082\n",
      "Epoch 104/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0321 - accuracy: 0.2220 - val_loss: 0.0449 - val_accuracy: 0.3182\n",
      "Epoch 105/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0321 - accuracy: 0.2219 - val_loss: 0.0447 - val_accuracy: 0.3163\n",
      "Epoch 106/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0320 - accuracy: 0.2225 - val_loss: 0.0452 - val_accuracy: 0.3090\n",
      "Epoch 107/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0319 - accuracy: 0.2221 - val_loss: 0.0444 - val_accuracy: 0.3041\n",
      "Epoch 108/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0319 - accuracy: 0.2215 - val_loss: 0.0439 - val_accuracy: 0.3019\n",
      "Epoch 109/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0318 - accuracy: 0.2217 - val_loss: 0.0448 - val_accuracy: 0.3206\n",
      "Epoch 110/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0318 - accuracy: 0.2229 - val_loss: 0.0446 - val_accuracy: 0.2932\n",
      "Epoch 111/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0317 - accuracy: 0.2221 - val_loss: 0.0448 - val_accuracy: 0.3273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0317 - accuracy: 0.2227 - val_loss: 0.0436 - val_accuracy: 0.2999\n",
      "Epoch 113/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0316 - accuracy: 0.2216 - val_loss: 0.0442 - val_accuracy: 0.3069\n",
      "Epoch 114/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0316 - accuracy: 0.2215 - val_loss: 0.0447 - val_accuracy: 0.2978\n",
      "Epoch 115/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0315 - accuracy: 0.2227 - val_loss: 0.0437 - val_accuracy: 0.2920\n",
      "Epoch 116/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0314 - accuracy: 0.2222 - val_loss: 0.0446 - val_accuracy: 0.3006\n",
      "Epoch 117/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0314 - accuracy: 0.2227 - val_loss: 0.0443 - val_accuracy: 0.3089\n",
      "Epoch 118/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0313 - accuracy: 0.2221 - val_loss: 0.0439 - val_accuracy: 0.3104\n",
      "Epoch 119/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0313 - accuracy: 0.2227 - val_loss: 0.0434 - val_accuracy: 0.3112\n",
      "Epoch 120/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0313 - accuracy: 0.2220 - val_loss: 0.0442 - val_accuracy: 0.3033\n",
      "Epoch 121/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0312 - accuracy: 0.2219 - val_loss: 0.0447 - val_accuracy: 0.3059\n",
      "Epoch 122/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0312 - accuracy: 0.2231 - val_loss: 0.0438 - val_accuracy: 0.3076\n",
      "Epoch 123/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0311 - accuracy: 0.2230 - val_loss: 0.0438 - val_accuracy: 0.3187\n",
      "Epoch 124/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0310 - accuracy: 0.2221 - val_loss: 0.0441 - val_accuracy: 0.3247\n",
      "Epoch 125/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0310 - accuracy: 0.2227 - val_loss: 0.0437 - val_accuracy: 0.3229\n",
      "Epoch 126/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0310 - accuracy: 0.2227 - val_loss: 0.0441 - val_accuracy: 0.2976\n",
      "Epoch 127/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0309 - accuracy: 0.2233 - val_loss: 0.0441 - val_accuracy: 0.2981\n",
      "Epoch 128/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0309 - accuracy: 0.2223 - val_loss: 0.0433 - val_accuracy: 0.3099\n",
      "Epoch 129/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0308 - accuracy: 0.2236 - val_loss: 0.0432 - val_accuracy: 0.2959\n",
      "Epoch 130/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0308 - accuracy: 0.2240 - val_loss: 0.0436 - val_accuracy: 0.3093\n",
      "Epoch 131/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0307 - accuracy: 0.2234 - val_loss: 0.0433 - val_accuracy: 0.3238\n",
      "Epoch 132/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0307 - accuracy: 0.2235 - val_loss: 0.0435 - val_accuracy: 0.3012\n",
      "Epoch 133/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0307 - accuracy: 0.2234 - val_loss: 0.0438 - val_accuracy: 0.3114\n",
      "Epoch 134/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0306 - accuracy: 0.2236 - val_loss: 0.0436 - val_accuracy: 0.3041\n",
      "Epoch 135/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0306 - accuracy: 0.2241 - val_loss: 0.0437 - val_accuracy: 0.3148\n",
      "Epoch 136/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0305 - accuracy: 0.2241 - val_loss: 0.0437 - val_accuracy: 0.3125\n",
      "Epoch 137/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0305 - accuracy: 0.2245 - val_loss: 0.0437 - val_accuracy: 0.3107\n",
      "Epoch 138/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0304 - accuracy: 0.2238 - val_loss: 0.0432 - val_accuracy: 0.2998\n",
      "Epoch 139/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0304 - accuracy: 0.2243 - val_loss: 0.0437 - val_accuracy: 0.3223\n",
      "Epoch 140/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0304 - accuracy: 0.2239 - val_loss: 0.0436 - val_accuracy: 0.3193\n",
      "Epoch 141/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0303 - accuracy: 0.2249 - val_loss: 0.0428 - val_accuracy: 0.3091\n",
      "Epoch 142/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0303 - accuracy: 0.2245 - val_loss: 0.0434 - val_accuracy: 0.3214\n",
      "Epoch 143/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0302 - accuracy: 0.2251 - val_loss: 0.0430 - val_accuracy: 0.3026\n",
      "Epoch 144/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0302 - accuracy: 0.2258 - val_loss: 0.0432 - val_accuracy: 0.3298\n",
      "Epoch 145/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0302 - accuracy: 0.2248 - val_loss: 0.0426 - val_accuracy: 0.3106\n",
      "Epoch 146/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0301 - accuracy: 0.2252 - val_loss: 0.0431 - val_accuracy: 0.3093\n",
      "Epoch 147/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0301 - accuracy: 0.2246 - val_loss: 0.0432 - val_accuracy: 0.3068\n",
      "Epoch 148/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0300 - accuracy: 0.2247 - val_loss: 0.0431 - val_accuracy: 0.2923\n",
      "Epoch 149/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0300 - accuracy: 0.2247 - val_loss: 0.0427 - val_accuracy: 0.2921\n",
      "Epoch 150/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0300 - accuracy: 0.2262 - val_loss: 0.0427 - val_accuracy: 0.3131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-1a1788afa7e3>:140: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "  tprs.append(interp(mean_fpr, fpr, tpr))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Epoch 1/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.1619 - accuracy: 0.0449 - val_loss: 0.1875 - val_accuracy: 0.1129\n",
      "Epoch 2/150\n",
      "7941/7941 [==============================] - 12s 2ms/step - loss: 0.1416 - accuracy: 0.1318 - val_loss: 0.1486 - val_accuracy: 0.2709\n",
      "Epoch 3/150\n",
      "7941/7941 [==============================] - 12s 2ms/step - loss: 0.1094 - accuracy: 0.1974 - val_loss: 0.1191 - val_accuracy: 0.2981\n",
      "Epoch 4/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0914 - accuracy: 0.2187 - val_loss: 0.1050 - val_accuracy: 0.3130\n",
      "Epoch 5/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0803 - accuracy: 0.2263 - val_loss: 0.0954 - val_accuracy: 0.3251\n",
      "Epoch 6/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0732 - accuracy: 0.2286 - val_loss: 0.0896 - val_accuracy: 0.3311\n",
      "Epoch 7/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0678 - accuracy: 0.2296 - val_loss: 0.0842 - val_accuracy: 0.3270\n",
      "Epoch 8/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0637 - accuracy: 0.2294 - val_loss: 0.0812 - val_accuracy: 0.3347\n",
      "Epoch 9/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0605 - accuracy: 0.2288 - val_loss: 0.0767 - val_accuracy: 0.3351\n",
      "Epoch 10/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0579 - accuracy: 0.2288 - val_loss: 0.0748 - val_accuracy: 0.3441\n",
      "Epoch 11/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0558 - accuracy: 0.2283 - val_loss: 0.0723 - val_accuracy: 0.3308\n",
      "Epoch 12/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0539 - accuracy: 0.2270 - val_loss: 0.0694 - val_accuracy: 0.3341\n",
      "Epoch 13/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0523 - accuracy: 0.2262 - val_loss: 0.0679 - val_accuracy: 0.3276\n",
      "Epoch 14/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0510 - accuracy: 0.2253 - val_loss: 0.0666 - val_accuracy: 0.3316\n",
      "Epoch 15/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0498 - accuracy: 0.2256 - val_loss: 0.0656 - val_accuracy: 0.3365\n",
      "Epoch 16/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0488 - accuracy: 0.2244 - val_loss: 0.0637 - val_accuracy: 0.3326\n",
      "Epoch 17/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0479 - accuracy: 0.2239 - val_loss: 0.0622 - val_accuracy: 0.3270\n",
      "Epoch 18/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0471 - accuracy: 0.2233 - val_loss: 0.0613 - val_accuracy: 0.3288\n",
      "Epoch 19/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0464 - accuracy: 0.2236 - val_loss: 0.0608 - val_accuracy: 0.3297\n",
      "Epoch 20/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0458 - accuracy: 0.2236 - val_loss: 0.0596 - val_accuracy: 0.3205\n",
      "Epoch 21/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0452 - accuracy: 0.2238 - val_loss: 0.0589 - val_accuracy: 0.3257\n",
      "Epoch 22/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0447 - accuracy: 0.2228 - val_loss: 0.0584 - val_accuracy: 0.3110\n",
      "Epoch 23/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0442 - accuracy: 0.2227 - val_loss: 0.0577 - val_accuracy: 0.3219\n",
      "Epoch 24/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0437 - accuracy: 0.2226 - val_loss: 0.0576 - val_accuracy: 0.3212\n",
      "Epoch 25/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0433 - accuracy: 0.2233 - val_loss: 0.0569 - val_accuracy: 0.3170\n",
      "Epoch 26/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0429 - accuracy: 0.2243 - val_loss: 0.0563 - val_accuracy: 0.3328\n",
      "Epoch 27/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0425 - accuracy: 0.2237 - val_loss: 0.0563 - val_accuracy: 0.2907\n",
      "Epoch 28/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0422 - accuracy: 0.2235 - val_loss: 0.0558 - val_accuracy: 0.3140\n",
      "Epoch 29/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0418 - accuracy: 0.2236 - val_loss: 0.0551 - val_accuracy: 0.2950\n",
      "Epoch 30/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0415 - accuracy: 0.2239 - val_loss: 0.0542 - val_accuracy: 0.3242\n",
      "Epoch 31/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0412 - accuracy: 0.2241 - val_loss: 0.0539 - val_accuracy: 0.3063\n",
      "Epoch 32/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0409 - accuracy: 0.2240 - val_loss: 0.0532 - val_accuracy: 0.3042\n",
      "Epoch 33/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0406 - accuracy: 0.2239 - val_loss: 0.0538 - val_accuracy: 0.3210\n",
      "Epoch 34/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0403 - accuracy: 0.2239 - val_loss: 0.0531 - val_accuracy: 0.3079\n",
      "Epoch 35/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0400 - accuracy: 0.2244 - val_loss: 0.0526 - val_accuracy: 0.3047\n",
      "Epoch 36/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0397 - accuracy: 0.2248 - val_loss: 0.0519 - val_accuracy: 0.3088\n",
      "Epoch 37/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0395 - accuracy: 0.2240 - val_loss: 0.0521 - val_accuracy: 0.3055\n",
      "Epoch 38/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0393 - accuracy: 0.2243 - val_loss: 0.0514 - val_accuracy: 0.3127\n",
      "Epoch 39/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0390 - accuracy: 0.2246 - val_loss: 0.0515 - val_accuracy: 0.3162\n",
      "Epoch 40/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0388 - accuracy: 0.2248 - val_loss: 0.0508 - val_accuracy: 0.2949\n",
      "Epoch 41/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0386 - accuracy: 0.2241 - val_loss: 0.0512 - val_accuracy: 0.3159\n",
      "Epoch 42/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0384 - accuracy: 0.2242 - val_loss: 0.0514 - val_accuracy: 0.3016\n",
      "Epoch 43/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0382 - accuracy: 0.2239 - val_loss: 0.0507 - val_accuracy: 0.3171\n",
      "Epoch 44/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0380 - accuracy: 0.2242 - val_loss: 0.0504 - val_accuracy: 0.3021\n",
      "Epoch 45/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0378 - accuracy: 0.2239 - val_loss: 0.0504 - val_accuracy: 0.3050\n",
      "Epoch 46/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0376 - accuracy: 0.2236 - val_loss: 0.0509 - val_accuracy: 0.3140\n",
      "Epoch 47/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0375 - accuracy: 0.2231 - val_loss: 0.0494 - val_accuracy: 0.2990\n",
      "Epoch 48/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0373 - accuracy: 0.2232 - val_loss: 0.0498 - val_accuracy: 0.3085\n",
      "Epoch 49/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0371 - accuracy: 0.2233 - val_loss: 0.0499 - val_accuracy: 0.3136\n",
      "Epoch 50/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0369 - accuracy: 0.2231 - val_loss: 0.0496 - val_accuracy: 0.3072\n",
      "Epoch 51/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0368 - accuracy: 0.2226 - val_loss: 0.0491 - val_accuracy: 0.3185\n",
      "Epoch 52/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0366 - accuracy: 0.2224 - val_loss: 0.0490 - val_accuracy: 0.2888\n",
      "Epoch 53/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0365 - accuracy: 0.2215 - val_loss: 0.0484 - val_accuracy: 0.2944\n",
      "Epoch 54/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0364 - accuracy: 0.2214 - val_loss: 0.0484 - val_accuracy: 0.3050\n",
      "Epoch 55/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0362 - accuracy: 0.2213 - val_loss: 0.0484 - val_accuracy: 0.2935\n",
      "Epoch 56/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0361 - accuracy: 0.2203 - val_loss: 0.0486 - val_accuracy: 0.3042\n",
      "Epoch 57/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0360 - accuracy: 0.2208 - val_loss: 0.0480 - val_accuracy: 0.2968\n",
      "Epoch 58/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0358 - accuracy: 0.2203 - val_loss: 0.0479 - val_accuracy: 0.3051\n",
      "Epoch 59/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0357 - accuracy: 0.2202 - val_loss: 0.0483 - val_accuracy: 0.3042\n",
      "Epoch 60/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0356 - accuracy: 0.2204 - val_loss: 0.0480 - val_accuracy: 0.3022\n",
      "Epoch 61/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0355 - accuracy: 0.2195 - val_loss: 0.0475 - val_accuracy: 0.3021\n",
      "Epoch 62/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0353 - accuracy: 0.2194 - val_loss: 0.0475 - val_accuracy: 0.3070\n",
      "Epoch 63/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0352 - accuracy: 0.2190 - val_loss: 0.0473 - val_accuracy: 0.3092\n",
      "Epoch 64/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0351 - accuracy: 0.2195 - val_loss: 0.0468 - val_accuracy: 0.2944\n",
      "Epoch 65/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0350 - accuracy: 0.2197 - val_loss: 0.0471 - val_accuracy: 0.2997\n",
      "Epoch 66/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0349 - accuracy: 0.2182 - val_loss: 0.0468 - val_accuracy: 0.2928\n",
      "Epoch 67/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0348 - accuracy: 0.2194 - val_loss: 0.0468 - val_accuracy: 0.2862\n",
      "Epoch 68/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0347 - accuracy: 0.2184 - val_loss: 0.0467 - val_accuracy: 0.2968\n",
      "Epoch 69/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0346 - accuracy: 0.2183 - val_loss: 0.0471 - val_accuracy: 0.3013\n",
      "Epoch 70/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0345 - accuracy: 0.2182 - val_loss: 0.0462 - val_accuracy: 0.2896\n",
      "Epoch 71/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0344 - accuracy: 0.2179 - val_loss: 0.0462 - val_accuracy: 0.3000\n",
      "Epoch 72/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0343 - accuracy: 0.2182 - val_loss: 0.0475 - val_accuracy: 0.3039\n",
      "Epoch 73/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0342 - accuracy: 0.2186 - val_loss: 0.0464 - val_accuracy: 0.2885\n",
      "Epoch 74/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0341 - accuracy: 0.2174 - val_loss: 0.0464 - val_accuracy: 0.3102\n",
      "Epoch 75/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0340 - accuracy: 0.2177 - val_loss: 0.0466 - val_accuracy: 0.2924\n",
      "Epoch 76/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0340 - accuracy: 0.2179 - val_loss: 0.0462 - val_accuracy: 0.2968\n",
      "Epoch 77/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0339 - accuracy: 0.2176 - val_loss: 0.0462 - val_accuracy: 0.3034\n",
      "Epoch 78/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0338 - accuracy: 0.2177 - val_loss: 0.0462 - val_accuracy: 0.2972\n",
      "Epoch 79/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0337 - accuracy: 0.2171 - val_loss: 0.0461 - val_accuracy: 0.3074\n",
      "Epoch 80/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0336 - accuracy: 0.2175 - val_loss: 0.0462 - val_accuracy: 0.3052\n",
      "Epoch 81/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0335 - accuracy: 0.2170 - val_loss: 0.0458 - val_accuracy: 0.2901\n",
      "Epoch 82/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0335 - accuracy: 0.2175 - val_loss: 0.0454 - val_accuracy: 0.2944\n",
      "Epoch 83/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0334 - accuracy: 0.2168 - val_loss: 0.0456 - val_accuracy: 0.2921\n",
      "Epoch 84/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0333 - accuracy: 0.2169 - val_loss: 0.0458 - val_accuracy: 0.2949\n",
      "Epoch 85/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0332 - accuracy: 0.2177 - val_loss: 0.0457 - val_accuracy: 0.2824\n",
      "Epoch 86/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0332 - accuracy: 0.2169 - val_loss: 0.0456 - val_accuracy: 0.2833\n",
      "Epoch 87/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0331 - accuracy: 0.2166 - val_loss: 0.0455 - val_accuracy: 0.3013\n",
      "Epoch 88/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0330 - accuracy: 0.2169 - val_loss: 0.0454 - val_accuracy: 0.3071\n",
      "Epoch 89/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0329 - accuracy: 0.2175 - val_loss: 0.0456 - val_accuracy: 0.3068\n",
      "Epoch 90/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0329 - accuracy: 0.2171 - val_loss: 0.0448 - val_accuracy: 0.3004\n",
      "Epoch 91/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0328 - accuracy: 0.2170 - val_loss: 0.0453 - val_accuracy: 0.2940\n",
      "Epoch 92/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0327 - accuracy: 0.2169 - val_loss: 0.0451 - val_accuracy: 0.3032\n",
      "Epoch 93/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0327 - accuracy: 0.2168 - val_loss: 0.0451 - val_accuracy: 0.3027\n",
      "Epoch 94/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0326 - accuracy: 0.2179 - val_loss: 0.0449 - val_accuracy: 0.3001\n",
      "Epoch 95/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0325 - accuracy: 0.2175 - val_loss: 0.0451 - val_accuracy: 0.2967\n",
      "Epoch 96/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0325 - accuracy: 0.2175 - val_loss: 0.0445 - val_accuracy: 0.2915\n",
      "Epoch 97/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0324 - accuracy: 0.2177 - val_loss: 0.0449 - val_accuracy: 0.3042\n",
      "Epoch 98/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0323 - accuracy: 0.2169 - val_loss: 0.0439 - val_accuracy: 0.2896\n",
      "Epoch 99/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0323 - accuracy: 0.2171 - val_loss: 0.0445 - val_accuracy: 0.3000\n",
      "Epoch 100/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0322 - accuracy: 0.2177 - val_loss: 0.0446 - val_accuracy: 0.2858\n",
      "Epoch 101/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0322 - accuracy: 0.2174 - val_loss: 0.0445 - val_accuracy: 0.2827\n",
      "Epoch 102/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0321 - accuracy: 0.2176 - val_loss: 0.0442 - val_accuracy: 0.2932\n",
      "Epoch 103/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0320 - accuracy: 0.2182 - val_loss: 0.0445 - val_accuracy: 0.2934\n",
      "Epoch 104/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0320 - accuracy: 0.2166 - val_loss: 0.0445 - val_accuracy: 0.3048\n",
      "Epoch 105/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0319 - accuracy: 0.2183 - val_loss: 0.0438 - val_accuracy: 0.2908\n",
      "Epoch 106/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0319 - accuracy: 0.2174 - val_loss: 0.0445 - val_accuracy: 0.3005\n",
      "Epoch 107/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0318 - accuracy: 0.2177 - val_loss: 0.0451 - val_accuracy: 0.3016\n",
      "Epoch 108/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0318 - accuracy: 0.2180 - val_loss: 0.0433 - val_accuracy: 0.2781\n",
      "Epoch 109/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0317 - accuracy: 0.2177 - val_loss: 0.0437 - val_accuracy: 0.2868\n",
      "Epoch 110/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0316 - accuracy: 0.2170 - val_loss: 0.0441 - val_accuracy: 0.3001\n",
      "Epoch 111/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0316 - accuracy: 0.2181 - val_loss: 0.0444 - val_accuracy: 0.2990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0315 - accuracy: 0.2176 - val_loss: 0.0441 - val_accuracy: 0.2889\n",
      "Epoch 113/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0315 - accuracy: 0.2169 - val_loss: 0.0438 - val_accuracy: 0.3186\n",
      "Epoch 114/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0314 - accuracy: 0.2181 - val_loss: 0.0439 - val_accuracy: 0.2840\n",
      "Epoch 115/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0314 - accuracy: 0.2178 - val_loss: 0.0441 - val_accuracy: 0.2885\n",
      "Epoch 116/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0313 - accuracy: 0.2184 - val_loss: 0.0444 - val_accuracy: 0.3072\n",
      "Epoch 117/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0313 - accuracy: 0.2176 - val_loss: 0.0435 - val_accuracy: 0.2974\n",
      "Epoch 118/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0312 - accuracy: 0.2188 - val_loss: 0.0439 - val_accuracy: 0.3125\n",
      "Epoch 119/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0312 - accuracy: 0.2180 - val_loss: 0.0442 - val_accuracy: 0.3200\n",
      "Epoch 120/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0311 - accuracy: 0.2189 - val_loss: 0.0433 - val_accuracy: 0.2928\n",
      "Epoch 121/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0311 - accuracy: 0.2195 - val_loss: 0.0443 - val_accuracy: 0.3182\n",
      "Epoch 122/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0310 - accuracy: 0.2189 - val_loss: 0.0435 - val_accuracy: 0.3117\n",
      "Epoch 123/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0310 - accuracy: 0.2186 - val_loss: 0.0426 - val_accuracy: 0.2947\n",
      "Epoch 124/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0309 - accuracy: 0.2194 - val_loss: 0.0436 - val_accuracy: 0.2943\n",
      "Epoch 125/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0309 - accuracy: 0.2189 - val_loss: 0.0432 - val_accuracy: 0.2947\n",
      "Epoch 126/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0308 - accuracy: 0.2187 - val_loss: 0.0436 - val_accuracy: 0.3046\n",
      "Epoch 127/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0308 - accuracy: 0.2194 - val_loss: 0.0435 - val_accuracy: 0.2954\n",
      "Epoch 128/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0308 - accuracy: 0.2189 - val_loss: 0.0431 - val_accuracy: 0.3083\n",
      "Epoch 129/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0307 - accuracy: 0.2195 - val_loss: 0.0433 - val_accuracy: 0.3030\n",
      "Epoch 130/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0307 - accuracy: 0.2195 - val_loss: 0.0432 - val_accuracy: 0.3137\n",
      "Epoch 131/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0306 - accuracy: 0.2207 - val_loss: 0.0431 - val_accuracy: 0.3027\n",
      "Epoch 132/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0306 - accuracy: 0.2199 - val_loss: 0.0429 - val_accuracy: 0.2978\n",
      "Epoch 133/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0305 - accuracy: 0.2205 - val_loss: 0.0431 - val_accuracy: 0.2882\n",
      "Epoch 134/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0305 - accuracy: 0.2201 - val_loss: 0.0433 - val_accuracy: 0.3107\n",
      "Epoch 135/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0304 - accuracy: 0.2203 - val_loss: 0.0430 - val_accuracy: 0.2910\n",
      "Epoch 136/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0304 - accuracy: 0.2201 - val_loss: 0.0433 - val_accuracy: 0.3033\n",
      "Epoch 137/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0304 - accuracy: 0.2198 - val_loss: 0.0425 - val_accuracy: 0.2915\n",
      "Epoch 138/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0303 - accuracy: 0.2201 - val_loss: 0.0428 - val_accuracy: 0.3016\n",
      "Epoch 139/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0303 - accuracy: 0.2204 - val_loss: 0.0431 - val_accuracy: 0.2962\n",
      "Epoch 140/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0302 - accuracy: 0.2202 - val_loss: 0.0430 - val_accuracy: 0.3072\n",
      "Epoch 141/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0302 - accuracy: 0.2203 - val_loss: 0.0429 - val_accuracy: 0.3157\n",
      "Epoch 142/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0301 - accuracy: 0.2208 - val_loss: 0.0425 - val_accuracy: 0.3029\n",
      "Epoch 143/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0301 - accuracy: 0.2209 - val_loss: 0.0434 - val_accuracy: 0.3129\n",
      "Epoch 144/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0301 - accuracy: 0.2215 - val_loss: 0.0424 - val_accuracy: 0.3002\n",
      "Epoch 145/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0300 - accuracy: 0.2209 - val_loss: 0.0423 - val_accuracy: 0.2855\n",
      "Epoch 146/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0300 - accuracy: 0.2210 - val_loss: 0.0421 - val_accuracy: 0.2975\n",
      "Epoch 147/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0300 - accuracy: 0.2210 - val_loss: 0.0424 - val_accuracy: 0.2981\n",
      "Epoch 148/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0299 - accuracy: 0.2212 - val_loss: 0.0421 - val_accuracy: 0.2984\n",
      "Epoch 149/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0299 - accuracy: 0.2216 - val_loss: 0.0424 - val_accuracy: 0.2905\n",
      "Epoch 150/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0298 - accuracy: 0.2207 - val_loss: 0.0430 - val_accuracy: 0.3107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-1a1788afa7e3>:140: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "  tprs.append(interp(mean_fpr, fpr, tpr))\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20000, 0]\n",
      "[0, 0]\n",
      "both b and c are zero\n",
      "[20000, 0]\n",
      "[0, 0]\n",
      "both b and c are zero\n",
      "[18630, 519]\n",
      "[738, 113]\n",
      "[18811, 338]\n",
      "[37, 814]\n",
      "[19269, 330]\n",
      "[229, 172]\n",
      "[19072, 527]\n",
      "[133, 268]\n",
      "[19682, 164]\n",
      "[64, 90]\n",
      "[19198, 648]\n",
      "[88, 66]\n",
      "[19755, 128]\n",
      "[51, 66]\n",
      "[19243, 640]\n",
      "[43, 74]\n",
      "[19642, 239]\n",
      "[66, 53]\n",
      "[19271, 610]\n",
      "[38, 81]\n",
      "[19537, 325]\n",
      "[65, 73]\n",
      "[19244, 618]\n",
      "[48, 90]\n",
      "[18297, 1126]\n",
      "[225, 352]\n",
      "[18873, 550]\n",
      "[226, 351]\n",
      "[18542, 919]\n",
      "[271, 268]\n",
      "[18941, 520]\n",
      "[164, 375]\n",
      "[19458, 309]\n",
      "[113, 120]\n",
      "[19109, 658]\n",
      "[98, 135]\n",
      "[19576, 222]\n",
      "[103, 99]\n",
      "[19195, 603]\n",
      "[70, 132]\n",
      "[19653, 196]\n",
      "[53, 98]\n",
      "[19268, 581]\n",
      "[44, 107]\n",
      "[19629, 203]\n",
      "[84, 84]\n",
      "[19212, 620]\n",
      "[45, 123]\n",
      "[19357, 478]\n",
      "[97, 68]\n",
      "[19208, 627]\n",
      "[46, 119]\n",
      "[19101, 392]\n",
      "[111, 396]\n",
      "[19027, 466]\n",
      "[91, 416]\n",
      "[19054, 408]\n",
      "[123, 415]\n",
      "[18982, 480]\n",
      "[101, 437]\n",
      "[19290, 506]\n",
      "[106, 98]\n",
      "[19151, 645]\n",
      "[96, 108]\n",
      "[19611, 232]\n",
      "[68, 89]\n",
      "[19243, 600]\n",
      "[48, 109]\n",
      "[19616, 244]\n",
      "[75, 65]\n",
      "[19270, 590]\n",
      "[64, 76]\n",
      "[19524, 320]\n",
      "[80, 76]\n",
      "[19232, 612]\n",
      "[62, 94]\n",
      "[19409, 367]\n",
      "[141, 83]\n",
      "[19070, 706]\n",
      "[146, 78]\n",
      "[18436, 990]\n",
      "[266, 308]\n",
      "[18910, 516]\n",
      "[200, 374]\n",
      "[18383, 1069]\n",
      "[313, 235]\n",
      "[18966, 486]\n",
      "[127, 421]\n",
      "[19409, 406]\n",
      "[96, 89]\n",
      "[19236, 579]\n",
      "[73, 112]\n",
      "[19571, 274]\n",
      "[81, 74]\n",
      "[19167, 678]\n",
      "[82, 73]\n",
      "[19637, 231]\n",
      "[64, 68]\n",
      "[19212, 656]\n",
      "[60, 72]\n",
      "[19618, 241]\n",
      "[64, 77]\n",
      "[19253, 606]\n",
      "[43, 98]\n",
      "[19357, 457]\n",
      "[103, 83]\n",
      "[19142, 672]\n",
      "[88, 98]\n",
      "[18868, 521]\n",
      "[337, 274]\n",
      "[18871, 518]\n",
      "[270, 341]\n",
      "[18874, 554]\n",
      "[282, 290]\n",
      "[18860, 568]\n",
      "[233, 339]\n",
      "[19317, 475]\n",
      "[119, 89]\n",
      "[19178, 614]\n",
      "[59, 149]\n",
      "[19629, 223]\n",
      "[70, 78]\n",
      "[19201, 651]\n",
      "[45, 103]\n",
      "[19602, 255]\n",
      "[74, 69]\n",
      "[19234, 623]\n",
      "[39, 104]\n",
      "[19521, 329]\n",
      "[88, 62]\n",
      "[19226, 624]\n",
      "[55, 95]\n",
      "[19481, 342]\n",
      "[75, 102]\n",
      "[19170, 653]\n",
      "[51, 126]\n",
      "[18512, 924]\n",
      "[302, 262]\n",
      "[18944, 492]\n",
      "[117, 447]\n",
      "[18557, 872]\n",
      "[293, 278]\n",
      "[18910, 519]\n",
      "[176, 395]\n",
      "[19478, 350]\n",
      "[95, 77]\n",
      "[19227, 601]\n",
      "[48, 124]\n",
      "[19603, 269]\n",
      "[79, 49]\n",
      "[19264, 608]\n",
      "[32, 96]\n",
      "[19668, 193]\n",
      "[47, 92]\n",
      "[19246, 615]\n",
      "[60, 79]\n",
      "[19598, 231]\n",
      "[85, 86]\n",
      "[19202, 627]\n",
      "[55, 116]\n",
      "[19514, 322]\n",
      "[77, 87]\n",
      "[19244, 592]\n",
      "[55, 109]\n",
      "[19027, 428]\n",
      "[221, 324]\n",
      "[18994, 461]\n",
      "[168, 377]\n",
      "[19074, 396]\n",
      "[156, 374]\n",
      "[19027, 443]\n",
      "[125, 405]\n",
      "[19671, 201]\n",
      "[61, 67]\n",
      "[19261, 611]\n",
      "[45, 83]\n",
      "[19817, 75]\n",
      "[51, 57]\n",
      "[19260, 632]\n",
      "[58, 50]\n",
      "[19736, 127]\n",
      "[62, 75]\n",
      "[19233, 630]\n",
      "[79, 58]\n",
      "[19774, 105]\n",
      "[41, 80]\n",
      "[19230, 649]\n",
      "[48, 73]\n",
      "[19704, 149]\n",
      "[65, 82]\n",
      "[19220, 633]\n",
      "[37, 110]\n",
      "[19530, 188]\n",
      "[112, 170]\n",
      "[19107, 611]\n",
      "[129, 153]\n",
      "[18254, 1087]\n",
      "[300, 359]\n",
      "[18943, 398]\n",
      "[235, 424]\n",
      "[19155, 255]\n",
      "[34, 556]\n",
      "[19007, 403]\n",
      "[48, 542]\n",
      "[18478, 975]\n",
      "[227, 320]\n",
      "[18953, 500]\n",
      "[176, 371]\n",
      "[19049, 400]\n",
      "[110, 441]\n",
      "[19007, 442]\n",
      "[101, 450]\n",
      "[18483, 1003]\n",
      "[261, 253]\n",
      "[19013, 473]\n",
      "[124, 390]\n",
      "[19133, 339]\n",
      "[43, 485]\n",
      "[19062, 410]\n",
      "[59, 469]\n",
      "[19553, 159]\n",
      "[122, 166]\n",
      "[19203, 509]\n",
      "[102, 186]\n",
      "[19556, 306]\n",
      "[73, 65]\n",
      "[19233, 629]\n",
      "[81, 57]\n",
      "[19266, 472]\n",
      "[135, 127]\n",
      "[19179, 559]\n",
      "[73, 189]\n",
      "[19478, 329]\n",
      "[99, 94]\n",
      "[19203, 604]\n",
      "[65, 128]\n",
      "[19355, 428]\n",
      "[105, 112]\n",
      "[19209, 574]\n",
      "[49, 168]\n",
      "[19404, 384]\n",
      "[115, 97]\n",
      "[19182, 606]\n",
      "[42, 170]\n",
      "[19371, 417]\n",
      "[124, 88]\n",
      "[19210, 578]\n",
      "[33, 179]\n",
      "[19700, 132]\n",
      "[80, 88]\n",
      "[19210, 622]\n",
      "[54, 114]\n",
      "[19680, 191]\n",
      "[67, 62]\n",
      "[19242, 629]\n",
      "[46, 83]\n",
      "[19571, 238]\n",
      "[88, 103]\n",
      "[19180, 629]\n",
      "[43, 148]\n",
      "[19444, 376]\n",
      "[100, 80]\n",
      "[19266, 554]\n",
      "[51, 129]\n",
      "[19501, 317]\n",
      "[94, 88]\n",
      "[19250, 568]\n",
      "[55, 127]\n",
      "[19518, 272]\n",
      "[121, 89]\n",
      "[19190, 600]\n",
      "[90, 120]\n",
      "[19645, 154]\n",
      "[83, 118]\n",
      "[19158, 641]\n",
      "[85, 116]\n",
      "[19728, 148]\n",
      "[66, 58]\n",
      "[19313, 563]\n",
      "[45, 79]\n",
      "[19671, 199]\n",
      "[71, 59]\n",
      "[19266, 604]\n",
      "[37, 93]\n",
      "[19562, 249]\n",
      "[85, 104]\n",
      "[19227, 584]\n",
      "[44, 145]\n",
      "[19591, 230]\n",
      "[78, 101]\n",
      "[19214, 607]\n",
      "[64, 115]\n",
      "[19607, 231]\n",
      "[81, 81]\n",
      "[19257, 581]\n",
      "[68, 94]\n",
      "[19659, 165]\n",
      "[103, 73]\n",
      "[19215, 609]\n",
      "[74, 102]\n",
      "[19648, 189]\n",
      "[85, 78]\n",
      "[19203, 634]\n",
      "[68, 95]\n",
      "[19784, 75]\n",
      "[69, 72]\n",
      "[19291, 568]\n",
      "[50, 91]\n",
      "[19671, 191]\n",
      "[87, 51]\n",
      "[19283, 579]\n",
      "[34, 104]\n",
      "[19591, 232]\n",
      "[86, 91]\n",
      "[19247, 576]\n",
      "[36, 141]\n",
      "[19550, 266]\n",
      "[108, 76]\n",
      "[19242, 574]\n",
      "[37, 147]\n",
      "[19493, 306]\n",
      "[106, 95]\n",
      "[19184, 615]\n",
      "[68, 133]\n",
      "[19568, 254]\n",
      "[90, 88]\n",
      "[19226, 596]\n",
      "[65, 113]\n",
      "[19625, 213]\n",
      "[85, 77]\n",
      "[19232, 606]\n",
      "[39, 123]\n",
      "[19771, 93]\n",
      "[85, 51]\n",
      "[19263, 601]\n",
      "[41, 95]\n",
      "[19483, 385]\n",
      "[80, 52]\n",
      "[19215, 653]\n",
      "[54, 78]\n",
      "[19578, 240]\n",
      "[86, 96]\n",
      "[19268, 550]\n",
      "[48, 134]\n",
      "[19353, 438]\n",
      "[109, 100]\n",
      "[19242, 549]\n",
      "[58, 151]\n",
      "[19463, 351]\n",
      "[99, 87]\n",
      "[19213, 601]\n",
      "[64, 122]\n",
      "[19354, 453]\n",
      "[115, 78]\n",
      "[19208, 599]\n",
      "[49, 144]\n",
      "[19506, 305]\n",
      "[92, 97]\n",
      "[19221, 590]\n",
      "[69, 120]\n",
      "[19678, 173]\n",
      "[77, 72]\n",
      "[19293, 558]\n",
      "[37, 112]\n",
      "[19273, 432]\n",
      "[173, 122]\n",
      "[19259, 446]\n",
      "[65, 230]\n",
      "[18520, 969]\n",
      "[290, 221]\n",
      "[19022, 467]\n",
      "[86, 425]\n",
      "[19134, 362]\n",
      "[79, 425]\n",
      "[19082, 414]\n",
      "[80, 424]\n",
      "[18467, 934]\n",
      "[287, 312]\n",
      "[18921, 480]\n",
      "[188, 411]\n",
      "[19079, 353]\n",
      "[86, 482]\n",
      "[19005, 427]\n",
      "[82, 486]\n",
      "[18534, 913]\n",
      "[326, 227]\n",
      "[18999, 448]\n",
      "[128, 425]\n",
      "[19071, 343]\n",
      "[141, 445]\n",
      "[18998, 416]\n",
      "[120, 466]\n",
      "------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds of MWPM:\n",
      "> Accuracy: 0.9740240740740888 (+- 3.447768702163095e-05)\n",
      "> F1: 0.6987362438110836(+- 0.0006966427848766909)\n",
      "> Time: 2130.54912663333 (+- 28.87378933361273)\n",
      "Average scores for all folds of PLUT:\n",
      "> Accuracy: 0.9620978645122641 (+- 1.8245896618399287e-05)\n",
      "> F1: 0.21589124360698222(+- 9.335238689092155e-05)\n",
      "> Time: 3.5051154666666666 (+- 0.022936989822894247)\n",
      "Average scores for all folds of NN:\n",
      "> Accuracy: 0.490395938517907 (+- 0.3506857778787787)\n",
      "> F1: 0.8202249687718891(+- 0.0009415129894640702)\n",
      "> Time: 2.8170823333333335 (+- 0.08801259313061718)\n",
      "> AUC for class : 0.9911802210257864 (+- 0.0)\n",
      "X^2 for MWPM and NN: 0\n",
      "X^2 for PLUT and NN: 0\n",
      "> AUC for class X00: 0.8774827733806959 (+- 0.003705540801808864)\n",
      "X^2 for MWPM and NN: 38.50437549721559\n",
      "X^2 for PLUT and NN: 240.0\n",
      "> AUC for class X01: 0.9863141864038659 (+- 0.0013923190414103488)\n",
      "X^2 for MWPM and NN: 17.88908765652952\n",
      "X^2 for PLUT and NN: 234.01363636363635\n",
      "> AUC for class X02: 0.998202627813443 (+- 0.0001126424073676256)\n",
      "X^2 for MWPM and NN: 42.98684210526316\n",
      "X^2 for PLUT and NN: 424.5665760869565\n",
      "> AUC for class X03: 0.998611722150729 (+- 2.3820055900544046e-05)\n",
      "X^2 for MWPM and NN: 32.26815642458101\n",
      "X^2 for PLUT and NN: 520.0819912152269\n",
      "> AUC for class X04: 0.998751832445945 (+- 0.00017124731330597724)\n",
      "X^2 for MWPM and NN: 96.99672131147541\n",
      "X^2 for PLUT and NN: 503.1496913580247\n",
      "> AUC for class X05: 0.9982927402893323 (+- 7.028158556914539e-05)\n",
      "X^2 for MWPM and NN: 172.0025641025641\n",
      "X^2 for PLUT and NN: 486.12762762762765\n",
      "> AUC for class X06: 0.9806230104015855 (+- 0.0003079704263952614)\n",
      "X^2 for MWPM and NN: 599.5558845299778\n",
      "X^2 for PLUT and NN: 134.44458762886597\n",
      "> AUC for class X10: 0.9795176075375851 (+- 0.00030131226053562)\n",
      "X^2 for MWPM and NN: 351.77226890756305\n",
      "X^2 for PLUT and NN: 184.24707602339183\n",
      "> AUC for class X11: 0.9956219133630166 (+- 0.00014973442455243132)\n",
      "X^2 for MWPM and NN: 90.10663507109005\n",
      "X^2 for PLUT and NN: 413.3346560846561\n",
      "> AUC for class X12: 0.9970042914212579 (+- 0.0002092421307010517)\n",
      "X^2 for MWPM and NN: 42.84307692307692\n",
      "X^2 for PLUT and NN: 420.5408618127786\n",
      "> AUC for class X13: 0.9979382728820717 (+- 7.064161040965837e-05)\n",
      "X^2 for MWPM and NN: 80.97991967871486\n",
      "X^2 for PLUT and NN: 459.6736\n",
      "> AUC for class X14: 0.9977896573775924 (+- 4.21805668062142e-05)\n",
      "X^2 for MWPM and NN: 48.51567944250871\n",
      "X^2 for PLUT and NN: 495.4526315789474\n",
      "> AUC for class X15: 0.9969598570498338 (+- 0.0001448925217843939)\n",
      "X^2 for MWPM and NN: 251.1304347826087\n",
      "X^2 for PLUT and NN: 499.851411589896\n",
      "> AUC for class X16: 0.9797291881256412 (+- 0.0011791116776803969)\n",
      "X^2 for MWPM and NN: 155.8648111332008\n",
      "X^2 for PLUT and NN: 251.12387791741472\n",
      "> AUC for class X20: 0.979357821317162 (+- 0.00024548202980732923)\n",
      "X^2 for MWPM and NN: 151.89453860640302\n",
      "X^2 for PLUT and NN: 245.9277108433735\n",
      "> AUC for class X21: 0.9966516455487561 (+- 0.00033097228133939223)\n",
      "X^2 for MWPM and NN: 260.13235294117646\n",
      "X^2 for PLUT and NN: 405.2685560053981\n",
      "> AUC for class X22: 0.9975172398315815 (+- 0.0003028378471528569)\n",
      "X^2 for MWPM and NN: 88.56333333333333\n",
      "X^2 for PLUT and NN: 468.52006172839504\n",
      "> AUC for class X23: 0.9978572672119671 (+- 0.00010988022120051134)\n",
      "X^2 for MWPM and NN: 88.47648902821317\n",
      "X^2 for PLUT and NN: 421.4449541284404\n",
      "> AUC for class X24: 0.9975686292908076 (+- 8.870402150928219e-05)\n",
      "X^2 for MWPM and NN: 142.8025\n",
      "X^2 for PLUT and NN: 447.1824925816024\n",
      "> AUC for class X25: 0.9968512518869387 (+- 3.974119302731932e-05)\n",
      "X^2 for MWPM and NN: 99.65551181102362\n",
      "X^2 for PLUT and NN: 366.76173708920186\n",
      "> AUC for class X26: 0.9802283683131926 (+- 0.0003945772183405357)\n",
      "X^2 for MWPM and NN: 416.18550955414014\n",
      "X^2 for PLUT and NN: 138.58240223463687\n",
      "> AUC for class X30: 0.9805549331085576 (+- 0.00021655438975399637)\n",
      "X^2 for MWPM and NN: 412.4638205499276\n",
      "X^2 for PLUT and NN: 209.07667210440457\n",
      "> AUC for class X31: 0.9968893619692768 (+- 0.00020076201524521907)\n",
      "X^2 for MWPM and NN: 190.20119521912352\n",
      "X^2 for PLUT and NN: 391.14263803680984\n",
      "> AUC for class X32: 0.9978247879900342 (+- 8.321575425973896e-05)\n",
      "X^2 for MWPM and NN: 103.84225352112676\n",
      "X^2 for PLUT and NN: 465.82236842105266\n",
      "> AUC for class X33: 0.9978577023501066 (+- 0.0001462052694539889)\n",
      "X^2 for MWPM and NN: 93.41016949152542\n",
      "X^2 for PLUT and NN: 494.44832402234636\n",
      "> AUC for class X34: 0.9977662100310852 (+- 0.0001214821994834402)\n",
      "X^2 for MWPM and NN: 101.56065573770492\n",
      "X^2 for PLUT and NN: 486.6625577812018\n",
      "> AUC for class X35: 0.9968440694313911 (+- 0.0001980096513828126)\n",
      "X^2 for MWPM and NN: 222.51607142857142\n",
      "X^2 for PLUT and NN: 447.22236842105264\n",
      "> AUC for class X36: 0.9794758483389382 (+- 0.00037201437910342773)\n",
      "X^2 for MWPM and NN: 39.03146853146853\n",
      "X^2 for PLUT and NN: 77.4225888324873\n",
      "> AUC for class X40: 0.9805516882095163 (+- 0.00022091082949538267)\n",
      "X^2 for MWPM and NN: 87.84808612440192\n",
      "X^2 for PLUT and NN: 139.270911360799\n",
      "> AUC for class X41: 0.9968807502742333 (+- 0.00040804331856033704)\n",
      "X^2 for MWPM and NN: 212.16329966329965\n",
      "X^2 for PLUT and NN: 456.0416047548291\n",
      "> AUC for class X42: 0.9977506657897122 (+- 0.0001460353753530687)\n",
      "X^2 for MWPM and NN: 78.8532423208191\n",
      "X^2 for PLUT and NN: 525.8979885057471\n",
      "> AUC for class X43: 0.9978703824306648 (+- 0.0001438571333933399)\n",
      "X^2 for MWPM and NN: 98.48024316109422\n",
      "X^2 for PLUT and NN: 513.4274924471299\n",
      "> AUC for class X44: 0.9978212461259982 (+- 6.236326343930132e-05)\n",
      "X^2 for MWPM and NN: 138.1294964028777\n",
      "X^2 for PLUT and NN: 475.1458026509573\n",
      "> AUC for class X45: 0.9966874438614474 (+- 0.00029048185328464307)\n",
      "X^2 for MWPM and NN: 169.67865707434052\n",
      "X^2 for PLUT and NN: 513.0696022727273\n",
      "> AUC for class X46: 0.9803449730645056 (+- 0.00019532186497763952)\n",
      "X^2 for MWPM and NN: 314.5522022838499\n",
      "X^2 for PLUT and NN: 229.68144499178982\n",
      "> AUC for class X50: 0.9801934817774871 (+- 0.0005160110584033476)\n",
      "X^2 for MWPM and NN: 286.7673819742489\n",
      "X^2 for PLUT and NN: 168.29352517985612\n",
      "> AUC for class X51: 0.9968481548234482 (+- 0.00020739839110133813)\n",
      "X^2 for MWPM and NN: 144.97977528089888\n",
      "X^2 for PLUT and NN: 469.49768875192603\n",
      "> AUC for class X52: 0.9977173846523378 (+- 0.00011069723844194319)\n",
      "X^2 for MWPM and NN: 102.64655172413794\n",
      "X^2 for PLUT and NN: 516.6015625\n",
      "> AUC for class X53: 0.99779235443984 (+- 0.0001224181602711518)\n",
      "X^2 for MWPM and NN: 87.60416666666667\n",
      "X^2 for PLUT and NN: 454.6903703703704\n",
      "> AUC for class X54: 0.9976616768011629 (+- 6.059709384889179e-05)\n",
      "X^2 for MWPM and NN: 66.53481012658227\n",
      "X^2 for PLUT and NN: 478.0659824046921\n",
      "> AUC for class X55: 0.997362942598282 (+- 0.00013935498352221417)\n",
      "X^2 for MWPM and NN: 149.21303258145363\n",
      "X^2 for PLUT and NN: 444.0432766615147\n",
      "> AUC for class X56: 0.9798954471802372 (+- 0.0004901859330572352)\n",
      "X^2 for MWPM and NN: 65.38674884437596\n",
      "X^2 for PLUT and NN: 135.55484896661366\n",
      "> AUC for class X60: 0.9813082968723393 (+- 0.00028886281262224513)\n",
      "X^2 for MWPM and NN: 103.48007246376811\n",
      "X^2 for PLUT and NN: 176.91725352112675\n",
      "> AUC for class X61: 0.9981784384162088 (+- 0.0001380464351841308)\n",
      "X^2 for MWPM and NN: 73.7442748091603\n",
      "X^2 for PLUT and NN: 486.6234756097561\n",
      "> AUC for class X62: 0.9986600583284965 (+- 0.00016831805938138098)\n",
      "X^2 for MWPM and NN: 4.198412698412699\n",
      "X^2 for PLUT and NN: 475.8391304347826\n",
      "> AUC for class X63: 0.998678481615929 (+- 9.207439344812215e-05)\n",
      "X^2 for MWPM and NN: 21.67195767195767\n",
      "X^2 for PLUT and NN: 426.657263751763\n",
      "> AUC for class X64: 0.9987111771924818 (+- 3.958900999207701e-05)\n",
      "X^2 for MWPM and NN: 27.184931506849313\n",
      "X^2 for PLUT and NN: 516.4992826398852\n",
      "> AUC for class X65: 0.997718336568631 (+- 8.676620060439921e-05)\n",
      "X^2 for MWPM and NN: 32.191588785046726\n",
      "X^2 for PLUT and NN: 528.3955223880597\n",
      "> AUC for class X66: 0.9936306857242284 (+- 0.0001865841256177411)\n",
      "X^2 for MWPM and NN: 18.75\n",
      "X^2 for PLUT and NN: 312.65\n",
      "> AUC for class Z00: 0.9734218571335364 (+- 0.0005062701058509945)\n",
      "X^2 for MWPM and NN: 445.41888968997836\n",
      "X^2 for PLUT and NN: 41.459715639810426\n",
      "> AUC for class Z01: 0.9745877870480538 (+- 0.00023026177667556758)\n",
      "X^2 for MWPM and NN: 167.47404844290656\n",
      "X^2 for PLUT and NN: 277.86252771618626\n",
      "> AUC for class Z02: 0.9769452891802967 (+- 0.0006805630686753409)\n",
      "X^2 for MWPM and NN: 464.2337770382695\n",
      "X^2 for PLUT and NN: 154.3328402366864\n",
      "> AUC for class Z03: 0.9775675764564827 (+- 0.0007640354077926777)\n",
      "X^2 for MWPM and NN: 163.76666666666668\n",
      "X^2 for PLUT and NN: 212.8913443830571\n",
      "> AUC for class Z04: 0.9777084716498256 (+- 0.00026559682121339524)\n",
      "X^2 for MWPM and NN: 434.3995253164557\n",
      "X^2 for PLUT and NN: 202.85427135678393\n",
      "> AUC for class Z05: 0.9774307301727627 (+- 0.0005837478785323237)\n",
      "X^2 for MWPM and NN: 227.81413612565444\n",
      "X^2 for PLUT and NN: 261.1940298507463\n",
      "> AUC for class Z06: 0.9919198376219809 (+- 0.00044985140956473534)\n",
      "X^2 for MWPM and NN: 4.612099644128114\n",
      "X^2 for PLUT and NN: 269.7806873977087\n",
      "> AUC for class Z10: 0.9978109098011504 (+- 0.00029104484454982755)\n",
      "X^2 for MWPM and NN: 142.01583113456465\n",
      "X^2 for PLUT and NN: 421.42112676056337\n",
      "> AUC for class Z11: 0.9951366491124176 (+- 0.00015627747088879136)\n",
      "X^2 for MWPM and NN: 185.99011532125206\n",
      "X^2 for PLUT and NN: 372.1914556962025\n",
      "> AUC for class Z12: 0.995565207529363 (+- 7.652913711792506e-05)\n",
      "X^2 for MWPM and NN: 122.52570093457943\n",
      "X^2 for PLUT and NN: 432.65171898355754\n",
      "> AUC for class Z13: 0.9957129950494569 (+- 4.275046434907151e-05)\n",
      "X^2 for MWPM and NN: 194.52908067542214\n",
      "X^2 for PLUT and NN: 440.7319422150883\n",
      "> AUC for class Z14: 0.9960739179318843 (+- 0.00015203064713624202)\n",
      "X^2 for MWPM and NN: 143.93587174348698\n",
      "X^2 for PLUT and NN: 489.1496913580247\n",
      "> AUC for class Z15: 0.996234425791967 (+- 0.0003372120256971147)\n",
      "X^2 for MWPM and NN: 157.60443622920516\n",
      "X^2 for PLUT and NN: 484.34697217675944\n",
      "> AUC for class Z16: 0.9971633044204807 (+- 0.0002844925744994952)\n",
      "X^2 for MWPM and NN: 12.268867924528301\n",
      "X^2 for PLUT and NN: 475.57544378698225\n",
      "> AUC for class Z20: 0.9983192645975095 (+- 0.00011326801510232799)\n",
      "X^2 for MWPM and NN: 58.63953488372093\n",
      "X^2 for PLUT and NN: 501.81333333333333\n",
      "> AUC for class Z21: 0.9965758720217038 (+- 0.00011015112351288854)\n",
      "X^2 for MWPM and NN: 68.10122699386503\n",
      "X^2 for PLUT and NN: 509.26339285714283\n",
      "> AUC for class Z22: 0.9966548427195588 (+- 4.7205163650435596e-05)\n",
      "X^2 for MWPM and NN: 158.87605042016807\n",
      "X^2 for PLUT and NN: 416.53553719008266\n",
      "> AUC for class Z23: 0.9965659574107475 (+- 0.00045531399598732576)\n",
      "X^2 for MWPM and NN: 119.91240875912409\n",
      "X^2 for PLUT and NN: 420.776886035313\n",
      "> AUC for class Z24: 0.9963863843239298 (+- 0.0002991224906279135)\n",
      "X^2 for MWPM and NN: 57.25190839694657\n",
      "X^2 for PLUT and NN: 375.47971014492754\n",
      "> AUC for class Z25: 0.996672372021793 (+- 0.0002237818300370828)\n",
      "X^2 for MWPM and NN: 20.675105485232066\n",
      "X^2 for PLUT and NN: 424.27685950413223\n",
      "> AUC for class Z26: 0.9980443824030408 (+- 0.00011185457252453694)\n",
      "X^2 for MWPM and NN: 30.6588785046729\n",
      "X^2 for PLUT and NN: 439.6200657894737\n",
      "> AUC for class Z30: 0.9982613629763645 (+- 9.299693764795388e-05)\n",
      "X^2 for MWPM and NN: 59.737037037037034\n",
      "X^2 for PLUT and NN: 499.77535101404055\n",
      "> AUC for class Z31: 0.9969181813588129 (+- 9.310131336458149e-05)\n",
      "X^2 for MWPM and NN: 79.54790419161677\n",
      "X^2 for PLUT and NN: 462.6130573248408\n",
      "> AUC for class Z32: 0.9963564546604284 (+- 0.0003322954280873239)\n",
      "X^2 for MWPM and NN: 74.02922077922078\n",
      "X^2 for PLUT and NN: 437.80029806259313\n",
      "> AUC for class Z33: 0.9966313607222115 (+- 0.00010413640175452734)\n",
      "X^2 for MWPM and NN: 71.15705128205128\n",
      "X^2 for PLUT and NN: 403.91987673343607\n",
      "> AUC for class Z34: 0.9965334604590304 (+- 8.91375509366838e-05)\n",
      "X^2 for MWPM and NN: 13.884328358208956\n",
      "X^2 for PLUT and NN: 417.5051244509517\n",
      "> AUC for class Z35: 0.9971081798083835 (+- 0.00011050063704346869)\n",
      "X^2 for MWPM and NN: 38.71897810218978\n",
      "X^2 for PLUT and NN: 454.73646723646726\n",
      "> AUC for class Z36: 0.998079941301291 (+- 4.0097178656665334e-05)\n",
      "X^2 for MWPM and NN: 0.1736111111111111\n",
      "X^2 for PLUT and NN: 432.5064724919094\n",
      "> AUC for class Z40: 0.998104583579571 (+- 0.00010316700914292098)\n",
      "X^2 for MWPM and NN: 38.16187050359712\n",
      "X^2 for PLUT and NN: 482.76672104404565\n",
      "> AUC for class Z41: 0.9965779212313924 (+- 0.00037213804926374716)\n",
      "X^2 for MWPM and NN: 66.11635220125787\n",
      "X^2 for PLUT and NN: 474.7075163398693\n",
      "> AUC for class Z42: 0.9965685206431889 (+- 0.000339449427206362)\n",
      "X^2 for MWPM and NN: 65.90641711229947\n",
      "X^2 for PLUT and NN: 470.2062193126023\n",
      "> AUC for class Z43: 0.9965508562518822 (+- 0.00011991387652603103)\n",
      "X^2 for MWPM and NN: 96.11893203883496\n",
      "X^2 for PLUT and NN: 436.4802342606149\n",
      "> AUC for class Z44: 0.9962644785600622 (+- 7.55954010842816e-05)\n",
      "X^2 for MWPM and NN: 77.23546511627907\n",
      "X^2 for PLUT and NN: 424.9621785173979\n",
      "> AUC for class Z45: 0.9967595827159554 (+- 3.3768883988293494e-05)\n",
      "X^2 for MWPM and NN: 54.124161073825505\n",
      "X^2 for PLUT and NN: 496.67596899224804\n",
      "> AUC for class Z46: 0.9983456566858252 (+- 3.5258646372158956e-05)\n",
      "X^2 for MWPM and NN: 0.2752808988764045\n",
      "X^2 for PLUT and NN: 486.7305295950156\n",
      "> AUC for class Z50: 0.997016544414547 (+- 0.0003646886638668276)\n",
      "X^2 for MWPM and NN: 198.74408602150538\n",
      "X^2 for PLUT and NN: 505.8048090523338\n",
      "> AUC for class Z51: 0.996101042306805 (+- 0.00025920362002967133)\n",
      "X^2 for MWPM and NN: 71.80674846625767\n",
      "X^2 for PLUT and NN: 419.7341137123746\n",
      "> AUC for class Z52: 0.9958469333391795 (+- 0.00015612445386823068)\n",
      "X^2 for MWPM and NN: 196.6800731261426\n",
      "X^2 for PLUT and NN: 395.5518945634267\n",
      "> AUC for class Z53: 0.9957132280657358 (+- 0.00020281130331587874)\n",
      "X^2 for MWPM and NN: 140.00222222222223\n",
      "X^2 for PLUT and NN: 432.02406015037593\n",
      "> AUC for class Z54: 0.9958601451231338 (+- 0.00014541510349404095)\n",
      "X^2 for MWPM and NN: 199.94542253521126\n",
      "X^2 for PLUT and NN: 465.125\n",
      "> AUC for class Z55: 0.9956124504831507 (+- 0.0002556480296945368)\n",
      "X^2 for MWPM and NN: 113.20906801007557\n",
      "X^2 for PLUT and NN: 410.3186646433991\n",
      "> AUC for class Z56: 0.9976799593836697 (+- 0.00012313834437542196)\n",
      "X^2 for MWPM and NN: 36.1\n",
      "X^2 for PLUT and NN: 454.453781512605\n",
      "> AUC for class Z60: 0.991924129572585 (+- 0.0006863129824408266)\n",
      "X^2 for MWPM and NN: 110.02314049586776\n",
      "X^2 for PLUT and NN: 282.5831702544031\n",
      "> AUC for class Z61: 0.9776339226313029 (+- 0.00014698071822325125)\n",
      "X^2 for MWPM and NN: 365.1183478951549\n",
      "X^2 for PLUT and NN: 261.121157323689\n",
      "> AUC for class Z62: 0.9771363236424623 (+- 0.000596693130469904)\n",
      "X^2 for MWPM and NN: 180.3265306122449\n",
      "X^2 for PLUT and NN: 224.47165991902833\n",
      "> AUC for class Z63: 0.9771938734753985 (+- 0.0002041736602518702)\n",
      "X^2 for MWPM and NN: 341.7821457821458\n",
      "X^2 for PLUT and NN: 126.76796407185628\n",
      "> AUC for class Z64: 0.9773273771120087 (+- 0.00022448069685538234)\n",
      "X^2 for MWPM and NN: 161.1753986332574\n",
      "X^2 for PLUT and NN: 232.48722986247543\n",
      "> AUC for class Z65: 0.9768674536040837 (+- 0.0003023855849850142)\n",
      "X^2 for MWPM and NN: 277.1557707828894\n",
      "X^2 for PLUT and NN: 176.66840277777777\n",
      "> AUC for class Z66: 0.9778529287140733 (+- 0.0010139899198560313)\n",
      "X^2 for MWPM and NN: 83.47314049586777\n",
      "X^2 for PLUT and NN: 162.36007462686567\n",
      "###################################################################################\n",
      "TOTAL F1 NN: [0.8201395877498923, 0.821418399316434, 0.8191169192493412]\n",
      "TOTAL F1 PLUT: [0.21600465905950167, 0.21577601491392678, 0.21589305684751825]\n",
      "TOTAL F1 MWPM: [0.6980389857833798, 0.6996876498871094, 0.6984820957627614]\n",
      "TOTAL ACC NN: [0.24212782084941864, 0.2427195906639099, 0.9863404040403925]\n",
      "TOTAL ACC PLUT: [0.9621109258977504, 0.9620720615785512, 0.9621106060604908]\n",
      "TOTAL ACC MWPM: [0.9739863636363807, 0.9740696969697087, 0.9740161616161771]\n",
      "TOTAL TIME NN: [2.9411304, 2.7462049, 2.7639117]\n",
      "TOTAL TIME PLUT: [3.5267561, 3.5152217, 3.4733686]\n",
      "TOTAL TIME MWPM: [2171.313826099998, 2112.2217765999876, 2108.1117772000052]\n",
      "------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "kfold = KFold(n_splits=3, shuffle=True)\n",
    "inputs = x_d7.copy()\n",
    "inputs = np.array(inputs)\n",
    "inputs = np.stack(inputs)\n",
    "targets = np.array(y_d7)\n",
    "targets = np.array([np.array(xi) for xi in targets])\n",
    "\n",
    "n_classes = 99\n",
    "\n",
    "acc_per_fold = []\n",
    "f1_per_fold = []\n",
    "time_per_fold = []\n",
    "\n",
    "acc_per_fold_plut = []\n",
    "f1_per_fold_plut = []\n",
    "time_per_fold_plut = []\n",
    "\n",
    "acc_per_fold_mwpm = []\n",
    "f1_per_fold_mwpm = []\n",
    "time_per_fold_mwpm = []\n",
    "\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "aucs_classes = {}\n",
    "for i in mlb_d7.classes_:\n",
    "    aucs_classes[i] = []\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "for i, (train, test) in enumerate(kfold.split(inputs, targets)):\n",
    "    i_train = inputs[train].copy()\n",
    "    i_test = inputs[test].copy()\n",
    "    t_test = targets[test].copy()\n",
    "    x_test_d7 = i_test[:,:2]\n",
    "    inputs_train = i_train[:,3:]\n",
    "    inputs_test = i_test[:,3:]\n",
    "    indices = np.random.choice(inputs[test].shape[0], 20000, replace=False)\n",
    "    x_test_d7 = x_test_d7[indices]\n",
    "    inputs_test_2 = inputs_test[indices]\n",
    "    targets_test_2 = targets[test][indices]\n",
    "    ####################################################################################################\n",
    "    #test MWPM decoder for this fold\n",
    "    #labels = targets[train], features = inputs[train]\n",
    "   # x_test_d7 = translate_to_graph(testData_d7_MWPM, targs, mlb_d7)\n",
    "    decoding_d7, time_mwpm = do_new_decoding(x_test_d7, 7, 0)\n",
    "    decoding_d7['combine'] = decoding_d7[[0, 1]].values.tolist()\n",
    "    decoding_d7['combine'].apply(lambda x: x[0].extend(x[1]))\n",
    "    decoding_d7 = np.array(decoding_d7[0])\n",
    "                                              \n",
    "    time_per_fold_mwpm.append(time_mwpm)\n",
    "                                              \n",
    "    pred_mwpm = mlb_d7.transform(decoding_d7)\n",
    "\n",
    "    if fold_no < 3:\n",
    "        acc = partial_accuracy(targets_test_2, pred_mwpm)\n",
    "    else:\n",
    "        acc, contingency_mwpm = partial_accuracy_and_contingency(targets_test_2, pred_mwpm, mlb_d7)\n",
    "        \n",
    "    acc_per_fold_mwpm.append(acc)\n",
    "    f1_per_fold_mwpm.append(f1_score(targets_test_2, pred_mwpm, average='micro'))\n",
    "\n",
    "    #####################################################################################################\n",
    "    #test the plut decoder for this fold\n",
    "    \n",
    "    lookup_d7 = lookup_decoder(7)\n",
    "    \n",
    "    lookup_d7 = train_plut(lookup_d7, inputs_train, targets[train])\n",
    "    \n",
    "    start = time.time_ns()\n",
    "    pred_plut_d7 = test_plut(lookup_d7, inputs_test)\n",
    "    end = time.time_ns() \n",
    "    time_per_fold_plut.append((end - start)/ (10 ** 9))\n",
    "    \n",
    "    if fold_no < 3:\n",
    "        acc = partial_accuracy(targets[test], pred_plut_d7)\n",
    "        f1 = f1_score(targets[test], pred_plut_d7, average='micro')\n",
    "    else:\n",
    "        pred_plut_d7 = test_plut(lookup_d7, inputs_test_2)\n",
    "        f1 = f1_score(targets_test_2, pred_plut_d7, average='micro')\n",
    "        acc, contingency_plut = partial_accuracy_and_contingency(targets_test_2, pred_plut_d7, mlb_d7)\n",
    "        \n",
    "    acc_per_fold_plut.append(acc)\n",
    "    f1_per_fold_plut.append(f1)\n",
    "    \n",
    "    #####################################################################################################\n",
    "    #Test the NN decoder for this fold\n",
    "    \n",
    "    model_d7 = compile_FFNN_model_DepthSeven(7)\n",
    "\n",
    "    # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    inputs_train = np.asarray(inputs_train).astype('int32')\n",
    "    inputs_test = np.asarray(inputs_test).astype('int32')\n",
    "    inputs_test_2 = np.asarray(inputs_test_2).astype('int32')\n",
    "    \n",
    "    # Fit data to model\n",
    "    history = model_d7.fit(\n",
    "        x=inputs_train ,\n",
    "        y=targets[train],\n",
    "        validation_split=.25,\n",
    "        epochs= 150)\n",
    "    \n",
    "   # Generate generalization metrics\n",
    "    scores = model_d7.evaluate(inputs_test, targets[test], verbose=0)\n",
    "    \n",
    "    start = time.time_ns()\n",
    "    predictions_d7 = model_d7.predict(inputs_test)\n",
    "    end = time.time_ns() \n",
    "    time_per_fold.append((end - start)/ (10 ** 9))\n",
    "    \n",
    "    #threshold based on previous tests with train_test_split in hyperparameter tuning\n",
    "    pred=predictions_d7.copy() #change here\n",
    "    pred[pred>=.5]=1 \n",
    "    pred[pred<.5]=0\n",
    "     \n",
    "    if fold_no < 3:\n",
    "        acc = scores[1]\n",
    "        f1 = f1_score(targets[test], pred, average='micro')\n",
    "    else:\n",
    "        pred = model_d7.predict(inputs_test_2)\n",
    "        pred[pred>=.5]=1 \n",
    "        pred[pred<.5]=0\n",
    "        acc, contingency_nn = partial_accuracy_and_contingency(targets_test_2, pred, mlb_d7)\n",
    "        f1 = f1_score(targets_test_2, pred, average='micro')\n",
    " \n",
    "    acc_per_fold.append(acc)\n",
    "    f1_per_fold.append(f1)\n",
    "\n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(targets[test].ravel(), predictions_d7.ravel())\n",
    "    tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "    tprs[-1][0] = 0.0\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "    plt.plot(fpr, tpr, lw=1, alpha=0.3, label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
    "    \n",
    "    #get the AUCs of each class, used to get average AUC of each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(targets[test][:, i], predictions_d7[:, i]) \n",
    "        aucs_classes[mlb_d7.classes_[i]].append(auc(fpr[i], tpr[i]))\n",
    "\n",
    "#########################################################################################################\n",
    "#compute McNemar's statistic on results of last fold\n",
    "\n",
    "mcnemar_results_mwpm = {}\n",
    "mcnemar_results_plut = {}\n",
    "\n",
    "for class_ in mlb_d7.classes_:\n",
    "    \n",
    "    #compute the x^2 for NN and MWPM\n",
    "    mcnemar_results_mwpm[class_]=contingency_table_and_t(contingency_nn[class_], contingency_mwpm[class_])[1]    \n",
    "    #comput the x^2 for NN and PLUT\n",
    "    mcnemar_results_plut[class_] = contingency_table_and_t(contingency_nn[class_], contingency_plut[class_])[1]\n",
    "        \n",
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds of MWPM:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold_mwpm)} (+- {np.std(acc_per_fold_mwpm)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold_mwpm)}(+- {np.std(f1_per_fold_mwpm)})')\n",
    "print(f'> Time: {np.mean(time_per_fold_mwpm)} (+- {np.std(time_per_fold_mwpm)})')\n",
    "print('Average scores for all folds of PLUT:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold_plut)} (+- {np.std(acc_per_fold_plut)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold_plut)}(+- {np.std(f1_per_fold_plut)})')\n",
    "print(f'> Time: {np.mean(time_per_fold_plut)} (+- {np.std(time_per_fold_plut)})')\n",
    "print('Average scores for all folds of NN:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold)}(+- {np.std(f1_per_fold)})')\n",
    "print(f'> Time: {np.mean(time_per_fold)} (+- {np.std(time_per_fold)})')\n",
    "for key in aucs_classes:\n",
    "    cleanedList = [x for x in aucs_classes[key] if str(x) != 'nan']\n",
    "    print(f'> AUC for class {key}: {np.mean(cleanedList)} (+- {np.std(cleanedList)})') #this has to be a for loop\n",
    "    print(\"X^2 for MWPM and NN: \" + str(mcnemar_results_mwpm[key]))  \n",
    "    print(\"X^2 for PLUT and NN: \" + str(mcnemar_results_plut[key]))\n",
    "print(\"###################################################################################\")\n",
    "print(\"TOTAL F1 NN: \" + str(f1_per_fold))\n",
    "print(\"TOTAL F1 PLUT: \" + str(f1_per_fold_plut))\n",
    "print(\"TOTAL F1 MWPM: \" + str(f1_per_fold_mwpm))\n",
    "print(\"TOTAL ACC NN: \" + str(acc_per_fold))\n",
    "print(\"TOTAL ACC PLUT: \" + str(acc_per_fold_plut))\n",
    "print(\"TOTAL ACC MWPM: \" + str(acc_per_fold_mwpm))\n",
    "print(\"TOTAL TIME NN: \" + str(time_per_fold))\n",
    "print(\"TOTAL TIME PLUT: \" + str(time_per_fold_plut))\n",
    "print(\"TOTAL TIME MWPM: \" + str(time_per_fold_mwpm))\n",
    "print('------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "         label='Chance', alpha=.8)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "         label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "         lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                 label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "plt.xlim([-0.01, 1.01])\n",
    "plt.ylim([-0.01, 1.01])\n",
    "plt.xlabel('False Positive Rate',fontsize=18)\n",
    "plt.ylabel('True Positive Rate',fontsize=18)\n",
    "plt.title('Cross-Validation ROC of Depth 7 NN',fontsize=18)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL F1 NN: [0.8201395877498923, 0.821418399316434, 0.8191169192493412]\n",
      "TOTAL F1 PLUT: [0.21600465905950167, 0.21577601491392678, 0.21589305684751825]\n",
      "TOTAL F1 MWPM: [0.6980389857833798, 0.6996876498871094, 0.6984820957627614]\n",
      "TOTAL ACC NN: [0.24212782084941864, 0.2427195906639099, 0.9863404040403925]\n",
      "TOTAL ACC PLUT: [0.9621109258977504, 0.9620720615785512, 0.9621106060604908]\n",
      "TOTAL ACC MWPM: [0.9739863636363807, 0.9740696969697087, 0.9740161616161771]\n",
      "TOTAL TIME NN: [2.9411304, 2.7462049, 2.7639117]\n",
      "TOTAL TIME PLUT: [3.5267561, 3.5152217, 3.4733686]\n",
      "TOTAL TIME MWPM: [2171.313826099998, 2112.2217765999876, 2108.1117772000052]\n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"TOTAL F1 NN: \" + str(f1_per_fold))\n",
    "print(\"TOTAL F1 PLUT: \" + str(f1_per_fold_plut))\n",
    "print(\"TOTAL F1 MWPM: \" + str(f1_per_fold_mwpm))\n",
    "print(\"TOTAL ACC NN: \" + str(acc_per_fold))\n",
    "print(\"TOTAL ACC PLUT: \" + str(acc_per_fold_plut))\n",
    "print(\"TOTAL ACC MWPM: \" + str(acc_per_fold_mwpm))\n",
    "print(\"TOTAL TIME NN: \" + str(time_per_fold))\n",
    "print(\"TOTAL TIME PLUT: \" + str(time_per_fold_plut))\n",
    "print(\"TOTAL TIME MWPM: \" + str(time_per_fold_mwpm))\n",
    "print('-----------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-ce8ecd95ec10>:58: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "  tprs.append(interp(mean_fpr, fpr, tpr))\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20000, 0]\n",
      "[0, 0]\n",
      "both b and c are zero\n",
      "[20000, 0]\n",
      "[0, 0]\n",
      "both b and c are zero\n",
      "[18630, 519]\n",
      "[738, 113]\n",
      "[18811, 338]\n",
      "[37, 814]\n",
      "[19269, 330]\n",
      "[229, 172]\n",
      "[19072, 527]\n",
      "[133, 268]\n",
      "[19682, 164]\n",
      "[64, 90]\n",
      "[19198, 648]\n",
      "[88, 66]\n",
      "[19755, 128]\n",
      "[51, 66]\n",
      "[19243, 640]\n",
      "[43, 74]\n",
      "[19642, 239]\n",
      "[66, 53]\n",
      "[19271, 610]\n",
      "[38, 81]\n",
      "[19537, 325]\n",
      "[65, 73]\n",
      "[19244, 618]\n",
      "[48, 90]\n",
      "[18297, 1126]\n",
      "[225, 352]\n",
      "[18873, 550]\n",
      "[226, 351]\n",
      "[18542, 919]\n",
      "[271, 268]\n",
      "[18941, 520]\n",
      "[164, 375]\n",
      "[19458, 309]\n",
      "[113, 120]\n",
      "[19109, 658]\n",
      "[98, 135]\n",
      "[19576, 222]\n",
      "[103, 99]\n",
      "[19195, 603]\n",
      "[70, 132]\n",
      "[19653, 196]\n",
      "[53, 98]\n",
      "[19268, 581]\n",
      "[44, 107]\n",
      "[19629, 203]\n",
      "[84, 84]\n",
      "[19212, 620]\n",
      "[45, 123]\n",
      "[19357, 478]\n",
      "[97, 68]\n",
      "[19208, 627]\n",
      "[46, 119]\n",
      "[19101, 392]\n",
      "[111, 396]\n",
      "[19027, 466]\n",
      "[91, 416]\n",
      "[19054, 408]\n",
      "[123, 415]\n",
      "[18982, 480]\n",
      "[101, 437]\n",
      "[19290, 506]\n",
      "[106, 98]\n",
      "[19151, 645]\n",
      "[96, 108]\n",
      "[19611, 232]\n",
      "[68, 89]\n",
      "[19243, 600]\n",
      "[48, 109]\n",
      "[19616, 244]\n",
      "[75, 65]\n",
      "[19270, 590]\n",
      "[64, 76]\n",
      "[19524, 320]\n",
      "[80, 76]\n",
      "[19232, 612]\n",
      "[62, 94]\n",
      "[19409, 367]\n",
      "[141, 83]\n",
      "[19070, 706]\n",
      "[146, 78]\n",
      "[18436, 990]\n",
      "[266, 308]\n",
      "[18910, 516]\n",
      "[200, 374]\n",
      "[18383, 1069]\n",
      "[313, 235]\n",
      "[18966, 486]\n",
      "[127, 421]\n",
      "[19409, 406]\n",
      "[96, 89]\n",
      "[19236, 579]\n",
      "[73, 112]\n",
      "[19571, 274]\n",
      "[81, 74]\n",
      "[19167, 678]\n",
      "[82, 73]\n",
      "[19637, 231]\n",
      "[64, 68]\n",
      "[19212, 656]\n",
      "[60, 72]\n",
      "[19618, 241]\n",
      "[64, 77]\n",
      "[19253, 606]\n",
      "[43, 98]\n",
      "[19357, 457]\n",
      "[103, 83]\n",
      "[19142, 672]\n",
      "[88, 98]\n",
      "[18868, 521]\n",
      "[337, 274]\n",
      "[18871, 518]\n",
      "[270, 341]\n",
      "[18874, 554]\n",
      "[282, 290]\n",
      "[18860, 568]\n",
      "[233, 339]\n",
      "[19317, 475]\n",
      "[119, 89]\n",
      "[19178, 614]\n",
      "[59, 149]\n",
      "[19629, 223]\n",
      "[70, 78]\n",
      "[19201, 651]\n",
      "[45, 103]\n",
      "[19602, 255]\n",
      "[74, 69]\n",
      "[19234, 623]\n",
      "[39, 104]\n",
      "[19521, 329]\n",
      "[88, 62]\n",
      "[19226, 624]\n",
      "[55, 95]\n",
      "[19481, 342]\n",
      "[75, 102]\n",
      "[19170, 653]\n",
      "[51, 126]\n",
      "[18512, 924]\n",
      "[302, 262]\n",
      "[18944, 492]\n",
      "[117, 447]\n",
      "[18557, 872]\n",
      "[293, 278]\n",
      "[18910, 519]\n",
      "[176, 395]\n",
      "[19478, 350]\n",
      "[95, 77]\n",
      "[19227, 601]\n",
      "[48, 124]\n",
      "[19603, 269]\n",
      "[79, 49]\n",
      "[19264, 608]\n",
      "[32, 96]\n",
      "[19668, 193]\n",
      "[47, 92]\n",
      "[19246, 615]\n",
      "[60, 79]\n",
      "[19598, 231]\n",
      "[85, 86]\n",
      "[19202, 627]\n",
      "[55, 116]\n",
      "[19514, 322]\n",
      "[77, 87]\n",
      "[19244, 592]\n",
      "[55, 109]\n",
      "[19027, 428]\n",
      "[221, 324]\n",
      "[18994, 461]\n",
      "[168, 377]\n",
      "[19074, 396]\n",
      "[156, 374]\n",
      "[19027, 443]\n",
      "[125, 405]\n",
      "[19671, 201]\n",
      "[61, 67]\n",
      "[19261, 611]\n",
      "[45, 83]\n",
      "[19817, 75]\n",
      "[51, 57]\n",
      "[19260, 632]\n",
      "[58, 50]\n",
      "[19736, 127]\n",
      "[62, 75]\n",
      "[19233, 630]\n",
      "[79, 58]\n",
      "[19774, 105]\n",
      "[41, 80]\n",
      "[19230, 649]\n",
      "[48, 73]\n",
      "[19704, 149]\n",
      "[65, 82]\n",
      "[19220, 633]\n",
      "[37, 110]\n",
      "[19530, 188]\n",
      "[112, 170]\n",
      "[19107, 611]\n",
      "[129, 153]\n",
      "[18254, 1087]\n",
      "[300, 359]\n",
      "[18943, 398]\n",
      "[235, 424]\n",
      "[19155, 255]\n",
      "[34, 556]\n",
      "[19007, 403]\n",
      "[48, 542]\n",
      "[18478, 975]\n",
      "[227, 320]\n",
      "[18953, 500]\n",
      "[176, 371]\n",
      "[19049, 400]\n",
      "[110, 441]\n",
      "[19007, 442]\n",
      "[101, 450]\n",
      "[18483, 1003]\n",
      "[261, 253]\n",
      "[19013, 473]\n",
      "[124, 390]\n",
      "[19133, 339]\n",
      "[43, 485]\n",
      "[19062, 410]\n",
      "[59, 469]\n",
      "[19553, 159]\n",
      "[122, 166]\n",
      "[19203, 509]\n",
      "[102, 186]\n",
      "[19556, 306]\n",
      "[73, 65]\n",
      "[19233, 629]\n",
      "[81, 57]\n",
      "[19266, 472]\n",
      "[135, 127]\n",
      "[19179, 559]\n",
      "[73, 189]\n",
      "[19478, 329]\n",
      "[99, 94]\n",
      "[19203, 604]\n",
      "[65, 128]\n",
      "[19355, 428]\n",
      "[105, 112]\n",
      "[19209, 574]\n",
      "[49, 168]\n",
      "[19404, 384]\n",
      "[115, 97]\n",
      "[19182, 606]\n",
      "[42, 170]\n",
      "[19371, 417]\n",
      "[124, 88]\n",
      "[19210, 578]\n",
      "[33, 179]\n",
      "[19700, 132]\n",
      "[80, 88]\n",
      "[19210, 622]\n",
      "[54, 114]\n",
      "[19680, 191]\n",
      "[67, 62]\n",
      "[19242, 629]\n",
      "[46, 83]\n",
      "[19571, 238]\n",
      "[88, 103]\n",
      "[19180, 629]\n",
      "[43, 148]\n",
      "[19444, 376]\n",
      "[100, 80]\n",
      "[19266, 554]\n",
      "[51, 129]\n",
      "[19501, 317]\n",
      "[94, 88]\n",
      "[19250, 568]\n",
      "[55, 127]\n",
      "[19518, 272]\n",
      "[121, 89]\n",
      "[19190, 600]\n",
      "[90, 120]\n",
      "[19645, 154]\n",
      "[83, 118]\n",
      "[19158, 641]\n",
      "[85, 116]\n",
      "[19728, 148]\n",
      "[66, 58]\n",
      "[19313, 563]\n",
      "[45, 79]\n",
      "[19671, 199]\n",
      "[71, 59]\n",
      "[19266, 604]\n",
      "[37, 93]\n",
      "[19562, 249]\n",
      "[85, 104]\n",
      "[19227, 584]\n",
      "[44, 145]\n",
      "[19591, 230]\n",
      "[78, 101]\n",
      "[19214, 607]\n",
      "[64, 115]\n",
      "[19607, 231]\n",
      "[81, 81]\n",
      "[19257, 581]\n",
      "[68, 94]\n",
      "[19659, 165]\n",
      "[103, 73]\n",
      "[19215, 609]\n",
      "[74, 102]\n",
      "[19648, 189]\n",
      "[85, 78]\n",
      "[19203, 634]\n",
      "[68, 95]\n",
      "[19784, 75]\n",
      "[69, 72]\n",
      "[19291, 568]\n",
      "[50, 91]\n",
      "[19671, 191]\n",
      "[87, 51]\n",
      "[19283, 579]\n",
      "[34, 104]\n",
      "[19591, 232]\n",
      "[86, 91]\n",
      "[19247, 576]\n",
      "[36, 141]\n",
      "[19550, 266]\n",
      "[108, 76]\n",
      "[19242, 574]\n",
      "[37, 147]\n",
      "[19493, 306]\n",
      "[106, 95]\n",
      "[19184, 615]\n",
      "[68, 133]\n",
      "[19568, 254]\n",
      "[90, 88]\n",
      "[19226, 596]\n",
      "[65, 113]\n",
      "[19625, 213]\n",
      "[85, 77]\n",
      "[19232, 606]\n",
      "[39, 123]\n",
      "[19771, 93]\n",
      "[85, 51]\n",
      "[19263, 601]\n",
      "[41, 95]\n",
      "[19483, 385]\n",
      "[80, 52]\n",
      "[19215, 653]\n",
      "[54, 78]\n",
      "[19578, 240]\n",
      "[86, 96]\n",
      "[19268, 550]\n",
      "[48, 134]\n",
      "[19353, 438]\n",
      "[109, 100]\n",
      "[19242, 549]\n",
      "[58, 151]\n",
      "[19463, 351]\n",
      "[99, 87]\n",
      "[19213, 601]\n",
      "[64, 122]\n",
      "[19354, 453]\n",
      "[115, 78]\n",
      "[19208, 599]\n",
      "[49, 144]\n",
      "[19506, 305]\n",
      "[92, 97]\n",
      "[19221, 590]\n",
      "[69, 120]\n",
      "[19678, 173]\n",
      "[77, 72]\n",
      "[19293, 558]\n",
      "[37, 112]\n",
      "[19273, 432]\n",
      "[173, 122]\n",
      "[19259, 446]\n",
      "[65, 230]\n",
      "[18520, 969]\n",
      "[290, 221]\n",
      "[19022, 467]\n",
      "[86, 425]\n",
      "[19134, 362]\n",
      "[79, 425]\n",
      "[19082, 414]\n",
      "[80, 424]\n",
      "[18467, 934]\n",
      "[287, 312]\n",
      "[18921, 480]\n",
      "[188, 411]\n",
      "[19079, 353]\n",
      "[86, 482]\n",
      "[19005, 427]\n",
      "[82, 486]\n",
      "[18534, 913]\n",
      "[326, 227]\n",
      "[18999, 448]\n",
      "[128, 425]\n",
      "[19071, 343]\n",
      "[141, 445]\n",
      "[18998, 416]\n",
      "[120, 466]\n",
      "------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds of MWPM:\n",
      "> Accuracy: 0.9740240740740888 (+- 3.447768702163095e-05)\n",
      "> F1: 0.6987362438110836(+- 0.0006966427848766909)\n",
      "> Time: 2130.54912663333 (+- 28.87378933361273)\n",
      "Average scores for all folds of PLUT:\n",
      "> Accuracy: 0.9620978645122641 (+- 1.8245896618399287e-05)\n",
      "> F1: 0.21589124360698222(+- 9.335238689092155e-05)\n",
      "> Time: 3.5051154666666666 (+- 0.022936989822894247)\n",
      "Average scores for all folds of NN:\n",
      "> Accuracy: 0.490395938517907 (+- 0.3506857778787787)\n",
      "> F1: 0.8202249687718891(+- 0.0009415129894640702)\n",
      "> Time: 2.8170823333333335 (+- 0.08801259313061718)\n",
      "> AUC for class : 0.9911802210257864 (+- 0.0)\n",
      "X^2 for MWPM and NN: 0\n",
      "X^2 for PLUT and NN: 0\n",
      "> AUC for class X00: 0.8786446666443656 (+- 0.0032036874870317476)\n",
      "X^2 for MWPM and NN: 38.50437549721559\n",
      "X^2 for PLUT and NN: 240.0\n",
      "> AUC for class X01: 0.9855683124312872 (+- 0.0014133730442808924)\n",
      "X^2 for MWPM and NN: 17.88908765652952\n",
      "X^2 for PLUT and NN: 234.01363636363635\n",
      "> AUC for class X02: 0.9981418942470979 (+- 0.00011465529331018379)\n",
      "X^2 for MWPM and NN: 42.98684210526316\n",
      "X^2 for PLUT and NN: 424.5665760869565\n",
      "> AUC for class X03: 0.9986209576894612 (+- 2.1642084034864097e-05)\n",
      "X^2 for MWPM and NN: 32.26815642458101\n",
      "X^2 for PLUT and NN: 520.0819912152269\n",
      "> AUC for class X04: 0.9988440040092457 (+- 0.00017418031177676894)\n",
      "X^2 for MWPM and NN: 96.99672131147541\n",
      "X^2 for PLUT and NN: 503.1496913580247\n",
      "> AUC for class X05: 0.9983324299982408 (+- 7.298363002708901e-05)\n",
      "X^2 for MWPM and NN: 172.0025641025641\n",
      "X^2 for PLUT and NN: 486.12762762762765\n",
      "> AUC for class X06: 0.9804743795367402 (+- 0.00030007360946518094)\n",
      "X^2 for MWPM and NN: 599.5558845299778\n",
      "X^2 for PLUT and NN: 134.44458762886597\n",
      "> AUC for class X10: 0.9793472304382395 (+- 0.0003130750404961166)\n",
      "X^2 for MWPM and NN: 351.77226890756305\n",
      "X^2 for PLUT and NN: 184.24707602339183\n",
      "> AUC for class X11: 0.9955778659232839 (+- 0.00012791600430806524)\n",
      "X^2 for MWPM and NN: 90.10663507109005\n",
      "X^2 for PLUT and NN: 413.3346560846561\n",
      "> AUC for class X12: 0.9969153684424491 (+- 0.00019526982817773847)\n",
      "X^2 for MWPM and NN: 42.84307692307692\n",
      "X^2 for PLUT and NN: 420.5408618127786\n",
      "> AUC for class X13: 0.9979690201295481 (+- 6.642463484144515e-05)\n",
      "X^2 for MWPM and NN: 80.97991967871486\n",
      "X^2 for PLUT and NN: 459.6736\n",
      "> AUC for class X14: 0.9977927306417991 (+- 3.288901882060368e-05)\n",
      "X^2 for MWPM and NN: 48.51567944250871\n",
      "X^2 for PLUT and NN: 495.4526315789474\n",
      "> AUC for class X15: 0.9969601284131565 (+- 0.00011223375685819325)\n",
      "X^2 for MWPM and NN: 251.1304347826087\n",
      "X^2 for PLUT and NN: 499.851411589896\n",
      "> AUC for class X16: 0.9791970308807023 (+- 0.0011220381495639325)\n",
      "X^2 for MWPM and NN: 155.8648111332008\n",
      "X^2 for PLUT and NN: 251.12387791741472\n",
      "> AUC for class X20: 0.9792260454354217 (+- 0.000249407659640991)\n",
      "X^2 for MWPM and NN: 151.89453860640302\n",
      "X^2 for PLUT and NN: 245.9277108433735\n",
      "> AUC for class X21: 0.9964738052269426 (+- 0.00033640208189471065)\n",
      "X^2 for MWPM and NN: 260.13235294117646\n",
      "X^2 for PLUT and NN: 405.2685560053981\n",
      "> AUC for class X22: 0.9976747958196126 (+- 0.00030374708401506497)\n",
      "X^2 for MWPM and NN: 88.56333333333333\n",
      "X^2 for PLUT and NN: 468.52006172839504\n",
      "> AUC for class X23: 0.9978373114635495 (+- 8.855250221760721e-05)\n",
      "X^2 for MWPM and NN: 88.47648902821317\n",
      "X^2 for PLUT and NN: 421.4449541284404\n",
      "> AUC for class X24: 0.9975686541241243 (+- 6.870984634091362e-05)\n",
      "X^2 for MWPM and NN: 142.8025\n",
      "X^2 for PLUT and NN: 447.1824925816024\n",
      "> AUC for class X25: 0.9968737184446439 (+- 4.128845815275554e-05)\n",
      "X^2 for MWPM and NN: 99.65551181102362\n",
      "X^2 for PLUT and NN: 366.76173708920186\n",
      "> AUC for class X26: 0.9802620493578011 (+- 0.00030840935116968886)\n",
      "X^2 for MWPM and NN: 416.18550955414014\n",
      "X^2 for PLUT and NN: 138.58240223463687\n",
      "> AUC for class X30: 0.9806438436341262 (+- 0.00019998776104651576)\n",
      "X^2 for MWPM and NN: 412.4638205499276\n",
      "X^2 for PLUT and NN: 209.07667210440457\n",
      "> AUC for class X31: 0.9967951016947018 (+- 0.00019367687291901078)\n",
      "X^2 for MWPM and NN: 190.20119521912352\n",
      "X^2 for PLUT and NN: 391.14263803680984\n",
      "> AUC for class X32: 0.9978535536982518 (+- 7.345826031229463e-05)\n",
      "X^2 for MWPM and NN: 103.84225352112676\n",
      "X^2 for PLUT and NN: 465.82236842105266\n",
      "> AUC for class X33: 0.9978993584257836 (+- 0.00012421123721659272)\n",
      "X^2 for MWPM and NN: 93.41016949152542\n",
      "X^2 for PLUT and NN: 494.44832402234636\n",
      "> AUC for class X34: 0.9977496841047928 (+- 9.625182717500336e-05)\n",
      "X^2 for MWPM and NN: 101.56065573770492\n",
      "X^2 for PLUT and NN: 486.6625577812018\n",
      "> AUC for class X35: 0.996936109273555 (+- 0.0001903462425233232)\n",
      "X^2 for MWPM and NN: 222.51607142857142\n",
      "X^2 for PLUT and NN: 447.22236842105264\n",
      "> AUC for class X36: 0.9794179457814618 (+- 0.000296758956392749)\n",
      "X^2 for MWPM and NN: 39.03146853146853\n",
      "X^2 for PLUT and NN: 77.4225888324873\n",
      "> AUC for class X40: 0.9806297275801663 (+- 0.00019600043829077677)\n",
      "X^2 for MWPM and NN: 87.84808612440192\n",
      "X^2 for PLUT and NN: 139.270911360799\n",
      "> AUC for class X41: 0.9969782118095294 (+- 0.00033785756789879315)\n",
      "X^2 for MWPM and NN: 212.16329966329965\n",
      "X^2 for PLUT and NN: 456.0416047548291\n",
      "> AUC for class X42: 0.9977603666172843 (+- 0.00011374074729878522)\n",
      "X^2 for MWPM and NN: 78.8532423208191\n",
      "X^2 for PLUT and NN: 525.8979885057471\n",
      "> AUC for class X43: 0.9978083439047806 (+- 0.00013487065261844799)\n",
      "X^2 for MWPM and NN: 98.48024316109422\n",
      "X^2 for PLUT and NN: 513.4274924471299\n",
      "> AUC for class X44: 0.9978144578694602 (+- 4.901659531546773e-05)\n",
      "X^2 for MWPM and NN: 138.1294964028777\n",
      "X^2 for PLUT and NN: 475.1458026509573\n",
      "> AUC for class X45: 0.9968510731953872 (+- 0.0003013132301127377)\n",
      "X^2 for MWPM and NN: 169.67865707434052\n",
      "X^2 for PLUT and NN: 513.0696022727273\n",
      "> AUC for class X46: 0.9802573560501833 (+- 0.0001854871703412548)\n",
      "X^2 for MWPM and NN: 314.5522022838499\n",
      "X^2 for PLUT and NN: 229.68144499178982\n",
      "> AUC for class X50: 0.9801598430543386 (+- 0.00040181810932226643)\n",
      "X^2 for MWPM and NN: 286.7673819742489\n",
      "X^2 for PLUT and NN: 168.29352517985612\n",
      "> AUC for class X51: 0.9969172633884369 (+- 0.00018158316610037691)\n",
      "X^2 for MWPM and NN: 144.97977528089888\n",
      "X^2 for PLUT and NN: 469.49768875192603\n",
      "> AUC for class X52: 0.9977540085198994 (+- 9.67692547865422e-05)\n",
      "X^2 for MWPM and NN: 102.64655172413794\n",
      "X^2 for PLUT and NN: 516.6015625\n",
      "> AUC for class X53: 0.9978488217355672 (+- 0.00011736505786906808)\n",
      "X^2 for MWPM and NN: 87.60416666666667\n",
      "X^2 for PLUT and NN: 454.6903703703704\n",
      "> AUC for class X54: 0.9976889426481593 (+- 5.760507172471197e-05)\n",
      "X^2 for MWPM and NN: 66.53481012658227\n",
      "X^2 for PLUT and NN: 478.0659824046921\n",
      "> AUC for class X55: 0.9973668707444086 (+- 0.0001080510636573028)\n",
      "X^2 for MWPM and NN: 149.21303258145363\n",
      "X^2 for PLUT and NN: 444.0432766615147\n",
      "> AUC for class X56: 0.9801589712336772 (+- 0.0004983339638159887)\n",
      "X^2 for MWPM and NN: 65.38674884437596\n",
      "X^2 for PLUT and NN: 135.55484896661366\n",
      "> AUC for class X60: 0.9811807256762884 (+- 0.00027290410367449615)\n",
      "X^2 for MWPM and NN: 103.48007246376811\n",
      "X^2 for PLUT and NN: 176.91725352112675\n",
      "> AUC for class X61: 0.9982285323766771 (+- 0.00012328097287625387)\n",
      "X^2 for MWPM and NN: 73.7442748091603\n",
      "X^2 for PLUT and NN: 486.6234756097561\n",
      "> AUC for class X62: 0.9987138216333327 (+- 0.00014606272929367262)\n",
      "X^2 for MWPM and NN: 4.198412698412699\n",
      "X^2 for PLUT and NN: 475.8391304347826\n",
      "> AUC for class X63: 0.9987092421591953 (+- 8.065936331484441e-05)\n",
      "X^2 for MWPM and NN: 21.67195767195767\n",
      "X^2 for PLUT and NN: 426.657263751763\n",
      "> AUC for class X64: 0.9987034095170365 (+- 3.210730446528726e-05)\n",
      "X^2 for MWPM and NN: 27.184931506849313\n",
      "X^2 for PLUT and NN: 516.4992826398852\n",
      "> AUC for class X65: 0.9976697728565046 (+- 8.974784311407937e-05)\n",
      "X^2 for MWPM and NN: 32.191588785046726\n",
      "X^2 for PLUT and NN: 528.3955223880597\n",
      "> AUC for class X66: 0.9935252039892394 (+- 0.00019384988053639928)\n",
      "X^2 for MWPM and NN: 18.75\n",
      "X^2 for PLUT and NN: 312.65\n",
      "> AUC for class Z00: 0.9734214696246595 (+- 0.0003921554249168573)\n",
      "X^2 for MWPM and NN: 445.41888968997836\n",
      "X^2 for PLUT and NN: 41.459715639810426\n",
      "> AUC for class Z01: 0.9746043908482017 (+- 0.00017951551673239512)\n",
      "X^2 for MWPM and NN: 167.47404844290656\n",
      "X^2 for PLUT and NN: 277.86252771618626\n",
      "> AUC for class Z02: 0.9769169244287349 (+- 0.000528305302797181)\n",
      "X^2 for MWPM and NN: 464.2337770382695\n",
      "X^2 for PLUT and NN: 154.3328402366864\n",
      "> AUC for class Z03: 0.9779938120968307 (+- 0.0007891547974071998)\n",
      "X^2 for MWPM and NN: 163.76666666666668\n",
      "X^2 for PLUT and NN: 212.8913443830571\n",
      "> AUC for class Z04: 0.9776705061493818 (+- 0.0002109195858594238)\n",
      "X^2 for MWPM and NN: 434.3995253164557\n",
      "X^2 for PLUT and NN: 202.85427135678393\n",
      "> AUC for class Z05: 0.9777350036468295 (+- 0.0005859440860231786)\n",
      "X^2 for MWPM and NN: 227.81413612565444\n",
      "X^2 for PLUT and NN: 261.1940298507463\n",
      "> AUC for class Z06: 0.9918840436805775 (+- 0.0003512002046898693)\n",
      "X^2 for MWPM and NN: 4.612099644128114\n",
      "X^2 for PLUT and NN: 269.7806873977087\n",
      "> AUC for class Z10: 0.9979175902394006 (+- 0.00026056733252906506)\n",
      "X^2 for MWPM and NN: 142.01583113456465\n",
      "X^2 for PLUT and NN: 421.42112676056337\n",
      "> AUC for class Z11: 0.995052411684231 (+- 0.00015905189469369265)\n",
      "X^2 for MWPM and NN: 185.99011532125206\n",
      "X^2 for PLUT and NN: 372.1914556962025\n",
      "> AUC for class Z12: 0.9955669184033322 (+- 5.9316236659253554e-05)\n",
      "X^2 for MWPM and NN: 122.52570093457943\n",
      "X^2 for PLUT and NN: 432.65171898355754\n",
      "> AUC for class Z13: 0.995725119567693 (+- 3.629142095486087e-05)\n",
      "X^2 for MWPM and NN: 194.52908067542214\n",
      "X^2 for PLUT and NN: 440.7319422150883\n",
      "> AUC for class Z14: 0.9959899942270407 (+- 0.00015630986198309955)\n",
      "X^2 for MWPM and NN: 143.93587174348698\n",
      "X^2 for PLUT and NN: 489.1496913580247\n",
      "> AUC for class Z15: 0.996320563182189 (+- 0.00028170311527607944)\n",
      "X^2 for MWPM and NN: 157.60443622920516\n",
      "X^2 for PLUT and NN: 484.34697217675944\n",
      "> AUC for class Z16: 0.997218226203594 (+- 0.00023040446687885141)\n",
      "X^2 for MWPM and NN: 12.268867924528301\n",
      "X^2 for PLUT and NN: 475.57544378698225\n",
      "> AUC for class Z20: 0.9983675312144765 (+- 0.0001057935981867192)\n",
      "X^2 for MWPM and NN: 58.63953488372093\n",
      "X^2 for PLUT and NN: 501.81333333333333\n",
      "> AUC for class Z21: 0.9965421472990169 (+- 9.47945008276101e-05)\n",
      "X^2 for MWPM and NN: 68.10122699386503\n",
      "X^2 for PLUT and NN: 509.26339285714283\n",
      "> AUC for class Z22: 0.9966770823313315 (+- 4.559492277495803e-05)\n",
      "X^2 for MWPM and NN: 158.87605042016807\n",
      "X^2 for PLUT and NN: 416.53553719008266\n",
      "> AUC for class Z23: 0.9968161517768819 (+- 0.00046720694797181103)\n",
      "X^2 for MWPM and NN: 119.91240875912409\n",
      "X^2 for PLUT and NN: 420.776886035313\n",
      "> AUC for class Z24: 0.9964168360079361 (+- 0.00023468173390339288)\n",
      "X^2 for MWPM and NN: 57.25190839694657\n",
      "X^2 for PLUT and NN: 375.47971014492754\n",
      "> AUC for class Z25: 0.9967581732698916 (+- 0.0002027061068042745)\n",
      "X^2 for MWPM and NN: 20.675105485232066\n",
      "X^2 for PLUT and NN: 424.27685950413223\n",
      "> AUC for class Z26: 0.998065580267505 (+- 9.044828037922176e-05)\n",
      "X^2 for MWPM and NN: 30.6588785046729\n",
      "X^2 for PLUT and NN: 439.6200657894737\n",
      "> AUC for class Z30: 0.9982734915190037 (+- 7.355073464597975e-05)\n",
      "X^2 for MWPM and NN: 59.737037037037034\n",
      "X^2 for PLUT and NN: 499.77535101404055\n",
      "> AUC for class Z31: 0.9969651447531425 (+- 9.224453013885462e-05)\n",
      "X^2 for MWPM and NN: 79.54790419161677\n",
      "X^2 for PLUT and NN: 462.6130573248408\n",
      "> AUC for class Z32: 0.9963041701404654 (+- 0.0002652407537658652)\n",
      "X^2 for MWPM and NN: 74.02922077922078\n",
      "X^2 for PLUT and NN: 437.80029806259313\n",
      "> AUC for class Z33: 0.9966139793891264 (+- 8.342541706189935e-05)\n",
      "X^2 for MWPM and NN: 71.15705128205128\n",
      "X^2 for PLUT and NN: 403.91987673343607\n",
      "> AUC for class Z34: 0.9965816270196569 (+- 9.081480130512722e-05)\n",
      "X^2 for MWPM and NN: 13.884328358208956\n",
      "X^2 for PLUT and NN: 417.5051244509517\n",
      "> AUC for class Z35: 0.9971473550952915 (+- 9.812384587191499e-05)\n",
      "X^2 for MWPM and NN: 38.71897810218978\n",
      "X^2 for PLUT and NN: 454.73646723646726\n",
      "> AUC for class Z36: 0.9981016694695258 (+- 4.09003689957943e-05)\n",
      "X^2 for MWPM and NN: 0.1736111111111111\n",
      "X^2 for PLUT and NN: 432.5064724919094\n",
      "> AUC for class Z40: 0.9981615322057605 (+- 0.00010606968512538511)\n",
      "X^2 for MWPM and NN: 38.16187050359712\n",
      "X^2 for PLUT and NN: 482.76672104404565\n",
      "> AUC for class Z41: 0.9967872334860143 (+- 0.00038575830066012287)\n",
      "X^2 for MWPM and NN: 66.11635220125787\n",
      "X^2 for PLUT and NN: 474.7075163398693\n",
      "> AUC for class Z42: 0.9967446389276354 (+- 0.0003400911985821053)\n",
      "X^2 for MWPM and NN: 65.90641711229947\n",
      "X^2 for PLUT and NN: 470.2062193126023\n",
      "> AUC for class Z43: 0.9965544006984175 (+- 9.298627491174606e-05)\n",
      "X^2 for MWPM and NN: 96.11893203883496\n",
      "X^2 for PLUT and NN: 436.4802342606149\n",
      "> AUC for class Z44: 0.9963015856506663 (+- 7.412289159710763e-05)\n",
      "X^2 for MWPM and NN: 77.23546511627907\n",
      "X^2 for PLUT and NN: 424.9621785173979\n",
      "> AUC for class Z45: 0.9967763756108822 (+- 3.32746825006586e-05)\n",
      "X^2 for MWPM and NN: 54.124161073825505\n",
      "X^2 for PLUT and NN: 496.67596899224804\n",
      "> AUC for class Z46: 0.9983291589876571 (+- 3.397299447608318e-05)\n",
      "X^2 for MWPM and NN: 0.2752808988764045\n",
      "X^2 for PLUT and NN: 486.7305295950156\n",
      "> AUC for class Z50: 0.9971010480770277 (+- 0.0003008487932346466)\n",
      "X^2 for MWPM and NN: 198.74408602150538\n",
      "X^2 for PLUT and NN: 505.8048090523338\n",
      "> AUC for class Z51: 0.996137768697683 (+- 0.0002057550768809122)\n",
      "X^2 for MWPM and NN: 71.80674846625767\n",
      "X^2 for PLUT and NN: 419.7341137123746\n",
      "> AUC for class Z52: 0.995776666269523 (+- 0.0001484287663311034)\n",
      "X^2 for MWPM and NN: 196.6800731261426\n",
      "X^2 for PLUT and NN: 395.5518945634267\n",
      "> AUC for class Z53: 0.9956328320693169 (+- 0.00018540423191477784)\n",
      "X^2 for MWPM and NN: 140.00222222222223\n",
      "X^2 for PLUT and NN: 432.02406015037593\n",
      "> AUC for class Z54: 0.9959250049340655 (+- 0.00013783150585273886)\n",
      "X^2 for MWPM and NN: 199.94542253521126\n",
      "X^2 for PLUT and NN: 465.125\n",
      "> AUC for class Z55: 0.9955647053827615 (+- 0.0002064774587425422)\n",
      "X^2 for MWPM and NN: 113.20906801007557\n",
      "X^2 for PLUT and NN: 410.3186646433991\n",
      "> AUC for class Z56: 0.9976105378939149 (+- 0.00012777654703815248)\n",
      "X^2 for MWPM and NN: 36.1\n",
      "X^2 for PLUT and NN: 454.453781512605\n",
      "> AUC for class Z60: 0.9923120088897746 (+- 0.0007129451963336755)\n",
      "X^2 for MWPM and NN: 110.02314049586776\n",
      "X^2 for PLUT and NN: 282.5831702544031\n",
      "> AUC for class Z61: 0.9775591290903517 (+- 0.00014612703230298648)\n",
      "X^2 for MWPM and NN: 365.1183478951549\n",
      "X^2 for PLUT and NN: 261.121157323689\n",
      "> AUC for class Z62: 0.9768126418770257 (+- 0.000608917435358987)\n",
      "X^2 for MWPM and NN: 180.3265306122449\n",
      "X^2 for PLUT and NN: 224.47165991902833\n",
      "> AUC for class Z63: 0.977309027240128 (+- 0.00021190260598079428)\n",
      "X^2 for MWPM and NN: 341.7821457821458\n",
      "X^2 for PLUT and NN: 126.76796407185628\n",
      "> AUC for class Z64: 0.9772978006717038 (+- 0.00017761502945435925)\n",
      "X^2 for MWPM and NN: 161.1753986332574\n",
      "X^2 for PLUT and NN: 232.48722986247543\n",
      "> AUC for class Z65: 0.976988424760787 (+- 0.00027715204551873843)\n",
      "X^2 for MWPM and NN: 277.1557707828894\n",
      "X^2 for PLUT and NN: 176.66840277777777\n",
      "> AUC for class Z66: 0.9775078124433636 (+- 0.0008919434931679691)\n",
      "X^2 for MWPM and NN: 83.47314049586777\n",
      "X^2 for PLUT and NN: 162.36007462686567\n",
      "###################################################################################\n",
      "TOTAL F1 NN: [0.8201395877498923, 0.821418399316434, 0.8191169192493412]\n",
      "TOTAL F1 PLUT: [0.21600465905950167, 0.21577601491392678, 0.21589305684751825]\n",
      "TOTAL F1 MWPM: [0.6980389857833798, 0.6996876498871094, 0.6984820957627614]\n",
      "TOTAL ACC NN: [0.24212782084941864, 0.2427195906639099, 0.9863404040403925]\n",
      "TOTAL ACC PLUT: [0.9621109258977504, 0.9620720615785512, 0.9621106060604908]\n",
      "TOTAL ACC MWPM: [0.9739863636363807, 0.9740696969697087, 0.9740161616161771]\n",
      "TOTAL TIME NN: [2.9411304, 2.7462049, 2.7639117]\n",
      "TOTAL TIME PLUT: [3.5267561, 3.5152217, 3.4733686]\n",
      "TOTAL TIME MWPM: [2171.313826099998, 2112.2217765999876, 2108.1117772000052]\n",
      "------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#test MWPM decoder for this fold\n",
    "#labels = targets[train], features = inputs[train]\n",
    "# x_test_d7 = translate_to_graph(testData_d7_MWPM, targs, mlb_d7)\n",
    "\"\"\"\n",
    "decoding_d7, time_mwpm = do_new_decoding(x_test_d7, 7, 0)\n",
    "decoding_d7['combine'] = decoding_d7[[0, 1]].values.tolist()\n",
    "decoding_d7['combine'].apply(lambda x: x[0].extend(x[1]))\n",
    "decoding_d7 = np.array(decoding_d7[0])\n",
    "\n",
    "time_per_fold_mwpm.append(time_mwpm)\n",
    "\n",
    "pred_mwpm = mlb_d7.transform(decoding_d7)\n",
    "\n",
    "if fold_no < 3:\n",
    "    acc = partial_accuracy(targets_test_2, pred_mwpm)\n",
    "else:\n",
    "    acc, contingency_mwpm = partial_accuracy_and_contingency(targets_test_2, pred_mwpm, mlb_d7)\"\"\"\n",
    "\n",
    "#acc_per_fold_mwpm.append(acc)\n",
    "#f1_per_fold_mwpm.append(f1_score(targets_test_2, pred_mwpm, average='micro'))\n",
    "\n",
    "#####################################################################################################\n",
    "#test the plut decoder for this fold\n",
    "\n",
    "#lookup_d7 = lookup_decoder(7)\n",
    "\n",
    "#lookup_d7 = train_plut(lookup_d7, inputs_train, targets[train])\n",
    "\n",
    "#start = time.time_ns()\n",
    "#pred_plut_d7 = test_plut(lookup_d7, inputs_test)\n",
    "#end = time.time_ns() \n",
    "#time_per_fold_plut.append((end - start)/ (10 ** 9))\n",
    "\n",
    "pred_plut_d7 = test_plut(lookup_d7, inputs_test_2)\n",
    "#f1 = f1_score(targets_test_2, pred_plut_d7, average='micro')\n",
    "acc, contingency_plut = partial_accuracy_and_contingency(targets_test_2, pred_plut_d7, mlb_d7)\n",
    "\n",
    "#acc_per_fold_plut.append(acc)\n",
    "#f1_per_fold_plut.append(f1)\n",
    "\n",
    "#####################################################################################################\n",
    "#Test the NN decoder for this fold\n",
    "\"\"\"\n",
    "model_d7 = compile_FFNN_model_DepthSeven(7)\n",
    "\n",
    "# Generate a print\n",
    "print('------------------------------------------------------------------------')\n",
    "print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "# Fit data to model\n",
    "history = model_d7.fit(\n",
    "    x=inputs_train ,\n",
    "    y=targets[train],\n",
    "    validation_split=.25,\n",
    "    epochs= 150)\"\"\"\n",
    "inputs_test_2 = np.asarray(inputs_test_2).astype('int32')\n",
    "fpr, tpr, thresholds = roc_curve(targets[test].ravel(), predictions_d7.ravel())\n",
    "tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "tprs[-1][0] = 0.0\n",
    "roc_auc = auc(fpr, tpr)\n",
    "aucs.append(roc_auc)\n",
    "plt.plot(fpr, tpr, lw=1, alpha=0.3, label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
    "\n",
    "#get the AUCs of each class, used to get average AUC of each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(targets[test][:, i], predictions_d7[:, i]) \n",
    "    aucs_classes[mlb_d7.classes_[i]].append(auc(fpr[i], tpr[i]))\n",
    "\n",
    "#########################################################################################################\n",
    "#compute McNemar's statistic on results of last fold\n",
    "\n",
    "mcnemar_results_mwpm = {}\n",
    "mcnemar_results_plut = {}\n",
    "\n",
    "for class_ in mlb_d7.classes_:\n",
    "    \n",
    "    #compute the x^2 for NN and MWPM\n",
    "    mcnemar_results_mwpm[class_]=contingency_table_and_t(contingency_nn[class_], contingency_mwpm[class_])[1]    \n",
    "    #comput the x^2 for NN and PLUT\n",
    "    mcnemar_results_plut[class_] = contingency_table_and_t(contingency_nn[class_], contingency_plut[class_])[1]\n",
    "        \n",
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds of MWPM:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold_mwpm)} (+- {np.std(acc_per_fold_mwpm)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold_mwpm)}(+- {np.std(f1_per_fold_mwpm)})')\n",
    "print(f'> Time: {np.mean(time_per_fold_mwpm)} (+- {np.std(time_per_fold_mwpm)})')\n",
    "print('Average scores for all folds of PLUT:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold_plut)} (+- {np.std(acc_per_fold_plut)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold_plut)}(+- {np.std(f1_per_fold_plut)})')\n",
    "print(f'> Time: {np.mean(time_per_fold_plut)} (+- {np.std(time_per_fold_plut)})')\n",
    "print('Average scores for all folds of NN:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold)}(+- {np.std(f1_per_fold)})')\n",
    "print(f'> Time: {np.mean(time_per_fold)} (+- {np.std(time_per_fold)})')\n",
    "for key in aucs_classes:\n",
    "    cleanedList = [x for x in aucs_classes[key] if str(x) != 'nan']\n",
    "    print(f'> AUC for class {key}: {np.mean(cleanedList)} (+- {np.std(cleanedList)})') #this has to be a for loop\n",
    "    print(\"X^2 for MWPM and NN: \" + str(mcnemar_results_mwpm[key]))  \n",
    "    print(\"X^2 for PLUT and NN: \" + str(mcnemar_results_plut[key]))\n",
    "print(\"###################################################################################\")\n",
    "print(\"TOTAL F1 NN: \" + str(f1_per_fold))\n",
    "print(\"TOTAL F1 PLUT: \" + str(f1_per_fold_plut))\n",
    "print(\"TOTAL F1 MWPM: \" + str(f1_per_fold_mwpm))\n",
    "print(\"TOTAL ACC NN: \" + str(acc_per_fold))\n",
    "print(\"TOTAL ACC PLUT: \" + str(acc_per_fold_plut))\n",
    "print(\"TOTAL ACC MWPM: \" + str(acc_per_fold_mwpm))\n",
    "print(\"TOTAL TIME NN: \" + str(time_per_fold))\n",
    "print(\"TOTAL TIME PLUT: \" + str(time_per_fold_plut))\n",
    "print(\"TOTAL TIME MWPM: \" + str(time_per_fold_mwpm))\n",
    "print('------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "         label='Chance', alpha=.8)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "         label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "         lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                 label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "plt.xlim([-0.01, 1.01])\n",
    "plt.ylim([-0.01, 1.01])\n",
    "plt.xlabel('False Positive Rate',fontsize=18)\n",
    "plt.ylabel('True Positive Rate',fontsize=18)\n",
    "plt.title('Cross-Validation ROC of Depth 7 NN',fontsize=18)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-18-383d0a3a81ae>:74: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "  tprs.append(interp(mean_fpr, fpr, tpr))\n",
      "C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:949: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20000, 0]\n",
      "[0, 0]\n",
      "both b and c are zero\n",
      "[20000, 0]\n",
      "[0, 0]\n",
      "both b and c are zero\n",
      "[18630, 519]\n",
      "[738, 113]\n",
      "[18811, 338]\n",
      "[37, 814]\n",
      "[19269, 330]\n",
      "[229, 172]\n",
      "[19072, 527]\n",
      "[133, 268]\n",
      "[19682, 164]\n",
      "[64, 90]\n",
      "[19198, 648]\n",
      "[88, 66]\n",
      "[19755, 128]\n",
      "[51, 66]\n",
      "[19243, 640]\n",
      "[43, 74]\n",
      "[19642, 239]\n",
      "[66, 53]\n",
      "[19271, 610]\n",
      "[38, 81]\n",
      "[19537, 325]\n",
      "[65, 73]\n",
      "[19244, 618]\n",
      "[48, 90]\n",
      "[18297, 1126]\n",
      "[225, 352]\n",
      "[18873, 550]\n",
      "[226, 351]\n",
      "[18542, 919]\n",
      "[271, 268]\n",
      "[18941, 520]\n",
      "[164, 375]\n",
      "[19458, 309]\n",
      "[113, 120]\n",
      "[19109, 658]\n",
      "[98, 135]\n",
      "[19576, 222]\n",
      "[103, 99]\n",
      "[19195, 603]\n",
      "[70, 132]\n",
      "[19653, 196]\n",
      "[53, 98]\n",
      "[19268, 581]\n",
      "[44, 107]\n",
      "[19629, 203]\n",
      "[84, 84]\n",
      "[19212, 620]\n",
      "[45, 123]\n",
      "[19357, 478]\n",
      "[97, 68]\n",
      "[19208, 627]\n",
      "[46, 119]\n",
      "[19101, 392]\n",
      "[111, 396]\n",
      "[19027, 466]\n",
      "[91, 416]\n",
      "[19054, 408]\n",
      "[123, 415]\n",
      "[18982, 480]\n",
      "[101, 437]\n",
      "[19290, 506]\n",
      "[106, 98]\n",
      "[19151, 645]\n",
      "[96, 108]\n",
      "[19611, 232]\n",
      "[68, 89]\n",
      "[19243, 600]\n",
      "[48, 109]\n",
      "[19616, 244]\n",
      "[75, 65]\n",
      "[19270, 590]\n",
      "[64, 76]\n",
      "[19524, 320]\n",
      "[80, 76]\n",
      "[19232, 612]\n",
      "[62, 94]\n",
      "[19409, 367]\n",
      "[141, 83]\n",
      "[19070, 706]\n",
      "[146, 78]\n",
      "[18436, 990]\n",
      "[266, 308]\n",
      "[18910, 516]\n",
      "[200, 374]\n",
      "[18383, 1069]\n",
      "[313, 235]\n",
      "[18966, 486]\n",
      "[127, 421]\n",
      "[19409, 406]\n",
      "[96, 89]\n",
      "[19236, 579]\n",
      "[73, 112]\n",
      "[19571, 274]\n",
      "[81, 74]\n",
      "[19167, 678]\n",
      "[82, 73]\n",
      "[19637, 231]\n",
      "[64, 68]\n",
      "[19212, 656]\n",
      "[60, 72]\n",
      "[19618, 241]\n",
      "[64, 77]\n",
      "[19253, 606]\n",
      "[43, 98]\n",
      "[19357, 457]\n",
      "[103, 83]\n",
      "[19142, 672]\n",
      "[88, 98]\n",
      "[18868, 521]\n",
      "[337, 274]\n",
      "[18871, 518]\n",
      "[270, 341]\n",
      "[18874, 554]\n",
      "[282, 290]\n",
      "[18860, 568]\n",
      "[233, 339]\n",
      "[19317, 475]\n",
      "[119, 89]\n",
      "[19178, 614]\n",
      "[59, 149]\n",
      "[19629, 223]\n",
      "[70, 78]\n",
      "[19201, 651]\n",
      "[45, 103]\n",
      "[19602, 255]\n",
      "[74, 69]\n",
      "[19234, 623]\n",
      "[39, 104]\n",
      "[19521, 329]\n",
      "[88, 62]\n",
      "[19226, 624]\n",
      "[55, 95]\n",
      "[19481, 342]\n",
      "[75, 102]\n",
      "[19170, 653]\n",
      "[51, 126]\n",
      "[18512, 924]\n",
      "[302, 262]\n",
      "[18944, 492]\n",
      "[117, 447]\n",
      "[18557, 872]\n",
      "[293, 278]\n",
      "[18910, 519]\n",
      "[176, 395]\n",
      "[19478, 350]\n",
      "[95, 77]\n",
      "[19227, 601]\n",
      "[48, 124]\n",
      "[19603, 269]\n",
      "[79, 49]\n",
      "[19264, 608]\n",
      "[32, 96]\n",
      "[19668, 193]\n",
      "[47, 92]\n",
      "[19246, 615]\n",
      "[60, 79]\n",
      "[19598, 231]\n",
      "[85, 86]\n",
      "[19202, 627]\n",
      "[55, 116]\n",
      "[19514, 322]\n",
      "[77, 87]\n",
      "[19244, 592]\n",
      "[55, 109]\n",
      "[19027, 428]\n",
      "[221, 324]\n",
      "[18994, 461]\n",
      "[168, 377]\n",
      "[19074, 396]\n",
      "[156, 374]\n",
      "[19027, 443]\n",
      "[125, 405]\n",
      "[19671, 201]\n",
      "[61, 67]\n",
      "[19261, 611]\n",
      "[45, 83]\n",
      "[19817, 75]\n",
      "[51, 57]\n",
      "[19260, 632]\n",
      "[58, 50]\n",
      "[19736, 127]\n",
      "[62, 75]\n",
      "[19233, 630]\n",
      "[79, 58]\n",
      "[19774, 105]\n",
      "[41, 80]\n",
      "[19230, 649]\n",
      "[48, 73]\n",
      "[19704, 149]\n",
      "[65, 82]\n",
      "[19220, 633]\n",
      "[37, 110]\n",
      "[19530, 188]\n",
      "[112, 170]\n",
      "[19107, 611]\n",
      "[129, 153]\n",
      "[18254, 1087]\n",
      "[300, 359]\n",
      "[18943, 398]\n",
      "[235, 424]\n",
      "[19155, 255]\n",
      "[34, 556]\n",
      "[19007, 403]\n",
      "[48, 542]\n",
      "[18478, 975]\n",
      "[227, 320]\n",
      "[18953, 500]\n",
      "[176, 371]\n",
      "[19049, 400]\n",
      "[110, 441]\n",
      "[19007, 442]\n",
      "[101, 450]\n",
      "[18483, 1003]\n",
      "[261, 253]\n",
      "[19013, 473]\n",
      "[124, 390]\n",
      "[19133, 339]\n",
      "[43, 485]\n",
      "[19062, 410]\n",
      "[59, 469]\n",
      "[19553, 159]\n",
      "[122, 166]\n",
      "[19203, 509]\n",
      "[102, 186]\n",
      "[19556, 306]\n",
      "[73, 65]\n",
      "[19233, 629]\n",
      "[81, 57]\n",
      "[19266, 472]\n",
      "[135, 127]\n",
      "[19179, 559]\n",
      "[73, 189]\n",
      "[19478, 329]\n",
      "[99, 94]\n",
      "[19203, 604]\n",
      "[65, 128]\n",
      "[19355, 428]\n",
      "[105, 112]\n",
      "[19209, 574]\n",
      "[49, 168]\n",
      "[19404, 384]\n",
      "[115, 97]\n",
      "[19182, 606]\n",
      "[42, 170]\n",
      "[19371, 417]\n",
      "[124, 88]\n",
      "[19210, 578]\n",
      "[33, 179]\n",
      "[19700, 132]\n",
      "[80, 88]\n",
      "[19210, 622]\n",
      "[54, 114]\n",
      "[19680, 191]\n",
      "[67, 62]\n",
      "[19242, 629]\n",
      "[46, 83]\n",
      "[19571, 238]\n",
      "[88, 103]\n",
      "[19180, 629]\n",
      "[43, 148]\n",
      "[19444, 376]\n",
      "[100, 80]\n",
      "[19266, 554]\n",
      "[51, 129]\n",
      "[19501, 317]\n",
      "[94, 88]\n",
      "[19250, 568]\n",
      "[55, 127]\n",
      "[19518, 272]\n",
      "[121, 89]\n",
      "[19190, 600]\n",
      "[90, 120]\n",
      "[19645, 154]\n",
      "[83, 118]\n",
      "[19158, 641]\n",
      "[85, 116]\n",
      "[19728, 148]\n",
      "[66, 58]\n",
      "[19313, 563]\n",
      "[45, 79]\n",
      "[19671, 199]\n",
      "[71, 59]\n",
      "[19266, 604]\n",
      "[37, 93]\n",
      "[19562, 249]\n",
      "[85, 104]\n",
      "[19227, 584]\n",
      "[44, 145]\n",
      "[19591, 230]\n",
      "[78, 101]\n",
      "[19214, 607]\n",
      "[64, 115]\n",
      "[19607, 231]\n",
      "[81, 81]\n",
      "[19257, 581]\n",
      "[68, 94]\n",
      "[19659, 165]\n",
      "[103, 73]\n",
      "[19215, 609]\n",
      "[74, 102]\n",
      "[19648, 189]\n",
      "[85, 78]\n",
      "[19203, 634]\n",
      "[68, 95]\n",
      "[19784, 75]\n",
      "[69, 72]\n",
      "[19291, 568]\n",
      "[50, 91]\n",
      "[19671, 191]\n",
      "[87, 51]\n",
      "[19283, 579]\n",
      "[34, 104]\n",
      "[19591, 232]\n",
      "[86, 91]\n",
      "[19247, 576]\n",
      "[36, 141]\n",
      "[19550, 266]\n",
      "[108, 76]\n",
      "[19242, 574]\n",
      "[37, 147]\n",
      "[19493, 306]\n",
      "[106, 95]\n",
      "[19184, 615]\n",
      "[68, 133]\n",
      "[19568, 254]\n",
      "[90, 88]\n",
      "[19226, 596]\n",
      "[65, 113]\n",
      "[19625, 213]\n",
      "[85, 77]\n",
      "[19232, 606]\n",
      "[39, 123]\n",
      "[19771, 93]\n",
      "[85, 51]\n",
      "[19263, 601]\n",
      "[41, 95]\n",
      "[19483, 385]\n",
      "[80, 52]\n",
      "[19215, 653]\n",
      "[54, 78]\n",
      "[19578, 240]\n",
      "[86, 96]\n",
      "[19268, 550]\n",
      "[48, 134]\n",
      "[19353, 438]\n",
      "[109, 100]\n",
      "[19242, 549]\n",
      "[58, 151]\n",
      "[19463, 351]\n",
      "[99, 87]\n",
      "[19213, 601]\n",
      "[64, 122]\n",
      "[19354, 453]\n",
      "[115, 78]\n",
      "[19208, 599]\n",
      "[49, 144]\n",
      "[19506, 305]\n",
      "[92, 97]\n",
      "[19221, 590]\n",
      "[69, 120]\n",
      "[19678, 173]\n",
      "[77, 72]\n",
      "[19293, 558]\n",
      "[37, 112]\n",
      "[19273, 432]\n",
      "[173, 122]\n",
      "[19259, 446]\n",
      "[65, 230]\n",
      "[18520, 969]\n",
      "[290, 221]\n",
      "[19022, 467]\n",
      "[86, 425]\n",
      "[19134, 362]\n",
      "[79, 425]\n",
      "[19082, 414]\n",
      "[80, 424]\n",
      "[18467, 934]\n",
      "[287, 312]\n",
      "[18921, 480]\n",
      "[188, 411]\n",
      "[19079, 353]\n",
      "[86, 482]\n",
      "[19005, 427]\n",
      "[82, 486]\n",
      "[18534, 913]\n",
      "[326, 227]\n",
      "[18999, 448]\n",
      "[128, 425]\n",
      "[19071, 343]\n",
      "[141, 445]\n",
      "[18998, 416]\n",
      "[120, 466]\n",
      "------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds of MWPM:\n",
      "> Accuracy: 0.9740240740740888 (+- 3.447768702163095e-05)\n",
      "> F1: 0.6987362438110836(+- 0.0006966427848766909)\n",
      "> Time: 2130.54912663333 (+- 28.87378933361273)\n",
      "Average scores for all folds of PLUT:\n",
      "> Accuracy: 0.9620978645122641 (+- 1.8245896618399287e-05)\n",
      "> F1: 0.21589124360698222(+- 9.335238689092155e-05)\n",
      "> Time: 3.5051154666666666 (+- 0.022936989822894247)\n",
      "Average scores for all folds of NN:\n",
      "> Accuracy: 0.490395938517907 (+- 0.3506857778787787)\n",
      "> F1: 0.8199479563912522(+- 0.0009460669157494656)\n",
      "> Time: 2.8170823333333335 (+- 0.08801259313061718)\n",
      "> AUC for class : 0.9911802210257864 (+- 0.0)\n",
      "X^2 for MWPM and NN: 0\n",
      "X^2 for PLUT and NN: 0\n",
      "> AUC for class X00: 0.878935139960283 (+- 0.002995811242124049)\n",
      "X^2 for MWPM and NN: 38.50437549721559\n",
      "X^2 for PLUT and NN: 240.0\n",
      "> AUC for class X01: 0.9853818439381424 (+- 0.0013559272214717505)\n",
      "X^2 for MWPM and NN: 17.88908765652952\n",
      "X^2 for PLUT and NN: 234.01363636363635\n",
      "> AUC for class X02: 0.9981267108555115 (+- 0.00011003426952704032)\n",
      "X^2 for MWPM and NN: 42.98684210526316\n",
      "X^2 for PLUT and NN: 424.5665760869565\n",
      "> AUC for class X03: 0.9986232665741444 (+- 2.0419873739598897e-05)\n",
      "X^2 for MWPM and NN: 32.26815642458101\n",
      "X^2 for PLUT and NN: 520.0819912152269\n",
      "> AUC for class X04: 0.9988670469000708 (+- 0.00016714422394082857)\n",
      "X^2 for MWPM and NN: 96.99672131147541\n",
      "X^2 for PLUT and NN: 503.1496913580247\n",
      "> AUC for class X05: 0.998342352425468 (+- 7.022189606523973e-05)\n",
      "X^2 for MWPM and NN: 172.0025641025641\n",
      "X^2 for PLUT and NN: 486.12762762762765\n",
      "> AUC for class X06: 0.9804372218205288 (+- 0.00028625214168298595)\n",
      "X^2 for MWPM and NN: 599.5558845299778\n",
      "X^2 for PLUT and NN: 134.44458762886597\n",
      "> AUC for class X10: 0.9793046361634029 (+- 0.0003012496396604948)\n",
      "X^2 for MWPM and NN: 351.77226890756305\n",
      "X^2 for PLUT and NN: 184.24707602339183\n",
      "> AUC for class X11: 0.9955668540633508 (+- 0.00011933870046101642)\n",
      "X^2 for MWPM and NN: 90.10663507109005\n",
      "X^2 for PLUT and NN: 413.3346560846561\n",
      "> AUC for class X12: 0.9968931376977469 (+- 0.00018505751776284982)\n",
      "X^2 for MWPM and NN: 42.84307692307692\n",
      "X^2 for PLUT and NN: 420.5408618127786\n",
      "> AUC for class X13: 0.9979767069414173 (+- 6.302614916368488e-05)\n",
      "X^2 for MWPM and NN: 80.97991967871486\n",
      "X^2 for PLUT and NN: 459.6736\n",
      "> AUC for class X14: 0.9977934989578507 (+- 3.0072543073839292e-05)\n",
      "X^2 for MWPM and NN: 48.51567944250871\n",
      "X^2 for PLUT and NN: 495.4526315789474\n",
      "> AUC for class X15: 0.9969601962539872 (+- 0.00010245504621041482)\n",
      "X^2 for MWPM and NN: 251.1304347826087\n",
      "X^2 for PLUT and NN: 499.851411589896\n",
      "> AUC for class X16: 0.9790639915694676 (+- 0.0010666014403545672)\n",
      "X^2 for MWPM and NN: 155.8648111332008\n",
      "X^2 for PLUT and NN: 251.12387791741472\n",
      "> AUC for class X20: 0.9791931014649866 (+- 0.00023929760379031966)\n",
      "X^2 for MWPM and NN: 151.89453860640302\n",
      "X^2 for PLUT and NN: 245.9277108433735\n",
      "> AUC for class X21: 0.9964293451464893 (+- 0.0003227828904318134)\n",
      "X^2 for MWPM and NN: 260.13235294117646\n",
      "X^2 for PLUT and NN: 405.2685560053981\n",
      "> AUC for class X22: 0.9977141848166204 (+- 0.0002909341986878105)\n",
      "X^2 for MWPM and NN: 88.56333333333333\n",
      "X^2 for PLUT and NN: 468.52006172839504\n",
      "> AUC for class X23: 0.997832322526445 (+- 8.160311784070186e-05)\n",
      "X^2 for MWPM and NN: 88.47648902821317\n",
      "X^2 for PLUT and NN: 421.4449541284404\n",
      "> AUC for class X24: 0.9975686603324535 (+- 6.272322280896758e-05)\n",
      "X^2 for MWPM and NN: 142.8025\n",
      "X^2 for PLUT and NN: 447.1824925816024\n",
      "> AUC for class X25: 0.9968793350840702 (+- 3.9728417697585704e-05)\n",
      "X^2 for MWPM and NN: 99.65551181102362\n",
      "X^2 for PLUT and NN: 366.76173708920186\n",
      "> AUC for class X26: 0.9802704696189534 (+- 0.000282166813363024)\n",
      "X^2 for MWPM and NN: 416.18550955414014\n",
      "X^2 for PLUT and NN: 138.58240223463687\n",
      "> AUC for class X30: 0.9806660712655183 (+- 0.00018920780056206057)\n",
      "X^2 for MWPM and NN: 412.4638205499276\n",
      "X^2 for PLUT and NN: 209.07667210440457\n",
      "> AUC for class X31: 0.996771536626058 (+- 0.0001844871401679308)\n",
      "X^2 for MWPM and NN: 190.20119521912352\n",
      "X^2 for PLUT and NN: 391.14263803680984\n",
      "> AUC for class X32: 0.9978607451253062 (+- 6.89590201169232e-05)\n",
      "X^2 for MWPM and NN: 103.84225352112676\n",
      "X^2 for PLUT and NN: 465.82236842105266\n",
      "> AUC for class X33: 0.9979097724447028 (+- 0.00011575528134524001)\n",
      "X^2 for MWPM and NN: 93.41016949152542\n",
      "X^2 for PLUT and NN: 494.44832402234636\n",
      "> AUC for class X34: 0.9977455526232197 (+- 8.834982113950125e-05)\n",
      "X^2 for MWPM and NN: 101.56065573770492\n",
      "X^2 for PLUT and NN: 486.6625577812018\n",
      "> AUC for class X35: 0.9969591192340959 (+- 0.00018121911632818823)\n",
      "X^2 for MWPM and NN: 222.51607142857142\n",
      "X^2 for PLUT and NN: 447.22236842105264\n",
      "> AUC for class X36: 0.9794034701420927 (+- 0.0002728295301254759)\n",
      "X^2 for MWPM and NN: 39.03146853146853\n",
      "X^2 for PLUT and NN: 77.4225888324873\n",
      "> AUC for class X40: 0.9806492374228287 (+- 0.00018416472602552958)\n",
      "X^2 for MWPM and NN: 87.84808612440192\n",
      "X^2 for PLUT and NN: 139.270911360799\n",
      "> AUC for class X41: 0.9970025771933534 (+- 0.0003131955828762724)\n",
      "X^2 for MWPM and NN: 212.16329966329965\n",
      "X^2 for PLUT and NN: 456.0416047548291\n",
      "> AUC for class X42: 0.9977627918241775 (+- 0.00010397214116824311)\n",
      "X^2 for MWPM and NN: 78.8532423208191\n",
      "X^2 for PLUT and NN: 525.8979885057471\n",
      "> AUC for class X43: 0.9977928342733096 (+- 0.00012791072716265568)\n",
      "X^2 for MWPM and NN: 98.48024316109422\n",
      "X^2 for PLUT and NN: 513.4274924471299\n",
      "> AUC for class X44: 0.9978127608053257 (+- 4.490644697943772e-05)\n",
      "X^2 for MWPM and NN: 138.1294964028777\n",
      "X^2 for PLUT and NN: 475.1458026509573\n",
      "> AUC for class X45: 0.9968919805288721 (+- 0.0002898708365204268)\n",
      "X^2 for MWPM and NN: 169.67865707434052\n",
      "X^2 for PLUT and NN: 513.0696022727273\n",
      "> AUC for class X46: 0.9802354517966027 (+- 0.00017626747740946523)\n",
      "X^2 for MWPM and NN: 314.5522022838499\n",
      "X^2 for PLUT and NN: 229.68144499178982\n",
      "> AUC for class X50: 0.9801514333735515 (+- 0.0003672897690799569)\n",
      "X^2 for MWPM and NN: 286.7673819742489\n",
      "X^2 for PLUT and NN: 168.29352517985612\n",
      "> AUC for class X51: 0.9969345405296841 (+- 0.00017020439641081246)\n",
      "X^2 for MWPM and NN: 144.97977528089888\n",
      "X^2 for PLUT and NN: 469.49768875192603\n",
      "> AUC for class X52: 0.9977631644867898 (+- 9.067928397254217e-05)\n",
      "X^2 for MWPM and NN: 102.64655172413794\n",
      "X^2 for PLUT and NN: 516.6015625\n",
      "> AUC for class X53: 0.9978629385594989 (+- 0.00011169252853308241)\n",
      "X^2 for MWPM and NN: 87.60416666666667\n",
      "X^2 for PLUT and NN: 454.6903703703704\n",
      "> AUC for class X54: 0.9976957591099085 (+- 5.475041242669475e-05)\n",
      "X^2 for MWPM and NN: 66.53481012658227\n",
      "X^2 for PLUT and NN: 478.0659824046921\n",
      "> AUC for class X55: 0.9973678527809402 (+- 9.866111498445762e-05)\n",
      "X^2 for MWPM and NN: 149.21303258145363\n",
      "X^2 for PLUT and NN: 444.0432766615147\n",
      "> AUC for class X56: 0.9802248522470373 (+- 0.0004781723774092135)\n",
      "X^2 for MWPM and NN: 65.38674884437596\n",
      "X^2 for PLUT and NN: 135.55484896661366\n",
      "> AUC for class X60: 0.9811488328772757 (+- 0.0002591324525790656)\n",
      "X^2 for MWPM and NN: 103.48007246376811\n",
      "X^2 for PLUT and NN: 176.91725352112675\n",
      "> AUC for class X61: 0.9982410558667941 (+- 0.00011597135099378202)\n",
      "X^2 for MWPM and NN: 73.7442748091603\n",
      "X^2 for PLUT and NN: 486.6234756097561\n",
      "> AUC for class X62: 0.9987272624595417 (+- 0.00013668167319179)\n",
      "X^2 for MWPM and NN: 4.198412698412699\n",
      "X^2 for PLUT and NN: 475.8391304347826\n",
      "> AUC for class X63: 0.9987169322950118 (+- 7.561284075042603e-05)\n",
      "X^2 for MWPM and NN: 21.67195767195767\n",
      "X^2 for PLUT and NN: 426.657263751763\n",
      "> AUC for class X64: 0.9987014675981752 (+- 2.96297330004058e-05)\n",
      "X^2 for MWPM and NN: 27.184931506849313\n",
      "X^2 for PLUT and NN: 516.4992826398852\n",
      "> AUC for class X65: 0.997657631928473 (+- 8.630898053137502e-05)\n",
      "X^2 for MWPM and NN: 32.191588785046726\n",
      "X^2 for PLUT and NN: 528.3955223880597\n",
      "> AUC for class X66: 0.9934988335554921 (+- 0.00018652563461020983)\n",
      "X^2 for MWPM and NN: 18.75\n",
      "X^2 for PLUT and NN: 312.65\n",
      "> AUC for class Z00: 0.9734213727474402 (+- 0.0003579873526663739)\n",
      "X^2 for MWPM and NN: 445.41888968997836\n",
      "X^2 for PLUT and NN: 41.459715639810426\n",
      "> AUC for class Z01: 0.9746085417982386 (+- 0.0001641371455647055)\n",
      "X^2 for MWPM and NN: 167.47404844290656\n",
      "X^2 for PLUT and NN: 277.86252771618626\n",
      "> AUC for class Z02: 0.9769098332408445 (+- 0.0004825351478025607)\n",
      "X^2 for MWPM and NN: 464.2337770382695\n",
      "X^2 for PLUT and NN: 154.3328402366864\n",
      "> AUC for class Z03: 0.9781003710069177 (+- 0.000758778679954592)\n",
      "X^2 for MWPM and NN: 163.76666666666668\n",
      "X^2 for PLUT and NN: 212.8913443830571\n",
      "> AUC for class Z04: 0.9776610147742709 (+- 0.00019370852009331253)\n",
      "X^2 for MWPM and NN: 434.3995253164557\n",
      "X^2 for PLUT and NN: 202.85427135678393\n",
      "> AUC for class Z05: 0.9778110720153462 (+- 0.0005612848742530202)\n",
      "X^2 for MWPM and NN: 227.81413612565444\n",
      "X^2 for PLUT and NN: 261.1940298507463\n",
      "> AUC for class Z06: 0.9918750951952267 (+- 0.0003212242676006006)\n",
      "X^2 for MWPM and NN: 4.612099644128114\n",
      "X^2 for PLUT and NN: 269.7806873977087\n",
      "> AUC for class Z10: 0.9979442603489632 (+- 0.00024522626158608686)\n",
      "X^2 for MWPM and NN: 142.01583113456465\n",
      "X^2 for PLUT and NN: 421.42112676056337\n",
      "> AUC for class Z11: 0.9950313523271843 (+- 0.0001526392378588517)\n",
      "X^2 for MWPM and NN: 185.99011532125206\n",
      "X^2 for PLUT and NN: 372.1914556962025\n",
      "> AUC for class Z12: 0.9955673461218245 (+- 5.41565138438935e-05)\n",
      "X^2 for MWPM and NN: 122.52570093457943\n",
      "X^2 for PLUT and NN: 432.65171898355754\n",
      "> AUC for class Z13: 0.9957281506972521 (+- 3.3815599375441824e-05)\n",
      "X^2 for MWPM and NN: 194.52908067542214\n",
      "X^2 for PLUT and NN: 440.7319422150883\n",
      "> AUC for class Z14: 0.9959690133008299 (+- 0.00015020532763845257)\n",
      "X^2 for MWPM and NN: 143.93587174348698\n",
      "X^2 for PLUT and NN: 489.1496913580247\n",
      "> AUC for class Z15: 0.9963420975297445 (+- 0.00026162793859489566)\n",
      "X^2 for MWPM and NN: 157.60443622920516\n",
      "X^2 for PLUT and NN: 484.34697217675944\n",
      "> AUC for class Z16: 0.9972319566493724 (+- 0.00021255855900072386)\n",
      "X^2 for MWPM and NN: 12.268867924528301\n",
      "X^2 for PLUT and NN: 475.57544378698225\n",
      "> AUC for class Z20: 0.9983795978687183 (+- 0.00010027425012115967)\n",
      "X^2 for MWPM and NN: 58.63953488372093\n",
      "X^2 for PLUT and NN: 501.81333333333333\n",
      "> AUC for class Z21: 0.9965337161183451 (+- 8.856497724021597e-05)\n",
      "X^2 for MWPM and NN: 68.10122699386503\n",
      "X^2 for PLUT and NN: 509.26339285714283\n",
      "> AUC for class Z22: 0.9966826422342746 (+- 4.343934571405591e-05)\n",
      "X^2 for MWPM and NN: 158.87605042016807\n",
      "X^2 for PLUT and NN: 416.53553719008266\n",
      "> AUC for class Z23: 0.9968787003684154 (+- 0.0004488469394367882)\n",
      "X^2 for MWPM and NN: 119.91240875912409\n",
      "X^2 for PLUT and NN: 420.776886035313\n",
      "> AUC for class Z24: 0.9964244489289377 (+- 0.00021490939103290666)\n",
      "X^2 for MWPM and NN: 57.25190839694657\n",
      "X^2 for PLUT and NN: 375.47971014492754\n",
      "> AUC for class Z25: 0.9967796235819163 (+- 0.0001911597522393913)\n",
      "X^2 for MWPM and NN: 20.675105485232066\n",
      "X^2 for PLUT and NN: 424.27685950413223\n",
      "> AUC for class Z26: 0.9980708797336212 (+- 8.341361534671092e-05)\n",
      "X^2 for MWPM and NN: 30.6588785046729\n",
      "X^2 for PLUT and NN: 439.6200657894737\n",
      "> AUC for class Z30: 0.9982765236546634 (+- 6.748378599658553e-05)\n",
      "X^2 for MWPM and NN: 59.737037037037034\n",
      "X^2 for PLUT and NN: 499.77535101404055\n",
      "> AUC for class Z31: 0.9969768856017248 (+- 8.820496250750799e-05)\n",
      "X^2 for MWPM and NN: 79.54790419161677\n",
      "X^2 for PLUT and NN: 462.6130573248408\n",
      "> AUC for class Z32: 0.9962910990104746 (+- 0.00024388826692718453)\n",
      "X^2 for MWPM and NN: 74.02922077922078\n",
      "X^2 for PLUT and NN: 437.80029806259313\n",
      "> AUC for class Z33: 0.9966096340558551 (+- 7.677397421100654e-05)\n",
      "X^2 for MWPM and NN: 71.15705128205128\n",
      "X^2 for PLUT and NN: 403.91987673343607\n",
      "> AUC for class Z34: 0.9965936686598136 (+- 8.716523925885293e-05)\n",
      "X^2 for MWPM and NN: 13.884328358208956\n",
      "X^2 for PLUT and NN: 417.5051244509517\n",
      "> AUC for class Z35: 0.9971571489170185 (+- 9.221262925442705e-05)\n",
      "X^2 for MWPM and NN: 38.71897810218978\n",
      "X^2 for PLUT and NN: 454.73646723646726\n",
      "> AUC for class Z36: 0.9981071015115845 (+- 3.92628181782341e-05)\n",
      "X^2 for MWPM and NN: 0.1736111111111111\n",
      "X^2 for PLUT and NN: 432.5064724919094\n",
      "> AUC for class Z40: 0.9981757693623079 (+- 0.00010192708939042161)\n",
      "X^2 for MWPM and NN: 38.16187050359712\n",
      "X^2 for PLUT and NN: 482.76672104404565\n",
      "> AUC for class Z41: 0.9968395615496698 (+- 0.00037107818583572355)\n",
      "X^2 for MWPM and NN: 66.11635220125787\n",
      "X^2 for PLUT and NN: 474.7075163398693\n",
      "> AUC for class Z42: 0.996788668498747 (+- 0.00032569623136016523)\n",
      "X^2 for MWPM and NN: 65.90641711229947\n",
      "X^2 for PLUT and NN: 470.2062193126023\n",
      "> AUC for class Z43: 0.9965552868100512 (+- 8.490758939835948e-05)\n",
      "X^2 for MWPM and NN: 96.11893203883496\n",
      "X^2 for PLUT and NN: 436.4802342606149\n",
      "> AUC for class Z44: 0.9963108624233173 (+- 7.077284156388251e-05)\n",
      "X^2 for MWPM and NN: 77.23546511627907\n",
      "X^2 for PLUT and NN: 424.9621785173979\n",
      "> AUC for class Z45: 0.9967805738346138 (+- 3.1793015356197886e-05)\n",
      "X^2 for MWPM and NN: 54.124161073825505\n",
      "X^2 for PLUT and NN: 496.67596899224804\n",
      "> AUC for class Z46: 0.9983250345631149 (+- 3.235518531974987e-05)\n",
      "X^2 for MWPM and NN: 0.2752808988764045\n",
      "X^2 for PLUT and NN: 486.7305295950156\n",
      "> AUC for class Z50: 0.9971221739926478 (+- 0.00027866919195748197)\n",
      "X^2 for MWPM and NN: 198.74408602150538\n",
      "X^2 for PLUT and NN: 505.8048090523338\n",
      "> AUC for class Z51: 0.9961469502954025 (+- 0.00018894655788159758)\n",
      "X^2 for MWPM and NN: 71.80674846625767\n",
      "X^2 for PLUT and NN: 419.7341137123746\n",
      "> AUC for class Z52: 0.9957590995021088 (+- 0.00014107517670515436)\n",
      "X^2 for MWPM and NN: 196.6800731261426\n",
      "X^2 for PLUT and NN: 395.5518945634267\n",
      "> AUC for class Z53: 0.9956127330702121 (+- 0.00017511555185548936)\n",
      "X^2 for MWPM and NN: 140.00222222222223\n",
      "X^2 for PLUT and NN: 432.02406015037593\n",
      "> AUC for class Z54: 0.9959412198867984 (+- 0.00013094232877624217)\n",
      "X^2 for MWPM and NN: 199.94542253521126\n",
      "X^2 for PLUT and NN: 465.125\n",
      "> AUC for class Z55: 0.9955527691076642 (+- 0.0001903676026095476)\n",
      "X^2 for MWPM and NN: 113.20906801007557\n",
      "X^2 for PLUT and NN: 410.3186646433991\n",
      "> AUC for class Z56: 0.9975931825214762 (+- 0.00012292985699140753)\n",
      "X^2 for MWPM and NN: 36.1\n",
      "X^2 for PLUT and NN: 454.453781512605\n",
      "> AUC for class Z60: 0.9924089787190719 (+- 0.0006859966835069585)\n",
      "X^2 for MWPM and NN: 110.02314049586776\n",
      "X^2 for PLUT and NN: 282.5831702544031\n",
      "> AUC for class Z61: 0.977540430705114 (+- 0.00013979415591231555)\n",
      "X^2 for MWPM and NN: 365.1183478951549\n",
      "X^2 for PLUT and NN: 261.121157323689\n",
      "> AUC for class Z62: 0.9767317214356664 (+- 0.0005845718875544105)\n",
      "X^2 for MWPM and NN: 180.3265306122449\n",
      "X^2 for PLUT and NN: 224.47165991902833\n",
      "> AUC for class Z63: 0.9773378156813104 (+- 0.000203869567157774)\n",
      "X^2 for MWPM and NN: 341.7821457821458\n",
      "X^2 for PLUT and NN: 126.76796407185628\n",
      "> AUC for class Z64: 0.9772904065616276 (+- 0.00016298040749066904)\n",
      "X^2 for MWPM and NN: 161.1753986332574\n",
      "X^2 for PLUT and NN: 232.48722986247543\n",
      "> AUC for class Z65: 0.9770186675499627 (+- 0.00026188581181820555)\n",
      "X^2 for MWPM and NN: 277.1557707828894\n",
      "X^2 for PLUT and NN: 176.66840277777777\n",
      "> AUC for class Z66: 0.977421533375686 (+- 0.0008367733963069108)\n",
      "X^2 for MWPM and NN: 83.47314049586777\n",
      "X^2 for PLUT and NN: 162.36007462686567\n",
      "###################################################################################\n",
      "TOTAL F1 NN: [0.8201395877498923, 0.821418399316434, 0.8191169192493412, 0.8191169192493412]\n",
      "TOTAL F1 PLUT: [0.21600465905950167, 0.21577601491392678, 0.21589305684751825]\n",
      "TOTAL F1 MWPM: [0.6980389857833798, 0.6996876498871094, 0.6984820957627614]\n",
      "TOTAL ACC NN: [0.24212782084941864, 0.2427195906639099, 0.9863404040403925]\n",
      "TOTAL ACC PLUT: [0.9621109258977504, 0.9620720615785512, 0.9621106060604908]\n",
      "TOTAL ACC MWPM: [0.9739863636363807, 0.9740696969697087, 0.9740161616161771]\n",
      "TOTAL TIME NN: [2.9411304, 2.7462049, 2.7639117]\n",
      "TOTAL TIME PLUT: [3.5267561, 3.5152217, 3.4733686]\n",
      "TOTAL TIME MWPM: [2171.313826099998, 2112.2217765999876, 2108.1117772000052]\n",
      "------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#test MWPM decoder for this fold\n",
    "#labels = targets[train], features = inputs[train]\n",
    "# x_test_d7 = translate_to_graph(testData_d7_MWPM, targs, mlb_d7)\n",
    "\"\"\"\n",
    "decoding_d7, time_mwpm = do_new_decoding(x_test_d7, 7, 0)\n",
    "decoding_d7['combine'] = decoding_d7[[0, 1]].values.tolist()\n",
    "decoding_d7['combine'].apply(lambda x: x[0].extend(x[1]))\n",
    "decoding_d7 = np.array(decoding_d7[0])\n",
    "\n",
    "time_per_fold_mwpm.append(time_mwpm)\n",
    "\n",
    "pred_mwpm = mlb_d7.transform(decoding_d7)\n",
    "\n",
    "if fold_no < 3:\n",
    "    acc = partial_accuracy(targets_test_2, pred_mwpm)\n",
    "else:\n",
    "    acc, contingency_mwpm = partial_accuracy_and_contingency(targets_test_2, pred_mwpm, mlb_d7)\"\"\"\n",
    "\n",
    "#acc_per_fold_mwpm.append(acc)\n",
    "#f1_per_fold_mwpm.append(f1_score(targets_test_2, pred_mwpm, average='micro'))\n",
    "\n",
    "#####################################################################################################\n",
    "#test the plut decoder for this fold\n",
    "\n",
    "#lookup_d7 = lookup_decoder(7)\n",
    "\n",
    "#lookup_d7 = train_plut(lookup_d7, inputs_train, targets[train])\n",
    "\n",
    "#start = time.time_ns()\n",
    "#pred_plut_d7 = test_plut(lookup_d7, inputs_test)\n",
    "#end = time.time_ns() \n",
    "#time_per_fold_plut.append((end - start)/ (10 ** 9))\n",
    "\n",
    "if fold_no < 3:\n",
    "    acc = partial_accuracy(targets[test], pred_plut_d7)\n",
    "    f1 = f1_score(targets[test], pred_plut_d7, average='micro')\n",
    "else:\n",
    "    pred_plut_d7 = test_plut(lookup_d7, inputs_test_2)\n",
    "    #f1 = f1_score(targets_test_2, pred_plut_d7, average='micro')\n",
    "    acc, contingency_plut = partial_accuracy_and_contingency(targets_test_2, pred_plut_d7, mlb_d7)\n",
    "\n",
    "#acc_per_fold_plut.append(acc)\n",
    "#f1_per_fold_plut.append(f1)\n",
    "\n",
    "#####################################################################################################\n",
    "#Test the NN decoder for this fold\n",
    "\"\"\"\n",
    "model_d7 = compile_FFNN_model_DepthSeven(7)\n",
    "\n",
    "# Generate a print\n",
    "print('------------------------------------------------------------------------')\n",
    "print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "# Fit data to model\n",
    "history = model_d7.fit(\n",
    "    x=inputs_train ,\n",
    "    y=targets[train],\n",
    "    validation_split=.25,\n",
    "    epochs= 150)\"\"\"\n",
    "inputs_test_2 = np.asarray(inputs_test_2).astype('int32')\n",
    "pred = model_d7.predict(inputs_test_2)\n",
    "pred[pred>=.5]=1 \n",
    "pred[pred<.5]=0\n",
    "acc, contingency_nn = partial_accuracy_and_contingency(targets_test_2, pred, mlb_d7)\n",
    "f1 = f1_score(targets_test_2, pred, average='micro')\n",
    "\n",
    "#acc_per_fold.append(acc)\n",
    "f1_per_fold.append(f1)\n",
    "\n",
    "# Increase fold number\n",
    "fold_no = fold_no + 1\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(targets[test].ravel(), predictions_d7.ravel())\n",
    "tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "tprs[-1][0] = 0.0\n",
    "roc_auc = auc(fpr, tpr)\n",
    "aucs.append(roc_auc)\n",
    "plt.plot(fpr, tpr, lw=1, alpha=0.3, label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
    "\n",
    "#get the AUCs of each class, used to get average AUC of each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(targets[test][:, i], predictions_d7[:, i]) \n",
    "    aucs_classes[mlb_d7.classes_[i]].append(auc(fpr[i], tpr[i]))\n",
    "\n",
    "#########################################################################################################\n",
    "#compute McNemar's statistic on results of last fold\n",
    "\n",
    "mcnemar_results_mwpm = {}\n",
    "mcnemar_results_plut = {}\n",
    "\n",
    "for class_ in mlb_d7.classes_:\n",
    "    \n",
    "    #compute the x^2 for NN and MWPM\n",
    "    mcnemar_results_mwpm[class_]=contingency_table_and_t(contingency_nn[class_], contingency_mwpm[class_])[1]    \n",
    "    #comput the x^2 for NN and PLUT\n",
    "    mcnemar_results_plut[class_] = contingency_table_and_t(contingency_nn[class_], contingency_plut[class_])[1]\n",
    "        \n",
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds of MWPM:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold_mwpm)} (+- {np.std(acc_per_fold_mwpm)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold_mwpm)}(+- {np.std(f1_per_fold_mwpm)})')\n",
    "print(f'> Time: {np.mean(time_per_fold_mwpm)} (+- {np.std(time_per_fold_mwpm)})')\n",
    "print('Average scores for all folds of PLUT:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold_plut)} (+- {np.std(acc_per_fold_plut)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold_plut)}(+- {np.std(f1_per_fold_plut)})')\n",
    "print(f'> Time: {np.mean(time_per_fold_plut)} (+- {np.std(time_per_fold_plut)})')\n",
    "print('Average scores for all folds of NN:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold)}(+- {np.std(f1_per_fold)})')\n",
    "print(f'> Time: {np.mean(time_per_fold)} (+- {np.std(time_per_fold)})')\n",
    "for key in aucs_classes:\n",
    "    cleanedList = [x for x in aucs_classes[key] if str(x) != 'nan']\n",
    "    print(f'> AUC for class {key}: {np.mean(cleanedList)} (+- {np.std(cleanedList)})') #this has to be a for loop\n",
    "    print(\"X^2 for MWPM and NN: \" + str(mcnemar_results_mwpm[key]))  \n",
    "    print(\"X^2 for PLUT and NN: \" + str(mcnemar_results_plut[key]))\n",
    "print(\"###################################################################################\")\n",
    "print(\"TOTAL F1 NN: \" + str(f1_per_fold))\n",
    "print(\"TOTAL F1 PLUT: \" + str(f1_per_fold_plut))\n",
    "print(\"TOTAL F1 MWPM: \" + str(f1_per_fold_mwpm))\n",
    "print(\"TOTAL ACC NN: \" + str(acc_per_fold))\n",
    "print(\"TOTAL ACC PLUT: \" + str(acc_per_fold_plut))\n",
    "print(\"TOTAL ACC MWPM: \" + str(acc_per_fold_mwpm))\n",
    "print(\"TOTAL TIME NN: \" + str(time_per_fold))\n",
    "print(\"TOTAL TIME PLUT: \" + str(time_per_fold_plut))\n",
    "print(\"TOTAL TIME MWPM: \" + str(time_per_fold_mwpm))\n",
    "print('------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "         label='Chance', alpha=.8)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "         label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "         lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                 label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "plt.xlim([-0.01, 1.01])\n",
    "plt.ylim([-0.01, 1.01])\n",
    "plt.xlabel('False Positive Rate',fontsize=18)\n",
    "plt.ylabel('True Positive Rate',fontsize=18)\n",
    "plt.title('Cross-Validation ROC of Depth 7 NN',fontsize=18)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "model_d5 = compile_FFNN_model_DepthFive(5)\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "print(\"Fit model on training data\")\n",
    "history = model_d5.fit(\n",
    "    x=x_train_d5.values,\n",
    "    y=Y_train_d5,\n",
    "    validation_split=.25,\n",
    "    epochs = 500\n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Make a quick epoch vs. loss plot\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.grid()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Depth 5 Model Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (MSE)')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Make a quick epoch vs. accuracy plot\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.grid()\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Depth 5 Model Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# predict\n",
    "from sklearn.metrics import precision_score,accuracy_score, f1_score, recall_score, hamming_loss\n",
    "\n",
    "start = time.time()\n",
    "predictions_d5 = model_d5.predict(x_test_d5.values)\n",
    "end = time.time()\n",
    "print(\"Time: \" + str(end - start))\n",
    "\n",
    "pred=predictions_d5.copy()\n",
    "\n",
    "thresholds=[0.1, 0.2, 0.3, .4, .5, .6, .7, .8, .9]\n",
    "for val in thresholds:\n",
    "    pred=predictions_d5.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "    \n",
    "  \n",
    "    precision = precision_score(Y_test_d5, pred, average='micro')\n",
    "    recall = recall_score(Y_test_d5, pred, average='micro')\n",
    "    f1 = f1_score(Y_test_d5, pred, average='micro')\n",
    "   \n",
    "    print(\"Micro-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "    print(\"Accuracy = \",accuracy_score(Y_test_d5, pred))\n",
    "    print(\"Accuracy = \",partial_accuracy(Y_test_d5, pred))\n",
    "    print(\"\\n\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "pred=predictions_d5.copy()\n",
    "from sklearn.metrics import multilabel_confusion_matrix, classification_report\n",
    "#look at confusion matrix to see what got misclassified    \n",
    "pred[pred>=.5]=1\n",
    "pred[pred<.5]=0\n",
    "multilabel_confusion_matrix(Y_test_d5, pred)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#look at classifcation report to see what got mislabeled\n",
    "print(classification_report(Y_test_d5, pred))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit model on training data\n",
      "Epoch 1/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.1609 - accuracy: 0.0565 - val_loss: 0.1849 - val_accuracy: 0.1491\n",
      "Epoch 2/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.1397 - accuracy: 0.1358 - val_loss: 0.1502 - val_accuracy: 0.2154\n",
      "Epoch 3/150\n",
      "7941/7941 [==============================] - 16s 2ms/step - loss: 0.1116 - accuracy: 0.1754 - val_loss: 0.1202 - val_accuracy: 0.2970\n",
      "Epoch 4/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0914 - accuracy: 0.2068 - val_loss: 0.1046 - val_accuracy: 0.3019\n",
      "Epoch 5/150\n",
      "7941/7941 [==============================] - 17s 2ms/step - loss: 0.0798 - accuracy: 0.2268 - val_loss: 0.0954 - val_accuracy: 0.3104\n",
      "Epoch 6/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0721 - accuracy: 0.2313 - val_loss: 0.0888 - val_accuracy: 0.3180\n",
      "Epoch 7/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0669 - accuracy: 0.2302 - val_loss: 0.0839 - val_accuracy: 0.3146\n",
      "Epoch 8/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0631 - accuracy: 0.2289 - val_loss: 0.0805 - val_accuracy: 0.3237\n",
      "Epoch 9/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0602 - accuracy: 0.2285 - val_loss: 0.0779 - val_accuracy: 0.3113\n",
      "Epoch 10/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0578 - accuracy: 0.2286 - val_loss: 0.0748 - val_accuracy: 0.3113\n",
      "Epoch 11/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0557 - accuracy: 0.2295 - val_loss: 0.0730 - val_accuracy: 0.3250\n",
      "Epoch 12/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0539 - accuracy: 0.2300 - val_loss: 0.0708 - val_accuracy: 0.3242\n",
      "Epoch 13/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0522 - accuracy: 0.2306 - val_loss: 0.0685 - val_accuracy: 0.3223\n",
      "Epoch 14/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0508 - accuracy: 0.2316 - val_loss: 0.0672 - val_accuracy: 0.3277\n",
      "Epoch 15/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0497 - accuracy: 0.2315 - val_loss: 0.0659 - val_accuracy: 0.3283\n",
      "Epoch 16/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0486 - accuracy: 0.2317 - val_loss: 0.0640 - val_accuracy: 0.3279\n",
      "Epoch 17/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0477 - accuracy: 0.2324 - val_loss: 0.0635 - val_accuracy: 0.3254\n",
      "Epoch 18/150\n",
      "7941/7941 [==============================] - 16s 2ms/step - loss: 0.0469 - accuracy: 0.2332 - val_loss: 0.0618 - val_accuracy: 0.3184\n",
      "Epoch 19/150\n",
      "7941/7941 [==============================] - 17s 2ms/step - loss: 0.0462 - accuracy: 0.2333 - val_loss: 0.0615 - val_accuracy: 0.3252\n",
      "Epoch 20/150\n",
      "7941/7941 [==============================] - 18s 2ms/step - loss: 0.0455 - accuracy: 0.2333 - val_loss: 0.0607 - val_accuracy: 0.3226\n",
      "Epoch 21/150\n",
      "7941/7941 [==============================] - 17s 2ms/step - loss: 0.0449 - accuracy: 0.2330 - val_loss: 0.0596 - val_accuracy: 0.3134\n",
      "Epoch 22/150\n",
      "7941/7941 [==============================] - 16s 2ms/step - loss: 0.0444 - accuracy: 0.2329 - val_loss: 0.0589 - val_accuracy: 0.3155\n",
      "Epoch 23/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0439 - accuracy: 0.2329 - val_loss: 0.0585 - val_accuracy: 0.3286\n",
      "Epoch 24/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0434 - accuracy: 0.2316 - val_loss: 0.0583 - val_accuracy: 0.3213\n",
      "Epoch 25/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0429 - accuracy: 0.2313 - val_loss: 0.0571 - val_accuracy: 0.3240\n",
      "Epoch 26/150\n",
      "7941/7941 [==============================] - 16s 2ms/step - loss: 0.0425 - accuracy: 0.2309 - val_loss: 0.0565 - val_accuracy: 0.3119\n",
      "Epoch 27/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0422 - accuracy: 0.2310 - val_loss: 0.0559 - val_accuracy: 0.3132\n",
      "Epoch 28/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0418 - accuracy: 0.2305 - val_loss: 0.0551 - val_accuracy: 0.3098\n",
      "Epoch 29/150\n",
      "7941/7941 [==============================] - 16s 2ms/step - loss: 0.0415 - accuracy: 0.2305 - val_loss: 0.0559 - val_accuracy: 0.3244\n",
      "Epoch 30/150\n",
      "7941/7941 [==============================] - 16s 2ms/step - loss: 0.0411 - accuracy: 0.2296 - val_loss: 0.0553 - val_accuracy: 0.2960\n",
      "Epoch 31/150\n",
      "7941/7941 [==============================] - 16s 2ms/step - loss: 0.0408 - accuracy: 0.2283 - val_loss: 0.0544 - val_accuracy: 0.3092\n",
      "Epoch 32/150\n",
      "7941/7941 [==============================] - 16s 2ms/step - loss: 0.0405 - accuracy: 0.2296 - val_loss: 0.0543 - val_accuracy: 0.3129\n",
      "Epoch 33/150\n",
      "7941/7941 [==============================] - 17s 2ms/step - loss: 0.0402 - accuracy: 0.2277 - val_loss: 0.0538 - val_accuracy: 0.3110\n",
      "Epoch 34/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0400 - accuracy: 0.2278 - val_loss: 0.0533 - val_accuracy: 0.3005\n",
      "Epoch 35/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0397 - accuracy: 0.2280 - val_loss: 0.0528 - val_accuracy: 0.3006\n",
      "Epoch 36/150\n",
      "7941/7941 [==============================] - 16s 2ms/step - loss: 0.0395 - accuracy: 0.2277 - val_loss: 0.0531 - val_accuracy: 0.3291\n",
      "Epoch 37/150\n",
      "7941/7941 [==============================] - 17s 2ms/step - loss: 0.0392 - accuracy: 0.2275 - val_loss: 0.0527 - val_accuracy: 0.2974\n",
      "Epoch 38/150\n",
      "7941/7941 [==============================] - 17s 2ms/step - loss: 0.0390 - accuracy: 0.2274 - val_loss: 0.0517 - val_accuracy: 0.2973\n",
      "Epoch 39/150\n",
      "7941/7941 [==============================] - 18s 2ms/step - loss: 0.0388 - accuracy: 0.2285 - val_loss: 0.0511 - val_accuracy: 0.3039\n",
      "Epoch 40/150\n",
      "7941/7941 [==============================] - 19s 2ms/step - loss: 0.0386 - accuracy: 0.2272 - val_loss: 0.0519 - val_accuracy: 0.3126\n",
      "Epoch 41/150\n",
      "7941/7941 [==============================] - 17s 2ms/step - loss: 0.0384 - accuracy: 0.2275 - val_loss: 0.0513 - val_accuracy: 0.3097\n",
      "Epoch 42/150\n",
      "7941/7941 [==============================] - 20s 3ms/step - loss: 0.0382 - accuracy: 0.2271 - val_loss: 0.0524 - val_accuracy: 0.3158\n",
      "Epoch 43/150\n",
      "7941/7941 [==============================] - 17s 2ms/step - loss: 0.0380 - accuracy: 0.2267 - val_loss: 0.0513 - val_accuracy: 0.3136\n",
      "Epoch 44/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0378 - accuracy: 0.2268 - val_loss: 0.0505 - val_accuracy: 0.3031\n",
      "Epoch 45/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0376 - accuracy: 0.2270 - val_loss: 0.0502 - val_accuracy: 0.3175\n",
      "Epoch 46/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0374 - accuracy: 0.2263 - val_loss: 0.0510 - val_accuracy: 0.3231\n",
      "Epoch 47/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0372 - accuracy: 0.2270 - val_loss: 0.0508 - val_accuracy: 0.3024\n",
      "Epoch 48/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0371 - accuracy: 0.2262 - val_loss: 0.0500 - val_accuracy: 0.3217\n",
      "Epoch 49/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0369 - accuracy: 0.2255 - val_loss: 0.0504 - val_accuracy: 0.3051\n",
      "Epoch 50/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0368 - accuracy: 0.2261 - val_loss: 0.0492 - val_accuracy: 0.3039\n",
      "Epoch 51/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0366 - accuracy: 0.2265 - val_loss: 0.0496 - val_accuracy: 0.3096\n",
      "Epoch 52/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0365 - accuracy: 0.2261 - val_loss: 0.0496 - val_accuracy: 0.3186\n",
      "Epoch 53/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0363 - accuracy: 0.2263 - val_loss: 0.0496 - val_accuracy: 0.3067\n",
      "Epoch 54/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0362 - accuracy: 0.2252 - val_loss: 0.0494 - val_accuracy: 0.3141\n",
      "Epoch 55/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0361 - accuracy: 0.2254 - val_loss: 0.0488 - val_accuracy: 0.3083\n",
      "Epoch 56/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0359 - accuracy: 0.2250 - val_loss: 0.0489 - val_accuracy: 0.3214\n",
      "Epoch 57/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0358 - accuracy: 0.2259 - val_loss: 0.0486 - val_accuracy: 0.3054\n",
      "Epoch 58/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0357 - accuracy: 0.2248 - val_loss: 0.0485 - val_accuracy: 0.3152\n",
      "Epoch 59/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0355 - accuracy: 0.2245 - val_loss: 0.0487 - val_accuracy: 0.3117\n",
      "Epoch 60/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0354 - accuracy: 0.2244 - val_loss: 0.0485 - val_accuracy: 0.3269\n",
      "Epoch 61/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0353 - accuracy: 0.2250 - val_loss: 0.0487 - val_accuracy: 0.2980\n",
      "Epoch 62/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0352 - accuracy: 0.2242 - val_loss: 0.0478 - val_accuracy: 0.3075\n",
      "Epoch 63/150\n",
      "7941/7941 [==============================] - 17s 2ms/step - loss: 0.0351 - accuracy: 0.2245 - val_loss: 0.0480 - val_accuracy: 0.3011\n",
      "Epoch 64/150\n",
      "7941/7941 [==============================] - 16s 2ms/step - loss: 0.0350 - accuracy: 0.2247 - val_loss: 0.0478 - val_accuracy: 0.3168\n",
      "Epoch 65/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0349 - accuracy: 0.2232 - val_loss: 0.0477 - val_accuracy: 0.3110\n",
      "Epoch 66/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0348 - accuracy: 0.2243 - val_loss: 0.0476 - val_accuracy: 0.3198\n",
      "Epoch 67/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0346 - accuracy: 0.2238 - val_loss: 0.0477 - val_accuracy: 0.3121\n",
      "Epoch 68/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0346 - accuracy: 0.2243 - val_loss: 0.0476 - val_accuracy: 0.3156\n",
      "Epoch 69/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0345 - accuracy: 0.2236 - val_loss: 0.0477 - val_accuracy: 0.3089\n",
      "Epoch 70/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0344 - accuracy: 0.2235 - val_loss: 0.0475 - val_accuracy: 0.3173\n",
      "Epoch 71/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0343 - accuracy: 0.2230 - val_loss: 0.0474 - val_accuracy: 0.3063\n",
      "Epoch 72/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0342 - accuracy: 0.2236 - val_loss: 0.0474 - val_accuracy: 0.3213\n",
      "Epoch 73/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0341 - accuracy: 0.2232 - val_loss: 0.0475 - val_accuracy: 0.3124\n",
      "Epoch 74/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0340 - accuracy: 0.2226 - val_loss: 0.0470 - val_accuracy: 0.2936\n",
      "Epoch 75/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0339 - accuracy: 0.2223 - val_loss: 0.0466 - val_accuracy: 0.3128\n",
      "Epoch 76/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0338 - accuracy: 0.2230 - val_loss: 0.0471 - val_accuracy: 0.3130\n",
      "Epoch 77/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0337 - accuracy: 0.2225 - val_loss: 0.0467 - val_accuracy: 0.3022\n",
      "Epoch 78/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0336 - accuracy: 0.2230 - val_loss: 0.0467 - val_accuracy: 0.3190\n",
      "Epoch 79/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0336 - accuracy: 0.2228 - val_loss: 0.0463 - val_accuracy: 0.3004\n",
      "Epoch 80/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0335 - accuracy: 0.2229 - val_loss: 0.0475 - val_accuracy: 0.3146\n",
      "Epoch 81/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0334 - accuracy: 0.2219 - val_loss: 0.0468 - val_accuracy: 0.3274\n",
      "Epoch 82/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0333 - accuracy: 0.2228 - val_loss: 0.0469 - val_accuracy: 0.3368\n",
      "Epoch 83/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0332 - accuracy: 0.2212 - val_loss: 0.0467 - val_accuracy: 0.3181\n",
      "Epoch 84/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0332 - accuracy: 0.2231 - val_loss: 0.0460 - val_accuracy: 0.3171\n",
      "Epoch 85/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0331 - accuracy: 0.2220 - val_loss: 0.0461 - val_accuracy: 0.3154\n",
      "Epoch 86/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0330 - accuracy: 0.2218 - val_loss: 0.0470 - val_accuracy: 0.3071\n",
      "Epoch 87/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0330 - accuracy: 0.2216 - val_loss: 0.0464 - val_accuracy: 0.3094\n",
      "Epoch 88/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0329 - accuracy: 0.2216 - val_loss: 0.0457 - val_accuracy: 0.3109\n",
      "Epoch 89/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0328 - accuracy: 0.2219 - val_loss: 0.0461 - val_accuracy: 0.3019\n",
      "Epoch 90/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0327 - accuracy: 0.2220 - val_loss: 0.0455 - val_accuracy: 0.3017\n",
      "Epoch 91/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0327 - accuracy: 0.2216 - val_loss: 0.0460 - val_accuracy: 0.3069\n",
      "Epoch 92/150\n",
      "7941/7941 [==============================] - 12s 2ms/step - loss: 0.0326 - accuracy: 0.2209 - val_loss: 0.0456 - val_accuracy: 0.3074\n",
      "Epoch 93/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0325 - accuracy: 0.2212 - val_loss: 0.0459 - val_accuracy: 0.3052\n",
      "Epoch 94/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0325 - accuracy: 0.2215 - val_loss: 0.0459 - val_accuracy: 0.2854\n",
      "Epoch 95/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0324 - accuracy: 0.2216 - val_loss: 0.0448 - val_accuracy: 0.2866\n",
      "Epoch 96/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0323 - accuracy: 0.2216 - val_loss: 0.0455 - val_accuracy: 0.3043\n",
      "Epoch 97/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0323 - accuracy: 0.2215 - val_loss: 0.0454 - val_accuracy: 0.3175\n",
      "Epoch 98/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0322 - accuracy: 0.2207 - val_loss: 0.0450 - val_accuracy: 0.3151\n",
      "Epoch 99/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0322 - accuracy: 0.2201 - val_loss: 0.0458 - val_accuracy: 0.3246\n",
      "Epoch 100/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0321 - accuracy: 0.2208 - val_loss: 0.0449 - val_accuracy: 0.3044\n",
      "Epoch 101/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0320 - accuracy: 0.2206 - val_loss: 0.0453 - val_accuracy: 0.2868\n",
      "Epoch 102/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0320 - accuracy: 0.2205 - val_loss: 0.0444 - val_accuracy: 0.3116\n",
      "Epoch 103/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0319 - accuracy: 0.2213 - val_loss: 0.0445 - val_accuracy: 0.2931\n",
      "Epoch 104/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0319 - accuracy: 0.2203 - val_loss: 0.0449 - val_accuracy: 0.3096\n",
      "Epoch 105/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0318 - accuracy: 0.2201 - val_loss: 0.0445 - val_accuracy: 0.2962\n",
      "Epoch 106/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0317 - accuracy: 0.2206 - val_loss: 0.0453 - val_accuracy: 0.2932\n",
      "Epoch 107/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0317 - accuracy: 0.2205 - val_loss: 0.0446 - val_accuracy: 0.3221\n",
      "Epoch 108/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0316 - accuracy: 0.2199 - val_loss: 0.0449 - val_accuracy: 0.3204\n",
      "Epoch 109/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0316 - accuracy: 0.2210 - val_loss: 0.0442 - val_accuracy: 0.2888\n",
      "Epoch 110/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0315 - accuracy: 0.2200 - val_loss: 0.0442 - val_accuracy: 0.3117\n",
      "Epoch 111/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0315 - accuracy: 0.2209 - val_loss: 0.0452 - val_accuracy: 0.3117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0314 - accuracy: 0.2211 - val_loss: 0.0441 - val_accuracy: 0.2996\n",
      "Epoch 113/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0314 - accuracy: 0.2203 - val_loss: 0.0447 - val_accuracy: 0.3197\n",
      "Epoch 114/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0313 - accuracy: 0.2204 - val_loss: 0.0446 - val_accuracy: 0.2980\n",
      "Epoch 115/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0313 - accuracy: 0.2206 - val_loss: 0.0441 - val_accuracy: 0.2935\n",
      "Epoch 116/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0312 - accuracy: 0.2205 - val_loss: 0.0440 - val_accuracy: 0.3194\n",
      "Epoch 117/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0312 - accuracy: 0.2197 - val_loss: 0.0448 - val_accuracy: 0.3178\n",
      "Epoch 118/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0311 - accuracy: 0.2206 - val_loss: 0.0441 - val_accuracy: 0.3080\n",
      "Epoch 119/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0311 - accuracy: 0.2202 - val_loss: 0.0439 - val_accuracy: 0.3187\n",
      "Epoch 120/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0310 - accuracy: 0.2202 - val_loss: 0.0444 - val_accuracy: 0.3342\n",
      "Epoch 121/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0310 - accuracy: 0.2198 - val_loss: 0.0445 - val_accuracy: 0.3110\n",
      "Epoch 122/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0309 - accuracy: 0.2198 - val_loss: 0.0441 - val_accuracy: 0.3085\n",
      "Epoch 123/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0309 - accuracy: 0.2212 - val_loss: 0.0441 - val_accuracy: 0.3015\n",
      "Epoch 124/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0308 - accuracy: 0.2196 - val_loss: 0.0435 - val_accuracy: 0.3056\n",
      "Epoch 125/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0308 - accuracy: 0.2205 - val_loss: 0.0439 - val_accuracy: 0.3264\n",
      "Epoch 126/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0307 - accuracy: 0.2199 - val_loss: 0.0437 - val_accuracy: 0.2958\n",
      "Epoch 127/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0307 - accuracy: 0.2202 - val_loss: 0.0438 - val_accuracy: 0.3185\n",
      "Epoch 128/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0306 - accuracy: 0.2207 - val_loss: 0.0442 - val_accuracy: 0.2939\n",
      "Epoch 129/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0306 - accuracy: 0.2209 - val_loss: 0.0434 - val_accuracy: 0.2998\n",
      "Epoch 130/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0306 - accuracy: 0.2205 - val_loss: 0.0433 - val_accuracy: 0.3093\n",
      "Epoch 131/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0305 - accuracy: 0.2205 - val_loss: 0.0435 - val_accuracy: 0.3044\n",
      "Epoch 132/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0305 - accuracy: 0.2208 - val_loss: 0.0435 - val_accuracy: 0.3043\n",
      "Epoch 133/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0304 - accuracy: 0.2206 - val_loss: 0.0440 - val_accuracy: 0.3198\n",
      "Epoch 134/150\n",
      "7941/7941 [==============================] - 14s 2ms/step - loss: 0.0304 - accuracy: 0.2203 - val_loss: 0.0440 - val_accuracy: 0.3118\n",
      "Epoch 135/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0303 - accuracy: 0.2196 - val_loss: 0.0432 - val_accuracy: 0.3143\n",
      "Epoch 136/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0303 - accuracy: 0.2213 - val_loss: 0.0437 - val_accuracy: 0.2975\n",
      "Epoch 137/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0303 - accuracy: 0.2195 - val_loss: 0.0428 - val_accuracy: 0.3151\n",
      "Epoch 138/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0302 - accuracy: 0.2210 - val_loss: 0.0437 - val_accuracy: 0.3288\n",
      "Epoch 139/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0302 - accuracy: 0.2201 - val_loss: 0.0429 - val_accuracy: 0.3216\n",
      "Epoch 140/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0301 - accuracy: 0.2194 - val_loss: 0.0436 - val_accuracy: 0.3050\n",
      "Epoch 141/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0301 - accuracy: 0.2211 - val_loss: 0.0432 - val_accuracy: 0.3066\n",
      "Epoch 142/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0301 - accuracy: 0.2193 - val_loss: 0.0427 - val_accuracy: 0.3151\n",
      "Epoch 143/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0300 - accuracy: 0.2209 - val_loss: 0.0425 - val_accuracy: 0.3170\n",
      "Epoch 144/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0300 - accuracy: 0.2212 - val_loss: 0.0432 - val_accuracy: 0.3236\n",
      "Epoch 145/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0300 - accuracy: 0.2214 - val_loss: 0.0429 - val_accuracy: 0.3170\n",
      "Epoch 146/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0299 - accuracy: 0.2204 - val_loss: 0.0430 - val_accuracy: 0.3100\n",
      "Epoch 147/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0299 - accuracy: 0.2207 - val_loss: 0.0430 - val_accuracy: 0.3003\n",
      "Epoch 148/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0298 - accuracy: 0.2204 - val_loss: 0.0428 - val_accuracy: 0.3095\n",
      "Epoch 149/150\n",
      "7941/7941 [==============================] - 15s 2ms/step - loss: 0.0298 - accuracy: 0.2207 - val_loss: 0.0435 - val_accuracy: 0.2978\n",
      "Epoch 150/150\n",
      "7941/7941 [==============================] - 13s 2ms/step - loss: 0.0298 - accuracy: 0.2199 - val_loss: 0.0431 - val_accuracy: 0.3230\n"
     ]
    }
   ],
   "source": [
    "model_d7 = compile_FFNN_model_DepthSeven(7)\n",
    "print(\"Fit model on training data\")\n",
    "history = model_d7.fit(\n",
    "    x=inputs_train,\n",
    "    y=targets[train],\n",
    "    validation_split=.25,\n",
    "    epochs= 150\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss (MSE)')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make a quick epoch vs. loss plot\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.grid()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Depth 7 Model Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (MSE)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Accuracy')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmcAAAJcCAYAAAC8DwN/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAACNDklEQVR4nO3dd3hc5ZU/8O/RqPdq2Zbcuw3GnQ6mhhIgjYRUUkmyyW6y6WV/KWzapvcEQlgISRYCBHASCKEJTHfBBfduS5YsyZKsLk15f3+cezV3RjPSzGiuNJa+n+fRM5o77epqyplz3vO+YowBEREREaWGtLHeASIiIiIKYnBGRERElEIYnBERERGlEAZnRERERCmEwRkRERFRCmFwRkRERJRCGJwR0agTkcMicvlY70esRGSmiBgRSY/huu8XkedHY7+IaHxicEY0wVmBUo+IdIhIm4i8KCIfE5GkvD+IyF0i8q0R3P4rItLp+OkRkYCIlEe5/mER6Q+/XEReswKsmYnuS7KISL71tzw21vtCRKmHwRkRAcB1xpgCADMAfA/AFwH8fmx3SRljvmOMybd/APwPgBpjTPMQNzsE4J32GRE5E0Cuy7saj7cC6ANwhYhMHs0HjiX7R0Rji8EZEQ0wxpwyxqwD8A4AN4vIGQAgIlki8kMROSoiJ0TktyKSY122VkRqrQxXs5W5erd12S0A3g3gC1am6G+Oh1smIttE5JSI3Cci2cPtn4gIgPcBuHuYq95jXc92M4A/hN1XkYj8QUSaROSIiPyXnS0UEY/19zaLyEEA10a47e9FpF5E6kTkWyLiGW7/w/bntwC2AXhP2H1fYGUv20TkmIi839qeIyI/svb1lIg8b21bKyK1YfcxUDYWkW+IyAMi8kcRaQfwfhFZIyIvWY9RLyK/FJFMx+2XiMgTItJi/b+/IiKTRaRbRMoc11thHb+MOP52IhoGgzMiGsQY8yqAWgAXWpu+B2A+gGUA5gKoAvA1x00mAyi3tt8M4HYRWWCMuR3AnwB838p8Xee4zdsBXAVgFoClAN4fw65dCGASgAeHud7LAApFZJEVNN0E4I9h1/kFgCIAswFcDA3mPmBd9hEAbwSwHMAqAG8Lu+1dAHzQY7EcwJUAPhzD/kNEZgBYCz0uf4IjiLQue8zatwro8d5iXfxDACsBnAegFMAXAARieUwANwB4AECx9Zh+AP8J/Z+dC+AyAP9m7UMBgCcB/BPAVOtvfMoY0wCgBvp/s70XwL3GGG+M+0FEMWBwRkTRHAdQamWrbgHwn8aYFmNMB4DvQAMep/9njOkzxjwL4B8I/RCP5OfGmOPGmBYAf4MGIsO5GcADxpjOGK5rZ8+uALALQJ19gSNg+7IxpsMYcxjAj6DBBqx9/6kx5pi1f9913LYSwDUAPm2M6TLGNAL4CQYfj2jeC2CbMWYngHsBLBGR5dZl7wLwpDHm/4wxXmPMSWPMFiuj90EAnzLG1Blj/MaYF40xfTE+5kvGmIeNMQFjTI8xZpMx5mVjjM/622+DBqiABqUNxpgfGWN6rePzinXZ3bAyfdYxfCf0OBNREnHsARFFUwWgBZrByQWwSeM0AIAAcJbxWo0xXY7zR6BZl6E0OH7vHu76IpIL4EZoFigW9wB4DpqZ+0PYZeUAMqz9tB2B/s2w9uVY2GW2GdZt6x3HIy3s+kN5H4DfAYAxpk5EnoUGna8BmAbgQITblAPIjnJZLEL2TUTmA/gxNCuYC/0s2GRdHG0fAOARAL8VkVkAFgA4ZWVZiSiJmDkjokFEZDU0UHkeQDOAHgBLjDHF1k+RNTjfViIieY7z06GZNwAwSdqtN0ODxZpYrmyMOQJtDLgGwF/DLm4G4IUGWrbpCGbX6qFBivMy2zHoYP5yx/EoNMYsGW6fROQ8APMAfFlEGkSkAcDZAN5lDdQ/BmBOhJs2A+iNclkXHM0OVkarIuw64f+D3wDYDWCeMaYQwFegAbf9982OtP/GmF4Af4Fmz94LZs2IXMHgjIgGiEihiLwRWm77ozFmuzEmAM30/EREJlnXqxKRN4Td/JsikikiF0JLY/db208gyod9nG4G8AdjTDzB3ocAXBqW1YMxxg8NMr4tIgXWWK/PIDgu7S8A/kNEqkWkBMCXHLetB/AvAD+yjleaiMwRkYsxvJsBPAFgMbSMuwzAGQByAFwNHQ92uYi8XUTSRaRMRJZZ/4M7AfxYRKZaDQvnikgWgL0AskXkWmtg/n8ByBpmPwoAtAPoFJGFAD7uuOzvAKaIyKdFG0EKRORsx+V/gI4PvB4MzohcweCMiADgbyLSAc2afBVa8vqA4/IvAtgP4GWr4+9JaFnL1gCgFZot+xOAjxljdluX/R7AYqsz8OFEdk5EqgBcisHlySEZYw4YYzZGufjfoVmng9AM4Z+hARCgwejjALYC2IzBmbf3AcgEsBP6dz8AYMowf0M2dCzbL4wxDY6fQ9Ag52ZjzFFopu+z0CzhFgBnWXfxOQDbAWywLvsfAGnGmFPQwfx3QDN/XdBmjqF8Djq+rcP6W++zL7DGFF4B4Dro/3UfgEscl78AbUTYbGUniSjJJL4voUREoURkLTTLVj3Gu0KjRESeBvBnY8wdY70vROMRGwKIiChm1njEFYi9MYOI4sSyJhERxURE7oaWtD9tlT+JyAUsaxIRERGlEGbOiIiIiFLIuBlzVl5ebmbOnOn643R1dSEvL2/4K04QPB6heDxC8XiE4vEIxeMRiscj1Hg/Hps2bWo2xoTPSQhgHAVnM2fOxMaN0Trmk6empgZr1651/XFOFzweoXg8QvF4hOLxCMXjEYrHI9R4Px4iEnUqGpY1iYiIiFIIgzMiIiKiFMLgjIiIiCiFMDgjIiIiSiEMzoiIiIhSCIMzIiIiohTC4IyIiIgohTA4IyIiIkohDM6IiIiIUgiDMyIiIqIU4mpwJiJXicgeEdkvIl+KcPnHRGS7iGwRkedFZLG1faaI9Fjbt4jIb93cTyIiIqJU4dramiLiAfArAFcAqAWwQUTWGWN2Oq72Z2PMb63rXw/gxwCusi47YIxZ5tb+EREREaUiNzNnawDsN8YcNMb0A7gXwA3OKxhj2h1n8wAYF/eHiIiIKOWJMe7EQyLyNgBXGWM+bJ1/L4CzjTGfDLveJwB8BkAmgEuNMftEZCaAHQD2AmgH8F/GmPURHuMWALcAQGVl5cp7773Xlb/FqbOzE/n5+a4/zumCxyMUj0coHo9QPB6heDxC8XiEGu/H45JLLtlkjFkV6TLXypqxMsb8CsCvRORdAP4LwM0A6gFMN8acFJGVAB4WkSVhmTYYY24HcDsArFq1yqxdu9b1/a2pqcFoPM7pgscjFI9HKB6PUDweoXg8QvF4hJrIx8PNsmYdgGmO89XWtmjuBfAmADDG9BljTlq/bwJwAMB8d3aTiIiIKHW4GZxtADBPRGaJSCaAmwCsc15BROY5zl4LYJ+1vcJqKICIzAYwD8BBF/eViIiIKCW4VtY0xvhE5JMAHgfgAXCnMWaHiNwKYKMxZh2AT4rI5QC8AFqhJU0AuAjArSLiBRAA8DFjTItb+0pERESUKlwdc2aMeRTAo2Hbvub4/VNRbvcggAfd3DciIiKiVMQVAoiIiIhSCIMzIiIiohTC4IyIiIgohTA4Ox20Hgb+8Cagmz0RRERE4x2Ds9PBzkeAg88AR18e6z0hIiIilzE4Ox0cfUVPm/eO7X4QERGR6xicpTpjgGNWcHZy39juCxEREbmOwVmqO3kA6G7W35sZnBEREY13DM5S3TFrnNn084CmPZpJIyIionGLwVmqO/oykF0MLLwW6G0Duk+O9R4RERGRixicpbpjrwDTzwEqFuh5ljaJiIjGNQZnqay7RTs0p50NlM/TbezYJCIiGtcYnKUyu0tz+jlA0TQgPZvBGRER0TjH4CyVHX0ZSMsApi4H0jxA2VyWNYmIiMY5Bmep7NgrwNRlQEaOni+by7nOiIiIxjkGZ6nK1wfUbdbxZrby+brOpq8v/vszBmjYnrTdIyIiIncwOEtVx7cA/j4db2Yrnw+YANByMP77e/1B4LcX6CkRERGlLAZnqcqefDYkczaCjs2Xf6OnT/034Osf2b4RERGRa9LHegfIYdv9wOH1wPHXgMZdQOkcIH9S8PKyuXoab3BWtwmo2wjMvxrY+xiw+W5gzUeSt99ERESUNMycpYoTO4G/fhjY+QiQWwac90ngbb8PvU5WPlBYDTTvD27raQOOvDj0fb9yO5CZD7zldmDG+cCz3wf6OpP+JxAREdHIMThLFQee1tOPvwC872Hg8m/oFBrhyucGM2fGAH/9CHDXtUBHQ+T77WwCdvwVWPYuILsQuPybQFdjsMxJREREKYXBWao4+AxQNg8oqh76euXzda4zY4CdDwP7/qVNArv/Efn6m+4C/P3Amlv0/LTVwMI3Ai/8DOhqTuZfkJj+rsS6T4mIiMYpBmepwNenpck5lwx/3fL5QH+HZs8e+yIw5SygdDaw62+Dr+v3Aht/D8y5NNhMAACXfQ3wdmmANtb+dKP+HURERASAwVlqOPYq4O0GZscSnFlB1oMfBrqagOt+Biy6XhsJelpDr7vrb0BHPbDmo6HbKxZo9mzLnzWAGyvG6JQhbUfGbh+IiIhSDIOz0bb7UaBpT+i2gzWAeICZ5w9/+/L5etqwTYOuqcuBRdcBAR+w9/Hg9YwBXvwFUDITmHfF4PtZ9i6guxnY/2Sif8nIdZ/UDF5/19jtAxERUYphcDaauluAv7xXB/EbE9x+8BmgehWQXTT8fRRM0c7LgqnApV/VbVNX6HlnaXPPY8DxzcBFn9d1OcPNvRzILQe2/t/I/qaRaD2sp+wcJSIiGsDgbDTt+ptmuOq3Anv/qdt6WnVes9lrY7sPEeD6XwA3/QnIKtBtaWnAwmuB/U8B/d1AIAA8822dJ23pTZHvx5MBnHmjBnHdLdEfz+91b8C+HZz1MzgjIiKyMTgbTa8/qIP3S2YBNd/V7Nmh9dptGct4M9sZbwGqVoRuW3Qd4OsBDjylXZwnXgfWfhnwDDHP8LJ3aifnjocGX9Z2FHjyG8CPFuhUHW5gcEZERDQIVwgYLR0ndND+hZ/TcWCP/Jtmzw7WaJmyetXI7n/G+UBOCbDjYc3MVSzSIG4ok5cCkxZraXP1h3Rbdwvw908HS6TF04HaDZrhyykZ2T6GsxsBWNYkIiIawMzZaNn5iGbIzngrsPQdwezZwWeAmRdomXEkPOnAgmuA1x8ATu4DLvlK5LFmTiLAWe/U4Kt5n2ayfn+lljrP/zTwqW3AdT/X69ZuHNn+RdJqBWf+vrHtGiUiIkohDM5Gy+sPApOWAJMWaiB10ec1w9VyMPbxZsNZ+EY9nbxUy5yxWPp2QNKAp24F7rhCp+d43yPA5V8HiqcBVSv18mOvJmcfneyyJsDSJhERkYXB2WhoOwYcezm0zGhnz4D4xpsNZc6lurj51d/XrFgsCibr7XatAzKygQ/9C5hxXvDyrHygcglQm+TgzO8DTtUCedbC7pxOg4iICADHnI0Oe8C9MzjzpAPX/FAzahULkvM4GdnAu+6N/3YXfwnIKQWu/BZQUDn48uo1wLb7gIB/+FJprNprAePXwO9gI8edERERWZg5Gw2vP6hzkZXODt0+73Lgzb+JPcvllmmrgbf+LnJgBgDT1mjZsXFX6Pa9/0J+x8HEHtMeb1a5RE+ZOSMiIgLA4Mx9Jw8A9Vu0EeB0Vb1aT52lzd5TwF/eh5mH/5zYfdqdmpVn6Gl/R+L7R0RENI4wOHPb4ef1dMHVY7sfI1E6W1cTOLYhuG3bXwBfD3J66hO7z9bDumRVhbUcVbyZs0AgscclIiJKcQzO3Na4C8jIDQ7+Px2JaGnz2Ct63hhg890AgJyeBh2LFq/WI9oNml2s5+MZc9Z2FPj2ZF1ZgYiIaJxhcOa2pl064D/tND/U1auBlgNA10kNihq2A5OXIs34gPa6+O+v9TBQPAPIzNPz8UylcXK/zo1WvzX+xyUiIkpxp3nEcBpo3KWz8J/upq3R09oNmjVLzwEu/oJua0mgKaDtiK6UkJmv5+MJznpa9fRUbfyPS0RElOIYnLmpuwXoPAFULBzrPRm5qSt0jNjBZ4DtD+i0IFOW6WUthwZfv+WQNg1E0t+lk92WzNCSr70tVgPBWQIZOyIiohTHec7c1LRbTyctGtv9SIbMXGDymcCG3wMBL7DiZqCwCgHJQFp45qz3FPDrc3Rs2oKrgDNvBOZdCaRn6eX2NBrFM7Tcm5kf35gzOzhrZ+aMiIjGH2bO3NS4U0/HQ3AGaGkz4NVM4LQ1QFoaenIqB5c1G14HfL3A7IuBwy8A970H+P0VwcYBexoNu0kiMy++qTR62vR0uLKmMcAjnwQOPhv7fRMREY0xBmduatwNZBUChVVjvSfJMe1sPV1x88DEuT05UwaXNRu26+n1vwA+u0eXk6rfGlwpwV5Ts2SGnmbmJ17WNGbo6712jy4GT0SUCvq7x3oP6DTA4MxNjbs0yzTWKwAky4JrgEv/H7Dy5oFNPTmTgdZDoUFSw3YgrwLIr9RlqlZ/BChfADz3Q52frPUIkJEH5Jbp9TPzEitr+vuArubo1zt1TE+b98V+30REbjmxE/jeNKB201jvCaU4BmduMUbLmpPGQTOALTMXuOhzwekvYGXOvN3a+GBr2Kbj0+ygNC1Nb9e0C9j992Cnpn15VkFimTNg6HFnbUf1tHlv7PdNROSWPY8CAR9wkl8YaWgMztzS1QT0tAAV42S8WRQ9OVP0F3vcma9fM4aTzwy94pK3AKVzgOd+oGVQu6QJJDDmrBUonq6/D9Wx2WZlzrpP6vxsE0XtRuDQ+rHeC6LT395/AS/9Knn3d+BpPe1uGXyZMUj3xlFBoHGNwZlb7EXCx0szQBS92WHBWfMebRqYvDT0ip504MLPalataZdmzmyJjDmz73+opgC7rAlMrG+q//wS8PhXxnoviE5/W/4IvPDz5NxXb3twlZXuCF8Wd/0N5770geCXSprQGJy5ZTxNozGE3uwKnf/MbgqwmwHCgzMAWPr2YMarOCxzFuuYM2M0OCubA6RnD1/WtCe5HevSZscJwNvj/uME/MCJHaGlXyJKTE8r0NeenPs6/LyWNAGtqoQ7uQ+eQL8O/aAJj8GZWxp3AjklOih+HDNp6Rpw2Zmzhu26ekDZnMFX9mQAF3xGf3deHs+YM2834O/XZoLCqUNnztqO6pQfnsyxDc6MAX57AbD+R+4/VsshPUYMzohGrqfVes/xjvy+DjyljVAlMyNnzuyhF7v+NvLHotMegzO3NO7W8WbjpVNzKKWzg8FZ/TagcgmQ5ol83RXvA276MzDn0uC2zHxdvmmoaTFsdtCRUwIUVQ895uzUMX0jLJubvI7NHQ8Bj34h8mVtR5HV2zR4e1cT0NWo87+57YSVuezv1PF/RJQ4e07F3iRkz/Y/Bcy8ACiYEnnMWbfVeX70JaAzwvsITSgMztxgjLWm5jjq1BxK6WzN2BijmbMpEUqatjQPsPDa0OAtMw+A0W+ow3EGZ4XV0Rdd7+sMNg6Uz0tO5qyzCfjbp4ANdwQn1HV6+N+wcPdPB2+353VLZA3SeDkDwN429x+PaDyzg7O+KEvRxarloE45NPcyIKc0cma7qxk+Tx5gAtrVOdEd34JzXvoQ0H58rPdkTDA4c0NHvb6Yx8OC57EonaV/b/0WPQ3v1BxOljUuLJZxZ+GZs456wO8bfD27GaBoms6x1noY8PXFt1/hnvqmLk1l/JHLEm1HkNcVYTCvHZy1HtZ53tx0whGcsbRJlDi/N9hFPtLMmd2lOecyILc08vtH90mcKlqo43E57gzY/Xdk9zUDx14d6z0ZEwzO3GAv2zQeFjyPRelsPd35iJ5GagYYij1ovz8sOOtuGRx42eWAnBKgqEq/ZXbUD75Pe46z4ulA+Xy93kgyV7WbdLWB8gV6vqMh9HJjgI4TyPSeGrzgux2c+fsi76tTT2vkkkesGl4PTu5rf+snovg5Xz/hr+l4HXgGKJquY21zS/U1Hj6Mo/skvBmFwKLrgIM1ySmlns6OvKSnY93MNUYYnLmhcWJ0ag5wBmeSFn/GMFJw5vcCP18ObPrf0OuGlzWByE0BdnBWNE3LmkDiL/JAAHj0c0D+ZOAN39FtnY2h1+lt0+ALGBwE2sEZoKWNoTzwQeCBDwy/T837gM33hG7rbtHu1ZkX6HlmzogS53z9jKRj0+/V9X3nXqpjkHNK9b0ivAmqqxnejCJg4Ru16WnfvxJ/zHhtunvw+8lY8vUBtRv096Y9Y7svY4TBmRuadunyRXnlY70no6N4BgDRoKRsrq4kEA97xQFnWbOrWQMeOwtpCy9rApHHnZ06pl2a+ZW6T0DiwdmWPwLHNwNX/newy7QzLHPW4Vgh4eSB0MtaDgEFU4O/R+P3Akde1KkwhvP4V4B1n9SlsGz27WZeqKcMzogS53z9jCSLVbtBy6NzLtPzA5ltR4a8vwvw9cCbUaAd5nmTRre0uf5HwPM/Gb3HG07dZsDfB39aps6dGa6vc3QarMYQgzM3NO0Nlr8mgozsYKAUb0kT0Kk0gNBvkl1WZip8MGhPq85vlpGjZU0gSubsmO5TWpqOaSusTqxj0+8DnroVmH4ucOaNQMFk3R5e1nQGa5EyZzMvANLShy6t1m8DfL3a3dk3xIoJbceAfU/o7843cHu8GYMzopFzNtSMJHN24GmdC3LWRXo+t1RPncMXrDFo/ZlFVtPUNfoa9/Ym/rix6mnTJfVaDg79vjOajr4IAGiqOA9o3j94rO4LPwV+d+m4XkSewZkbWg8BZbPHei9GV+ksPY23GQAIZs6cSzjZreSRgrOcEv09qwDILoocnJ06piVNW6Idm7UbNFg6+2NaksjIAbKKBpc1rWDNIC00c+btBTqO6+MXTRu6rGnPHg4MnWHb/Ac9LZoG7FwX3N7wumZsy+cBEHZrEo1ESOZsBGPOTuzQca85xXrezpw5mwK6dBoNb0ahnl94nQ7zOFiT+OPGyp44HEYXZk8FR14EKhbhVNFiwNcDnDoaevmxV7Q0PI5XfmFwlmx9HfphXjJrrPdkdJWMJDizx5w5M2cxBGdA9Ok02o4Cxc7gbL5mzmKZS81p/xP6rXf22uC2gsoIZU0931EwG2hxBGf22LeSmRrADhV0HXtFHwuIHsT5fdqYMPdynTPu2CvBLF7DNqDyDP3mnV3EzBnRSAy8fmRkZc2WQ8Evr4COOQu5fwwEat6MIj0/6yLAkwUceSHxx41Vw7bIv4+VgB84+gow41x051rv4U2OL9aBAHB8y+Dt4wyDs2SzP3xLJ1hwNmmRlu2mnBX/bSOOObMyU93Noan9nrbQ4KyoKnQNTUCv33lCu6Ns5fP0m+hw3ZLh9j0BTDs7+K0X0HFszjFmgD5eRh468+eEZs7sZoCSmRrARgu6jNFAa641LiVaELfvcf0bVr5fu7pgtLTp9+qSYZPP0OvlFDM4IxoJ+/WTX5n4PGfG6HuA88v6QFkzUubMCs7SM61Jtkdhnc36bfo35pQ4smgx8PUDf//P5K8F2rBdqygzzkdXnjVcxjnurOVgsMxsL5OYbE9/C3jww+7cd4wYnCWb/eFbOsHKmis/ANzybGJNEENlzoDQgCo8cxZplQA7k1bsDM7m62k8pc2OE/pNct7lodvzKzUYC7luA1BQiZ6cKTrQ135jdwZnpbO1PBJpqoxTx/TvnHelfrOOFsRtuku7RudfpVO1lM3T5V6a92mHV6WVucwpYXAWr6OvAHdcMTproFLq62nVDHROceKZs84TWpZzflnPLgYgEceceTMKgtuKp43OIugN2/RL9eQz4wvOmnYDG+8E9v4zuftzRMebYfq58GUUArnloR2bxzfraXq2e8FZ3abBjV2jjMFZstkDvidaWTMjO5i1iVd6pnZWRhpzBkQIzoqD5wurNBhyDgwdmOMsrKwJxNcUsP9JPZ17Rej2gsn6pusskXaeAPIna3AGACet50HrIV1PL68i+AYdKStmT7Q4bU308mfbUc3krXgv4EnXMXCLrwcOrQcOr9frDGTOUjw4a69PvXmcDq8Hal8dnZUcKPXZXwSzixIfc2a/jp2fB550a9iBMzhrBtIy4EvPC24bjcyZt0cDn8lL9adxZ+RJvSOxqxvxViOGc+QF/TJrN3xVLAj9Un38NV2/efYl7s2B1tGgazePIQZnydZySCP97MKx3pPTS2b+4G7NLOsYOsedDcqcWQGYc9yZc44zW8FkILMg+ot55zpgb9i8Qvuf0CxV+Di6/Em61JSzs8nKnHXn2lNmWN+6Wg/rG41I8A06Ulbs2CsaxE1aEr38ac9DtOJ9wW2LrtMVC174mQa4dhCaU5K6k9DWbQJ+uRr455fHek9C2U0ekRpMaOKxh1BkFSberWm/jktmhm4PXyWgq1kbBZxrMRdN1y99bnZsNu7U948pS/V9ztcLnNwf223t10t7EoMzY3Rt0ennBbeVz9cA0v4yXLdZ97dysWa33FhDuKM+2Jk/RhicJVvLwYk33iwZMvPDxpw1BaflsAMvb4+WCMLHnAGhH6injulkuM5vPiLROzbb64G/fgT4y/scs/n7tAV+7uWDF6/Pt160ztKmlTnrzZ4MQIIpcTs4A4KnkbJiR18Gqlfpt+rSWfr3ON90jAG2/FnHpDnLtVOW6Zt4e51+w/Rk6PZUzZw1vA7c8xbNkobPYTfW7P/naIzzodQ3kDkrTDzL23JI34ucr1lAA7Hwsmb4kJDiCF88R+Lw88Az3w3dVm81AExeGvwSGmtp0w7OOpK49mXzXj0WMxzBWcUC7TzvatL35fqtwNQVOqzD+JOf6fb26v+ewdk4Ez74k2KTlR+6QkBnkwYpWYXBzJmdCQrp1owQnLUd0+12oGKrWKBvRuFreD7/Ex1QL2nAo5/XQKhuo5YywsebAdqtCQQ/zPs6dd8LKhHwWAN5Ww44BgPP1Otl5gIFUwZnxfo6dY6yaWfr+ZJZutyUM0g4dUxn/59/VehtRazGAATHmwE6rqW3zf21POPRvB+4501ARq6OrUu18uFAcDaCD0Nvj84GH29XMKWenlZ9HY0oc3ZYO8rTM0O350TKnJWGXsfO/NuVgJHaeCfw7PdCp8to2KZTA5XM1AyVJzP2jk03Mmd2d+qMsMwZoNmz5j36Bb1qhWN7ksed2Z34BVOSe79xYnCWTL4+DRImWjNAMmQ6grNAQMdg5FXoC2QgOHOsDmArnApAQr9dhs9xZlv5fr2PJ7/huG6dDrJf9i7g0v/SJVN2rdOxXeLRcQ3h7MyZPYWF/aFuby+drZmzriYtfzpLGiURxpPVbdJgzA7OIo1Nq92op9WrBu/P4uv11Fl+zSnR+xzJ5JnJ1NkI/OF6DVre94hOFdDbNrJ1RJNtIDgbQVnztT/q3/ncD5OzT/HYdBdw//t10uTX/qQZBkpcPGPODj8P/O6ywZOith4CSmcOvn5u2eCpNHKjZM6Slcm1p514zbFMU/02fd8Q0S+zkxbFnjkbGHPWMPT14nHkJX0fdX6GVlgTujfv0ZImAExdbgVnkvzlney/h5mzcaT1CADDsmYiMvOCY85624CAT8d2FU4dOjhLz9LrOd/A2o6FNgPYpp8DnPNxYMPvdBA9ADz/Y02NX/R5YM0t+kb12Bd1eoppa0KbD2z5k/TU/jAfeDFbGbWyOZoVcnZq2kpnDc4Y2c0AduAVaWxa7UbtTqqM0HQx7WzgLXcAy98T3GYfo1SZiPbV3+n/8T0PAhXzg2++w601OpoGMgEjyJzVbdLTZ74FbPzfoa+bTN5e4F9fA/Y9CTz/U+CRfwNuuyi+7jsKCgT0tWOXNf39Q4/9OrRes+3HXwvd3nIociXFXvzc1t08uKxZWKXZ/GR0bAb8wQlbt/6fJhL8Pp0gd4pjVRe7YzOWzO9A5eDU4HVCE1W/BahaGTqUpLBKv7w37dVOzaxCoHSOViKKp0Ve3mkk7M8bZs7GkYk6jUYyZDnGnNkfknkV+sIcKjgDtA389b/qty6/Tz9cI2XOAODS/6dvlo98Ql/sm/+gQU3JDB3v9cafabDVtFvHm0WSU6ITRNpvTnYafCBzNkff2O1vec5gvWSWXt/5DfvYy0DFomAgWDBZu5FCMmcbdHxZeKkW0DeypTeGNqHYxygVxp35+jWrM+9KYOoy3VYSITs4luzSNDCyTMXxLZptnXsF8I/P6DQno2Hfv/RD8u13Af91AvjY85r53fHQ6Dz+eNPfoZlnuyEAGDoLbQf09jQPgDYMdTcPbgYA9H69XRrw+b2amQvPnHkyNEBIRuas7agO9l/8Jn1P2P13DdZ8PaFL7k1eqvscSwdmZxMAK4hKRmmzv0u76Z3BIuAYL7xHg98pZ+myfICOO3Mtc8bgbPyYqNNoJIOzrGnPcZZXoZmzzgYNuqIFZzf8Sq/3pxv1Tcf4Bw/AHXicXOBNv9Y3qzuv1G+IF34ueHn1SmD1h/T3eVdGvg+R0Ilo7VM7DW4vjr7/SQASGijagZqdVQsEgGMbgOlnh95/qaNj09evJapIJc1o7EBvqOCsvyuxGbbrNlul2BjHVe3+m5ZAVjsmdRyqOWIs2IF2wRT9oAn447+P/i79AJm2Bnj73ZoBeOBDwZK0m7b/RRfLnrVWP9QnnwnMPF+7kDn+LX7O95psa2LYoZoC7ODMzpwCwdd4pEqKc/Fze+xZ+JgzQN87kpE5sxuhzv6YNhBtvifYDBCeOQNiy7h2NQbf65LRFNC4C4CJvMpM+QLN8jW8ruPNBrZbK78k8nqNpqNev3yHf86MMgZnydRySIOMRCZinehCgjMrc5Y/CSicot9guxqjB2f5k4D3rdM3twc+qNsilTVtM87TEmZPq05LEX7dK7+t9xf+Dc7JuYRTZ4MOpLX3q9R6wzr8vAaNGdnB2w0EZ1ZQsuOvmvGYcX7o/TvHpp3YruvIVa+Ovj/hYsmcvfAz4LYL41s8OOAH/vgWXXT4tou0dBfeYBHu1Ts0GHNmIjNzgYKpqdMUYGdrq1YCAe/gtVNj0bBdn6tTlmmZ/l1/0UBp671J3dVBelqBvY8DZ7xVs7+2RddrdsStiTpHy4kdwHenje6koAPvNcXBzNlQ487s7L4zOIs0x5nNuUqAHZxF+twonhZf5qyzScc7hgcrdnZp0kKtFBx8Rr80ebKCA+uB4LCJ4ZoC/F7d7ynL9HwyMmf2Y0YKzioW6Jf2gFc7NQe2L9T3RjsQToaOBv2iHd6lP8oYnCVTq7WG2hj/U09LmXnBD3lrKZOBsiagb349rUBaRnBFAaeiKuDmvwVT0UVRMme2y78OXP5NbQIIl5ENzL546NuHZ87yK4P/95KZOlbE1zO4pOEs57Ud09JX9WpgyVtCr2dnzgKBoZsBohkIztqiX6d2g5Y6In14P/ZF4M/vGLz9xOv6f1j6Dg1E/v5p4GdnRR18ntd5GDj6IrDqQ8FShC3S+Du3eXuBV24bvAqAnTmzv5Un0hRgr/c3dbme5pbqEAe3x9XtXKdjopa+PXT7ousAiF7upr4O4JdrguM4k+3Qei0pOgMft0XKnA21hFP7cf2C1nY0+P41MMxliMxZd0vw+uFlTUA7v9vrYs8Mbfkj8PR/D87WNu/RzGpOCbD83QBES+6Vi0OHSmQX6nvUcJkze5/tL7DJmIi2Ybse60hDUuymACD4+gI0OAOSOxltR/2YlzQBBmfJ1XKQJc1EZeXrtyJfv2YtxKPt5vZcZe11WgLIKYke/JbMAN7/d+CKW4Pp9mgy84ALPh25lBAL5xJOnQ163paeGXyDiTT5ZHaRTvT40Mf0Tfctt4dmPOzb+Xr1vms36puFHajGIrtYT6NlzowJljVO7Bh8+d7H9Se8m9L+AL78Gzqu6YOPAxk5wN3XDx4MDWDq8ce0kcHZrGBzlm5Hyyu/AR77gv5tTs7MGaDTlkTT1wGs+/fB2YL6Lfo8KHS8sZfOdr90u+0vuoyX80ML0G//087W7uOhNO4KftgmouF1/fA/9Fzi9zGUE6/raayToyZDSHBmZ86ilDV72zV4nHOpnrfHmrYcCg3unAYWP2/RMV5A5MxZ0TRtjoq1I9J+7GOvhG5v2hsMcIqqg1nsyRGqA7Es42S/95XO1sm9w4Ozvk5tAopnKp+G7bo/kd7fy619zy0LHbJS4cJ0GnbmbIwxOEuWgF+7NdkMkJiB9TU7tYSZV66ZlvDM2XDjAEpnAed/yv3sZcFkfWP19WvmLPzFbAeHkYL10tk6oeyR54Grvx/5OeOcTqN2g2bN4vmbMrK1qSBacNZ+PPihYH/42XpPWUGT0fKH0+HntWxbOFX3Z/o5wPv/oR9gf7ghNLvRewqTG2q03BYpCC6drW/yw5VFk6WnTTsZgcFlkM4G/UJgf1gNlTnbuU4bSTbdFbr9+GvBMo+tdJZmU5I5Jsap7Zg+j5a+I/LzY/H1+v+NVhJsOQj8+lzgB3N01Ya/fUrXGI2H/cHoVhbU/vIwqmXNNj2NpSHADkwWXK0Zc/s1MNScl86yZpc95qxs8PXsQCTW0qb9BckZnBmjZU1n+dJeZWTKWYPvY/JS/V8ONcbOHhdsfxlpDxtztuOvwKOfGxwkRhPw6/85UkkT0C+rnkwtaTqf59lF+sU1mU0BzJyNM+11mvnhNBqJCQnOrDnOgGBnZHtdbMHZaHFOpxGeOQOC484idWqVzNJxEovfpPOrRWK/qddt1EApnvFmtqGWcLLLkBm5gzNnzm/N+58O/h7w66LEMy8I29cZVoBWDPzhTcBfPwr8/TPAQx+DJ9Ab2ggQcrsIU4a46aVfahetJytCcHZC/6c5JZoJGGoi2t3/0NOdjwS39XdpaSU8e1UyS98X3FoS6vUH9PTMt0W+3J6gOFr27MhLAAxw3r/rc/X1v2qQHc8caXZJyY3gLOC3BopjbDJn2cWOzFmUsqb9vy2fr2W2geDsUPTPAztz1t0aHHOWE+ELzMBEtDEEZ52NGsSlpesXOrsRpPOElmSdpcGF1wLX/nhwKRzQL1wAsOfRIR7LypzZc1GGZ84arYC9MUJWPpKTB3ROyGjBmScduOSrOhVSOHt5p2To67AmFGfmbPyw35iYOUtMprXgb1+nvsnYwZmINddZfYoFZ9aL99SxyEt9lA0RnM26UN/E3/iT6Nmw4umaydl+v56vimO8mW2oJZzqt+q3/IXXambF2dFnlzunnwcceCp4WcN2fZOfeWHk/f3AozpVxtEXNXA58AxaSpaHdlc52a+VZJT9jBl6QtvOJuClX+vYvsolEYKzRg3ORHT8YrRMRX+3LuuVWwY07Qp+KNjNAPZUIbbwBpBkMgbYep+WLqMFAcXTNWCMNqXHsVc0ALn8VuDd9wP/vkmzOve+25oqIQZuZs5aDunYzewivf+hOk+Pv6ZDBSKV6ePV06qZ54xsDdYh0TNJdtaocKo+149v1gHzbceiZ87SM/V+u09qBjunZPDQBsAxEW0MqwTYJc0lb9bgqe2Inrefo87MWZpHu9KzCgbfz4zzgbK5wIY7oj+WPQzAnosyvOzaZAXUdmDtVL8VWP/j0G1DNQPYLvi0Ll8XrmKhfkFIRley/XeM8aLngMvBmYhcJSJ7RGS/iHwpwuUfE5HtIrJFRJ4XkcWOy75s3W6PiLzBzf1MiqE6c2h49ptEf5emzO3gDAjOdWYvRJwK7Aln7SxTeOZs4bXAsvdEfrNZ9UHgE68MPd7Nk6FjQxq2a5AW/qEfi5yS6JPQ1m/VN+vqNfpB5Pzm27BN/55l79Tt9hqYh5/X0/DMma2oWpsyPr0d+MIB4L8asO2sb0Tfv4HSbRI+1F/+NfCTJdEzhet/pGP4LvmqBswRM2dWgF1UHT3TdfAZDRbe8B3oYHsre2aXk8LLmm7O59Z6WD8Ez4iSNbMtul6zOZH+pmOv6tQfdrNG/iTgpj/pa/D+mzXIGE7TXmjw0pb8FR/skvuCa7Ws2BUhYGyvBx76OHD7JTrB6u4hMj6xcr7XpKUNvYSTc9LSqpUacB1+Xqf0ifTlzJZbokMjupojNwMA+qU1pzS2zNnxzfqFa/VH9Lw9ubWd2bQHzw8nLU2z3bUbgk0u4TobtdqRmadfTDvqQ8eX2QGhc6ko2yu3AU99MzQ727Bdm73KFwy+/nAq5mu2KxlrkNrvg+M5cyYiHgC/AnA1gMUA3ukMvix/NsacaYxZBuD7AH5s3XYxgJsALAFwFYBfW/eXuloOak08BSLu05KdOevv0Ddgu2wIWJmzVCtrWi9e+w0m/MVcPB14069Cp9GIl51ZqlwSPD7xyCkeOnM25Sy9byA022BfNsf6lrr/KT09/Lx+oy5M0niM7CLNQI00OPP16bQg3u7IWZO2Y8DG32sJuXyufmCeOqZz59k6TgSfc4VV0d/od/9D9/uMt2r5Z8fDuv34Fn1OhB+bwqn6vuBG5swOMCvD31bDLL5BT8OzZz1tGtxNWxO6fepy4Ppf6DqH//zy0Pfd16HNE3YncbKzZyd2WBnea/R8eGnz8AvAL1Zoeff8/9Agyp6KZyg9rbrvQ13ufK/JLoxe1myv007I9KzgNA92uXmoYS724ueRFj13inU6jbrNGoBVr9LAyR7v1bRHj0s8AcdZ79QhD9GyZ12NwddLwVRtWrDHsPae0mOSlq5lzfCMlp3he+1PwW0N23Waj/A1SGNhB53JaApIkQloAXczZ2sA7DfGHDTG9AO4F8ANzisYY5xfRfIA2P/FGwDca4zpM8YcArDfur/U1XpI3/TTUjuGTFn2mLPORv2QDcmcTdEXe39n6gRneRUAJBichWfOksF+Y49nCg2naMFZZ6NOGjl5afCD3c5QeHv0zXzyUi3vVSzS0ma08WYjFT7VRCAAPPsDXSQ9VtvvD46BaYzwTf3lX+vpxV+0HnOWfpjYAVjAb30hsP6HRdOsdVHDluvx+4A9jwHz3qCZzcU36IdP8z7t1IyU3UzzAMUz3Mmc2Zmw4bp4y+YAk5YMnlLDnm7BXtPVaenbgXM/qUud7fln9Pu2szLzr9LT8ODs+Bbgf2Ylvnj3iR3aiWp/iQhvCth2r/4vPvGqdmnnV0bOroX7ww3Aj5cAz34/crnSXrrJllU4dFnT/lJeuUTHNNqB8FCVFHvx867myM0AtlgmojVGM2dTV+hzrnpVMDhrtpoB4mkoyikGzrwR2P5A9PeQPPvLjBXI2BlEO2s2e60VqDmaBfo6dX/EA2y7L/gaszs1E2Fn25r3JXZ7pxTKnEUocidNFQDnM6oWwKB3ARH5BIDPAMgEcKnjti+H3XbQO5CI3ALgFgCorKxETU1NMvZ7SJ2dnREfZ9XR19GXVYbto7APqSTa8YhXdk89zgFwePMzmAlg97GTaLDut+pEN+YFNMuxt7YJx1PkGJ+XUYj0xl1IA/Di9oPo39uWtOMBANNaApgDYFdnAU4kcJ+zmztR1XUS68NuW3pyM5YC2HLCoO2VrTgnqxyntj2NXb7lKGjfh5XGj9dbPGiuqcGcrPmoOvQPbF33WyzvO4Wd3WVojGNfhjseC715KG7agZet6xS3bseyrd/C8d2vYu+CTwz/ACaA1Ru+B5M3C1l9TWh67Qns7Z4XcpUVO/4Ff8FCbN1yAMABFLe2YRmALTWPoK1kKTL623C+8WNffTvqampQ2dCBRQBeeeIh9OQGv0EXte3A8p4W7PDPRFNNDbJ6K3AugCPrvofpTXtwOG85jkT4W880hcg8th2bamqS+vyYcfgFzITguS37YdKODH3d3KWYefhevPT4Q+jP0qBj5qH7MANpeP5AF/xHBu+TZFyCVbmPIO2h/8CG1b9AwJM16DqVDU9jEYAN7WVYBcHhzU/jSEsw6z39yAOY3dOCHf+6B02Tzh90++GOx9lHNqKjYB52bTmECyUdta89jYPtwXmwVu15Dv05s7Ft2xEAR7DMlwHU7sOWIe5TAl5cVL8d/ZlFyHrm2/Cu/zkOzXo3jlddHbzf5lr05EzBDut+lvUZmBNHsDXSe3/9XvRmT8br1mXL82aiqH0PApKB5zbvASRy0LCow4fC9uPw+HvRnD4Ne6M8P+Z0Cqa2HMb6Z56JGmBl95zAOd0nsbcrH8drajDTX4kZDc/h+ScfxZq67WgpXY49cT7v8mUZVvnuxv4HbkXttJC8ClY3HkJ37jTsqKlBQXs9VgLY/uITOFnehsn1T2AhgN2exViIJ7HtyXvRUqZT1BS17cByE0Bt1bWorvsHdjz0Q5wqWoLzuhqxrzMHdWH7GNPrxRhc4MlFw/b12N+7KK6/MdzcfRsw2ZON518axTn1ojHGuPID4G0A7nCcfy+AXw5x/XcBuNv6/ZcA3uO47PcA3jbU461cudKMhmeeeWbwxkDAmG9NMebRL47KPqSSiMcjER0njPl6oTEPfkRP9/4reNnOdbrt64XGbLs/OY+XDL8+X/fpG8XG+H3GmCQeD2OMOfyiMf9daUzrkcRu/9wPdf/6u0O3P/sD3d7dquf/eKMxvzpHf9/we72s5ZCe3/eknr/rOj09dTyuXRj2eDz9HWO+XmSMt1fP2///H8wzxu8f/gH2/FOvv+VeY35/lTF3XBF6ubfPmFvLjXn8v4LbWo/obTb+r56v367ndzys5w8+q+cP1ITe12NfNubWCmN624Pbfne5vva/XmjM7sci7+M/Pm/Mt6uMCQSS+/x4+BN6nGLRsEP38dU7gtvuus6Y31ww9O0OPqe3e+pbkS//19eM+WaZMT6vMT9eov8/p/veq7d/7ocRbz7k8eg5pbd99gd6/ucrjbn33cHLezv0tff0t0Mf7xerh/6bGncHnzO1G42582o933o0eJ0fLjDm4X8Lnv/T26Mfq+9OM+bvnw2ef/SLen+/WDX0fjz6BX1efKPEmCe/aYyJcjxe/JXeX2dz9Pva/qBep26znt/7hJ633zvX/2TofYnmjiuM+dmywa/F70435u+f0d/bavUxNvxezz/2ZX3f6jo5+LFf/GXwfeRHi4255y3BfT20ftDDx/x6+e2Fel+x6Osy5sQufb2Gv8bve58xP18R2/0kAYCNJkpM42ZZsw6Ac6rfamtbNPcCeFOCtx1bXU26iC2n0UicXda0yz/OMRjOcXypUtYEgk0BeZPcKWfPOBf4cm30dUKHE22VgIZtWm6x19+sXKLlKV+fdmpmF2kpDtClrtKzgUPPJne8ma10NgCjcwT2tOkA+8IqLVPWD57UdpAXf6HXP8PqwjyxM3SMS9NunT3fWXIsrNLxMPaYrYGF6+2yZrWeOsedGQPs+YeuHOHscFvyJn3tA9GbNkpn6VhKe8qEZGmvi31i4kmL9P9nT6nh92mTQKSSptOsC7W89cJPI88z1rRH79eTHnnFB3tAeSJzlNmdfvaSQmVzQ++nfot2yDo7mfMqhi9r2qXY8nk6gP+K/7b21bFoefiYs6woY876OnW78z3K7k4erjksp9RaYN0fvSEACD4fh+rYPL5ZxzZOssq/9lCIzffoaUUCA+0BbS5oORg636GvT8u+9uslv1LHBdqTMjft0kH6uaU6Hs3ZsVm3WZ+zhVN0DOj+p4C9j+ll9v85EaWzo4939Ht1guR/fhn4+XLgO1OAX58N/N87tLxtr/QCWBPQjv14M8DdMWcbAMwTkVkikgkd4B8y6EFEnPWHawHY+d91AG4SkSwRmQVgHoBXXdzXkRlYNHns69SnrYwcfYHb44/yHA0BBSkanNlvTgUujDezRWqvj1W0VQLsAf+2yiU6Bqt5rwZuzlm6M3KC48ySPd4McEyncVAHUft6dTC6pOn4rqHUbQYOr9e5jzwZOn6uvyN0fFP9Fj11dlGmeTTgHQjOHNMCAMGAx9nd2LhTr7/w2tB9WHS9nhZMif76d6tj81SdjguMhYju66H1Ogi9caeO4RwuOAOAK7+lAfqjnx88uLt5T3CW9tI5oR+Q3S3B6RwSCc7scZD2eLMy6/7trkB7zJxzqpa8Cu2AHKrL1B6bVDZXTyefoZ2Cdsett0efh+ENAZG6Ne0xSs4g2V5lYrgv685u7aHGnA1MpzHEXHl1r2lnuD2gPqdYx4vuf0LPO6fRiMfi6/V9ZMdfg9vs4NceF+xJ1/frDseYswqrvFi5OHSus+Obg3MBLnsXAKPr8xZPD35ZTETpbH3dh//fm/YCP5wP3H0dsOH3+j+/5KvAW+4A3vQbfXznyhYpMgEt4GJwZozxAfgkgMcB7ALwF2PMDhG5VUSsdzR8UkR2iMgW6Lizm63b7gDwFwA7AfwTwCeMMS5NsZ0Edvv4UC8wGpqIZs/CX/iANf+UlZlKxeAsP0WD8kiLn/e0apAREpxZ31jrt+oA7PBZw+2uzUjzm42Ucx6wzfcAlWfqMjjTzx0+OHvpl5rRWHGznrezBs6mgPqtep3wLIZzOg37y5X9/0zP0g8bZ4fcrr8BEGD+1QhRPE0HPs9eG9vfmCzG6Id1pHUIo1l8vWZp9jwaHCwe3qkZScFk4JKvaGOIs+PT26PH0O6WK52t2UE7U2vPXVU0HWhJMDjLKgpmjsrmaNBkZzTrNun/0Zllt983hspSNu/TD2B7ctn0LA0i7ODMuXSTLbtIGwLCg1N7X5yZs9LZOgP/kjcP/fc5g7O8oRoCrMx5tKaAgF+/hNhBoW3aGs0serKGntJjKOlZOgF2rWMM1sCXGceX0oLJmnWyOzXtTN2kxRog+X16XFsOBoOz0ln6nmL8iTcD2Epn6xfM8K7Ww+s1WH/z7cAXDupcfhd/AVh6o66qkV0EHKrR6xqTMks3AS7Pc2aMedQYM98YM8cY821r29eMMeus3z9ljFlijFlmjLnECsrs237but0CY8ww79JjbKgZnil29nQR2UWhLdVpnuC3mVQKzuwXsZuZs5Gwj5VzrjN7XjZnAFY2V9/AdzykH37hb5RL36HljfkuTDeYW6bB0851+gGz4r0aqC+4Wj+co3X5GQPse1LLivaH7CTr27pzOo3jW/TvCV903RmcdZzQCUGd05UUVQdXCfD16bf72RdH/l+/+0Hghl9H/xuLZwCQ5GbOetu0nBrPeqtTlmmGYuc6nQMrf3LsJfPVH9GuyRd+Ftx2cr9++NsfxM4sKBAsaS55k37pGmo5oEhO7NCsmZ3FtTNd9nQadZsGT85sB2d2ABFJ814taTpNXa7BmTGhSzfZsgo1iPB2h97O7kR0ZjBFNPtrz7QfjfPzYqiyZm6pTmsRbTqN5r2aBZ26InS7nRUtnzeyYRfVq3R4gP3/C880A8GJwu1OTfu1WLlEV0NpORB8Pjgznfaau0NNPhuL8OeerXGX/u+Wvl3Xb3ZK8wCzLgIOPmv931t1X8d75mxC6WHmLCnscWfOkqatcKqWuux17lKB/eZ0OmXO7Kk/nMGZJ13nGDrw9ODLAP1Wf+0PI88mPlIi+g366IsaIJ55o263M1TRpnFoPayrFTg/kLILNctgZ878Pg3wIo0FK5mlx6WnLbh0k1NRVbCMtPVeHZd2/qcj74snfXDw55SRrc/fZGbO7MAx1rImECxtHnxGSznT1sQ+vYInXSdPrtuoC50DjpnnowRn9Vs0+LOXHosnexYI6PhBu6QJBJdEazmggUB73eBskf1/jDbuzBjg5D4NNJ2mLg+uKetcuskWbQkn+//gHHoRK+fnxVDznIlY02lE+aJizxsWvhKHnRVNtKRpq1oFwAQzi10RgrOCKVrWtOcas7Opzi9M9u2dS5wtvgE4613DZxmHE221kcZdug/RnuezLtagt+VgSk2jATA4Sw67rJlKWZ3TkZ25cJY0bYVT9M1yqA/B0Zaf6pmzYj0ND84Kqwd/GFSeoVmQ9JzBWQW32SXHRW8MlnrK5+oHaLT1/SIFmYCWp+xZyZv3aCYwfNZ+IFjmaTtiLd0U9j8smmatl+vXbNGUZUOXLocz1IDlcI27gHX/oRm7aAbKadXx7cei67VBorMhtvFmTmfdpAH05rv1fNMe/cJkZ7TsY2p/QB7fov8feymzeMadnTqq4wedwVnBFM0gnTwQXL8yfA5A+70jWnDW1aQBVnjAYgf5dZsjlzXtL4Xh2b/2Og2yEplsOtYxZ4CWz1sO6fJhTn6ffrHJLBgccJbN1SXY5l0Z/7452UFfnTXGb2BdTWfmbIoet+Nb9D3EbigqX6BDUhp36nizklmhxzUjB3jzbxJvWLDlV+pzw/kaM0bHu9kBYiT2a/rQs47gjJmz8aO7RV8cicxuTEF2ZiY/QnC26oM6ViCVTFqoY6SmDVO+GCtZhfrG6AzOjr82OKABgh+ClUtGfyJl+1uvXeKwLbhaVyWIVA5r2KYdl5PCZseftFgzI77+YBkl0t9rBxKth/XDJjzALqrWUtFr92im5oL/jG8Sz0iP5/xW33tKAzDnIvOALl9233s1ABpq8XG7xFUUZ3BWvTr44RNvcJZbquPWtt2n482a9+jfZQcmmbmaQWo5qBnJ1kMa1A40RMSxeoBdmnZ28KWlafbs5H4NFNLSB5fg7S8d0YIzZ6em06RFGngefy3KmLNiPQ1vCnBOQBsvu6yZkadBylDK5mmg8d0q4Jerde3T2y7S86/9UYPU8C+uIsAHH9Nl2EYit1SPuz3urLNJxwI6A1L7OXXwGW0QsfclI1uD8xM7tWkh2jq7IyUy+AtQ5wn9X4a/RziVzdWhAQdrHKsDMHM2fvS0DL1OIsVmIHMWoaw5e6125aWSnBLg489rt1cqEgldJeDETv1gm3XR4OvawVmkQMZtS9+hwc+staHbF1wNBLw6ED1c/VbtCAvPWDg7T+u3aqnczuw4lVjf7O3gLDxzZo/leupW/WBadF0Cf5hD6SygqxEeX4+ef/k3GoDd8+bQjNJjX9DgEhh6OZpT1vI44eXY4aSlaQkpqxCYksAg7BU3a2C58xGrKy9svUb7A9JuBpi6TIO2wqrYM2d+X7BUF571KJut91O7UQO38P9/VqEGWVGDM+vYhgdnngwd93R8S5TgLFrm7Hh84/6cMnM1yzRUM4Dtsv8HvONPwEVf0Odz0x7Ntq3+MPCm3wJvvi2xfYhV9SoNiI2xXi9hX6Dt4KzlYLBT0zZpsWb32msHj4tLpvCpXOzhDUNlzkS0tHlovWON1NQIztxcIWDi6D7J4CwZBsacRcicUWJySoIDnLfdqx/oZ7x18PUmL9UPttkXj+ruAdAM5OXfGLy9eo1mF/Y8FjomxRj9EI3UoGB/S27cqWOeIjUDANp0klOqJcS+9ghjzqwuyO6TwGVfH3k20coeZfee0A/4l3+tGdeT+4A/vAn40OO6PNZrf9RA9eXfBsd0RdJep1mqRPbrsq8BZ39MO/HiNfMCDVY33KFBkr1sk610FrD3cUfWcpm1ffbQY868vTrdQfOe4Liu0jmDB3GXzQV2/V0DhLNuGnw/Ivr+0TlEcJaeE7kcPHW5Lpw+bbVmnJ1jLAfKmm2ht2mv0+snKrd06GYAW2aelv0XvTHxxxqJqlWaMT1VG7rUmc2ZPZwUFrBXLgF2Pqy/O8ebJVvpbH3uBfz6urDnVxsqcwboe97WPwP7ntD3y+GymKOEwVkydLewGSAZ7MxZpLImJSa7WDMBAT+w7X5g7uWRj29uKfD5A5pBSBWedA3A9jym2RR7zreOel1kOVKWr3yezlnVsE1LhvY0G5GUzAxOKTFozJn14Z0/OXIQEC9rOo2cnnpdr7L3FHDVd/Wyu6+zJsNs0ID0kq/q5JzDZc7iLWnaMnKCmcN4iQArbwae+JqeD8+clc3RAeOHnw8d21g2R7Nt0TRsB2pf1QzlpCX6fJx+7uDrlc3Vrsn+zsGdmra88uiZs5P7dDxjpIC9aoX+b2o3asbZWca2M2fOsmZ/t1ZNEs2cAfo/dGNd3mSzx/bVbdTAOHzCWGe2Kfw5YQdHkuZuZr50to6nbK/TRpTGnVqFGarZAtDMGaDPv+ECuVHEsmYydJ/kNBrJYH9TjVTWpMTklGhwdni9dlMtfUf066ZnjmxclRsWXK3ZCjuIAqI3AwAaXJbP1+kivN3RZ+0HrHFgVhkkvOM2r0KDhEu+nFiGadBjaXCW13UEeOlXGiRXrdCfd/6frpAAAd56h/4NFQuHzpydOhZfp2YynfUuDYCB4AS0Nnv84IGnQ4996Rx9HtrNU+HsyYLf8F095md/NHLZ1e7YBAZ3atryJwU7CsM17x08cN5mZ3WOvjy4uSu7SE+dZc1IE9DG6213Atf+KPHbj5bKM7RcXLtRs5LhAWV2sWYkgQjBmVVWLF8wOBOaTOHdwnan5nAKpwQ7jlOkpAkwOEuOnlaWNZOBZc3ks4OzrfdpaWbB1cPfJpXMuVSXpXF2bdZvBSDRl3upXBycmX6ob+rOiTnDy5ppacC/vQisfH8COx1BTjGQU4Jpxx7RL3MXOZpbZl0EfPCfwPv/HsxoVSzQAKyvY/B9BQIjG+s0UvkV1koJMrjr0f6ADHhDu2Ttjs1oTQH1W7T6MFw20B4/mFUUeSwhYC3h1Dx4u7dXg+BoU0uUz9fB+QHv4OAsI1dLnc7MWaQJaONVVB3/uMGxkJ6pr6UjL+gUNuHZdxENcpydmraSWfreEy2YThZncBYIAI27Y8+E2cM5UqRTE2BwNnK+fn3Bsqw5cixrJl9OiX5Q7VqncwqlyHiKmGUV6CziztUC6rdp+TLat3C7uSEjd+g5npzL64xGaalkFtL93RqMTQ/rlKxaEZopsrMPTXsH309XkwYQiZY1k+EN3wbefvfgue+cKzE4A2M74xWtKcBeUmy4zG1uqWZpqlZEn1bHXl8zfDb/loMATPSpYtI8wX0OD85EtLTpnOfMHkA+VkHyaKteFZyrLNLrpXiGfjEK/7+kpQHve1jHOrqpYKpm91oO6pczb1dsmTMgOKUGM2fjSKTOHkrMzPOBeW+If+4mii6nWN+k+juTM3ZqLCy4WgeT25124WuDhrOXcZp85tAD5u3MmaQNPy4lGexg0Jk1i2YgOIsw7qzdmhx3LIOzomoN9sNl5Qc/uEPKmrOgqyRECM68vVqCimU8koiWAdd+Ofp18ip07FH4hLHRptFwskubkd7PswpDy5oDmbPUyba4ypn5ijT05LqfAm/5XfTbuj0fZFqa1bF5KPZmANvMC/R562bDQpzYEDBSXB0geapWAu/+y1jvxfhif8gUTdcJKU9H868CHv2cljZz3q3ByVAf5JXWG/JwH/Z2cJZXMTpzuy17N46eMpgeywLyJTO1nBspOLNXLkjVjE3pbC0BOst16Vk6kWqkzFnjTp3+JNJkwZGc+bahLx+YiLY5dDHt8AXPI7E/nJ2rA9iyi0LLmqfq9HrOZb/GM+eEv5FKsYmu35lM9lQu9jQasU5um10EfHZPSo25ZXA2Uva6mhxzRqnIDs6Wvj21VleIR/E0zYLteSw4zmyowKuwCrj4S5EzO+HXS2SusETNvQwHaz2YHssHgCddB65HagoYWLopRTPMa780OGsFaGkzUuZsqAaPRNjDIroatTPTdnKfZuWHCqaGypzZi5/b2o+n7v/ADcUzdNqP7ubUHSdXOhs48IxOYlw0PdhlG4sUCswAljVHrpuZM0phlWdoELL83WO9JyOz4Brt2LTX/xxqoWQR7firHKakkebRD5xUXRt10sIoZc06HXidqkMpZq+NHBiXzQFOHhw8Fqx+iwY+ycq8RFvCKdKC55H28fxP6UoI4bIcY86M0QmdR9IMcLoRCWbPUrVpq3QW4OvR5ZiGe/2nOAZnI2VnzjiVBqWiyWcAn9kZ7GQ6XS24Wtf+3Pi/GlAlKzC5/hfuD1ROVMVCXey6vyt0+6lazdik2Df9YZXO0U4/+z3TFmszQKzs8VDO4MwYLWsOtwi4CHDFraFretqyC4NlzUPPaSZuwTXJ2efTxVnv1EmskzG9jBvs97nuk7E3A6QoBmcjNTDmjMEZkWumLNM2d29XcieynHl+YssYjYaKBQBMcKyU7VTt2M1xNhIDC6DvH9gkAZ+WoGIdbxYLu4rhXCWgo0GbYobLnA3FWdZ88eeaPTprhOtWnm6WvEnnZktVzi+hKTShbCIYnI1Ud4u27J9uUxQQnU5EgksFjcX6n2NhoGMzbNxZe93p2dEcYTqNvK6j2lmZzP+pJ10rGc7MmT1AfLjM2VCyrMxZw3Zg/5O6BFb42p40tgqrgxMkM3M2wXW3sKRJNBrsxcerR7CW4emkdLY2LDjHnfm9mgU6HTNnJTO0i9OROcvvtCalTWbmDBi8SsCxV3TKlJFMlZBdCMAAz3xHJ6td/aER7yYlmSc9+DyLthLEaYLB2Uj1tLCkSTQa5l4GfHS9TuI6EXgydNoHZ+asox6AOT27BD0ZOl3Oa38EunTcWUHHfiCzIPljIsNXCTjyojaRxNO9F85e/HzPo7q+aKo2ZEx0FQs1a3aaZzUZnI1U90kGZ0SjZcrS028g/EhULAjNnNnTaKTqHGfDeeOPdeLuRz8LwMqcTVma/GlenIuf+726JmSkhdTjYa+vKR7gnI+P7L7IPdf+GLjpz2O9FyPG4GykWNYkIrdULARaD+ks+kBwAtrTMXMGaPZq7ZeAHQ8B2/6C/M5DyS9pAtqxaTcE1G/V6RVGHJxZmbMz3goUTx/ZfZF7CiqDa9SexhicjVT3Sc5xRkTuqFioU4jY47TsJYhO18wZAJz/aaBqFbDu3+EJJLkZwJZXodN2+Pq0pAmMPDirPAOoXgNc9LmR7x/RMLhCwEgE/DopIcuaROQGu2Pzld8AjbuBuo26dmi0Rd9PB5504M2/BX57oZ53rsGZLPmOiWiPvqxj2ka6tmP+JODDT4x834hiwMzZSPS0ATDMnBGRO8rmaMfma3/UL4JX/Q/wwcfGeq9GrnwecM0PcKpwwdBrXSbKnsG+8wRw9KXTd11ZmrCYORsJrg5ARG5KzwLe8UcgPVuXRRpPzRAr3ovX2qdhrRuLztvB2ZEXtaN++jnJfwwiFzE4GwmuDkBEbltw9VjvwenHDs52rtPTGcyc0emFZc2RsDNnDM6IiFKHHZzVvqq/n+5ry9KEw+BsJLqtzBnLmkREqSMrX5fVA7RLczyVg2lCYHA2EgOZMzYEEBGllLxyPR3pFBpEY4DB2Uj0tACeTCAzb6z3hIiInOzS5gwGZ3T6YXA2Et0tmjVjypyIKLXkTQIy84HKM8d6T4jixm7NkeDSTUREqemcjwOLrtNJb4lOM3zWjkRPCzs1iYhS0eyLx3oPiBLGsuZIdJ9kcEZERERJxeBsJFjWJCIioiRjcJaoQMAqa3IaDSIiIkoeBmeJ6jsFmADLmkRERJRUDM4SZa8OwMwZERERJRGDs0Rx6SYiIiJyAYOzRPXYmTMGZ0RERJQ8DM4SNbCuJoMzIiIiSh4GZ4liWZOIiIhcwOAsUT2tgKQB2UVjvSdEREQ0jjA4S5SvF0jP5qLnRERElFQMzhJlAoB4xnoviIiIaJxhcJaogB9I4+EjIiKi5GJ0kaiAD0hLH+u9ICIionGGwVmijJ9lTSIiIko6BmeJCviBNAZnRERElFwMzhIV8LOsSUREREnH4CxRxq/znBERERElEaOLRLGsSURERC5gcJYodmsSERGRCxicJYrdmkREROQCBmeJCgRY1iQiIqKkY3CWqICPwRkRERElHYOzRLGsSURERC5gcJYodmsSERGRCxicJYrdmkREROQCBmeJMgGWNYmIiCjpGJwlKuAH0nj4iIiIKLkYXSSKZU0iIiJyAYOzRLFbk4iIiFzA4CxR7NYkIiIiFzA4S1TAz7ImERERJR2Ds0QZPyA8fERERJRcjC4SxcwZERERuYDBWaK4tiYRERG5gMFZotitSURERC5gcJaoQIBlTSIiIko6BmeJCvi4QgARERElHaOLRLGsSURERC5gcJYodmsSERGRCxicJYorBBAREZELXA3OROQqEdkjIvtF5EsRLv+MiOwUkW0i8pSIzHBc5heRLdbPOjf3MyEsaxIREZELXKvLiYgHwK8AXAGgFsAGEVlnjNnpuNprAFYZY7pF5OMAvg/gHdZlPcaYZW7t34gxc0ZEREQucDNztgbAfmPMQWNMP4B7AdzgvIIx5hljTLd19mUA1S7uT3JxEloiIiJygRhj3LljkbcBuMoY82Hr/HsBnG2M+WSU6/8SQIMx5lvWeR+ALQB8AL5njHk4wm1uAXALAFRWVq689957XfhLQnV2diI/Px8X17wZR6e/FYdmv8f1x0xl9vEgxeMRiscjFI9HKB6PUDweocb78bjkkks2GWNWRbosJdoNReQ9AFYBuNixeYYxpk5EZgN4WkS2G2MOOG9njLkdwO0AsGrVKrN27VrX97WmpgZrL74YqAlgxqw5mDEKj5nKampqMBrH/XTB4xGKxyMUj0coHo9QPB6hJvLxcLOsWQdgmuN8tbUthIhcDuCrAK43xvTZ240xddbpQQA1AJa7uK/xCfj1lGVNIiIiSjI3g7MNAOaJyCwRyQRwE4CQrksRWQ7gNmhg1ujYXiIiWdbv5QDOB+BsJBhbxgrOhDOREBERUXK5VtY0xvhE5JMAHgfgAXCnMWaHiNwKYKMxZh2AHwDIB3C/iADAUWPM9QAWAbhNRALQAPJ7YV2eY2sgc5YSVWEiIiIaR1yNLowxjwJ4NGzb1xy/Xx7ldi8CONPNfRuRgE9PWdYkIiKiJGNdLhEDZU0GZ0RERJRcDM4SEQjoKcuaRERElGQMzhIxUNbk4SMiIqLkYnSRCJY1iYiIyCUMzhLBbk0iIiJyCYOzRLBbk4iIiFzC4CwRxmoIYFmTiIiIkozBWSK4fBMRERG5hMFZIljWJCIiIpcwOEsEuzWJiIjIJQzOEjGQOWO3JhERESUXg7NEDKwQwMwZERERJReDs0SwrElEREQuYXCWCDYEEBERkUsYnCWCU2kQERGRSxicJYJlTSIiInIJg7NEsFuTiIiIXMLgLBHs1iQiIiKXMDhLxEBZk4ePiIiIkovRRSJY1iQiIiKXMDhLBLs1iYiIyCUMzhLBbk0iIiJyCYOzRAxkzljWJCIiouRicJaIgeCMh4+IiIiSi9FFIljWJCIiIpcwOEsEuzWJiIjIJQzOEsFuTSIiInIJUz+JMNYKASMoa7Z19+OuFw/j1UMtKMrJQEleJsrzMnHO7DKsmVWKdA/jZiIioomIwVkiBsqa8QdnzZ19uGP9Idzz0mF09fuxtLoITR19aO3uR0tXP37+9H6U5mXiysWVOLO6CD6/Qb8vABHgzKoinDWtGNkZzNgRERGNVwzOEpFgWXN/YyfefttLaOvux7VLp+ITl8zBwsmFA5f39Pvx7N5GPLq9AX/fVo97NxwbdB+ZnjQsm1aMd6yehresqIKIjOhPISIiotTC4CwRCXRr1p/qwc13voo0AR791IUhQZktJ9ODq86YgqvOmII+nx+tXV5kpqchMz0NfV4/Nh9tw6uHTuK5vc347P1b8fCWOnznzWdiWmlusv4yIiIiGmMc2JSIOLs1W7v68b7fv4pTPV7c9YE1EQOzcFnpHkwuykZpXibys9JRlp+FKxZX4qvXLsZjn7oQ//2mM/Da0TZc+ZPn8Oua/TjZ2TeSv4iIiIhSBIOzRASshoAYypo9/X588O4NOHKyG7e/byXOqCoa8cOnpQnee84MPPGZi3D+3DJ8/597cPZ3nsIH79qAR7bUoc/nH/FjEBER0dhgWTMRcZQ1122tw2tH2/Crd63AeXPKk7obU4pycMfNq7Grvh0Pb6nDui3H8fTuRlQWZuHDF8zGu86ejrws/Rf3ev1o6/ZiclF2UveBiIiIkovBWSICPgAS0/JNz+1rRmVhFq45c7Jru7NoSiEWTSnEF9+wEM/vb8Zvnz2Abz+6C798Zj+WTy/GwaYuHGvthjHAG5ZU4v+9cTGqSzhOjYiIKBUxOEtEwB9TSTMQMHhxfzMuWThpVLoq09IEF82vwEXzK/Da0Vb89tkDOHKyG2dWF+HNy6sQMAZ3rD+Ey3/8LD6xdi4+fOFs5GROzGk5+n0BbK1tw9ZjbdhWewqv151CaV4m3rlmOq5dOiVkupI+nx9HT3bjQFMXDjV34XhbD1q7+9HW7UVnnw8rZ5TgqjMmY8X0kjH8i4iIaLxgcJYI44+ppLmzvh2t3V5cMDe55cxYLJ9egtveu2rQ9pvWTMe3/7ETP3piL3721D4sqSrCyuklWD2zBOfOKUNxbuaw993r9aO1ux+tXV50e01C+9fr9ePh1+pw14uH4QsYnDenDOfNKcPqmaUozcuMGMwaY9DU0YcjLd04erIbJ7v60NLlRWtXPwLGIC8rHflZ6cjJ9CDL6nLN8KQh05OGjHQ9PdHei+f2NuGlgyfR3a/l6SlF2TijqggHmjrx2fu34r//sROXLpiE5q5+HGruRF1rDwKOP7MoJwOleZkozs1AZnoa7nnpCH7//CGU52eiItOHX+9+Cd5AAMbofVcV52BqcQ46+3w4au17Z58P1SU5mF6ai2mluQOn1SU5nMeOiGiCY3CWiIA/pk7N5/c3A8CYBGfRVBXn4NfvXokNh1vw9O5GbDrSij+9cgR3vnAIIsDS6mKcN6cMgYBBXVuPlSXyotfrR58vgJ5+P3q8wYaDbA/wWv9OfOiC2TGNZ2s41Yv7Nx7D3S8dQXNnHxZPKUR1STYe2FSLP7x0BACQlZ6GioIsVBRkwRigu9+H7n4/Wrr6BwIqW4ZHUJKbCU+aoLPPh64+X0ggFcn00ly8ZUUVLphbgRUzijGpQPfbGIOXDp7En14+ipq9TZhanI1l00rw5uXVmFORh1nleZhZnofC7IyQ++vo9aJmTxP+tfMEdh1pgCdNkJWRDmOAvSc68PTuRvT5tImksjALM0rzUFmYhUPNXXh2b9PAZbbZ5Xk4f245zp9bjpUzSpAmQJ8vgF6vH6d6vGju7MfJzj54/QFUlzCoIyIabxicJSLGsubz+5oxvzIfkwpTbxD+6pmlWD2zFICW+LbVtmH9vmY8v78Ztz93EB4RTC3OxtTiHJxZlYvsjDRkpXuQnZGG4txMlORmoiA7HX+q2YY7XziMu148jDcsmYwZZbkoy8tCeUEW8rM8yE73ICvDgwONnXjotTq8fOgkjAEunl+Bj140G+fOKYOIDOzD1tpTaGzvRWNHH5o6+iCiAU1eZjqKczMxoywXM8o001RRkIX8rPSQLJsxBn2+APr9AfT7AvA6Tvt8ARRmZ0SdF05EcN6c8rgbNwqyM3DdWVNx3VlTUVNTg7Vrzwm53BiDlq5+5GWlDwqgjDFo6uzDsZZuK6vWg621bXhwcy3ueflIzPsgAiycXIizZ5VizaxSLJlaiMrCbAZsRESnIQZniTB+QIZuBuj1+vHq4Ra85+wZo7RTictMT8OqmaVYNbMU/3nFfPR6/cj0pCEtbfhxcgWte/GDpWvwu/UH8fiOBjz2egP8UVJXs8rz8KnL5uGGZVWYVZ4XdR9GQkSQneFJqaBERFCWnxX1skkF2ZhUkI2VM4J/uz0mblvtKaSnyUBwXJSTgbL8TJTlZyE9TVDb2o1jLT042NyFzUdacd+GY7jrxcMD91OUk4Hqkhy8celU3LiqGuVR9oOIiFIHg7NEBHzDljU3Hm5Fvy+AC+aVjdJOJU+8gc200lzcesMZuPWGMxAIGLT1eHGysw9d/X70evWnLC8LZ1QVcrmpGGWmp4VkN6OpLMzGSkf87/UHsL3uFA40dqKxow8n2nux83g7/uefu/HjJ/bgyiWTsWZmKYpyMlCYk66n2RkozMlAUU5GSgW1RETD6ej1YvPRNkwvzcXMsty4PmOM0eE7O46340BTJ1bPLMWqGSUp8TnF4CwRMZQ11+9vQnqa4OxZp19wNhJpaYLSvEyU5g3fWEDJl+FJw4rpJYM6R/c3duDPrxzDg5tr8Y9t9VFvf+nCSfi3tXNCMpjd/T4cbOrC3En5DN6IJgBjTEoEKLZAwGB3QweaOvvQ79OhKrWt3ajZ04QNh1vgs6o1U4qyce7sMswqz0OPV8dH93r96On3o7vfcd6r50929uNUjzfksZZWF+GD58/CNWdOQWb62M3Tz+AsEWb4hoAX9jdjxfSSgUlgicbS3EkF+Np1i/GVaxairceL9h4vTvV40d7r09MeL2pbe/CXjcfwtt++hDUzS3HWtCJsONyK1+tOwRcwyPAIllYXY/XMUkwrzUGmJw1ZGR4YY9DYrlm6lq5+XDi/HG9cOhUZHi5AQnQ6qWvrwfce243HdzTgonkVeNvKaly6cBJ8gQBeOnASNXuaUH+qB4unFOLM6mIsnlqIrj7fQPNYwAAV+drMVVmYhSlFOfCEDY/xBwwa2ntR29KNY609qG/rQUZ6GgqzM1CQnY4MTxp8gQB8foMNR7y474+b8NLBk2jr9g7a3wWVBfjQhbNw/pxyHG3pxksHTuLZvU3462t18KQJcjM8yM70IDfTgxxruEtOhgeVBRnIztRhIoumFGLJ1ELMLMvDo9vrcecLh/Dp+7bgF0/vwxP/eXFMw3vcwMghEYGhp9Jo6erHjuPt+M/L54/iThENL92ThvL8rKhjz/7jsrm499Vj+N36g9hyrA1nTSvCLRfNxoLJBdhZ344Nh1pwx/qDA99UnbIz0pCXmY6/vlaHHz6+Fx++cBYq+g18/gDShwjUjDFo7uzHifZeNHX0oaWrHxfNr0BFAcfH0cQSMIlNTZSIxvZe9PkCA8HHfRuO4bZnDwAArjljMl48cBJP7jqBopwM9Hj96PcFkJPhQVVJDp7e3ThsVzyg3fTTSnMxsywPfT4/jrVoEBfp/SOaquJTuGJRJc6dU4YZZbk6PVJ6GkpzMwc1273nnBkwxsDrNwllvd5zzgy8a810PLevCcfbescsMAMYnCUm4B9ydYAX9jfDGOCCeakzhQZRLHIz0/HBC2bh5vNmwhcIICs9+CXkhmVVAIJLgfX7Auj369QmFQXZKMzW6UOe2dOI39QcwDf/tlNv+PRjyEpPQ0F2BuZX5mPxlEIsnlqItm4vNh1pxYbDLWjs6AvZj7K8TPzgxqW4dGElAG2QuPvFw3hwcy3WLpiED14wc2AKFKLTVZ/Pj1cPteDZPU2o2duEQ03dWL3vJVyyYBIuWTgJcyvyQwKEPQ0d+OvmWjyzpxFTi3OwtLoYS6uKsGByAaYUZSPdkwZjDLbVnsJfN9fi0dcbUJKbgQvmVuDC+eUozsnAk7tO4F87TmBfY+eg/bnurKn40tULUVWcA58/gPX7m/H3rfUoyc3A2gWTsHpWCbLSPeju92FXfTt21XegMCcDVVZnv0dEO+07+9BwqhdHTnbjcHMXDp/sQk6mB8umFeONS6cMTP8zrSQXU4qz4fMbdPT60NHrRb8/gAxPGtLTBFs3vYo3veGSuEqsIoLM9MSDqrQ0wdoFkxK+fbIwOEvEMGXNF/Y3oyA7HUuTsMg50VjwpAk8UcZVZmd4MLko8mUiwGWLKnHZokpsOtKC+5/ehKnTZ6Krz4fW7n7saejAPS8fGZjbrao4B+fOKcNZ1cWYWpyDysIsBAzw1Ye244N3bcT7z5uJc+eU4XuP7cah5i4snFyA2547gDtfOIQbV1bj8kWVyMtKR16WB/lZ6QMTEWelp0V9Q/f6Azjc3IXdDR3Yd6IDcybl47qlU8f0WzLFrrPPhwyPhHxxGGunur041tqNxVMKQ55Hh5q78J1Hd+FwcxeuXToFb11RjWmluTjc3IU/vXIE92+qRVu3F5meNJw9uxSzcnpxrNuL7z62G999bDcy09NQXZyDqpKcgYpMeprg7NmlaDjVi+f27hvIYHnSdPqjNBEcOdmNzPQ0XLZwEjr7fPijNZelfb01M0vxjtXTUJSTgYAx8AUMFk8pxHLHWNV0T5oGiRECldzMdKycURrSYW5LZOqorHQgLyt90FyZR7Ojv47HOwZniQj4hixr7j3RgTOmFg1ZyiEa71bOKEXHrAysXTsvZLvPH8Ch5i7kZ6djSlFOxNs+/Inz8b3HduOuF3UOvdkVefjfD6zGJQsm4VBzF25/7iDu31iLP71yNOLt09O0nDJvUj4WTC5AdoYHe090YE9DBw42daHfHzrx7/++cBjfvH4JzppWDAADkzA3dvRak/72w8Bgdnk+5k7KR3m+rmJhz6sX69QziWjt6kdulmfUg5Gmjj7U7GnE7Ip8LJlaGLUZJBAw6Oz3oSBszsFImjv74BFBSYSGoX5fAJuOtGL9viY8v78ZpXmZ+J+3LkWl48N+w+EW3PKHjfCkCd537ky855wZA81HfT4/DjZ14WhLN2pbe1Db2o1AwGDOpHzMqcjHvMr8QdnWnn4/7nzhENbva8LiKUVYOaMEK2eUDAoSjDH4x/Z6/O8Lh1GWl4kFkwswr7IA9W09eMqazNsfMKguycFbV1Tj6jMn46HNdbjzhUPISvdg8dRC/Oypffjpk/swd1I+9jd2Ij1N8IYlk/GWFVU4d04ZcjPTrXkSL0L9qR6s39uMA02dONaqf09mehq+ft1iXH/W1IGpebr7fdhxvB0HmzpxtEWn1enq8+HjF8/B1WdOQVGOTpjd69UMXWt3Py6aVxHx+FNqYXCWiGG6NRs7+rBqBtdZJIok3ZOGeZUFQ14nO8ODb1y/BJctmoRjLT24cVX1QIPBrPI8fPctZ+Lzb1iAIye70NXnH1gdoqvfh84+Hzp6fTjc3IW9Jzrw1O5G+AMGVcU5mF+Zj7ULJmHB5HwsqCzE7Io8/H1bPb732G7c8KsXcPH8CrR09WN/Y2fIShjh8jI9MAB6vH4YA5TmZeLCeeW4aF4FFk8txO6Gdmw52oZtdadQkZ+FNbN0WpTmngD++Xo9ttaewr4TnVhaXYQrFldi4eSCkMCm3xfAU7tO4P82HMP6fU0oyc3Ejauq8a410zGjLC/qfvX5/HjtaBte3N+MvSc6Masib6CMPKUoGzkZnmEDqNauftz23EHc/eLhgWOQniZYMLkAJbmZ8PoD8AUMuvv9aO7UMYL+gMGCygK8ZUUV3rS8CpWF2TBGr3O0pRtP7TqBJ3aewNbaUwCASQVZWDC5AIGuPvxu/8uoa+3B8bZe9PsDSE8TLJtWjFcPteCNv3gev373CqyeWYpHttTh8/dvQ1VJDmaU5eLHT+zFr2v247w55TjW0o2DzV0hcyzmZerf2tnnG9h2ZlURrjlzCq46YzJePXQSP3liHxrae7GgsmBgpRQAOKu6CDeumobrzpqKtu5+/L9HduC5vU2YXZGH1u7+gecUACyeUoiPXzwH00tzsW7rcfz86X342VP7AAA3rqzG569agEkF2ahr68FDm2uxfl8zrr9iPm5aPS1qlmlKUQ7evnrakP8nQDNYsUy5k53hwUXzK4a9P0odYkZxAKKbVq1aZTZu3Oj649TU1GBt/W3AqWPAx54fdLkxBgv+3z/x/vNm4ivXLHJ9f8aaftNbO9a7kTJ4PEKlwvHo8+lg5oKwZbecOnq9+MXT+/HP1xswvTQX8yrzMb+yAFOLc1CWl4ny/Cz4jcGBxk7sb9QsRXqaICdTO8AONHbiuX1NaO7sH7jP3EwPzphahIb2Xhxt6Q55vAyPoLokF4dPdsEYoLokB/Mm5Q+0+9e29qClqx9TirLxlhVVONDYhSd2nYA/YLBiejFmV+RjemkuJhdlo6mjD4eau3CouQs7jp9CrzeANNFlympbQwdfpwmQn5WO3Mx0q3QtA39HXlY6cjM92Hi4FV39Plx/1lR88PxZONHeOzAhclefD+meNGR4BNnpHm0uKchEbmY6ntp1ApuPtiFNgMmF2Wjp7kevN5ihPGtaMa5YNAmZ6WnY3aBZzLqTHZg5qQhVJTmoLsnByum6xm9Bdgb2NHTgo/dsRG1rD95wxmT8Y1s91swqxW3vWYmSvEzsO9GB360/iI1HWjG7PB8LJxdg/uQCzCrLQ3VJDopz9f/d2NGH/Y2d2F53Co+93oCtx9oG9mnZtGJ85ZpFWDOrFP2+AHbWt+Plgyfx8Gt12N3Qgaz0NBgAmZ40fO7K+XjvuTPhSRP0ev041NyF4tyMQdnfurYePLnzBJZPL8bS6uK4nqup8HpJJeP9eIjIJmPM4EWwwcxZYoYoa7b3+tDvC2ASO82IUkJW+vAlwYLsDHzlmkXDfqGqKs6JmoEIBAx21rdjX2MHFk0pxLxJBQPTCJxo78XGw614ZcvrePPaVVg0RcuEjR29eGpXI57ceQKNHX3IyfSgJDcTcyvycd1ZU3HR/IqQ+7j31WN4fn8T1u9rwon2YBNFZWEWZpbl4abV03H+3HKcPbsUhdkZ6PP5sb+xEzuPt+NkVz86ezWz2N3vgz8A+AMBeAMGvf2afWzu7MPaBRX4j8vmYb4ju3nlksnDHudPXDIXh5q78NBrdaht7UZZnq5kMakgC+fPLQ8pT9r0w/f8iPe3YHIBHvnkBfjMfVvwj231ePPyKnzvrWcO/C/nVRbg+287a9j9qizMRmVhNs6fW46PXTwHx1q68eSuE6gqzsEViysHMomZ6WlYNq0Yy6YV46MXzcb2ulN4YFMtvH6DT18+L2T/szM8WDSlMOLjVRXn4ObzZg67X0RDYXCWiCHKmk0dvQDAaQCIJpi0NMEZVUU4I0IjUGVhNq5dOgV5LXtCBl1PKsjGO9dMxzvXTB/2/isLs/Gpy+fhU5frGL5erx8Np3qtdWwjv5VnpXuwZGoRlkwdneakWeV5+MwVyZtCqCgnA7973yrsbujAoikFSRkcPq00Fx84f9aQ1xHROf3izXwRJQuDs0QM0a3ZaH2bZXBGRG7KzvBgZnn08WfjRVqaYPHUyFkqovGK7YSJGGISWnu+Js7BRERERIlgcJaIIcqajVZZc1IhM2dEREQUPwZniTBDBGftfcjOSEMB19QkIiKiBDA4S8QQ3ZqNHX2YVJA9YWc1JiIiopFhcJaIYcqanEaDiIiIEsXgLBFDdGs2dfRxvBkREREljMFZIgJ+QCIfOrusSURERJQIBmeJiFLW7PX60dHr4xxnRERElDAGZ4mIUtbkBLREREQ0UgzOEhGlW3NgjjMGZ0RERJQgBmeJCAQiljW5OgARERGNFIOzRESZhLaxnasDEBER0cgwOEtE1LJmH9LTBKW5mWOwU0RERDQeMDhLRJRuzaaOPpTnZyEtjasDEBERUWKGDc5E5DqRKJN6TVTRujU5AS0RERGNUCxB1zsA7BOR74vIQrd36LQQ8Ecta7JTk4iIiEZi2ODMGPMeAMsBHABwl4i8JCK3iEiB63uXqqKWNXs5xxkRERGNSEzlSmNMO4AHANwLYAqANwPYLCL/7uK+pa4I3Zo+fwAnu/pRwWk0iIiIaARiGXN2vYg8BKAGQAaANcaYqwGcBeCz7u5eiorQrdnc2Q9jOAEtERERjczgUe2DvRXAT4wxzzk3GmO6ReRD7uxWiotQ1uTqAERERJQMsQRn3wBQb58RkRwAlcaYw8aYp9zasZRlAgDMoG7NJnt1gEKWNYmIiChxsYw5ux9AwHHeb22bkMRYh0LCM2f20k3MnBEREVHiYgnO0o0x/fYZ6/eYpsAXkatEZI+I7BeRL0W4/DMislNEtonIUyIyw3HZzSKyz/q5OZbHGw0DwVla6KFrbNfgrDyfwRkRERElLpbgrElErrfPiMgNAJqHu5GIeAD8CsDVABYDeKeILA672msAVhljlkK7Qb9v3bYUwNcBnA1gDYCvi0hJDPvqOjF+/SWsrNnY0YvSvExkpnO+XiIiIkpcLJHExwB8RUSOisgxAF8E8NEYbrcGwH5jzEEr23YvgBucVzDGPGOM6bbOvgyg2vr9DQCeMMa0GGNaATwB4KoYHnMURC9rsqRJREREIzVsQ4Ax5gCAc0Qk3zrfGeN9VwE45jhfC82ERfMhAI8Ncduq8BuIyC0AbgGAyspK1NTUxLhrievr7AAA7Dt4CHX9wcc7UNeDnHSMyj6kks7Ozgn3Nw+FxyMUj0coHo9QPB6heDxCTeTjEUu3JkTkWgBLAGSL6KLexphbk7UTIvIeAKsAXBzP7YwxtwO4HQBWrVpl1q5dm6xdiuqFfz0MAJg3fyHmrQk+3ldeegrLZpRh7dplru9DKqmpqcFoHPfTBY9HKB6PUDweoXg8QvF4hJrIxyOWSWh/C11f898BCIAbAcwY8kaqDsA0x/lqa1v4/V8O4KsArjfG9MVz27EQ7NYMHjpjDJo6+zCJqwMQERHRCMUy5uw8Y8z7ALQaY74J4FwA82O43QYA80RklohkArgJwDrnFURkOYDboIFZo+OixwFcKSIlViPAlda2MRfs1gyOOWvr9sLrNxxzRkRERCMWS1mz1zrtFpGpAE5C19cckjHGJyKfhAZVHgB3GmN2iMitADYaY9YB+AGAfAD3W+XSo8aY640xLSLy39AADwBuNca0xPWXuSRSt+apHi8AoDg3Yyx2iYiIiMaRWIKzv4lIMTSQ2gzAAPhdLHdujHkUwKNh277m+P3yIW57J4A7Y3mc0TW4W7PPp9uyMzyRbkBEREQUsyGDMxFJA/CUMaYNwIMi8ncA2caYU6Oxc6koUlmzz6fZtCzOcUZEREQjNGQ0YYwJQCeStc/3TeTADHCWNQdnzrLSmTkjIiKikYkl1fOUiLxV7Dk0JrhIa2v2ee2yJjNnRERENDKxRBMfhS503ici7SLSISLtLu9Xyhq6rMnMGREREY1MLCsEFIzGjpwuInVrDpQ1mTkjIiKiERo2OBORiyJtN8Y8l/zdOR0MLmv2etkQQERERMkRy1Qan3f8ng1d0HwTgEtd2aMUFyxrBgMxNgQQERFRssRS1rzOeV5EpgH4qVs7lOqCwZmjrMnMGRERESVJItFELYBFyd6R08XAmLMIk9ByzBkRERGNVCxjzn4BXRUA0GBuGXSlgAkpcrembsv0MDgjIiKikYllzNlGx+8+AP9njHnBpf1JeRHLmj4/0tME6QzOiIiIaIRiCc4eANBrjNbzRMQjIrnGmG53dy1VRZ6EluPNiIiIKBliWiEAQI7jfA6AJ93ZndQXnOcstFszi4ueExERURLEEpxlG2M67TPW77nu7VJqi1bWZOaMiIiIkiGWiKJLRFbYZ0RkJYAe93YptUVcW9PHsiYRERElRyxjzj4N4H4ROQ5AAEwG8A43dyqVBcua4WPOWNYkIiKikYtlEtoNIrIQwAJr0x5jjNfd3UpdUcuanOOMiIiIkmDYiEJEPgEgzxjzujHmdQD5IvJv7u9aqrLLmmENASxrEhERURLEElF8xBjTZp8xxrQC+Ihre5Tiok1Cy7ImERERJUMswZlHRMQ+IyIeAJnu7VJqC445Y7cmERERJV8sDQH/BHCfiNxmnf8ogMfc26XUFrFb0xvgmDMiIiJKiliCsy8CuAXAx6zz26AdmxNS5IYAljWJiIgoOYZN9xhjAgBeAXAYwBoAlwLY5e5upa7IKwSwrElERETJETVzJiLzAbzT+mkGcB8AGGMuGZ1dS1WchJaIiIjcM1RZczeA9QDeaIzZDwAi8p+jslcpLGJZ08u1NYmIiCg5hkr3vAVAPYBnROR3InIZdIWACS18hQBjDMuaRERElDRRIwpjzMPGmJsALATwDHQZp0ki8hsRuXKU9i/lhHdr+gIGAQMGZ0RERJQUsTQEdBlj/myMuQ5ANYDXoB2cE1L4JLR9Pj2fzbImERERJUFc6R5jTKsx5nZjzGVu7VCqE+PXpZuseXl7vVrmZOaMiIiIkoERRdwCgzo1AXCeMyIiIkoKBmdxEhMI69S0MmdcIYCIiIiSgBFFnMT4By16DrCsSURERMnBiCJOYljWJCIiIvcwOIuTljWdi56zIYCIiIiShxFFnKKWNTnmjIiIiJKAEUXcWNYkIiIi9zA4i9Ogbk0fy5pERESUPIwo4qRlzeBh6/Myc0ZERETJw+AsTlG7NTnmjIiIiJKAEUWcWNYkIiIiNzGiiFP0SWhZ1iQiIqKRY3AWt7CypjXmLJOZMyIiIkoCRhRxGjQJrc+PDI/AkyZjuFdEREQ0XjA4i9Pg4CzAkiYRERElDYOzOInxh3Vr+tkMQEREREnDqCJOg7o1vQEGZ0RERJQ0jCriFLGsmcGyJhERESUHg7O4BQBxrBDAsiYRERElEaOKOOk8Z85JaFnWJCIiouRhVBGnQWVNL7s1iYiIKHkYnMUpfG3NXp+f62oSERFR0jCqiBO7NYmIiMhNjCripGPOwhoC2K1JREREScLgLG5ha2uyIYCIiIiSiFFFnAaVNbl8ExERESURg7M4aVnT2a3Jec6IiIgoeRhVxCm8W1NXCOBhJCIiouRgVBEn5zxnxhiWNYmIiCipGJzFyVnW7PcHAIBlTSIiIkoaRhVxC5Y1+3wMzoiIiCi5GFXEybm2Zp/XCs44zxkRERElCYOzODnHnPX5/ACYOSMiIqLkYVQRJ+3W1MPGsiYRERElG6OKOEUsa7Jbk4iIiJKEwVmcIpY1Oc8ZERERJQmjingYA2G3JhEREbmIUUU8jAZjA2VNH8uaRERElFwMzuIR0DIm0qyGAC+7NYmIiCi5GFXEw1jBWVhZM5tjzoiIiChJGFXEI+DTU5Y1iYiIyCUMzuIxUNbkJLRERETkDkYV8bAbAuyyJuc5IyIioiRjcBaPgbKmBmO9nOeMiIiIkszVqEJErhKRPSKyX0S+FOHyi0Rks4j4RORtYZf5RWSL9bPOzf2MWXhZ08qcZXoYnBEREVFypLt1xyLiAfArAFcAqAWwQUTWGWN2Oq52FMD7AXwuwl30GGOWubV/CYnQrZnpSUNamozhThEREdF44lpwBmANgP3GmIMAICL3ArgBwEBwZow5bF0WcHE/kmdQt6afzQBERESUVG4GZ1UAjjnO1wI4O47bZ4vIRgA+AN8zxjwcfgURuQXALQBQWVmJmpqahHc2Fjnd9TgbwK49e3HiVA0OHe2DGJ/rj5vKOjs7J/TfH47HIxSPRygej1A8HqF4PEJN5OPhZnA2UjOMMXUiMhvA0yKy3RhzwHkFY8ztAG4HgFWrVpm1a9e6u0fN+4BXgUWLz8CipWvxt8atKOg4CdcfN4XV1NRM6L8/HI9HKB6PUDweoXg8QvF4hJrIx8PNmlwdgGmO89XWtpgYY+qs04MAagAsT+bOJSSsW7PP52enJhERESWVm5HFBgDzRGSWiGQCuAlATF2XIlIiIlnW7+UAzodjrNqYGTQJbYBznBEREVFSuRacGWN8AD4J4HEAuwD8xRizQ0RuFZHrAUBEVotILYAbAdwmIjusmy8CsFFEtgJ4BjrmbOyDM7tb07F8ExsCiIiIKJlcHXNmjHkUwKNh277m+H0DtNwZfrsXAZzp5r4lxC5rDqwQwG5NIiIiSi5GFvEIWDN+OMuaGSxrEhERUfIwOIuHiTTmjIeQiIiIkoeRRTzCy5qchJaIiIiSjJFFPCKsrcluTSIiIkomBmfxiNStyXnOiIiIKIkYWcQjEL7wOcuaRERElFyMLOIxUNbUw8ZJaImIiCjZGJzFw1HWNMagn92aRERElGSMLOLh6Nbs8+mcZxxzRkRERMnEyCIejm7NgeCMZU0iIiJKIgZn8TD2CgHp6PNpoMayJhERESUTI4t4DJQ109DntTNnPIRERESUPIws4hGprMm1NYmIiCiJGJzFw9Gt2etlWZOIiIiSj5FFPCJ1azI4IyIioiRiZBGPkLKmnTljWZOIiIiSh8FZPEK6NTnPGRERESUfI4t4sFuTiIiIXMbIIh4saxIREZHLGJzFw9GtyYYAIiIicgMji3hE6NbM5jxnRERElEQMzuIRsBsCPOiz5zljQwARERElESOLeNhlTUljWZOIiIhcwcgiHgEfDNIAkYHgLNPDQ0hERETJw8giHgE/jOgh6/P5kZmeBhEZ450iIiKi8YTBWTyMH0a0AcDrM8hi1oyIiIiSjNFFPByZM68/gAyONyMiIqIkY3QRj/DgzMOSJhERESUXg7N4OMqa/f4AMljWJCIioiRjdBGPgA/2IfP6DTs1iYiIKOkYXcTDWdb0MXNGREREycfoIh4mEOzW9AeQkc4xZ0RERJRcDM7iEfANZM445oyIiIjcwOgiHo6yZj/LmkREROQCRhfxcE5C6w+wIYCIiIiSjtFFPMK6NTnPGRERESUbg7N4BAJhk9Dy8BEREVFyMbqIR/gktFy+iYiIiJKM0UU8HN2aXn+AC58TERFR0jG6iEfIJLSGZU0iIiJKOkYX8Qjr1uQktERERJRsDM7iEfDDPmSchJaIiIjcwOgiHs6yJuc5IyIiIhcwuoiHcQZnHHNGREREycfoIh4BH4x44A8Y+AMMzoiIiCj5GF3Ewyprev0BAGBDABERESUdg7N4hAVnHHNGREREycboIh5GuzW9fgMALGsSERFR0jG6iIeVOev3WWVNBmdERESUZIwu4mE1BAyMOfNwzBkRERElF4OzeFhTafTbY8648DkRERElGaOLeAQCod2aLGsSERFRkjG6iIdd1vSxIYCIiIjcwegiHla3JsuaRERE5BZGF/EIn4SWDQFERESUZAzO4hHwh3RrchJaIiIiSrb0sd6B08qFn8HJxgAbAoiIiMg1jC7iccGn0Vq6Av1sCCAiIiKXMLpIwEBZkwufExERUZIxOEsAy5pERETkFkYXCWBwRkRERG5hdJGAfj/HnBEREZE7GF0kwOvjVBpERETkDkYXCbBXCMhgQwARERElGYOzBNiZM5Y1iYiIKNkYXSTAbghIT2PmjIiIiJKLwVkC+v0GmZ40iDA4IyIiouRicJYArz/ARc+JiIjIFQzOEuD1B5CZzkNHREREyccIIwGaOeOhIyIiouRjhJGAfp9hcEZERESuYISRAJY1iYiIyC2MMBLAhgAiIiJyC4OzBHDMGREREbnF1QhDRK4SkT0isl9EvhTh8otEZLOI+ETkbWGX3Swi+6yfm93cz3j1+znmjIiIiNzhWoQhIh4AvwJwNYDFAN4pIovDrnYUwPsB/DnstqUAvg7gbABrAHxdRErc2td4eX0BLnpORERErnAzwlgDYL8x5qAxph/AvQBucF7BGHPYGLMNQCDstm8A8IQxpsUY0wrgCQBXubivcfH6A1z0nIiIiFyR7uJ9VwE45jhfC82EJXrbqvAricgtAG4BgMrKStTU1CS0o/Ho7OzEyTYPCjJkVB4v1XV2dvI4OPB4hOLxCMXjEYrHIxSPR6iJfDzcDM5cZ4y5HcDtALBq1Sqzdu1a1x+zpqYGWTlpqCzNxdq1q1x/vFRXU1OD0Tjupwsej1A8HqF4PELxeITi8Qg1kY+Hm2XNOgDTHOerrW1u39Z1Xj/HnBEREZE73IwwNgCYJyKzRCQTwE0A1sV428cBXCkiJVYjwJXWtpTg9RvOc0ZERESucC04M8b4AHwSGlTtAvAXY8wOEblVRK4HABFZLSK1AG4EcJuI7LBu2wLgv6EB3gYAt1rbUgLnOSMiIiK3uDrmzBjzKIBHw7Z9zfH7BmjJMtJt7wRwp5v7lygu30RERERuYYSRgH4fM2dERETkDkYYCfD6DTNnRERE5ApGGAngwudERETkFgZncQoYA1+Aa2sSERGROxhhxMlv9JTBGREREbmBEUacfNYqoJyEloiIiNzACCNOfis445gzIiIicgODszj5AlrXzGC3JhEREbmAEUacfBxzRkRERC5ihBEnjjkjIiIiNzHCiJNvYMwZDx0RERElHyOMOA2MOWNDABEREbmAwVmcBsacsSGAiIiIXMAII05+jjkjIiIiFzHCiNNAQwAzZ0REROQCRhhx8hl7zBkPHRERESUfI4w4+bhCABEREbmIwVmcOOaMiIiI3MQII05cIYCIiIjcxAgjTlxbk4iIiNzECCNOHHNGREREbmJwFieOOSMiIiI3McKIE8ecERERkZsYYcQpuLYmDx0RERElHyOMOHHMGREREbmJwVmcfAENzEQYnBEREVHyMTiLk88YljSJiIjINYwy4qSZMx42IiIicgejjDj5GZwRERGRixhlxMlngCyuDkBEREQuYZQRJ1/AsFOTiIiIXMPgLE4cc0ZERERuYpQRJ79hcEZERETuYZQRJ18AyOCYMyIiInIJo4w4+QIGmRxzRkRERC5hcBYnljWJiIjITYwy4sSGACIiInITo4w4MTgjIiIiNzHKiJPPGGSmc8wZERERuYPBWZy4fBMRERG5iVFGnLwMzoiIiMhFjDLixDFnRERE5CZGGXHyG85zRkRERO5hcBYnZs6IiIjITYwy4uQLAJlcvomIiIhcwigjDsYYrhBARERErmKUEQev3wBg5oyIiIjcwygjDl5/AACQwYYAIiIicgmDszgEgzMeNiIiInIHo4w49DM4IyIiIpcxyojDwJgzBmdERETkEkYZcfD6rMwZFz4nIiIilzA4iwPHnBEREZHbGGXEgWPOiIiIyG2MMuLAMWdERETkNkYZcej3MXNGRERE7mKUEQdOQktERERuY3AWh4ExZ1y+iYiIiFzCKCMO9lQaHHNGREREbmGUEQcufE5ERERuY5QRB85zRkRERG5jlBGHfjYEEBERkcsYnMXBzpxxzBkRERG5hVFGHLyc54yIiIhcxigjDnZDAKfSICIiIrcwyogDx5wRERGR2xicxWGgWzONh42IiIjcwSgjDl5/AB4B0tKYOSMiIiJ3MDiLg9dvwF4AIiIichNDjTj0+wJIZ9KMiIiIXMTgLA79/gDYqElERERuYqgRB68vgHSONyMiIiIXMTiLg90QQEREROQWBmdx8PoNy5pERETkKldDDRG5SkT2iMh+EflShMuzROQ+6/JXRGSmtX2miPSIyBbr57du7mesdMwZU2dERETknnS37lhEPAB+BeAKALUANojIOmPMTsfVPgSg1RgzV0RuAvA/AN5hXXbAGLPMrf1LhJcNAUREROQyN0ONNQD2G2MOGmP6AdwL4Iaw69wA4G7r9wcAXCYiKZua8vo5lQYRERG5y7XMGYAqAMcc52sBnB3tOsYYn4icAlBmXTZLRF4D0A7gv4wx68MfQERuAXALAFRWVqKmpiapf0C4ppM9EON3/XFOJ52dnTweDjweoXg8QvF4hOLxCMXjEWoiHw83g7ORqAcw3RhzUkRWAnhYRJYYY9qdVzLG3A7gdgBYtWqVWbt2ras79dMdL8DX0wG3H+d0UlNTw+PhwOMRiscjFI9HKB6PUDweoSby8XCzrFkHYJrjfLW1LeJ1RCQdQBGAk8aYPmPMSQAwxmwCcADAfBf3NSYsaxIREZHb3AzONgCYJyKzRCQTwE0A1oVdZx2Am63f3wbgaWOMEZEKq6EAIjIbwDwAB13c15iwIYCIiIjc5lpZ0xpD9kkAjwPwALjTGLNDRG4FsNEYsw7A7wHcIyL7AbRAAzgAuAjArSLiBRAA8DFjTItb+xorr98gPWOs94KIiIjGM1fHnBljHgXwaNi2rzl+7wVwY4TbPQjgQTf3LRH9vgA8maxrEhERkXtYpIsDy5pERETkNoYacWBwRkRERG5jqBGH6aW5KM5iWZOIiIjcw+AsDo988gJcNydzrHeDiIiIxjEGZ0REREQphMEZERERUQphcEZERESUQhicEREREaUQBmdEREREKYTBGREREVEKYXBGRERElEIYnBERERGlEAZnRERERCmEwRkRERFRCmFwRkRERJRCGJwRERERpRAGZ0REREQphMEZERERUQphcEZERESUQhicEREREaUQBmdEREREKYTBGREREVEKYXBGRERElEIYnBERERGlEAZnRERERCmEwRkRERFRCmFwRkRERJRCxBgz1vuQFCLSBODIKDxUOYDmUXic0wWPRygej1A8HqF4PELxeITi8Qg13o/HDGNMRaQLxk1wNlpEZKMxZtVY70eq4PEIxeMRiscjFI9HKB6PUDweoSby8WBZk4iIiCiFMDgjIiIiSiEMzuJ3+1jvQIrh8QjF4xGKxyMUj0coHo9QPB6hJuzx4JgzIiIiohTCzBkRERFRCmFwRkRERJRCGJzFSESuEpE9IrJfRL401vsz2kRkmog8IyI7RWSHiHzK2l4qIk+IyD7rtGSs93U0iYhHRF4Tkb9b52eJyCvW8+Q+Eckc630cTSJSLCIPiMhuEdklIudO5OeIiPyn9Xp5XUT+T0SyJ9JzRETuFJFGEXndsS3i80HUz63jsk1EVozdnrsjyvH4gfV62SYiD4lIseOyL1vHY4+IvGFMdtpFkY6H47LPiogRkXLr/Lh/fjgxOIuBiHgA/ArA1QAWA3iniCwe270adT4AnzXGLAZwDoBPWMfgSwCeMsbMA/CUdX4i+RSAXY7z/wPgJ8aYuQBaAXxoTPZq7PwMwD+NMQsBnAU9NhPyOSIiVQD+A8AqY8wZADwAbsLEeo7cBeCqsG3Rng9XA5hn/dwC4DejtI+j6S4MPh5PADjDGLMUwF4AXwYA6/31JgBLrNv82vosGk/uwuDjARGZBuBKAEcdmyfC82MAg7PYrAGw3xhz0BjTD+BeADeM8T6NKmNMvTFms/V7B/RDtwp6HO62rnY3gDeNyQ6OARGpBnAtgDus8wLgUgAPWFeZaMejCMBFAH4PAMaYfmNMGybwcwRAOoAcEUkHkAugHhPoOWKMeQ5AS9jmaM+HGwD8waiXARSLyJRR2dFREul4GGP+ZYzxWWdfBlBt/X4DgHuNMX3GmEMA9kM/i8aNKM8PAPgJgC8AcHYsjvvnhxODs9hUATjmOF9rbZuQRGQmgOUAXgFQaYypty5qAFA5Vvs1Bn4KfQMJWOfLALQ53mgn2vNkFoAmAP9rlXrvEJE8TNDniDGmDsAPod/+6wGcArAJE/s5AkR/PvB9FvgggMes3yfk8RCRGwDUGWO2hl00oY4HgzOKi4jkA3gQwKeNMe3Oy4zOyzIh5mYRkTcCaDTGbBrrfUkh6QBWAPiNMWY5gC6ElTAn2HOkBPptfxaAqQDyEKGEM5FNpOfDcETkq9DhI38a630ZKyKSC+ArAL421vsy1hicxaYOwDTH+Wpr24QiIhnQwOxPxpi/WptP2Kll67RxrPZvlJ0P4HoROQwtc18KHW9VbJWwgIn3PKkFUGuMecU6/wA0WJuoz5HLARwyxjQZY7wA/gp93kzk5wgQ/fkwYd9nReT9AN4I4N0mOPnoRDwec6BfZrZa763VADaLyGRMsOPB4Cw2GwDMs7qsMqGDNNeN8T6NKms81e8B7DLG/Nhx0ToAN1u/3wzgkdHet7FgjPmyMabaGDMT+nx42hjzbgDPAHibdbUJczwAwBjTAOCYiCywNl0GYCcm6HMEWs48R0RyrdePfTwm7HPEEu35sA7A+6yuvHMAnHKUP8ctEbkKOjziemNMt+OidQBuEpEsEZkFHQj/6ljs42gxxmw3xkwyxsy03ltrAayw3lsm1vPDGMOfGH4AXAPtpDkA4KtjvT9j8PdfAC0/bAOwxfq5BjrO6ikA+wA8CaB0rPd1DI7NWgB/t36fDX0D3Q/gfgBZY71/o3wslgHYaD1PHgZQMpGfIwC+CWA3gNcB3AMgayI9RwD8H3S8nRf6QfuhaM8HAALtij8AYDu0y3XM/4ZROB77oWOp7PfV3zqu/1XreOwBcPVY7/9oHI+wyw8DKJ8ozw/nD5dvIiIiIkohLGsSERERpRAGZ0REREQphMEZERERUQphcEZERESUQhicEREREaUQBmdENK6JiF9Etjh+krbwuojMFJHXk3V/RESALrdCRDSe9Rhjlo31ThARxYqZMyKakETksIh8X0S2i8irIjLX2j5TRJ4WkW0i8pSITLe2V4rIQyKy1fo5z7orj4j8TkR2iMi/RCTHuv5/iMhO637uHaM/k4hOQwzOiGi8ywkra77DcdkpY8yZAH4J4KfWtl8AuNsYsxS6CPXPre0/B/CsMeYs6JqhO6zt8wD8yhizBEAbgLda278EYLl1Px9z508jovGIKwQQ0bgmIp3GmPwI2w8DuNQYc1BEMgA0GGPKRKQZwBRjjNfaXm+MKReRJgDVxpg+x33MBPCEMWaedf6LADKMMd8SkX8C6IQuY/WwMabT5T+ViMYJZs6IaCIzUX6PR5/jdz+CY3mvha4FuALABhHhGF8iigmDMyKayN7hOH3J+v1FADdZv78bwHrr96cAfBwARMQjIkXR7lRE0gBMM8Y8A+CLAIoADMreERFFwm9yRDTe5YjIFsf5fxpj7Ok0SkRkGzT79U5r278D+F8R+TyAJgAfsLZ/CsDtIvIhaIbs4wDqozymB8AfrQBOAPzcGNOWpL+HiMY5jjkjognJGnO2yhjTPNb7QkTkxLImERERUQph5oyIiIgohTBzRkRERJRCGJwRERERpRAGZ0REREQphMEZERERUQphcEZERESUQv4/WUR3u0c1LcgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make a quick epoch vs. accuracy plot\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.grid()\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Depth 7 Model Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1569 predict_function  *\n        return step_function(self, iterator)\n    C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1559 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1285 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2833 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3608 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1552 run_step  **\n        outputs = model.predict_step(data)\n    C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1525 predict_step\n        return self(x, training=False)\n    C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1013 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:251 assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer sequential_13 is incompatible with the layer: expected axis -1 of input shape to have value 47 but received input with shape (32, 2)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-5e63e6ecd2d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mpredictions_d7\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_d7\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test_d7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Time: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mend\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1725\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1726\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1727\u001b[1;33m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1728\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1729\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    931\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 933\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    934\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    761\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    762\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m--> 763\u001b[1;33m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    764\u001b[0m             *args, **kwds))\n\u001b[0;32m    765\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3048\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3049\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3050\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3051\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3052\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3442\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3443\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3444\u001b[1;33m           \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3445\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3277\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3278\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3279\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3280\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3281\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    997\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    998\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 999\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1000\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1001\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    670\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 672\u001b[1;33m           \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    673\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    674\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    984\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1569 predict_function  *\n        return step_function(self, iterator)\n    C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1559 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1285 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2833 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3608 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1552 run_step  **\n        outputs = model.predict_step(data)\n    C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1525 predict_step\n        return self(x, training=False)\n    C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1013 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    C:\\Users\\Bensom2\\miniconda3\\envs\\my_qiskit\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:251 assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer sequential_13 is incompatible with the layer: expected axis -1 of input shape to have value 47 but received input with shape (32, 2)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"\"\"\n",
    "# predict\n",
    "from sklearn.metrics import precision_score,accuracy_score, f1_score, recall_score, hamming_loss\n",
    "\n",
    "start = time.time()\n",
    "predictions_d7 = model_d7.predict(x_test_d7.values)\n",
    "end = time.time()\n",
    "print(\"Time: \" + str(end - start))\n",
    "\n",
    "pred=predictions_d7.copy()\n",
    "\n",
    "\n",
    "thresholds=[0.1, 0.2, 0.3, .4, .5, .6, .7, .8, .9]\n",
    "for val in thresholds:\n",
    "    pred=predictions_d7.copy()\n",
    "\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "    \n",
    "  \n",
    "    precision = precision_score(Y_test_d7, pred, average='micro')\n",
    "    recall = recall_score(Y_test_d7, pred, average='micro')\n",
    "    f1 = f1_score(Y_test_d7, pred, average='micro')\n",
    "   \n",
    "    print(\"Micro-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "    print(\"Accuracy = \",accuracy_score(Y_test_d7, pred))\n",
    "    print(\"Accuracy = \",partial_accuracy(Y_test_d7, pred))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npredictions_d7 = model_d7.predict(x_test_d7.values)\\npred=predictions_d7.copy()\\n\\nn_classes = 99\\n\\nfpr = dict()\\ntpr = dict()\\nroc_auc = dict()\\nfor i in range(n_classes):\\n    fpr[i], tpr[i], _ = roc_curve(Y_test_d7[:, i], pred[:, i])\\n    roc_auc[i] = auc(fpr[i], tpr[i])\\n\\nfpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(Y_test_d7.ravel(), pred.ravel())\\nroc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\\n    \\n# Plot of a ROC curve for a specific class\\nfor i in range(n_classes):\\n    plt.figure()\\n    plt.plot(fpr[i], tpr[i], label=\\'ROC curve (area = %0.2f)\\' % roc_auc[i])\\n    plt.plot([0, 1], [0, 1], \\'k--\\')\\n    plt.xlim([0.0, 1.0])\\n    plt.ylim([0.0, 1.05])\\n    plt.xlabel(\\'False Positive Rate\\')\\n    plt.ylabel(\\'True Positive Rate\\')\\n    plt.title(\\'Receiver operating characteristic example\\')\\n    plt.legend(loc=\"lower right\")\\n    plt.show()'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "predictions_d7 = model_d7.predict(x_test_d7.values)\n",
    "pred=predictions_d7.copy()\n",
    "\n",
    "n_classes = 99\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(Y_test_d7[:, i], pred[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(Y_test_d7.ravel(), pred.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "    \n",
    "# Plot of a ROC curve for a specific class\n",
    "for i in range(n_classes):\n",
    "    plt.figure()\n",
    "    plt.plot(fpr[i], tpr[i], label='ROC curve (area = %0.2f)' % roc_auc[i])\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# First aggregate all false positive rates\\nall_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\\n\\n# Then interpolate all ROC curves at this points\\nmean_tpr = np.zeros_like(all_fpr)\\n\\nfor i in range(n_classes):\\n    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\\n\\n# Finally average it and compute AUC\\nmean_tpr /= n_classes\\n\\nfpr[\"macro\"] = all_fpr\\ntpr[\"macro\"] = mean_tpr\\nroc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\\n\\n# Plot all ROC curves\\nplt.figure()\\nplt.plot(fpr[\"micro\"], tpr[\"micro\"],\\n         label=\\'micro-average ROC curve (area = {0:0.2f})\\'\\n               \\'\\'.format(roc_auc[\"micro\"]),\\n         color=\\'deeppink\\', linestyle=\\':\\', linewidth=4)\\n\\nplt.plot([0, 1], [0, 1], \\'k--\\', lw=2)\\nplt.xlim([0.0, 1.0])\\nplt.ylim([0.0, 1.05])\\nplt.xlabel(\\'False Positive Rate\\')\\nplt.ylabel(\\'True Positive Rate\\')\\nplt.title(\\'Some extension of Receiver operating characteristic to multi-class\\')\\nplt.legend(loc=\"lower right\")\\nplt.show()'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure()\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Some extension of Receiver operating characteristic to multi-class')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nx_train_d3.to_csv(\"x_train_d3_01.csv\")\\nx_test_d3.to_csv(\"x_test_d3_01.csv\")\\npd.DataFrame(Y_train_d3).to_csv(\"Y_train_d3_01.csv\")\\npd.DataFrame(Y_test_d3).to_csv(\"Y_test_d3_01.csv\")\\n\\nx_train_d5.to_csv(\"x_train_d5_01.csv\")\\nx_test_d5.to_csv(\"x_test_d5_01.csv\")\\npd.DataFrame(Y_train_d5).to_csv(\"Y_train_d5_01.csv\")\\npd.DataFrame(Y_test_d5).to_csv(\"Y_test_d5_01.csv\")\\n\\nx_train_d7.to_csv(\"x_train_d7_01.csv\")\\nx_test_d7.to_csv(\"x_test_d7_01.csv\")\\npd.DataFrame(Y_train_d7).to_csv(\"Y_train_d7_01.csv\")\\npd.DataFrame(Y_test_d7).to_csv(\"Y_test_d7_01.csv\")'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "x_train_d3.to_csv(\"x_train_d3_01.csv\")\n",
    "x_test_d3.to_csv(\"x_test_d3_01.csv\")\n",
    "pd.DataFrame(Y_train_d3).to_csv(\"Y_train_d3_01.csv\")\n",
    "pd.DataFrame(Y_test_d3).to_csv(\"Y_test_d3_01.csv\")\n",
    "\n",
    "x_train_d5.to_csv(\"x_train_d5_01.csv\")\n",
    "x_test_d5.to_csv(\"x_test_d5_01.csv\")\n",
    "pd.DataFrame(Y_train_d5).to_csv(\"Y_train_d5_01.csv\")\n",
    "pd.DataFrame(Y_test_d5).to_csv(\"Y_test_d5_01.csv\")\n",
    "\n",
    "x_train_d7.to_csv(\"x_train_d7_01.csv\")\n",
    "x_test_d7.to_csv(\"x_test_d7_01.csv\")\n",
    "pd.DataFrame(Y_train_d7).to_csv(\"Y_train_d7_01.csv\")\n",
    "pd.DataFrame(Y_test_d7).to_csv(\"Y_test_d7_01.csv\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmodel_d7.save(\"model_d7_01.h5\")\\nmodel_d5.save(\"model_d5_01.h5\")\\nmodel.save(\"model_d3_01.h5\")'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "model_d7.save(\"model_d7_01.h5\")\n",
    "model_d5.save(\"model_d5_01.h5\")\n",
    "model.save(\"model_d3_01.h5\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
