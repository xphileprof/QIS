{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import time\n",
    "import random\n",
    "import copy\n",
    "\n",
    "from scipy import interp\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.metrics import make_scorer #CHANGE (updated to be consistent with scikit_learn .24)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, ShuffleSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.layers import Dense, Input, Dropout\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import precision_score,accuracy_score, f1_score, recall_score, hamming_loss\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras import optimizers\n",
    "from keras import callbacks\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras import backend as K\n",
    "\n",
    "#%matplotlib inline\n",
    "#import mpld3\n",
    "#mpld3.enable_notebook()\n",
    "\n",
    "print (pd.__version__)\n",
    "\n",
    "######### DEFINITION OF GLOBAL VARIABLES #########\n",
    "RUN_CONFIGURATION_LOOP = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "#sys.path.append('/')\n",
    "import circuits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are super long functions to be hard coded because i dont have time to properly fix them, sorry bout it\n",
    "#[(0, 0.5, 0.5), (0, 0.5, 2.5), (0, 1.5, -0.5), (0, 1.5, 1.5)]\n",
    "def graph_with_errs_d3(df):\n",
    "    x_data = []\n",
    "    z_data = []\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        x_data.append([])\n",
    "        z_data.append([])\n",
    "        \n",
    "        if df.loc[i].at[\"X0\"] == -1:\n",
    "            x_data[i].append((0, -0.5, 0.5))\n",
    "        if df.loc[i].at[\"Z1\"] == -1:\n",
    "            z_data[i].append((0, 0.5, 0.5))\n",
    "        if df.loc[i].at[\"X2\"] == -1:\n",
    "            x_data[i].append((0, 0.5, 1.5))\n",
    "        if df.loc[i].at[\"Z3\"] == -1:\n",
    "            z_data[i].append((0, 0.5, 2.5))\n",
    "        if df.loc[i].at[\"Z4\"] == -1:\n",
    "            z_data[i].append((0, 1.5, -0.5))\n",
    "        if df.loc[i].at[\"X5\"] == -1:\n",
    "            x_data[i].append((0, 1.5, 0.5))\n",
    "        if df.loc[i].at[\"X6\"] == -1:\n",
    "            x_data[i].append((0, 2.5, 1.5))\n",
    "        if df.loc[i].at[\"Z7\"] == -1:\n",
    "            z_data[i].append((0, 1.5, 1.5))\n",
    "        x_data[i] = str(x_data[i])\n",
    "        z_data[i] = str(z_data[i])\n",
    "    #graph_df = pd.DataFrame(df[\"Labels\"], x_data, z_data, columns=[\"Labels\", \"XSyn\", \"ZSyn\"])\n",
    "    graph_df = pd.DataFrame({\"XSyn\":x_data, \"ZSyn\":z_data})\n",
    "    return graph_df\n",
    "        \n",
    "\n",
    "def graph_with_errs_d5(df):\n",
    "    x_data = []\n",
    "    z_data = []\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        x_data.append([])\n",
    "        z_data.append([])\n",
    "        if df.loc[i].at[\"X0\"] == -1:\n",
    "             x_data[i].append((0, -0.5, 0.5))\n",
    "        if df.loc[i].at[\"Z1\"] == -1:\n",
    "            z_data[i].append((0, 0.5, 0.5))\n",
    "        if df.loc[i].at[\"X2\"] == -1:\n",
    "            x_data[i].append((0, 0.5, 1.5))\n",
    "        if df.loc[i].at[\"X3\"] == -1:\n",
    "            x_data[i].append((0, -0.5, 2.5))\n",
    "        if df.loc[i].at[\"Z4\"] == -1:\n",
    "            z_data[i].append((0, 0.5, 2.5))\n",
    "        if df.loc[i].at[\"X5\"] == -1:\n",
    "            x_data[i].append((0, 0.5, 3.5))\n",
    "        if df.loc[i].at[\"Z6\"] == -1:\n",
    "            z_data[i].append((0, 0.5, 4.5))\n",
    "        if df.loc[i].at[\"Z7\"] == -1:\n",
    "            z_data[i].append((0, 1.5, -0.5))\n",
    "        if df.loc[i].at[\"X8\"] == -1:\n",
    "            x_data[i].append((0, 1.5, 0.5))\n",
    "        if df.loc[i].at[\"Z9\"] == -1:\n",
    "            z_data[i].append((0, 1.5, 1.5))\n",
    "        if df.loc[i].at[\"X10\"] == -1:\n",
    "            x_data[i].append((0, 1.5, 2.5))\n",
    "        if df.loc[i].at[\"Z11\"] == -1:\n",
    "            z_data[i].append((0, 1.5, 3.5))\n",
    "        if df.loc[i].at[\"Z12\"] == -1:\n",
    "            z_data[i].append((0, 2.5, 0.5))\n",
    "        if df.loc[i].at[\"X13\"] == -1:\n",
    "            x_data[i].append((0, 2.5, 1.5))\n",
    "        if df.loc[i].at[\"Z14\"] == -1:\n",
    "            z_data[i].append((0, 2.5, 2.5))\n",
    "        if df.loc[i].at[\"X15\"] == -1:\n",
    "            x_data[i].append((0, 2.5, 3.5))\n",
    "        if df.loc[i].at[\"Z16\"] == -1:\n",
    "            z_data[i].append((0, 2.5, 4.5))\n",
    "        if df.loc[i].at[\"Z17\"] == -1:\n",
    "            z_data[i].append((0, 3.5, -0.5))\n",
    "        if df.loc[i].at[\"X18\"] == -1:\n",
    "            x_data[i].append((0, 3.5, 0.5))\n",
    "        if df.loc[i].at[\"X19\"] == -1:\n",
    "            x_data[i].append((0, 4.5, 1.5))\n",
    "        if df.loc[i].at[\"Z20\"] == -1:\n",
    "            z_data[i].append((0, 3.5, 1.5))\n",
    "        if df.loc[i].at[\"X21\"] == -1:\n",
    "            x_data[i].append((0, 3.5, 2.5))\n",
    "        if df.loc[i].at[\"X22\"] == -1:\n",
    "            x_data[i].append((0, 4.5, 3.5))\n",
    "        if df.loc[i].at[\"Z23\"] == -1:\n",
    "            z_data[i].append((0, 3.5, 3.5))\n",
    "        x_data[i] = str(x_data[i])\n",
    "        z_data[i] = str(z_data[i])\n",
    "            \n",
    "    graph_df = pd.DataFrame({\"XSyn\":x_data, \"ZSyn\":z_data})\n",
    "    return graph_df\n",
    "    \n",
    "def graph_with_errs_d7(df):\n",
    "    x_data = []\n",
    "    z_data = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        x_data.append([])\n",
    "        z_data.append([])\n",
    "        \n",
    "        if df.loc[i].at[\"X0\"] == -1:\n",
    "            x_data[i].append((0, -0.5, 0.5))\n",
    "        if df.loc[i].at[\"Z1\"] == -1:\n",
    "            z_data[i].append((0, 0.5, 0.5))\n",
    "        if df.loc[i].at[\"X2\"] == -1:\n",
    "            x_data[i].append((0, 0.5, 1.5))\n",
    "        if df.loc[i].at[\"X3\"] == -1:\n",
    "            x_data[i].append((0, -0.5, 2.5))\n",
    "        if df.loc[i].at[\"Z4\"] == -1:\n",
    "            z_data[i].append((0, 0.5, 2.5))\n",
    "        if df.loc[i].at[\"X5\"] == -1:\n",
    "            x_data[i].append((0, 0.5, 3.5))\n",
    "        if df.loc[i].at[\"X6\"] == -1:\n",
    "            x_data[i].append((0, -0.5, 4.5))\n",
    "        if df.loc[i].at[\"Z7\"] == -1:\n",
    "            z_data[i].append((0, 0.5, 4.5))\n",
    "        if df.loc[i].at[\"X8\"] == -1:\n",
    "            x_data[i].append((0, 0.5, 5.5))\n",
    "        if df.loc[i].at[\"Z9\"] == -1:\n",
    "            z_data[i].append((0, 0.5, 6.5))\n",
    "        if df.loc[i].at[\"Z10\"] == -1:\n",
    "            z_data[i].append((0, 1.5, -0.5))\n",
    "        if df.loc[i].at[\"X11\"] == -1:\n",
    "            x_data[i].append((0, 1.5, 0.5))\n",
    "        if df.loc[i].at[\"Z12\"] == -1:\n",
    "            z_data[i].append((0, 1.5, 1.5))\n",
    "        if df.loc[i].at[\"X13\"] == -1:\n",
    "            x_data[i].append((0, 1.5, 2.5))\n",
    "        if df.loc[i].at[\"Z14\"] == -1:\n",
    "            z_data[i].append((0, 1.5, 3.5))\n",
    "        if df.loc[i].at[\"X15\"] == -1:\n",
    "            x_data[i].append((0, 1.5, 4.5))\n",
    "        if df.loc[i].at[\"Z16\"] == -1:\n",
    "            z_data[i].append((0, 1.5, 5.5))\n",
    "        if df.loc[i].at[\"Z17\"] == -1:\n",
    "            z_data[i].append((0, 2.5, 0.5))\n",
    "        if df.loc[i].at[\"X18\"] == -1:\n",
    "            x_data[i].append((0, 2.5, 1.5))\n",
    "        if df.loc[i].at[\"Z19\"] == -1:\n",
    "            z_data[i].append((0, 2.5, 2.5))\n",
    "        if df.loc[i].at[\"X20\"] == -1:\n",
    "            x_data[i].append((0, 2.5, 3.5))\n",
    "        if df.loc[i].at[\"Z21\"] == -1:\n",
    "            z_data[i].append((0, 2.5, 4.5))\n",
    "        if df.loc[i].at[\"X22\"] == -1:\n",
    "            x_data[i].append((0, 2.5, 5.5))\n",
    "        if df.loc[i].at[\"Z23\"] == -1:\n",
    "            z_data[i].append((0, 2.5, 6.5))\n",
    "        if df.loc[i].at[\"Z24\"] == -1:\n",
    "            z_data[i].append((0, 3.5, -0.5))\n",
    "        if df.loc[i].at[\"X25\"] == -1:\n",
    "            x_data[i].append((0, 3.5, 0.5))\n",
    "        if df.loc[i].at[\"Z26\"] == -1:\n",
    "            z_data[i].append((0, 3.5, 1.5))\n",
    "        if df.loc[i].at[\"X27\"] == -1:\n",
    "            x_data[i].append((0, 3.5, 2.5))\n",
    "        if df.loc[i].at[\"Z28\"] == -1:\n",
    "            z_data[i].append((0, 3.5, 3.5))\n",
    "        if df.loc[i].at[\"X29\"] == -1:\n",
    "            x_data[i].append((0, 3.5, 4.5))\n",
    "        if df.loc[i].at[\"Z30\"] == -1:\n",
    "            z_data[i].append((0, 3.5, 5.5))\n",
    "        if df.loc[i].at[\"Z31\"] == -1:\n",
    "            z_data[i].append((0, 4.5, 0.5))\n",
    "        if df.loc[i].at[\"X32\"] == -1:\n",
    "            x_data[i].append((0, 4.5, 1.5))\n",
    "        if df.loc[i].at[\"Z33\"] == -1:\n",
    "            z_data[i].append((0, 4.5, 2.5))\n",
    "        if df.loc[i].at[\"X34\"] == -1:\n",
    "            x_data[i].append((0, 4.5, 3.5))\n",
    "        if df.loc[i].at[\"Z35\"] == -1:\n",
    "            z_data[i].append((0, 4.5, 4.5))\n",
    "        if df.loc[i].at[\"X36\"] == -1:\n",
    "            x_data[i].append((0, 4.5, 5.5))\n",
    "        if df.loc[i].at[\"Z37\"] == -1:\n",
    "            z_data[i].append((0, 4.5, 6.5))\n",
    "        if df.loc[i].at[\"Z38\"] == -1:\n",
    "            z_data[i].append((0, 5.5, -0.5))\n",
    "        if df.loc[i].at[\"X39\"] == -1:\n",
    "            x_data[i].append((0, 5.5, 0.5))\n",
    "        if df.loc[i].at[\"X40\"] == -1:\n",
    "            x_data[i].append((0, 6.5, 1.5))\n",
    "        if df.loc[i].at[\"Z41\"] == -1:\n",
    "            z_data[i].append((0, 5.5, 1.5))\n",
    "        if df.loc[i].at[\"X42\"] == -1:\n",
    "            x_data[i].append((0, 5.5, 2.5))\n",
    "        if df.loc[i].at[\"X43\"] == -1:\n",
    "            x_data[i].append((0, 6.5, 3.5))\n",
    "        if df.loc[i].at[\"Z44\"] == -1:\n",
    "            z_data[i].append((0, 5.5, 3.5))\n",
    "        if df.loc[i].at[\"X45\"] == -1:\n",
    "            x_data[i].append((0, 5.5, 4.5))\n",
    "        if df.loc[i].at[\"X46\"] == -1:\n",
    "            x_data[i].append((0, 6.5, 5.5))\n",
    "        if df.loc[i].at[\"Z47\"] == -1: \n",
    "            z_data[i].append((0, 5.5, 5.5))\n",
    "        x_data[i] = str(x_data[i])\n",
    "        z_data[i] = str(z_data[i])\n",
    "    graph_df = pd.DataFrame({\"XSyn\":x_data, \"ZSyn\":z_data})\n",
    "    return graph_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These functions needed to work with the GraphDecoder/MWPM module\n",
    "import time\n",
    "\n",
    "def add_measurement_errs(curr_syn, prob_err, x_syn, depth):\n",
    "    #x_syn is True if it is x syndrome, False if it is Z syndrome\n",
    "    total_time = 0\n",
    "    new_syn = []\n",
    "    if x_syn:\n",
    "        for i in curr_syn:\n",
    "            rand = random.random()\n",
    "            if rand > prob_err:\n",
    "                new_syn.append(i)\n",
    "        return (new_syn + return_xmeasurement_errs(depth, prob_err))\n",
    "    else:\n",
    "        for i in curr_syn:\n",
    "            rand = random.random()\n",
    "            if rand > prob_err:\n",
    "                new_syn.append(i)\n",
    "        return (new_syn + return_zmeasurement_errs(depth, prob_err))\n",
    "    \n",
    "def do_new_decoding(data, depth, prob_err):\n",
    "    decoder = circuits.GraphDecoder(depth,1)\n",
    "    G = decoder.S['Z']\n",
    "    #decoder.graph_2D(G,'distance')\n",
    "    df = pd.DataFrame()\n",
    "    syn = []\n",
    "    total_time = 0\n",
    "    \n",
    "    for row in data:\n",
    "        x_input = []\n",
    "        z_input = []\n",
    "        x_type = True\n",
    "        for col in row:\n",
    "            if not col == \"[]\":\n",
    "                col = eval(col)\n",
    "                for c in col:\n",
    "                    if x_type:\n",
    "                        x_input.append(c)\n",
    "                    else:\n",
    "                        z_input.append(c)\n",
    "            x_type = not x_type  \n",
    "            \n",
    "        if prob_err > 0:\n",
    "            syndromes_x = add_measurement_errs(x_input, prob_err, True, depth)\n",
    "            syndromes_z = add_measurement_errs(z_input, prob_err, False, depth)\n",
    "        else:\n",
    "            syndromes_x = x_input\n",
    "            syndromes_z = z_input\n",
    "\n",
    "        start = time.time_ns()\n",
    "        error_graph_x, paths_x = decoder.make_error_graph(syndromes_x,'X')\n",
    "        matching_graph_x = decoder.matching_graph(error_graph_x,'X')\n",
    "        matches_x = decoder.matching(matching_graph_x,'X')\n",
    "        flips_x = decoder.calculate_qubit_flips(matches_x, paths_x,'X')\n",
    "        syn_x = (translate_errors(flips_x))\n",
    "\n",
    "        error_graph_z, paths_z = decoder.make_error_graph(syndromes_z,'Z')\n",
    "        matching_graph_z = decoder.matching_graph(error_graph_z,'Z')\n",
    "        matches_z = decoder.matching(matching_graph_z,'Z')\n",
    "        flips_z = decoder.calculate_qubit_flips(matches_z, paths_z,'Z')\n",
    "        syn_z = translate_errors(flips_z)\n",
    "        df = df.append(pd.Series([syn_x, syn_z]), ignore_index=True)\n",
    "        end = time.time_ns()\n",
    "        total_time += (end - start)/ (10 ** 9)\n",
    "    return (df, total_time) \n",
    "\n",
    "import random\n",
    "def return_xmeasurement_errs(depth, prob):\n",
    "    \n",
    "    new_errs = []\n",
    "    \n",
    "    if depth == 3:\n",
    "        errs = [(0, -0.5, 0.5), (0, 0.5, 1.5), (0, 1.5, 0.5), (0, 2.5, 1.5)]\n",
    "    elif depth == 5:\n",
    "        errs = [(0, -0.5, 0.5), (0, 0.5, 1.5), (0, -0.5, 2.5), (0, 0.5, 3.5), (0, 1.5, 0.5), (0, 1.5, 2.5),\n",
    "                        (0, 2.5, 1.5), (0, 2.5, 3.5), (0, 3.5, 0.5), (0, 4.5, 1.5), (0, 3.5, 2.5), (0, 4.5, 3.5)]\n",
    "    else:\n",
    "        errs = [(0, -0.5, 0.5), (0, 0.5, 1.5), (0, -0.5, 2.5), (0, 0.5, 3.5), (0, -0.5, 4.5), (0, 0.5, 5.5),\n",
    "                        (0, 1.5, 0.5), (0, 1.5, 2.5), (0, 1.5, 4.5), (0, 2.5, 1.5), (0, 2.5, 3.5), (0, 2.5, 5.5),\n",
    "                        (0, 3.5, 0.5),  (0, 3.5, 2.5), (0, 3.5, 4.5), (0, 4.5, 1.5), (0, 4.5, 3.5), (0, 4.5, 5.5),\n",
    "                       (0, 5.5, 0.5), (0, 6.5, 1.5), (0, 5.5, 2.5), (0, 6.5, 3.5), (0, 5.5, 4.5), (0, 6.5, 5.5)]\n",
    "    for e in errs:\n",
    "        rand = random.random()\n",
    "        if rand <= prob:\n",
    "            new_errs.append(e)\n",
    "            \n",
    "    return new_errs\n",
    "            \n",
    "\n",
    "def return_zmeasurement_errs(depth, prob):\n",
    "    \n",
    "    new_errs = []\n",
    "    \n",
    "    if depth == 3:\n",
    "        errs = [(0, 0.5, 0.5), (0, 0.5, 2.5), (0, 1.5, -0.5), (0, 1.5, 1.5)]\n",
    "    elif depth == 5:\n",
    "        errs = [(0, 0.5, 0.5), (0, 0.5, 2.5), (0, 0.5, 4.5), (0, 1.5, -0.5), (0, 1.5, 1.5), (0, 1.5, 3.5),\n",
    "                        (0, 2.5, 0.5), (0, 2.5, 2.5), (0, 2.5, 4.5), (0, 3.5, -0.5), (0, 3.5, 1.5), (0, 3.5, 3.5)]\n",
    "    else:\n",
    "        errs = [(0, 0.5, 0.5), (0, 0.5, 2.5), (0, 0.5, 4.5), (0, 0.5, 6.5), (0, 1.5, -0.5), (0, 1.5, 1.5),\n",
    "                        (0, 1.5, 3.5), (0, 1.5, 5.5), (0, 2.5, 0.5), (0, 2.5, 2.5), (0, 2.5, 4.5), (0, 2.5, 6.5),\n",
    "                        (0, 3.5, -0.5),  (0, 3.5, 1.5), (0, 3.5, 3.5), (0, 3.5, 5.5), (0, 4.5, 0.5), (0, 4.5, 2.5),\n",
    "                       (0, 4.5, 4.5), (0, 4.5, 6.5), (0, 5.5, -0.5), (0, 5.5, 1.5), (0, 5.5, 3.5), (0, 5.5, 5.5)]\n",
    "        \n",
    "    for e in errs:\n",
    "        rand = random.random()\n",
    "        if rand <= prob:\n",
    "            new_errs.append(e)\n",
    "            \n",
    "    return new_errs\n",
    "\n",
    "def translate_errors (phys_errs):\n",
    "    flipX = np.array([(0, 1),(1, 0)])\n",
    "    flipZ = np.array([(1, 0), (0, -1)])\n",
    "    errs = []\n",
    "    str2 = \"\"\n",
    "    for qubit, flip in phys_errs.items():\n",
    "        row = int(qubit[1])\n",
    "        col = int(qubit[2])\n",
    "        if str(flip) == \"X\":\n",
    "            str1 = \"X\"\n",
    "        elif str(flip) == \"Z\":\n",
    "            str1 = \"Z\"\n",
    "        else:\n",
    "            str1 = \"X\"\n",
    "            str2 = \"Z\"\n",
    "        str1 += str(row) + str(col)\n",
    "        errs.append(str1)\n",
    "        if str2 != \"\":\n",
    "            str2 += str(row) +str(col)\n",
    "            errs.append(str2)\n",
    "            str2 = \"\"\n",
    "    return errs   \n",
    "\n",
    "def translate_to_graph(df_graph, labels, mlb):\n",
    "#go through labels given\n",
    "    indices = []\n",
    "    labels = mlb.inverse_transform(labels)\n",
    "    \n",
    "    for row in labels:\n",
    "        label_str = str(row)\n",
    "        for index, r in df_graph.iterrows():\n",
    "            if label_str == \"('',)\":\n",
    "                if str(r[\"Labels\"]) == \"[' ']\":\n",
    "                    indices.append([index])\n",
    "                    break\n",
    "            if set(row) == set(r[\"Labels\"]):\n",
    "                indices.append([index])\n",
    "                break\n",
    "\n",
    "    df_syns = df_graph.drop(['Labels'], axis=1)\n",
    "    return_df = pd.DataFrame()\n",
    "    for i in indices:\n",
    "        return_df = return_df.append(df_syns.loc[i])\n",
    "    return return_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function needed for preprocessing. CSV file reads in a string, needs to be a list for labels \n",
    "#for preprocessing csv files\n",
    "def create_list_from_string(err_list):\n",
    "    newstring = err_list.replace(\"'\", \"\")\n",
    "    new_err_list = newstring.strip('][').split(', ')\n",
    "    return sorted(set(new_err_list))\n",
    "\n",
    "\n",
    "def create_string_from_list(err_list):\n",
    "    return_string = \"[\"\n",
    "    if err_list[0] == \"''\":\n",
    "        return \"[' ']\"\n",
    "    else:\n",
    "        for index, i in enumerate(err_list):\n",
    "            return_string = return_string +  \"'\" + i + \"'\"\n",
    "            if index < (len(err_list)-1):\n",
    "                return_string += \", \"\n",
    "    return return_string + \"]\"\n",
    "        \n",
    "\n",
    "#take in two 2d arrays of predicted values, true values, and threshold to determine labels. \n",
    "#calculates the partial accuracy of the predicted values, averaged out for all obersvations\n",
    "def partial_accuracy(y_pred, y_true):\n",
    "    total = 0\n",
    "    rows = y_pred.shape[0]\n",
    "    cols = y_pred.shape[1]\n",
    "    for i in range(0, rows):\n",
    "        row_correct = 0\n",
    "        for j in range(0, cols):\n",
    "            if y_pred[i,j] == y_true[i,j]:\n",
    "                row_correct += 1\n",
    "        total += row_correct/cols\n",
    "    return total/rows\n",
    "\n",
    "def partial_accuracy_and_contingency(y_pred, y_true, mlb):\n",
    "    total = 0\n",
    "    a= np.zeros(shape=y_true.shape)\n",
    "    rows = y_pred.shape[0]\n",
    "    cols = y_pred.shape[1]\n",
    "    df = pd.DataFrame(a, columns = mlb.classes_)\n",
    "    for i in range(0, rows):\n",
    "        row_correct = 0\n",
    "        for j, label in enumerate(mlb.classes_):\n",
    "            if y_pred[i,j] == y_true[i,j]:\n",
    "                row_correct += 1\n",
    "                df.at[i, label] = 1\n",
    "            else:\n",
    "                df.at[i, label] = 0\n",
    "\n",
    "        total += row_correct/cols\n",
    "\n",
    "    return (total/rows, df)\n",
    "\n",
    "def contingency_table_and_t (clf1, clf2):\n",
    "    a = 0 #clf1 pos, clf2 pos\n",
    "    b = 0 #clf1 pos, clf2 neg\n",
    "    c = 0 #clf1 neg, clf2 pos\n",
    "    d = 0 #clf1 neg, clf2 neg\n",
    "    \n",
    "    for index, value in clf1.items():\n",
    "        if value == 1 and clf2[index] == 1:\n",
    "            a+=1\n",
    "        elif value == 1 and clf2[index] == 0: #classifier 1 right, classifier 2 wrong\n",
    "            b+=1\n",
    "        elif value == 0 and clf2[index] == 1: #classifier 1 wrong, classifier 2 right\n",
    "            c+=1\n",
    "        else:\n",
    "            d+=1\n",
    "    print(\"[\"+str(a)+\", \"+str(b)+\"]\")\n",
    "    print(\"[\"+str(c)+\", \"+str(d)+\"]\")\n",
    "    if b == 0 and c ==0:\n",
    "        print(\"both b and c are zero\")\n",
    "        t=0\n",
    "    else:\n",
    "        t = (((b-c)-1)**2)/(b+c)\n",
    "    return ([[a,b],[c,d]], t)\n",
    "\n",
    "def add_noise(val, noise_level):\n",
    "    rand = random.uniform(0, 1)\n",
    "    if rand <= noise_level:\n",
    "        if val == -1:\n",
    "            val = 1\n",
    "        elif val == 1:\n",
    "            val = -1\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_d7= trainData_d7.dropna()\n",
    "#######################################################################################################\n",
    "\n",
    "trainData_d7 = pd.read_csv(\"depth7_all_combos.csv\")\n",
    "trainData_d7 = trainData_d7.applymap(lambda x: add_noise(x,.01))\n",
    "\n",
    "#These four lines remove duplicates\n",
    "trainData_d7['Labels'] = trainData_d7['Labels'].map(lambda x: create_list_from_string(x))\n",
    "trainData_d7['Labels'] = trainData_d7['Labels'].astype(str)\n",
    "trainData_d7 = trainData_d7.drop_duplicates('Labels', keep='first', ignore_index=True)\n",
    "trainData_d7['Labels'] = trainData_d7['Labels'].map(lambda x: create_list_from_string(x))\n",
    "\n",
    "testData_d7_MWPM = graph_with_errs_d7(trainData_d7)\n",
    "\n",
    "#transforms the data to encoding for ML\n",
    "mlb_d7 = MultiLabelBinarizer()\n",
    "mlb_d7.fit(trainData_d7['Labels'])\n",
    "df = pd.DataFrame(mlb_d7.transform(trainData_d7['Labels']))\n",
    "df['Labels']= df.values.tolist()\n",
    "trainData_d7 = trainData_d7.drop(['Labels'], axis=1)\n",
    "trainData_d7 = pd.concat([df[\"Labels\"],testData_d7_MWPM, trainData_d7], axis=1, ignore_index=True)\n",
    "trainData_d7.columns = [\"Labels\",\"XSyn\", \"ZSyn\",\"X0\", \"Z1\", \"X2\", \"X3\", \"Z4\", \"X5\", \"X6\", \"Z7\", \"X8\", \"Z9\", \"Z10\", \"X11\", \"Z12\", \"X13\", \"Z14\", \"X15\", \"Z16\", \"Z17\", \"X18\", \"Z19\",\"X20\", \"Z21\", \"X22\", \"Z23\", \"Z24\", \"X25\", \"Z26\", \"X27\", \"Z28\", \"X29\", \"Z30\", \"Z31\", \"X32\", \"Z33\", \"X34\", \"Z35\", \"X36\", \"Z37\", \"Z38\", \"X39\", \"X40\", \"Z41\", \"X42\", \"X43\", \"Z44\", \"X45\", \"X46\", \"Z47\"]\n",
    "\n",
    "#######################################################################################\n",
    "\n",
    "trainData_d5 = pd.read_csv(\"depth5_all_combos.csv\")\n",
    "trainData_d5 = trainData_d5.applymap(lambda x: add_noise(x,.01)) #was .05\n",
    "#These four lines remove duplicates\n",
    "trainData_d5['Labels'] = trainData_d5['Labels'].map(lambda x: create_list_from_string(x))\n",
    "trainData_d5['Labels'] = trainData_d5['Labels'].astype(str)\n",
    "trainData_d5 = trainData_d5.drop_duplicates('Labels', keep='first', ignore_index=True)\n",
    "trainData_d5['Labels'] = trainData_d5['Labels'].map(lambda x: create_list_from_string(x))\n",
    "\n",
    "\n",
    "testData_d5_MWPM = graph_with_errs_d5(trainData_d5)\n",
    "\n",
    "\n",
    "#transforms the data to encoding for ML\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(trainData_d5['Labels'])\n",
    "df = pd.DataFrame(mlb.transform(trainData_d5['Labels']))\n",
    "df['Labels']= df.values.tolist()\n",
    "trainData_d5 = trainData_d5.drop(['Labels'], axis=1)\n",
    "trainData_d5 = pd.concat([df[\"Labels\"], testData_d5_MWPM, trainData_d5], axis=1, ignore_index=True)\n",
    "trainData_d5.columns = [\"Labels\",\"XSyn\", \"ZSyn\",\"X0\",\"Z1\",\"X2\",\"X3\",\"Z4\",\"X5\",\"Z6\",\"Z7\",\"X8\",\"Z9\",\"X10\",\"Z11\",\"Z12\",\"X13\",\"Z14\",\"X15\",\"Z16\",\"Z17\",\"X18\",\"X19\",\"Z20\",\"X21\",\"X22\",\"Z23\"]\n",
    "#########################################################################################\n",
    "\n",
    "#Has no duplicates, small enough to check manually\n",
    "trainData_d3 = pd.read_csv(\"depth3_all_combos.csv\")\n",
    "\n",
    "trainData_d3[\"Labels\"] = trainData_d3['Labels'].map(lambda x: create_list_from_string(x))\n",
    "trainData_d3 = trainData_d3.applymap(lambda x: add_noise(x,.01))\n",
    "\n",
    "testData_d3_MWPM = graph_with_errs_d3(trainData_d3)\n",
    "\n",
    "mlb_d3 = MultiLabelBinarizer()\n",
    "mlb_d3.fit(trainData_d3[\"Labels\"])\n",
    "df = pd.DataFrame(mlb_d3.transform(trainData_d3['Labels']))\n",
    "df['Labels']= df.values.tolist()\n",
    "trainData_d3 = trainData_d3.drop(['Labels'], axis=1)\n",
    "trainData_d3 = pd.concat([df['Labels'], testData_d3_MWPM, trainData_d3], axis=1, ignore_index=True)\n",
    "trainData_d3.columns = [\"Labels\",\"XSyn\", \"ZSyn\", \"X0\", \"Z1\", \"X2\", \"Z3\", \"Z4\", \"X5\", \"X6\", \"Z7\"]\n",
    "#########################################################################################\n",
    "y_d3 = trainData_d3[\"Labels\"]\n",
    "x_d3 = trainData_d3.drop([\"Labels\"], axis=1)\n",
    "\n",
    "y_d5 = trainData_d5[\"Labels\"] \n",
    "x_d5 = trainData_d5.drop([\"Labels\"], axis=1) \n",
    "\n",
    "y_d7 = trainData_d7[\"Labels\"]\n",
    "x_d7 = trainData_d7.drop([\"Labels\"], axis=1)\n",
    "\n",
    "\n",
    "x_d3 = x_d3.replace([-1], 0)\n",
    "x_d5 = x_d5.replace([-1], 0)\n",
    "x_d7 = x_d7.replace([-1], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_d7 = trainData_d7[\"Labels\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions for creating lookup tables here:\n",
    "from collections import defaultdict\n",
    "from math import log2\n",
    "\n",
    "def generateAllBinaryStrings(n, arr, i, lookup):  \n",
    "    if i == n: \n",
    "        lookup.setBitStringArray(arr, n)  \n",
    "        return\n",
    "      \n",
    "    # First assign \"0\" at ith position  \n",
    "    # and try for all other permutations  \n",
    "    # for remaining positions  \n",
    "    arr[i] = 0\n",
    "    generateAllBinaryStrings(n, arr, i + 1, lookup)  \n",
    "  \n",
    "    # And then assign \"1\" at ith position  , \n",
    "    # and try for all other permutations  \n",
    "    # for remaining positions  \n",
    "    arr[i] = 1\n",
    "    generateAllBinaryStrings(n, arr, i + 1, lookup)\n",
    "\n",
    "class lookup_decoder:\n",
    "    \n",
    "    def __init__(self, depth):\n",
    "        #self.lookupTable = defaultdict()\n",
    "        self.lookupTable = {}\n",
    "        self.distributions = {}\n",
    "        self.depth = depth\n",
    "        #generating all possible syndrome observations\n",
    "        #arr = [None] * (depth**2 - 1)\n",
    "        #generateAllBinaryStrings((depth**2 - 1), arr, 0, self)\n",
    "        \n",
    "    def setBitStringArray(self, arr, n): \n",
    "        new_str = \"\"\n",
    "        for i in range(0, n):  \n",
    "            new_str += str(arr[i])\n",
    "        self.lookupTable.update({new_str:defaultdict()})  \n",
    "        \n",
    "    def update_table (self, syndrome, phys_errs):\n",
    "        #all the keys are made in the init, so simply update the physical error combinations for the given syndrome\n",
    "        if syndrome not in self.lookupTable:\n",
    "            self.lookupTable[syndrome] = {}\n",
    "            self.lookupTable[syndrome][phys_errs] = 1\n",
    "            return\n",
    "        \n",
    "        if phys_errs not in self.lookupTable[syndrome]:\n",
    "            self.lookupTable[syndrome].update({phys_errs: 1})\n",
    "        else:\n",
    "            self.lookupTable[syndrome][phys_errs] += 1\n",
    "     \n",
    "    def get_probable_error(self, syndrome):\n",
    "        return_key = []\n",
    "\n",
    "        if syndrome not in self.lookupTable.keys():\n",
    "            for i in range(2* int(self.depth**2) + 1):\n",
    "                return_key.append(0)\n",
    "            return return_key\n",
    "        \n",
    "        key, value = max(self.lookupTable[syndrome].items(), key=lambda x:x[1])\n",
    "        \n",
    "        for character in key:\n",
    "            if character == '0' or character == '1':\n",
    "                return_key.append(int(character))\n",
    "                \n",
    "        return return_key\n",
    "        \n",
    "    def make_distribution_graph(self, syn):\n",
    "        \n",
    "        plt.bar(list(self.lookupTable[syn].keys()), self.lookupTable[syn].values(), color='g')\n",
    "        plt.show()\n",
    "        \n",
    "    def syndrome_count_graph(self):\n",
    "        graph_dict = {}\n",
    "        for syn in self.lookupTable:\n",
    "            graph_dict.update({syn:sum(self.lookupTable[syn].values())})\n",
    "        plt.ylim((0,4))\n",
    "        plt.bar(graph_dict.keys(), graph_dict.values())\n",
    "        plt.show()\n",
    "        \n",
    "    \n",
    "    def length_of_lookup(self):\n",
    "        print(len(self.lookupTable))\n",
    "        \n",
    "            \n",
    "    def get_entropies(self):\n",
    "        entropies = {}\n",
    "        for syn in self.lookupTable:\n",
    "            total = sum(self.lookupTable[syn].values())\n",
    "            h = 0\n",
    "            for key in self.lookupTable[syn]:\n",
    "                p = self.lookupTable[syn][key]/total\n",
    "                h += p+log2(p)\n",
    "            entropies[syn] = -h\n",
    "        print(entropies)\n",
    "            \n",
    "    def get_syndromes(self):\n",
    "        return self.lookupTable.keys()\n",
    "                     \n",
    "    def print_lookup(self):\n",
    "        for syn in self.lookupTable:\n",
    "            print(self.lookupTable[syn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_plut(table, data_x, data_y):\n",
    "    i = 0\n",
    "    for index, x in enumerate(data_x):\n",
    "        syn = \"\".join([str(i) for i in x])\n",
    "        syn = syn.replace(\".0\",\"\")\n",
    "        labels = np.array2string(np.array(data_y[i]), precision=1, separator='',suppress_small=True).replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "        table.update_table(syn, labels)\n",
    "        i+=1\n",
    "    return table\n",
    "\n",
    "\n",
    "def test_plut(table, test_set):\n",
    "    predictions_lookup = []\n",
    "    for index, x in enumerate(test_set):\n",
    "        syn_x = \"\".join([str(i) for i in x])\n",
    "        syn_x = syn_x.replace(\".0\",\"\")\n",
    "        predictions_lookup.append(table.get_probable_error(syn_x))\n",
    "    return np.array(predictions_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_FFNN_model_DepthThree(depth):\n",
    "    model = Sequential()\n",
    "    layers = 2\n",
    "    #input layer\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    \n",
    "    #hidden layers go here\n",
    "    for i in range(layers):\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "    \n",
    "    model.add(Dense(19 , activation='sigmoid'))\n",
    "    model.compile(loss=keras.losses.binary_crossentropy, optimizer=tf.keras.optimizers.SGD(learning_rate=0.05),\n",
    "                  metrics=['accuracy']) #CHANGE  (added tf., changed lr to learning_rate)\n",
    "    return model\n",
    "\n",
    "#make any needed changes here\n",
    "def compile_FFNN_model_DepthFive(depth):\n",
    "    model = Sequential()\n",
    "    layers = 4\n",
    "    \n",
    "    #input layer\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    \n",
    "    #hidden layers go here\n",
    "    for i in range(layers):\n",
    "        model.add(Dense(240, activation='relu'))\n",
    "    \n",
    "    #model.add(Dense((2*depth**2) , activation='sigmoid'))\n",
    "    model.add(Dense(51, activation='sigmoid'))\n",
    "    model.compile(loss=keras.losses.binary_crossentropy, optimizer=tf.keras.optimizers.SGD(learning_rate=0.05),\n",
    "                  metrics=['accuracy']) #CHANGE (added tf.)\n",
    "    return model\n",
    "\n",
    "\n",
    "#make any needed changes here\n",
    "def compile_FFNN_model_DepthSeven(depth):\n",
    "    model = Sequential()\n",
    "    layers = 4\n",
    "    \n",
    "    #input layer\n",
    "    model.add(Dense(48, activation='relu'))\n",
    "    \n",
    "    #hidden layers go here\n",
    "    for i in range(layers):\n",
    "        model.add(Dense(400, activation='relu'))\n",
    "    \n",
    "    #model.add(Dense((2*depth**2) , activation='sigmoid'))\n",
    "    model.add(Dense(99, activation='sigmoid'))\n",
    "    model.compile(loss=keras.losses.binary_crossentropy, optimizer=tf.keras.optimizers.SGD(learning_rate=0.05),\n",
    "                  metrics=['accuracy']) #CHANGE  (added tf.)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Epoch 1/200\n",
      "1/1 [==============================] - 1s 509ms/step - loss: 0.6542 - accuracy: 0.1176 - val_loss: 0.6586 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.6520 - accuracy: 0.1176 - val_loss: 0.6566 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.6497 - accuracy: 0.1176 - val_loss: 0.6547 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.6474 - accuracy: 0.1176 - val_loss: 0.6527 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.6450 - accuracy: 0.1176 - val_loss: 0.6506 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6426 - accuracy: 0.1176 - val_loss: 0.6486 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.6402 - accuracy: 0.1176 - val_loss: 0.6465 - val_accuracy: 0.2000\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.6378 - accuracy: 0.1176 - val_loss: 0.6445 - val_accuracy: 0.2000\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.6354 - accuracy: 0.1176 - val_loss: 0.6424 - val_accuracy: 0.2000\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6329 - accuracy: 0.1176 - val_loss: 0.6403 - val_accuracy: 0.2000\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6304 - accuracy: 0.1176 - val_loss: 0.6381 - val_accuracy: 0.2000\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.6278 - accuracy: 0.1176 - val_loss: 0.6359 - val_accuracy: 0.2000\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6251 - accuracy: 0.1176 - val_loss: 0.6336 - val_accuracy: 0.2000\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.6225 - accuracy: 0.1176 - val_loss: 0.6314 - val_accuracy: 0.2000\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.6198 - accuracy: 0.1176 - val_loss: 0.6291 - val_accuracy: 0.2000\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.6170 - accuracy: 0.1176 - val_loss: 0.6269 - val_accuracy: 0.2000\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.6143 - accuracy: 0.1176 - val_loss: 0.6246 - val_accuracy: 0.2000\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6115 - accuracy: 0.1176 - val_loss: 0.6223 - val_accuracy: 0.2000\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6087 - accuracy: 0.1176 - val_loss: 0.6199 - val_accuracy: 0.2000\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.6058 - accuracy: 0.1176 - val_loss: 0.6176 - val_accuracy: 0.2000\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.6029 - accuracy: 0.1176 - val_loss: 0.6151 - val_accuracy: 0.2000\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.5999 - accuracy: 0.1176 - val_loss: 0.6127 - val_accuracy: 0.2000\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5970 - accuracy: 0.1176 - val_loss: 0.6102 - val_accuracy: 0.2000\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5940 - accuracy: 0.1176 - val_loss: 0.6076 - val_accuracy: 0.2000\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.5909 - accuracy: 0.1176 - val_loss: 0.6050 - val_accuracy: 0.2000\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.5878 - accuracy: 0.1176 - val_loss: 0.6024 - val_accuracy: 0.2000\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5847 - accuracy: 0.1176 - val_loss: 0.5997 - val_accuracy: 0.2000\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5815 - accuracy: 0.1176 - val_loss: 0.5969 - val_accuracy: 0.2000\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.5783 - accuracy: 0.1176 - val_loss: 0.5942 - val_accuracy: 0.2000\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.5750 - accuracy: 0.1176 - val_loss: 0.5913 - val_accuracy: 0.2000\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5717 - accuracy: 0.1176 - val_loss: 0.5885 - val_accuracy: 0.2000\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.5683 - accuracy: 0.1176 - val_loss: 0.5855 - val_accuracy: 0.2000\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5649 - accuracy: 0.1176 - val_loss: 0.5826 - val_accuracy: 0.2000\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5615 - accuracy: 0.1176 - val_loss: 0.5796 - val_accuracy: 0.2000\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5580 - accuracy: 0.1176 - val_loss: 0.5766 - val_accuracy: 0.2000\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.5545 - accuracy: 0.1176 - val_loss: 0.5736 - val_accuracy: 0.2000\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5510 - accuracy: 0.1176 - val_loss: 0.5705 - val_accuracy: 0.2000\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5474 - accuracy: 0.1176 - val_loss: 0.5674 - val_accuracy: 0.2000\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5437 - accuracy: 0.1176 - val_loss: 0.5643 - val_accuracy: 0.2000\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5400 - accuracy: 0.1176 - val_loss: 0.5611 - val_accuracy: 0.2000\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.5363 - accuracy: 0.1176 - val_loss: 0.5579 - val_accuracy: 0.2000\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.5325 - accuracy: 0.1176 - val_loss: 0.5547 - val_accuracy: 0.2000\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.5287 - accuracy: 0.1176 - val_loss: 0.5513 - val_accuracy: 0.2000\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5248 - accuracy: 0.1176 - val_loss: 0.5479 - val_accuracy: 0.2000\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5209 - accuracy: 0.1176 - val_loss: 0.5445 - val_accuracy: 0.2000\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5170 - accuracy: 0.1176 - val_loss: 0.5411 - val_accuracy: 0.2000\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5130 - accuracy: 0.1176 - val_loss: 0.5376 - val_accuracy: 0.2000\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.5090 - accuracy: 0.1176 - val_loss: 0.5341 - val_accuracy: 0.2000\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5049 - accuracy: 0.1176 - val_loss: 0.5306 - val_accuracy: 0.2000\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5008 - accuracy: 0.1176 - val_loss: 0.5270 - val_accuracy: 0.2000\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4967 - accuracy: 0.1176 - val_loss: 0.5235 - val_accuracy: 0.2000\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4925 - accuracy: 0.1176 - val_loss: 0.5199 - val_accuracy: 0.2000\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4883 - accuracy: 0.1176 - val_loss: 0.5163 - val_accuracy: 0.2000\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4841 - accuracy: 0.1176 - val_loss: 0.5127 - val_accuracy: 0.2000\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.4799 - accuracy: 0.1176 - val_loss: 0.5091 - val_accuracy: 0.2000\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4756 - accuracy: 0.1176 - val_loss: 0.5054 - val_accuracy: 0.2000\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4713 - accuracy: 0.1176 - val_loss: 0.5017 - val_accuracy: 0.2000\n",
      "Epoch 58/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4669 - accuracy: 0.1176 - val_loss: 0.4980 - val_accuracy: 0.2000\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4625 - accuracy: 0.1176 - val_loss: 0.4943 - val_accuracy: 0.2000\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4581 - accuracy: 0.1176 - val_loss: 0.4905 - val_accuracy: 0.2000\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.4537 - accuracy: 0.1176 - val_loss: 0.4868 - val_accuracy: 0.2000\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4493 - accuracy: 0.1176 - val_loss: 0.4830 - val_accuracy: 0.2000\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4449 - accuracy: 0.1176 - val_loss: 0.4792 - val_accuracy: 0.2000\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4404 - accuracy: 0.1176 - val_loss: 0.4755 - val_accuracy: 0.2000\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4360 - accuracy: 0.1176 - val_loss: 0.4717 - val_accuracy: 0.2000\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4315 - accuracy: 0.1176 - val_loss: 0.4679 - val_accuracy: 0.2000\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4270 - accuracy: 0.0588 - val_loss: 0.4642 - val_accuracy: 0.2000\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4226 - accuracy: 0.0588 - val_loss: 0.4604 - val_accuracy: 0.2000\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4181 - accuracy: 0.0588 - val_loss: 0.4566 - val_accuracy: 0.2000\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4136 - accuracy: 0.0588 - val_loss: 0.4529 - val_accuracy: 0.2000\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4091 - accuracy: 0.0588 - val_loss: 0.4492 - val_accuracy: 0.2000\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.4047 - accuracy: 0.0588 - val_loss: 0.4455 - val_accuracy: 0.2000\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4003 - accuracy: 0.0588 - val_loss: 0.4418 - val_accuracy: 0.2000\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3959 - accuracy: 0.0588 - val_loss: 0.4381 - val_accuracy: 0.2000\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3915 - accuracy: 0.0588 - val_loss: 0.4345 - val_accuracy: 0.2000\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3871 - accuracy: 0.0588 - val_loss: 0.4310 - val_accuracy: 0.2000\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3828 - accuracy: 0.0588 - val_loss: 0.4274 - val_accuracy: 0.2000\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.3785 - accuracy: 0.0588 - val_loss: 0.4239 - val_accuracy: 0.2000\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3743 - accuracy: 0.0588 - val_loss: 0.4205 - val_accuracy: 0.2000\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3701 - accuracy: 0.0588 - val_loss: 0.4171 - val_accuracy: 0.2000\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3659 - accuracy: 0.0588 - val_loss: 0.4137 - val_accuracy: 0.2000\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3618 - accuracy: 0.0588 - val_loss: 0.4104 - val_accuracy: 0.2000\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3578 - accuracy: 0.0588 - val_loss: 0.4072 - val_accuracy: 0.2000\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3538 - accuracy: 0.0588 - val_loss: 0.4040 - val_accuracy: 0.2000\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.3499 - accuracy: 0.0588 - val_loss: 0.4009 - val_accuracy: 0.2000\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3460 - accuracy: 0.0588 - val_loss: 0.3978 - val_accuracy: 0.2000\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3422 - accuracy: 0.0588 - val_loss: 0.3948 - val_accuracy: 0.2000\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3385 - accuracy: 0.0588 - val_loss: 0.3919 - val_accuracy: 0.2000\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3349 - accuracy: 0.0588 - val_loss: 0.3890 - val_accuracy: 0.2000\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3313 - accuracy: 0.0588 - val_loss: 0.3862 - val_accuracy: 0.2000\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3278 - accuracy: 0.0588 - val_loss: 0.3835 - val_accuracy: 0.2000\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3244 - accuracy: 0.0588 - val_loss: 0.3809 - val_accuracy: 0.2000\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3210 - accuracy: 0.0588 - val_loss: 0.3783 - val_accuracy: 0.2000\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3177 - accuracy: 0.0588 - val_loss: 0.3758 - val_accuracy: 0.2000\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3146 - accuracy: 0.0588 - val_loss: 0.3734 - val_accuracy: 0.2000\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3115 - accuracy: 0.0588 - val_loss: 0.3710 - val_accuracy: 0.2000\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3085 - accuracy: 0.0588 - val_loss: 0.3688 - val_accuracy: 0.2000\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3055 - accuracy: 0.0588 - val_loss: 0.3666 - val_accuracy: 0.2000\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.3027 - accuracy: 0.0588 - val_loss: 0.3645 - val_accuracy: 0.2000\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3000 - accuracy: 0.0588 - val_loss: 0.3625 - val_accuracy: 0.2000\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2973 - accuracy: 0.0588 - val_loss: 0.3605 - val_accuracy: 0.2000\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2947 - accuracy: 0.0588 - val_loss: 0.3586 - val_accuracy: 0.2000\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2922 - accuracy: 0.0588 - val_loss: 0.3568 - val_accuracy: 0.2000\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2898 - accuracy: 0.0588 - val_loss: 0.3551 - val_accuracy: 0.2000\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2875 - accuracy: 0.0588 - val_loss: 0.3535 - val_accuracy: 0.2000\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2853 - accuracy: 0.0588 - val_loss: 0.3519 - val_accuracy: 0.2000\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2831 - accuracy: 0.0588 - val_loss: 0.3504 - val_accuracy: 0.2000\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2810 - accuracy: 0.0588 - val_loss: 0.3490 - val_accuracy: 0.2000\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2790 - accuracy: 0.0588 - val_loss: 0.3476 - val_accuracy: 0.2000\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2771 - accuracy: 0.0588 - val_loss: 0.3463 - val_accuracy: 0.2000\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2753 - accuracy: 0.0588 - val_loss: 0.3451 - val_accuracy: 0.2000\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2735 - accuracy: 0.0588 - val_loss: 0.3439 - val_accuracy: 0.2000\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2718 - accuracy: 0.0588 - val_loss: 0.3428 - val_accuracy: 0.2000\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2702 - accuracy: 0.0588 - val_loss: 0.3417 - val_accuracy: 0.2000\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2686 - accuracy: 0.0588 - val_loss: 0.3407 - val_accuracy: 0.2000\n",
      "Epoch 116/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2671 - accuracy: 0.0588 - val_loss: 0.3398 - val_accuracy: 0.2000\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.2657 - accuracy: 0.0588 - val_loss: 0.3389 - val_accuracy: 0.2000\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.2643 - accuracy: 0.0588 - val_loss: 0.3381 - val_accuracy: 0.2000\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2630 - accuracy: 0.0588 - val_loss: 0.3373 - val_accuracy: 0.2000\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2617 - accuracy: 0.0588 - val_loss: 0.3366 - val_accuracy: 0.2000\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2605 - accuracy: 0.0588 - val_loss: 0.3359 - val_accuracy: 0.2000\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2594 - accuracy: 0.0588 - val_loss: 0.3352 - val_accuracy: 0.2000\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2583 - accuracy: 0.0588 - val_loss: 0.3346 - val_accuracy: 0.2000\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2572 - accuracy: 0.0588 - val_loss: 0.3341 - val_accuracy: 0.2000\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2562 - accuracy: 0.0588 - val_loss: 0.3335 - val_accuracy: 0.2000\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2553 - accuracy: 0.0588 - val_loss: 0.3331 - val_accuracy: 0.2000\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2543 - accuracy: 0.0588 - val_loss: 0.3326 - val_accuracy: 0.2000\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2535 - accuracy: 0.0588 - val_loss: 0.3322 - val_accuracy: 0.2000\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2526 - accuracy: 0.0588 - val_loss: 0.3318 - val_accuracy: 0.2000\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2518 - accuracy: 0.0588 - val_loss: 0.3314 - val_accuracy: 0.2000\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2510 - accuracy: 0.0588 - val_loss: 0.3311 - val_accuracy: 0.2000\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2503 - accuracy: 0.0588 - val_loss: 0.3308 - val_accuracy: 0.2000\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2496 - accuracy: 0.0588 - val_loss: 0.3305 - val_accuracy: 0.2000\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2489 - accuracy: 0.0588 - val_loss: 0.3302 - val_accuracy: 0.2000\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2483 - accuracy: 0.0588 - val_loss: 0.3300 - val_accuracy: 0.2000\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2477 - accuracy: 0.0588 - val_loss: 0.3298 - val_accuracy: 0.2000\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2471 - accuracy: 0.0588 - val_loss: 0.3296 - val_accuracy: 0.2000\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2465 - accuracy: 0.0588 - val_loss: 0.3294 - val_accuracy: 0.2000\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2460 - accuracy: 0.0588 - val_loss: 0.3293 - val_accuracy: 0.2000\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2455 - accuracy: 0.0588 - val_loss: 0.3291 - val_accuracy: 0.2000\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2450 - accuracy: 0.0588 - val_loss: 0.3290 - val_accuracy: 0.2000\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2445 - accuracy: 0.0588 - val_loss: 0.3289 - val_accuracy: 0.2000\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.2440 - accuracy: 0.0588 - val_loss: 0.3288 - val_accuracy: 0.2000\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2436 - accuracy: 0.0588 - val_loss: 0.3287 - val_accuracy: 0.2000\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2432 - accuracy: 0.0588 - val_loss: 0.3286 - val_accuracy: 0.2000\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2428 - accuracy: 0.0588 - val_loss: 0.3286 - val_accuracy: 0.2000\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2424 - accuracy: 0.0588 - val_loss: 0.3285 - val_accuracy: 0.2000\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2420 - accuracy: 0.0588 - val_loss: 0.3285 - val_accuracy: 0.2000\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2416 - accuracy: 0.0588 - val_loss: 0.3284 - val_accuracy: 0.2000\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2413 - accuracy: 0.0588 - val_loss: 0.3284 - val_accuracy: 0.2000\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2410 - accuracy: 0.0588 - val_loss: 0.3284 - val_accuracy: 0.2000\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2406 - accuracy: 0.0588 - val_loss: 0.3284 - val_accuracy: 0.2000\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2403 - accuracy: 0.0588 - val_loss: 0.3284 - val_accuracy: 0.2000\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2400 - accuracy: 0.0588 - val_loss: 0.3284 - val_accuracy: 0.2000\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2397 - accuracy: 0.0588 - val_loss: 0.3284 - val_accuracy: 0.2000\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2394 - accuracy: 0.0588 - val_loss: 0.3285 - val_accuracy: 0.2000\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2392 - accuracy: 0.0588 - val_loss: 0.3285 - val_accuracy: 0.2000\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2389 - accuracy: 0.1176 - val_loss: 0.3285 - val_accuracy: 0.2000\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2387 - accuracy: 0.1176 - val_loss: 0.3286 - val_accuracy: 0.2000\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2384 - accuracy: 0.1176 - val_loss: 0.3286 - val_accuracy: 0.2000\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2382 - accuracy: 0.1176 - val_loss: 0.3286 - val_accuracy: 0.2000\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2379 - accuracy: 0.1176 - val_loss: 0.3287 - val_accuracy: 0.2000\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2377 - accuracy: 0.1176 - val_loss: 0.3287 - val_accuracy: 0.2000\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2375 - accuracy: 0.1176 - val_loss: 0.3288 - val_accuracy: 0.2000\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.2373 - accuracy: 0.1176 - val_loss: 0.3289 - val_accuracy: 0.2000\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2371 - accuracy: 0.1176 - val_loss: 0.3289 - val_accuracy: 0.2000\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2369 - accuracy: 0.1176 - val_loss: 0.3290 - val_accuracy: 0.2000\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2367 - accuracy: 0.1176 - val_loss: 0.3291 - val_accuracy: 0.2000\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2365 - accuracy: 0.1176 - val_loss: 0.3291 - val_accuracy: 0.2000\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2363 - accuracy: 0.1176 - val_loss: 0.3292 - val_accuracy: 0.2000\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2361 - accuracy: 0.1176 - val_loss: 0.3293 - val_accuracy: 0.2000\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2360 - accuracy: 0.1176 - val_loss: 0.3294 - val_accuracy: 0.2000\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2358 - accuracy: 0.1176 - val_loss: 0.3294 - val_accuracy: 0.2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2356 - accuracy: 0.1176 - val_loss: 0.3295 - val_accuracy: 0.2000\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2355 - accuracy: 0.1176 - val_loss: 0.3296 - val_accuracy: 0.2000\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2353 - accuracy: 0.1176 - val_loss: 0.3297 - val_accuracy: 0.2000\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2352 - accuracy: 0.1176 - val_loss: 0.3298 - val_accuracy: 0.2000\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2350 - accuracy: 0.0588 - val_loss: 0.3299 - val_accuracy: 0.2000\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2349 - accuracy: 0.0000e+00 - val_loss: 0.3299 - val_accuracy: 0.2000\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.2347 - accuracy: 0.0000e+00 - val_loss: 0.3300 - val_accuracy: 0.2000\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2346 - accuracy: 0.0000e+00 - val_loss: 0.3301 - val_accuracy: 0.2000\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2345 - accuracy: 0.0000e+00 - val_loss: 0.3302 - val_accuracy: 0.2000\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2343 - accuracy: 0.0000e+00 - val_loss: 0.3303 - val_accuracy: 0.2000\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2342 - accuracy: 0.0000e+00 - val_loss: 0.3304 - val_accuracy: 0.2000\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2341 - accuracy: 0.0000e+00 - val_loss: 0.3305 - val_accuracy: 0.2000\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2339 - accuracy: 0.0000e+00 - val_loss: 0.3306 - val_accuracy: 0.0000e+00\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2338 - accuracy: 0.0000e+00 - val_loss: 0.3307 - val_accuracy: 0.0000e+00\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2337 - accuracy: 0.0000e+00 - val_loss: 0.3308 - val_accuracy: 0.0000e+00\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2336 - accuracy: 0.0000e+00 - val_loss: 0.3309 - val_accuracy: 0.0000e+00\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2335 - accuracy: 0.0588 - val_loss: 0.3309 - val_accuracy: 0.0000e+00\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2333 - accuracy: 0.0588 - val_loss: 0.3310 - val_accuracy: 0.0000e+00\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2332 - accuracy: 0.0588 - val_loss: 0.3311 - val_accuracy: 0.0000e+00\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2331 - accuracy: 0.0588 - val_loss: 0.3312 - val_accuracy: 0.0000e+00\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2330 - accuracy: 0.0588 - val_loss: 0.3313 - val_accuracy: 0.0000e+00\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2329 - accuracy: 0.0588 - val_loss: 0.3314 - val_accuracy: 0.0000e+00\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2328 - accuracy: 0.0588 - val_loss: 0.3315 - val_accuracy: 0.0000e+00\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2327 - accuracy: 0.0588 - val_loss: 0.3316 - val_accuracy: 0.0000e+00\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2326 - accuracy: 0.0588 - val_loss: 0.3317 - val_accuracy: 0.0000e+00\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2325 - accuracy: 0.0588 - val_loss: 0.3318 - val_accuracy: 0.0000e+00\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2324 - accuracy: 0.0588 - val_loss: 0.3319 - val_accuracy: 0.0000e+00\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-31-8784861b343f>:134: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "  tprs.append(interp(mean_fpr, fpr, tpr))\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:806: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1/1 [==============================] - 1s 513ms/step - loss: 0.6918 - accuracy: 0.0000e+00 - val_loss: 0.6957 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6907 - accuracy: 0.1176 - val_loss: 0.6948 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.6895 - accuracy: 0.1176 - val_loss: 0.6940 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.6884 - accuracy: 0.1176 - val_loss: 0.6932 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6873 - accuracy: 0.1176 - val_loss: 0.6923 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6862 - accuracy: 0.1176 - val_loss: 0.6915 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6851 - accuracy: 0.1176 - val_loss: 0.6907 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.6840 - accuracy: 0.1176 - val_loss: 0.6898 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.6829 - accuracy: 0.1176 - val_loss: 0.6890 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6819 - accuracy: 0.1176 - val_loss: 0.6881 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6808 - accuracy: 0.1176 - val_loss: 0.6873 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6797 - accuracy: 0.1176 - val_loss: 0.6864 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6786 - accuracy: 0.1176 - val_loss: 0.6856 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.6776 - accuracy: 0.1176 - val_loss: 0.6848 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6765 - accuracy: 0.1176 - val_loss: 0.6839 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6755 - accuracy: 0.1176 - val_loss: 0.6831 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6744 - accuracy: 0.1176 - val_loss: 0.6822 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.6733 - accuracy: 0.1176 - val_loss: 0.6814 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6723 - accuracy: 0.1176 - val_loss: 0.6806 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.6712 - accuracy: 0.1176 - val_loss: 0.6797 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.6702 - accuracy: 0.1176 - val_loss: 0.6789 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6692 - accuracy: 0.1176 - val_loss: 0.6781 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6681 - accuracy: 0.1176 - val_loss: 0.6772 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6671 - accuracy: 0.1176 - val_loss: 0.6764 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.6661 - accuracy: 0.1176 - val_loss: 0.6756 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6651 - accuracy: 0.1176 - val_loss: 0.6748 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.6641 - accuracy: 0.1176 - val_loss: 0.6740 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.6631 - accuracy: 0.1176 - val_loss: 0.6732 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6621 - accuracy: 0.1176 - val_loss: 0.6723 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6611 - accuracy: 0.1176 - val_loss: 0.6715 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.6601 - accuracy: 0.1176 - val_loss: 0.6707 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6590 - accuracy: 0.1176 - val_loss: 0.6698 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6580 - accuracy: 0.1176 - val_loss: 0.6690 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.6570 - accuracy: 0.1176 - val_loss: 0.6682 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6559 - accuracy: 0.1176 - val_loss: 0.6673 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6549 - accuracy: 0.1176 - val_loss: 0.6665 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6539 - accuracy: 0.1176 - val_loss: 0.6656 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6528 - accuracy: 0.1176 - val_loss: 0.6648 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6518 - accuracy: 0.1176 - val_loss: 0.6640 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.6508 - accuracy: 0.1176 - val_loss: 0.6631 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6497 - accuracy: 0.1176 - val_loss: 0.6623 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6487 - accuracy: 0.1176 - val_loss: 0.6614 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6476 - accuracy: 0.1176 - val_loss: 0.6605 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6465 - accuracy: 0.1176 - val_loss: 0.6597 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6455 - accuracy: 0.1176 - val_loss: 0.6588 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6444 - accuracy: 0.1176 - val_loss: 0.6580 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6433 - accuracy: 0.1176 - val_loss: 0.6571 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6422 - accuracy: 0.1176 - val_loss: 0.6562 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.6411 - accuracy: 0.1176 - val_loss: 0.6553 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6400 - accuracy: 0.1176 - val_loss: 0.6545 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6389 - accuracy: 0.1176 - val_loss: 0.6536 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6378 - accuracy: 0.1176 - val_loss: 0.6527 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6367 - accuracy: 0.1176 - val_loss: 0.6518 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.6356 - accuracy: 0.1176 - val_loss: 0.6510 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.6344 - accuracy: 0.1176 - val_loss: 0.6501 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6333 - accuracy: 0.1176 - val_loss: 0.6492 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6321 - accuracy: 0.1176 - val_loss: 0.6483 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.6310 - accuracy: 0.1176 - val_loss: 0.6474 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.6298 - accuracy: 0.1176 - val_loss: 0.6465 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.6287 - accuracy: 0.1176 - val_loss: 0.6456 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6275 - accuracy: 0.1176 - val_loss: 0.6447 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6263 - accuracy: 0.1176 - val_loss: 0.6438 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6251 - accuracy: 0.1176 - val_loss: 0.6429 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6239 - accuracy: 0.1176 - val_loss: 0.6419 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6227 - accuracy: 0.1176 - val_loss: 0.6410 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6214 - accuracy: 0.1176 - val_loss: 0.6400 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6202 - accuracy: 0.1176 - val_loss: 0.6391 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.6189 - accuracy: 0.1176 - val_loss: 0.6381 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6177 - accuracy: 0.1176 - val_loss: 0.6371 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6164 - accuracy: 0.1176 - val_loss: 0.6361 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.6151 - accuracy: 0.1176 - val_loss: 0.6351 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6138 - accuracy: 0.1176 - val_loss: 0.6341 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6124 - accuracy: 0.1176 - val_loss: 0.6331 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6111 - accuracy: 0.1176 - val_loss: 0.6321 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.6097 - accuracy: 0.1176 - val_loss: 0.6311 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.6084 - accuracy: 0.1176 - val_loss: 0.6300 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6070 - accuracy: 0.1176 - val_loss: 0.6290 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6056 - accuracy: 0.1176 - val_loss: 0.6279 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.6041 - accuracy: 0.1176 - val_loss: 0.6268 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6027 - accuracy: 0.1176 - val_loss: 0.6257 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.6012 - accuracy: 0.1176 - val_loss: 0.6246 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5997 - accuracy: 0.1176 - val_loss: 0.6235 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.5982 - accuracy: 0.1176 - val_loss: 0.6224 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5967 - accuracy: 0.1176 - val_loss: 0.6212 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.5952 - accuracy: 0.1176 - val_loss: 0.6201 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5936 - accuracy: 0.1176 - val_loss: 0.6189 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5920 - accuracy: 0.1176 - val_loss: 0.6177 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.5904 - accuracy: 0.1176 - val_loss: 0.6165 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5888 - accuracy: 0.1176 - val_loss: 0.6153 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5872 - accuracy: 0.1176 - val_loss: 0.6141 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5855 - accuracy: 0.1176 - val_loss: 0.6128 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5838 - accuracy: 0.1176 - val_loss: 0.6116 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.5821 - accuracy: 0.1176 - val_loss: 0.6103 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5803 - accuracy: 0.1176 - val_loss: 0.6090 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5786 - accuracy: 0.1176 - val_loss: 0.6077 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5768 - accuracy: 0.1176 - val_loss: 0.6063 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5749 - accuracy: 0.1176 - val_loss: 0.6050 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5731 - accuracy: 0.1176 - val_loss: 0.6036 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5712 - accuracy: 0.1176 - val_loss: 0.6022 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.5693 - accuracy: 0.1176 - val_loss: 0.6008 - val_accuracy: 0.0000e+00\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.5674 - accuracy: 0.1176 - val_loss: 0.5994 - val_accuracy: 0.0000e+00\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5654 - accuracy: 0.1176 - val_loss: 0.5979 - val_accuracy: 0.0000e+00\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5634 - accuracy: 0.1176 - val_loss: 0.5964 - val_accuracy: 0.0000e+00\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5614 - accuracy: 0.1176 - val_loss: 0.5949 - val_accuracy: 0.0000e+00\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5593 - accuracy: 0.1176 - val_loss: 0.5933 - val_accuracy: 0.0000e+00\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.5572 - accuracy: 0.1176 - val_loss: 0.5918 - val_accuracy: 0.0000e+00\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.5551 - accuracy: 0.1176 - val_loss: 0.5902 - val_accuracy: 0.0000e+00\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5529 - accuracy: 0.1176 - val_loss: 0.5886 - val_accuracy: 0.0000e+00\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.5507 - accuracy: 0.1176 - val_loss: 0.5870 - val_accuracy: 0.0000e+00\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5485 - accuracy: 0.1176 - val_loss: 0.5853 - val_accuracy: 0.0000e+00\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5462 - accuracy: 0.1176 - val_loss: 0.5837 - val_accuracy: 0.0000e+00\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5439 - accuracy: 0.1176 - val_loss: 0.5819 - val_accuracy: 0.0000e+00\n",
      "Epoch 113/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 45ms/step - loss: 0.5416 - accuracy: 0.1176 - val_loss: 0.5802 - val_accuracy: 0.0000e+00\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5392 - accuracy: 0.1176 - val_loss: 0.5785 - val_accuracy: 0.0000e+00\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5368 - accuracy: 0.1176 - val_loss: 0.5767 - val_accuracy: 0.0000e+00\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.5344 - accuracy: 0.1176 - val_loss: 0.5749 - val_accuracy: 0.0000e+00\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5319 - accuracy: 0.1176 - val_loss: 0.5730 - val_accuracy: 0.2000\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.5293 - accuracy: 0.1176 - val_loss: 0.5711 - val_accuracy: 0.2000\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5268 - accuracy: 0.1176 - val_loss: 0.5692 - val_accuracy: 0.2000\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5241 - accuracy: 0.1176 - val_loss: 0.5673 - val_accuracy: 0.2000\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5215 - accuracy: 0.0588 - val_loss: 0.5654 - val_accuracy: 0.2000\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5188 - accuracy: 0.0588 - val_loss: 0.5634 - val_accuracy: 0.2000\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.5160 - accuracy: 0.0588 - val_loss: 0.5614 - val_accuracy: 0.2000\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.5133 - accuracy: 0.1176 - val_loss: 0.5593 - val_accuracy: 0.2000\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.5104 - accuracy: 0.0588 - val_loss: 0.5573 - val_accuracy: 0.2000\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5076 - accuracy: 0.0588 - val_loss: 0.5552 - val_accuracy: 0.2000\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5046 - accuracy: 0.0588 - val_loss: 0.5531 - val_accuracy: 0.2000\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.5017 - accuracy: 0.0588 - val_loss: 0.5509 - val_accuracy: 0.2000\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4986 - accuracy: 0.0588 - val_loss: 0.5487 - val_accuracy: 0.2000\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4956 - accuracy: 0.0588 - val_loss: 0.5465 - val_accuracy: 0.2000\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4925 - accuracy: 0.0588 - val_loss: 0.5442 - val_accuracy: 0.2000\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4893 - accuracy: 0.0588 - val_loss: 0.5419 - val_accuracy: 0.2000\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.4861 - accuracy: 0.0588 - val_loss: 0.5396 - val_accuracy: 0.2000\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4829 - accuracy: 0.0588 - val_loss: 0.5372 - val_accuracy: 0.2000\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4796 - accuracy: 0.0588 - val_loss: 0.5349 - val_accuracy: 0.2000\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4762 - accuracy: 0.0588 - val_loss: 0.5324 - val_accuracy: 0.2000\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.4728 - accuracy: 0.0588 - val_loss: 0.5300 - val_accuracy: 0.2000\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4694 - accuracy: 0.0588 - val_loss: 0.5275 - val_accuracy: 0.2000\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4660 - accuracy: 0.0588 - val_loss: 0.5250 - val_accuracy: 0.2000\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4624 - accuracy: 0.0588 - val_loss: 0.5225 - val_accuracy: 0.2000\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4589 - accuracy: 0.0588 - val_loss: 0.5199 - val_accuracy: 0.2000\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4553 - accuracy: 0.0588 - val_loss: 0.5173 - val_accuracy: 0.2000\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4516 - accuracy: 0.0588 - val_loss: 0.5147 - val_accuracy: 0.2000\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4480 - accuracy: 0.0588 - val_loss: 0.5121 - val_accuracy: 0.2000\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4443 - accuracy: 0.0588 - val_loss: 0.5094 - val_accuracy: 0.2000\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4405 - accuracy: 0.0588 - val_loss: 0.5068 - val_accuracy: 0.2000\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4367 - accuracy: 0.0588 - val_loss: 0.5041 - val_accuracy: 0.2000\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4329 - accuracy: 0.0588 - val_loss: 0.5014 - val_accuracy: 0.2000\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4290 - accuracy: 0.0588 - val_loss: 0.4986 - val_accuracy: 0.2000\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.4251 - accuracy: 0.0588 - val_loss: 0.4959 - val_accuracy: 0.2000\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4212 - accuracy: 0.0588 - val_loss: 0.4931 - val_accuracy: 0.2000\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.4173 - accuracy: 0.0588 - val_loss: 0.4903 - val_accuracy: 0.2000\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4133 - accuracy: 0.0588 - val_loss: 0.4875 - val_accuracy: 0.2000\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4093 - accuracy: 0.0588 - val_loss: 0.4847 - val_accuracy: 0.2000\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.4053 - accuracy: 0.0588 - val_loss: 0.4819 - val_accuracy: 0.2000\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4013 - accuracy: 0.0588 - val_loss: 0.4791 - val_accuracy: 0.2000\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3973 - accuracy: 0.0588 - val_loss: 0.4763 - val_accuracy: 0.2000\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3933 - accuracy: 0.0588 - val_loss: 0.4735 - val_accuracy: 0.2000\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.3892 - accuracy: 0.0588 - val_loss: 0.4707 - val_accuracy: 0.2000\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3852 - accuracy: 0.0588 - val_loss: 0.4679 - val_accuracy: 0.2000\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3811 - accuracy: 0.0588 - val_loss: 0.4651 - val_accuracy: 0.2000\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3771 - accuracy: 0.0588 - val_loss: 0.4623 - val_accuracy: 0.2000\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3730 - accuracy: 0.0588 - val_loss: 0.4595 - val_accuracy: 0.2000\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3690 - accuracy: 0.0588 - val_loss: 0.4568 - val_accuracy: 0.2000\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3650 - accuracy: 0.0588 - val_loss: 0.4540 - val_accuracy: 0.2000\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3610 - accuracy: 0.0588 - val_loss: 0.4513 - val_accuracy: 0.2000\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3570 - accuracy: 0.0588 - val_loss: 0.4486 - val_accuracy: 0.2000\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3530 - accuracy: 0.0588 - val_loss: 0.4460 - val_accuracy: 0.2000\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3491 - accuracy: 0.0588 - val_loss: 0.4433 - val_accuracy: 0.2000\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3452 - accuracy: 0.0588 - val_loss: 0.4408 - val_accuracy: 0.2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3413 - accuracy: 0.0588 - val_loss: 0.4382 - val_accuracy: 0.2000\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3375 - accuracy: 0.0588 - val_loss: 0.4357 - val_accuracy: 0.2000\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3337 - accuracy: 0.0588 - val_loss: 0.4332 - val_accuracy: 0.2000\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3300 - accuracy: 0.0588 - val_loss: 0.4307 - val_accuracy: 0.2000\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3263 - accuracy: 0.0588 - val_loss: 0.4284 - val_accuracy: 0.2000\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3227 - accuracy: 0.0588 - val_loss: 0.4260 - val_accuracy: 0.2000\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3191 - accuracy: 0.0588 - val_loss: 0.4237 - val_accuracy: 0.2000\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3156 - accuracy: 0.0588 - val_loss: 0.4215 - val_accuracy: 0.2000\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3122 - accuracy: 0.0588 - val_loss: 0.4193 - val_accuracy: 0.2000\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3088 - accuracy: 0.0588 - val_loss: 0.4172 - val_accuracy: 0.2000\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3055 - accuracy: 0.0588 - val_loss: 0.4151 - val_accuracy: 0.2000\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3022 - accuracy: 0.0588 - val_loss: 0.4131 - val_accuracy: 0.2000\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2991 - accuracy: 0.0588 - val_loss: 0.4111 - val_accuracy: 0.2000\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2960 - accuracy: 0.0588 - val_loss: 0.4092 - val_accuracy: 0.2000\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2929 - accuracy: 0.0588 - val_loss: 0.4074 - val_accuracy: 0.2000\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2900 - accuracy: 0.0588 - val_loss: 0.4056 - val_accuracy: 0.2000\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2871 - accuracy: 0.0588 - val_loss: 0.4039 - val_accuracy: 0.2000\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2843 - accuracy: 0.0588 - val_loss: 0.4023 - val_accuracy: 0.2000\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2816 - accuracy: 0.0588 - val_loss: 0.4007 - val_accuracy: 0.2000\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2789 - accuracy: 0.0588 - val_loss: 0.3991 - val_accuracy: 0.2000\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2764 - accuracy: 0.0588 - val_loss: 0.3977 - val_accuracy: 0.2000\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2739 - accuracy: 0.0588 - val_loss: 0.3963 - val_accuracy: 0.2000\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2715 - accuracy: 0.0588 - val_loss: 0.3949 - val_accuracy: 0.2000\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2691 - accuracy: 0.0588 - val_loss: 0.3936 - val_accuracy: 0.2000\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.2669 - accuracy: 0.0588 - val_loss: 0.3924 - val_accuracy: 0.2000\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2647 - accuracy: 0.0588 - val_loss: 0.3912 - val_accuracy: 0.2000\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2626 - accuracy: 0.0588 - val_loss: 0.3901 - val_accuracy: 0.2000\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2606 - accuracy: 0.0588 - val_loss: 0.3890 - val_accuracy: 0.2000\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2586 - accuracy: 0.0588 - val_loss: 0.3880 - val_accuracy: 0.2000\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2568 - accuracy: 0.0588 - val_loss: 0.3871 - val_accuracy: 0.2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-31-8784861b343f>:134: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "  tprs.append(interp(mean_fpr, fpr, tpr))\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:806: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Epoch 1/200\n",
      "1/1 [==============================] - 1s 501ms/step - loss: 0.7107 - accuracy: 0.0000e+00 - val_loss: 0.7066 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.7075 - accuracy: 0.0000e+00 - val_loss: 0.7038 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.7045 - accuracy: 0.0000e+00 - val_loss: 0.7010 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.7016 - accuracy: 0.0000e+00 - val_loss: 0.6984 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.6987 - accuracy: 0.0000e+00 - val_loss: 0.6959 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6959 - accuracy: 0.0000e+00 - val_loss: 0.6934 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6931 - accuracy: 0.0000e+00 - val_loss: 0.6909 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6904 - accuracy: 0.0000e+00 - val_loss: 0.6885 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.6877 - accuracy: 0.0000e+00 - val_loss: 0.6862 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6852 - accuracy: 0.0000e+00 - val_loss: 0.6839 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6826 - accuracy: 0.0000e+00 - val_loss: 0.6816 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6801 - accuracy: 0.0000e+00 - val_loss: 0.6794 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6776 - accuracy: 0.0000e+00 - val_loss: 0.6772 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6752 - accuracy: 0.0000e+00 - val_loss: 0.6751 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6728 - accuracy: 0.0000e+00 - val_loss: 0.6730 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6705 - accuracy: 0.0588 - val_loss: 0.6710 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.6684 - accuracy: 0.0588 - val_loss: 0.6690 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.6662 - accuracy: 0.0588 - val_loss: 0.6671 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6641 - accuracy: 0.0588 - val_loss: 0.6651 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6621 - accuracy: 0.0588 - val_loss: 0.6633 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6601 - accuracy: 0.0588 - val_loss: 0.6614 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.6582 - accuracy: 0.0588 - val_loss: 0.6596 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6562 - accuracy: 0.0588 - val_loss: 0.6578 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6542 - accuracy: 0.0588 - val_loss: 0.6560 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.6523 - accuracy: 0.1176 - val_loss: 0.6542 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6503 - accuracy: 0.1176 - val_loss: 0.6524 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6484 - accuracy: 0.1176 - val_loss: 0.6506 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6465 - accuracy: 0.1176 - val_loss: 0.6489 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6445 - accuracy: 0.1176 - val_loss: 0.6470 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6425 - accuracy: 0.1176 - val_loss: 0.6452 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6405 - accuracy: 0.1176 - val_loss: 0.6433 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6385 - accuracy: 0.1176 - val_loss: 0.6415 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6365 - accuracy: 0.1176 - val_loss: 0.6396 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6345 - accuracy: 0.1176 - val_loss: 0.6377 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6324 - accuracy: 0.1176 - val_loss: 0.6359 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6303 - accuracy: 0.1176 - val_loss: 0.6339 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6282 - accuracy: 0.1176 - val_loss: 0.6320 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6260 - accuracy: 0.1176 - val_loss: 0.6300 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.6238 - accuracy: 0.1176 - val_loss: 0.6281 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6216 - accuracy: 0.1176 - val_loss: 0.6261 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6194 - accuracy: 0.1176 - val_loss: 0.6241 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6171 - accuracy: 0.1176 - val_loss: 0.6221 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.6148 - accuracy: 0.1176 - val_loss: 0.6201 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.6125 - accuracy: 0.1176 - val_loss: 0.6180 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6102 - accuracy: 0.1176 - val_loss: 0.6159 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6078 - accuracy: 0.1176 - val_loss: 0.6138 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6054 - accuracy: 0.1176 - val_loss: 0.6117 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6030 - accuracy: 0.1176 - val_loss: 0.6095 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.6005 - accuracy: 0.1176 - val_loss: 0.6073 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5980 - accuracy: 0.1176 - val_loss: 0.6051 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5954 - accuracy: 0.1176 - val_loss: 0.6028 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.5928 - accuracy: 0.1176 - val_loss: 0.6005 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5901 - accuracy: 0.1176 - val_loss: 0.5981 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5874 - accuracy: 0.1176 - val_loss: 0.5957 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.5846 - accuracy: 0.1176 - val_loss: 0.5933 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 28ms/step - loss: 0.5818 - accuracy: 0.1176 - val_loss: 0.5908 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5790 - accuracy: 0.1176 - val_loss: 0.5883 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5761 - accuracy: 0.1176 - val_loss: 0.5858 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5731 - accuracy: 0.1176 - val_loss: 0.5832 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5701 - accuracy: 0.1176 - val_loss: 0.5806 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5671 - accuracy: 0.1176 - val_loss: 0.5779 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5640 - accuracy: 0.1176 - val_loss: 0.5752 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.5609 - accuracy: 0.1176 - val_loss: 0.5724 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.5577 - accuracy: 0.1176 - val_loss: 0.5696 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5544 - accuracy: 0.1176 - val_loss: 0.5668 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.5511 - accuracy: 0.1176 - val_loss: 0.5639 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5478 - accuracy: 0.1176 - val_loss: 0.5610 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5444 - accuracy: 0.1176 - val_loss: 0.5580 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.5409 - accuracy: 0.1176 - val_loss: 0.5550 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5374 - accuracy: 0.1176 - val_loss: 0.5519 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5338 - accuracy: 0.1176 - val_loss: 0.5488 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5301 - accuracy: 0.1176 - val_loss: 0.5457 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5264 - accuracy: 0.1176 - val_loss: 0.5425 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5226 - accuracy: 0.1176 - val_loss: 0.5392 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.5188 - accuracy: 0.0588 - val_loss: 0.5359 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5149 - accuracy: 0.0588 - val_loss: 0.5326 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5109 - accuracy: 0.0588 - val_loss: 0.5292 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5069 - accuracy: 0.0588 - val_loss: 0.5258 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.5029 - accuracy: 0.0588 - val_loss: 0.5224 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4988 - accuracy: 0.0588 - val_loss: 0.5189 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4946 - accuracy: 0.0588 - val_loss: 0.5154 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.4904 - accuracy: 0.0588 - val_loss: 0.5118 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4861 - accuracy: 0.0588 - val_loss: 0.5082 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4818 - accuracy: 0.0588 - val_loss: 0.5046 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4774 - accuracy: 0.0588 - val_loss: 0.5009 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4730 - accuracy: 0.0588 - val_loss: 0.4973 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4685 - accuracy: 0.0588 - val_loss: 0.4935 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4640 - accuracy: 0.0588 - val_loss: 0.4898 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4595 - accuracy: 0.0588 - val_loss: 0.4860 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.4549 - accuracy: 0.0588 - val_loss: 0.4822 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.4503 - accuracy: 0.0588 - val_loss: 0.4784 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4457 - accuracy: 0.0588 - val_loss: 0.4746 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4410 - accuracy: 0.0588 - val_loss: 0.4708 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4363 - accuracy: 0.0588 - val_loss: 0.4670 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4316 - accuracy: 0.05 - 0s 25ms/step - loss: 0.4316 - accuracy: 0.0588 - val_loss: 0.4632 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4269 - accuracy: 0.0588 - val_loss: 0.4593 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4222 - accuracy: 0.0588 - val_loss: 0.4555 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.4174 - accuracy: 0.0588 - val_loss: 0.4517 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.4127 - accuracy: 0.0588 - val_loss: 0.4479 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4080 - accuracy: 0.0588 - val_loss: 0.4441 - val_accuracy: 0.0000e+00\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4033 - accuracy: 0.0588 - val_loss: 0.4404 - val_accuracy: 0.0000e+00\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3986 - accuracy: 0.0588 - val_loss: 0.4367 - val_accuracy: 0.0000e+00\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3939 - accuracy: 0.0588 - val_loss: 0.4330 - val_accuracy: 0.0000e+00\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3892 - accuracy: 0.0588 - val_loss: 0.4293 - val_accuracy: 0.0000e+00\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3846 - accuracy: 0.0588 - val_loss: 0.4257 - val_accuracy: 0.0000e+00\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3800 - accuracy: 0.0588 - val_loss: 0.4221 - val_accuracy: 0.0000e+00\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3754 - accuracy: 0.0588 - val_loss: 0.4186 - val_accuracy: 0.0000e+00\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3709 - accuracy: 0.0588 - val_loss: 0.4152 - val_accuracy: 0.0000e+00\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3665 - accuracy: 0.0588 - val_loss: 0.4118 - val_accuracy: 0.0000e+00\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.3620 - accuracy: 0.0588 - val_loss: 0.4084 - val_accuracy: 0.0000e+00\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3577 - accuracy: 0.0588 - val_loss: 0.4051 - val_accuracy: 0.0000e+00\n",
      "Epoch 112/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 34ms/step - loss: 0.3534 - accuracy: 0.0588 - val_loss: 0.4019 - val_accuracy: 0.0000e+00\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.3491 - accuracy: 0.0588 - val_loss: 0.3988 - val_accuracy: 0.0000e+00\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.3450 - accuracy: 0.0588 - val_loss: 0.3957 - val_accuracy: 0.0000e+00\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.3409 - accuracy: 0.0588 - val_loss: 0.3927 - val_accuracy: 0.0000e+00\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.3369 - accuracy: 0.0588 - val_loss: 0.3898 - val_accuracy: 0.0000e+00\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.3330 - accuracy: 0.0588 - val_loss: 0.3870 - val_accuracy: 0.0000e+00\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.3291 - accuracy: 0.0588 - val_loss: 0.3843 - val_accuracy: 0.0000e+00\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.3254 - accuracy: 0.0588 - val_loss: 0.3816 - val_accuracy: 0.0000e+00\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.3218 - accuracy: 0.0588 - val_loss: 0.3791 - val_accuracy: 0.0000e+00\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.3182 - accuracy: 0.0588 - val_loss: 0.3766 - val_accuracy: 0.0000e+00\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.3148 - accuracy: 0.0588 - val_loss: 0.3743 - val_accuracy: 0.0000e+00\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.3114 - accuracy: 0.0588 - val_loss: 0.3720 - val_accuracy: 0.0000e+00\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3081 - accuracy: 0.0588 - val_loss: 0.3699 - val_accuracy: 0.0000e+00\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.3050 - accuracy: 0.0588 - val_loss: 0.3678 - val_accuracy: 0.0000e+00\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.3019 - accuracy: 0.0588 - val_loss: 0.3659 - val_accuracy: 0.0000e+00\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.2990 - accuracy: 0.0588 - val_loss: 0.3640 - val_accuracy: 0.0000e+00\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2961 - accuracy: 0.0588 - val_loss: 0.3622 - val_accuracy: 0.0000e+00\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2934 - accuracy: 0.0588 - val_loss: 0.3606 - val_accuracy: 0.0000e+00\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2907 - accuracy: 0.0588 - val_loss: 0.3590 - val_accuracy: 0.0000e+00\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2882 - accuracy: 0.0588 - val_loss: 0.3575 - val_accuracy: 0.0000e+00\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2858 - accuracy: 0.0588 - val_loss: 0.3560 - val_accuracy: 0.0000e+00\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2834 - accuracy: 0.0588 - val_loss: 0.3547 - val_accuracy: 0.0000e+00\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2811 - accuracy: 0.0588 - val_loss: 0.3535 - val_accuracy: 0.0000e+00\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2790 - accuracy: 0.0588 - val_loss: 0.3523 - val_accuracy: 0.0000e+00\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2769 - accuracy: 0.0588 - val_loss: 0.3512 - val_accuracy: 0.0000e+00\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2749 - accuracy: 0.0588 - val_loss: 0.3502 - val_accuracy: 0.0000e+00\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2730 - accuracy: 0.0588 - val_loss: 0.3492 - val_accuracy: 0.0000e+00\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2712 - accuracy: 0.0588 - val_loss: 0.3483 - val_accuracy: 0.0000e+00\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2694 - accuracy: 0.0588 - val_loss: 0.3475 - val_accuracy: 0.0000e+00\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2678 - accuracy: 0.0588 - val_loss: 0.3468 - val_accuracy: 0.0000e+00\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2662 - accuracy: 0.1176 - val_loss: 0.3461 - val_accuracy: 0.0000e+00\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2647 - accuracy: 0.1176 - val_loss: 0.3455 - val_accuracy: 0.0000e+00\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2632 - accuracy: 0.1176 - val_loss: 0.3449 - val_accuracy: 0.0000e+00\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2618 - accuracy: 0.1176 - val_loss: 0.3444 - val_accuracy: 0.0000e+00\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2605 - accuracy: 0.1176 - val_loss: 0.3439 - val_accuracy: 0.0000e+00\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2592 - accuracy: 0.1176 - val_loss: 0.3435 - val_accuracy: 0.0000e+00\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2580 - accuracy: 0.1176 - val_loss: 0.3432 - val_accuracy: 0.0000e+00\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2569 - accuracy: 0.1176 - val_loss: 0.3428 - val_accuracy: 0.0000e+00\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2558 - accuracy: 0.1176 - val_loss: 0.3426 - val_accuracy: 0.0000e+00\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2547 - accuracy: 0.1176 - val_loss: 0.3423 - val_accuracy: 0.0000e+00\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2537 - accuracy: 0.1176 - val_loss: 0.3421 - val_accuracy: 0.0000e+00\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2528 - accuracy: 0.1176 - val_loss: 0.3420 - val_accuracy: 0.0000e+00\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2519 - accuracy: 0.1765 - val_loss: 0.3418 - val_accuracy: 0.0000e+00\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2510 - accuracy: 0.1765 - val_loss: 0.3418 - val_accuracy: 0.0000e+00\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2502 - accuracy: 0.1765 - val_loss: 0.3417 - val_accuracy: 0.0000e+00\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2494 - accuracy: 0.1765 - val_loss: 0.3416 - val_accuracy: 0.0000e+00\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2486 - accuracy: 0.1765 - val_loss: 0.3416 - val_accuracy: 0.0000e+00\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2479 - accuracy: 0.1765 - val_loss: 0.3416 - val_accuracy: 0.0000e+00\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2472 - accuracy: 0.1765 - val_loss: 0.3417 - val_accuracy: 0.0000e+00\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2466 - accuracy: 0.1765 - val_loss: 0.3417 - val_accuracy: 0.0000e+00\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2459 - accuracy: 0.1765 - val_loss: 0.3418 - val_accuracy: 0.0000e+00\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2453 - accuracy: 0.1765 - val_loss: 0.3419 - val_accuracy: 0.0000e+00\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2447 - accuracy: 0.1765 - val_loss: 0.3420 - val_accuracy: 0.0000e+00\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2442 - accuracy: 0.1765 - val_loss: 0.3422 - val_accuracy: 0.0000e+00\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2436 - accuracy: 0.1765 - val_loss: 0.3423 - val_accuracy: 0.0000e+00\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2431 - accuracy: 0.2353 - val_loss: 0.3425 - val_accuracy: 0.0000e+00\n",
      "Epoch 168/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2427 - accuracy: 0.2353 - val_loss: 0.3426 - val_accuracy: 0.0000e+00\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2422 - accuracy: 0.2353 - val_loss: 0.3428 - val_accuracy: 0.0000e+00\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2417 - accuracy: 0.2353 - val_loss: 0.3430 - val_accuracy: 0.0000e+00\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2413 - accuracy: 0.1765 - val_loss: 0.3432 - val_accuracy: 0.0000e+00\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2409 - accuracy: 0.1765 - val_loss: 0.3435 - val_accuracy: 0.0000e+00\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2405 - accuracy: 0.1765 - val_loss: 0.3437 - val_accuracy: 0.0000e+00\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2401 - accuracy: 0.1765 - val_loss: 0.3439 - val_accuracy: 0.0000e+00\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2397 - accuracy: 0.1176 - val_loss: 0.3442 - val_accuracy: 0.0000e+00\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2394 - accuracy: 0.1176 - val_loss: 0.3444 - val_accuracy: 0.0000e+00\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2390 - accuracy: 0.1176 - val_loss: 0.3447 - val_accuracy: 0.0000e+00\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2387 - accuracy: 0.1176 - val_loss: 0.3450 - val_accuracy: 0.0000e+00\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2383 - accuracy: 0.1176 - val_loss: 0.3452 - val_accuracy: 0.0000e+00\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2380 - accuracy: 0.1176 - val_loss: 0.3455 - val_accuracy: 0.0000e+00\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2377 - accuracy: 0.1176 - val_loss: 0.3458 - val_accuracy: 0.0000e+00\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2374 - accuracy: 0.1176 - val_loss: 0.3461 - val_accuracy: 0.0000e+00\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2372 - accuracy: 0.1176 - val_loss: 0.3464 - val_accuracy: 0.0000e+00\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2369 - accuracy: 0.1176 - val_loss: 0.3467 - val_accuracy: 0.0000e+00\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2366 - accuracy: 0.1176 - val_loss: 0.3470 - val_accuracy: 0.0000e+00\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2364 - accuracy: 0.1176 - val_loss: 0.3473 - val_accuracy: 0.0000e+00\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2361 - accuracy: 0.1176 - val_loss: 0.3476 - val_accuracy: 0.0000e+00\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2359 - accuracy: 0.1176 - val_loss: 0.3479 - val_accuracy: 0.0000e+00\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2356 - accuracy: 0.1176 - val_loss: 0.3482 - val_accuracy: 0.0000e+00\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.2354 - accuracy: 0.1176 - val_loss: 0.3485 - val_accuracy: 0.0000e+00\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2352 - accuracy: 0.1176 - val_loss: 0.3489 - val_accuracy: 0.0000e+00\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2350 - accuracy: 0.1176 - val_loss: 0.3492 - val_accuracy: 0.0000e+00\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2348 - accuracy: 0.1176 - val_loss: 0.3495 - val_accuracy: 0.0000e+00\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2345 - accuracy: 0.1176 - val_loss: 0.3498 - val_accuracy: 0.0000e+00\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2343 - accuracy: 0.1176 - val_loss: 0.3501 - val_accuracy: 0.0000e+00\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2342 - accuracy: 0.1176 - val_loss: 0.3505 - val_accuracy: 0.0000e+00\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2340 - accuracy: 0.1176 - val_loss: 0.3508 - val_accuracy: 0.0000e+00\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2338 - accuracy: 0.1176 - val_loss: 0.3511 - val_accuracy: 0.0000e+00\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2336 - accuracy: 0.1176 - val_loss: 0.3514 - val_accuracy: 0.0000e+00\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2334 - accuracy: 0.1176 - val_loss: 0.3517 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-31-8784861b343f>:134: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "  tprs.append(interp(mean_fpr, fpr, tpr))\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:806: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Epoch 1/200\n",
      "1/1 [==============================] - 1s 504ms/step - loss: 0.6768 - accuracy: 0.0556 - val_loss: 0.6873 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6749 - accuracy: 0.0556 - val_loss: 0.6857 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.6731 - accuracy: 0.0000e+00 - val_loss: 0.6841 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6712 - accuracy: 0.0000e+00 - val_loss: 0.6825 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6694 - accuracy: 0.0556 - val_loss: 0.6809 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6675 - accuracy: 0.0556 - val_loss: 0.6793 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6656 - accuracy: 0.0556 - val_loss: 0.6777 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6638 - accuracy: 0.0556 - val_loss: 0.6761 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6619 - accuracy: 0.0556 - val_loss: 0.6746 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.6600 - accuracy: 0.0556 - val_loss: 0.6730 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.6582 - accuracy: 0.0556 - val_loss: 0.6714 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6563 - accuracy: 0.0556 - val_loss: 0.6698 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6544 - accuracy: 0.0556 - val_loss: 0.6682 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6525 - accuracy: 0.0556 - val_loss: 0.6666 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6506 - accuracy: 0.0556 - val_loss: 0.6651 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6487 - accuracy: 0.0556 - val_loss: 0.6635 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6467 - accuracy: 0.0556 - val_loss: 0.6619 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6448 - accuracy: 0.0556 - val_loss: 0.6602 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.6428 - accuracy: 0.0556 - val_loss: 0.6586 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.6408 - accuracy: 0.0556 - val_loss: 0.6570 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6388 - accuracy: 0.0556 - val_loss: 0.6553 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6368 - accuracy: 0.0556 - val_loss: 0.6536 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6348 - accuracy: 0.0556 - val_loss: 0.6519 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6327 - accuracy: 0.0556 - val_loss: 0.6502 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6306 - accuracy: 0.0556 - val_loss: 0.6485 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.6285 - accuracy: 0.0556 - val_loss: 0.6468 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6264 - accuracy: 0.0556 - val_loss: 0.6451 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6243 - accuracy: 0.0556 - val_loss: 0.6433 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6221 - accuracy: 0.0556 - val_loss: 0.6415 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6199 - accuracy: 0.0556 - val_loss: 0.6397 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6177 - accuracy: 0.0556 - val_loss: 0.6379 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6155 - accuracy: 0.0556 - val_loss: 0.6361 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6132 - accuracy: 0.0556 - val_loss: 0.6342 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6110 - accuracy: 0.0556 - val_loss: 0.6324 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6086 - accuracy: 0.0556 - val_loss: 0.6305 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6063 - accuracy: 0.0556 - val_loss: 0.6285 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6039 - accuracy: 0.0556 - val_loss: 0.6266 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6015 - accuracy: 0.0556 - val_loss: 0.6246 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.5991 - accuracy: 0.0556 - val_loss: 0.6226 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.5966 - accuracy: 0.0556 - val_loss: 0.6206 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5941 - accuracy: 0.0556 - val_loss: 0.6186 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5916 - accuracy: 0.0556 - val_loss: 0.6165 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5890 - accuracy: 0.0556 - val_loss: 0.6144 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5864 - accuracy: 0.0556 - val_loss: 0.6122 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.5837 - accuracy: 0.0556 - val_loss: 0.6101 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5810 - accuracy: 0.0556 - val_loss: 0.6079 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5783 - accuracy: 0.0556 - val_loss: 0.6057 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.5755 - accuracy: 0.0556 - val_loss: 0.6034 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5727 - accuracy: 0.0556 - val_loss: 0.6011 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5699 - accuracy: 0.0556 - val_loss: 0.5988 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.5670 - accuracy: 0.0556 - val_loss: 0.5965 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.5640 - accuracy: 0.0556 - val_loss: 0.5941 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5610 - accuracy: 0.0556 - val_loss: 0.5917 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.5580 - accuracy: 0.0556 - val_loss: 0.5892 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5549 - accuracy: 0.05 - 0s 27ms/step - loss: 0.5549 - accuracy: 0.0556 - val_loss: 0.5868 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5518 - accuracy: 0.0556 - val_loss: 0.5843 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5487 - accuracy: 0.0556 - val_loss: 0.5817 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5454 - accuracy: 0.0556 - val_loss: 0.5791 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5422 - accuracy: 0.0556 - val_loss: 0.5765 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5389 - accuracy: 0.0556 - val_loss: 0.5738 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.5355 - accuracy: 0.0556 - val_loss: 0.5711 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.5321 - accuracy: 0.0556 - val_loss: 0.5684 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5287 - accuracy: 0.0556 - val_loss: 0.5656 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.5252 - accuracy: 0.0556 - val_loss: 0.5628 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5216 - accuracy: 0.0556 - val_loss: 0.5599 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5180 - accuracy: 0.0556 - val_loss: 0.5570 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.5144 - accuracy: 0.0556 - val_loss: 0.5541 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.5107 - accuracy: 0.0556 - val_loss: 0.5511 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.5070 - accuracy: 0.0556 - val_loss: 0.5482 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5032 - accuracy: 0.0556 - val_loss: 0.5451 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4993 - accuracy: 0.0556 - val_loss: 0.5421 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4955 - accuracy: 0.0556 - val_loss: 0.5390 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.4916 - accuracy: 0.0556 - val_loss: 0.5358 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4876 - accuracy: 0.0556 - val_loss: 0.5326 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4836 - accuracy: 0.0556 - val_loss: 0.5295 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4796 - accuracy: 0.0556 - val_loss: 0.5262 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4755 - accuracy: 0.0556 - val_loss: 0.5230 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.4714 - accuracy: 0.0556 - val_loss: 0.5197 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.4672 - accuracy: 0.0556 - val_loss: 0.5164 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4631 - accuracy: 0.0556 - val_loss: 0.5131 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4588 - accuracy: 0.0556 - val_loss: 0.5098 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4546 - accuracy: 0.0556 - val_loss: 0.5065 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.4503 - accuracy: 0.0556 - val_loss: 0.5031 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4461 - accuracy: 0.0556 - val_loss: 0.4998 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4417 - accuracy: 0.0556 - val_loss: 0.4964 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4374 - accuracy: 0.0556 - val_loss: 0.4930 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4331 - accuracy: 0.0556 - val_loss: 0.4896 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4287 - accuracy: 0.0556 - val_loss: 0.4863 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4243 - accuracy: 0.0556 - val_loss: 0.4829 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4200 - accuracy: 0.0556 - val_loss: 0.4795 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4156 - accuracy: 0.0556 - val_loss: 0.4761 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4112 - accuracy: 0.0556 - val_loss: 0.4727 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4069 - accuracy: 0.0556 - val_loss: 0.4694 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.4025 - accuracy: 0.0556 - val_loss: 0.4660 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3982 - accuracy: 0.0556 - val_loss: 0.4627 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3939 - accuracy: 0.0556 - val_loss: 0.4594 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3896 - accuracy: 0.0556 - val_loss: 0.4561 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3853 - accuracy: 0.0556 - val_loss: 0.4529 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.3810 - accuracy: 0.0556 - val_loss: 0.4497 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3768 - accuracy: 0.0556 - val_loss: 0.4465 - val_accuracy: 0.0000e+00\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3727 - accuracy: 0.0556 - val_loss: 0.4434 - val_accuracy: 0.0000e+00\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3685 - accuracy: 0.0556 - val_loss: 0.4403 - val_accuracy: 0.0000e+00\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3644 - accuracy: 0.0556 - val_loss: 0.4372 - val_accuracy: 0.0000e+00\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.3604 - accuracy: 0.0556 - val_loss: 0.4342 - val_accuracy: 0.0000e+00\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3564 - accuracy: 0.0556 - val_loss: 0.4313 - val_accuracy: 0.0000e+00\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3525 - accuracy: 0.0556 - val_loss: 0.4284 - val_accuracy: 0.0000e+00\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3487 - accuracy: 0.0556 - val_loss: 0.4255 - val_accuracy: 0.0000e+00\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3449 - accuracy: 0.0556 - val_loss: 0.4227 - val_accuracy: 0.0000e+00\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3411 - accuracy: 0.0556 - val_loss: 0.4200 - val_accuracy: 0.0000e+00\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.3375 - accuracy: 0.0556 - val_loss: 0.4173 - val_accuracy: 0.0000e+00\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3339 - accuracy: 0.0556 - val_loss: 0.4147 - val_accuracy: 0.0000e+00\n",
      "Epoch 112/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3303 - accuracy: 0.0556 - val_loss: 0.4122 - val_accuracy: 0.0000e+00\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.3269 - accuracy: 0.0556 - val_loss: 0.4098 - val_accuracy: 0.0000e+00\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3235 - accuracy: 0.0556 - val_loss: 0.4074 - val_accuracy: 0.0000e+00\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3202 - accuracy: 0.0556 - val_loss: 0.4051 - val_accuracy: 0.0000e+00\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3170 - accuracy: 0.0556 - val_loss: 0.4028 - val_accuracy: 0.0000e+00\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3139 - accuracy: 0.0556 - val_loss: 0.4006 - val_accuracy: 0.0000e+00\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3109 - accuracy: 0.0556 - val_loss: 0.3986 - val_accuracy: 0.0000e+00\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.3079 - accuracy: 0.0556 - val_loss: 0.3965 - val_accuracy: 0.0000e+00\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3050 - accuracy: 0.0556 - val_loss: 0.3946 - val_accuracy: 0.0000e+00\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3022 - accuracy: 0.0556 - val_loss: 0.3927 - val_accuracy: 0.0000e+00\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2995 - accuracy: 0.0556 - val_loss: 0.3909 - val_accuracy: 0.0000e+00\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.2969 - accuracy: 0.0556 - val_loss: 0.3892 - val_accuracy: 0.0000e+00\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.2944 - accuracy: 0.0556 - val_loss: 0.3875 - val_accuracy: 0.0000e+00\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2919 - accuracy: 0.0556 - val_loss: 0.3859 - val_accuracy: 0.0000e+00\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2896 - accuracy: 0.0556 - val_loss: 0.3844 - val_accuracy: 0.0000e+00\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2873 - accuracy: 0.0556 - val_loss: 0.3830 - val_accuracy: 0.0000e+00\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2851 - accuracy: 0.0556 - val_loss: 0.3816 - val_accuracy: 0.0000e+00\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2829 - accuracy: 0.0556 - val_loss: 0.3803 - val_accuracy: 0.0000e+00\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2809 - accuracy: 0.0556 - val_loss: 0.3790 - val_accuracy: 0.0000e+00\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2789 - accuracy: 0.0556 - val_loss: 0.3779 - val_accuracy: 0.0000e+00\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2770 - accuracy: 0.0556 - val_loss: 0.3768 - val_accuracy: 0.0000e+00\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2752 - accuracy: 0.0556 - val_loss: 0.3757 - val_accuracy: 0.0000e+00\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2735 - accuracy: 0.0556 - val_loss: 0.3747 - val_accuracy: 0.0000e+00\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2718 - accuracy: 0.0556 - val_loss: 0.3738 - val_accuracy: 0.0000e+00\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2702 - accuracy: 0.0556 - val_loss: 0.3729 - val_accuracy: 0.0000e+00\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2686 - accuracy: 0.0556 - val_loss: 0.3720 - val_accuracy: 0.0000e+00\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2671 - accuracy: 0.0556 - val_loss: 0.3713 - val_accuracy: 0.0000e+00\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2657 - accuracy: 0.0556 - val_loss: 0.3705 - val_accuracy: 0.0000e+00\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2644 - accuracy: 0.0556 - val_loss: 0.3698 - val_accuracy: 0.0000e+00\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2631 - accuracy: 0.0556 - val_loss: 0.3692 - val_accuracy: 0.0000e+00\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2618 - accuracy: 0.0556 - val_loss: 0.3686 - val_accuracy: 0.0000e+00\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2606 - accuracy: 0.0556 - val_loss: 0.3680 - val_accuracy: 0.0000e+00\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2595 - accuracy: 0.0556 - val_loss: 0.3675 - val_accuracy: 0.0000e+00\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2584 - accuracy: 0.0556 - val_loss: 0.3670 - val_accuracy: 0.0000e+00\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2573 - accuracy: 0.0556 - val_loss: 0.3666 - val_accuracy: 0.0000e+00\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2563 - accuracy: 0.0556 - val_loss: 0.3662 - val_accuracy: 0.0000e+00\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.2554 - accuracy: 0.0556 - val_loss: 0.3658 - val_accuracy: 0.0000e+00\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2545 - accuracy: 0.0556 - val_loss: 0.3655 - val_accuracy: 0.0000e+00\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2536 - accuracy: 0.0556 - val_loss: 0.3651 - val_accuracy: 0.0000e+00\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2528 - accuracy: 0.0556 - val_loss: 0.3649 - val_accuracy: 0.0000e+00\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2520 - accuracy: 0.0556 - val_loss: 0.3646 - val_accuracy: 0.0000e+00\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2512 - accuracy: 0.0556 - val_loss: 0.3643 - val_accuracy: 0.0000e+00\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2505 - accuracy: 0.0556 - val_loss: 0.3641 - val_accuracy: 0.0000e+00\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2498 - accuracy: 0.0556 - val_loss: 0.3639 - val_accuracy: 0.0000e+00\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2491 - accuracy: 0.0556 - val_loss: 0.3638 - val_accuracy: 0.0000e+00\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2484 - accuracy: 0.0556 - val_loss: 0.3636 - val_accuracy: 0.0000e+00\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2478 - accuracy: 0.1111 - val_loss: 0.3635 - val_accuracy: 0.0000e+00\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2472 - accuracy: 0.1111 - val_loss: 0.3633 - val_accuracy: 0.0000e+00\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2467 - accuracy: 0.1111 - val_loss: 0.3632 - val_accuracy: 0.0000e+00\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2461 - accuracy: 0.1111 - val_loss: 0.3631 - val_accuracy: 0.0000e+00\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2456 - accuracy: 0.1111 - val_loss: 0.3631 - val_accuracy: 0.0000e+00\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2451 - accuracy: 0.1111 - val_loss: 0.3630 - val_accuracy: 0.0000e+00\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2446 - accuracy: 0.1111 - val_loss: 0.3629 - val_accuracy: 0.0000e+00\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2442 - accuracy: 0.0556 - val_loss: 0.3629 - val_accuracy: 0.0000e+00\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.2437 - accuracy: 0.0556 - val_loss: 0.3629 - val_accuracy: 0.0000e+00\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2433 - accuracy: 0.0556 - val_loss: 0.3629 - val_accuracy: 0.0000e+00\n",
      "Epoch 168/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2429 - accuracy: 0.0556 - val_loss: 0.3628 - val_accuracy: 0.0000e+00\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2425 - accuracy: 0.0556 - val_loss: 0.3628 - val_accuracy: 0.0000e+00\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.2421 - accuracy: 0.0556 - val_loss: 0.3628 - val_accuracy: 0.0000e+00\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2417 - accuracy: 0.0556 - val_loss: 0.3629 - val_accuracy: 0.0000e+00\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2414 - accuracy: 0.0556 - val_loss: 0.3629 - val_accuracy: 0.0000e+00\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2411 - accuracy: 0.0556 - val_loss: 0.3629 - val_accuracy: 0.0000e+00\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2407 - accuracy: 0.0556 - val_loss: 0.3629 - val_accuracy: 0.0000e+00\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2404 - accuracy: 0.0556 - val_loss: 0.3630 - val_accuracy: 0.0000e+00\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2401 - accuracy: 0.0556 - val_loss: 0.3630 - val_accuracy: 0.0000e+00\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2398 - accuracy: 0.0556 - val_loss: 0.3631 - val_accuracy: 0.0000e+00\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2395 - accuracy: 0.1667 - val_loss: 0.3631 - val_accuracy: 0.0000e+00\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2392 - accuracy: 0.1667 - val_loss: 0.3632 - val_accuracy: 0.0000e+00\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2390 - accuracy: 0.1667 - val_loss: 0.3632 - val_accuracy: 0.0000e+00\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2387 - accuracy: 0.1667 - val_loss: 0.3633 - val_accuracy: 0.0000e+00\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2385 - accuracy: 0.1667 - val_loss: 0.3634 - val_accuracy: 0.0000e+00\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2382 - accuracy: 0.1667 - val_loss: 0.3635 - val_accuracy: 0.0000e+00\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2380 - accuracy: 0.1667 - val_loss: 0.3635 - val_accuracy: 0.0000e+00\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2378 - accuracy: 0.1667 - val_loss: 0.3636 - val_accuracy: 0.0000e+00\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2375 - accuracy: 0.1111 - val_loss: 0.3637 - val_accuracy: 0.0000e+00\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.2373 - accuracy: 0.1111 - val_loss: 0.3638 - val_accuracy: 0.0000e+00\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2371 - accuracy: 0.1111 - val_loss: 0.3639 - val_accuracy: 0.0000e+00\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2369 - accuracy: 0.1111 - val_loss: 0.3639 - val_accuracy: 0.0000e+00\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2367 - accuracy: 0.1111 - val_loss: 0.3640 - val_accuracy: 0.0000e+00\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2365 - accuracy: 0.1111 - val_loss: 0.3641 - val_accuracy: 0.0000e+00\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2363 - accuracy: 0.1111 - val_loss: 0.3642 - val_accuracy: 0.0000e+00\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2362 - accuracy: 0.1667 - val_loss: 0.3643 - val_accuracy: 0.0000e+00\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2360 - accuracy: 0.1667 - val_loss: 0.3644 - val_accuracy: 0.0000e+00\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2358 - accuracy: 0.1667 - val_loss: 0.3645 - val_accuracy: 0.0000e+00\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2357 - accuracy: 0.1667 - val_loss: 0.3646 - val_accuracy: 0.0000e+00\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2355 - accuracy: 0.1667 - val_loss: 0.3647 - val_accuracy: 0.0000e+00\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2353 - accuracy: 0.1667 - val_loss: 0.3648 - val_accuracy: 0.0000e+00\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.2352 - accuracy: 0.1667 - val_loss: 0.3648 - val_accuracy: 0.0000e+00\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2350 - accuracy: 0.1667 - val_loss: 0.3649 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:5 out of the last 629 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000022DEC4A2790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-31-8784861b343f>:134: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "  tprs.append(interp(mean_fpr, fpr, tpr))\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:806: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Epoch 1/200\n",
      "1/1 [==============================] - 0s 497ms/step - loss: 0.6967 - accuracy: 0.0556 - val_loss: 0.6873 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6931 - accuracy: 0.0556 - val_loss: 0.6844 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6893 - accuracy: 0.0556 - val_loss: 0.6813 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6854 - accuracy: 0.0556 - val_loss: 0.6782 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6815 - accuracy: 0.1111 - val_loss: 0.6750 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6775 - accuracy: 0.1111 - val_loss: 0.6718 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6736 - accuracy: 0.1111 - val_loss: 0.6688 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6697 - accuracy: 0.1111 - val_loss: 0.6658 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.6659 - accuracy: 0.0556 - val_loss: 0.6628 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6620 - accuracy: 0.0556 - val_loss: 0.6599 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6582 - accuracy: 0.0556 - val_loss: 0.6569 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6543 - accuracy: 0.0556 - val_loss: 0.6539 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.6504 - accuracy: 0.0556 - val_loss: 0.6508 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.6465 - accuracy: 0.0556 - val_loss: 0.6477 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6426 - accuracy: 0.0556 - val_loss: 0.6446 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6387 - accuracy: 0.0556 - val_loss: 0.6413 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6347 - accuracy: 0.1111 - val_loss: 0.6381 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.6306 - accuracy: 0.0556 - val_loss: 0.6348 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6265 - accuracy: 0.0556 - val_loss: 0.6315 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6223 - accuracy: 0.0556 - val_loss: 0.6281 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6180 - accuracy: 0.0556 - val_loss: 0.6247 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.6137 - accuracy: 0.0556 - val_loss: 0.6213 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.6094 - accuracy: 0.0556 - val_loss: 0.6179 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.6051 - accuracy: 0.0556 - val_loss: 0.6145 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6006 - accuracy: 0.0556 - val_loss: 0.6110 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.5960 - accuracy: 0.0556 - val_loss: 0.6074 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5914 - accuracy: 0.0556 - val_loss: 0.6038 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5866 - accuracy: 0.0556 - val_loss: 0.6001 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5817 - accuracy: 0.0556 - val_loss: 0.5963 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5768 - accuracy: 0.0556 - val_loss: 0.5925 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5717 - accuracy: 0.0556 - val_loss: 0.5886 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.5666 - accuracy: 0.0556 - val_loss: 0.5846 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5613 - accuracy: 0.0556 - val_loss: 0.5805 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.5560 - accuracy: 0.0556 - val_loss: 0.5764 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5505 - accuracy: 0.0556 - val_loss: 0.5722 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5450 - accuracy: 0.0556 - val_loss: 0.5679 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.5393 - accuracy: 0.0556 - val_loss: 0.5636 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5336 - accuracy: 0.0556 - val_loss: 0.5592 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5277 - accuracy: 0.0556 - val_loss: 0.5547 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.5218 - accuracy: 0.0556 - val_loss: 0.5501 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5158 - accuracy: 0.0556 - val_loss: 0.5455 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5096 - accuracy: 0.0556 - val_loss: 0.5408 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.5034 - accuracy: 0.0556 - val_loss: 0.5360 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4971 - accuracy: 0.0556 - val_loss: 0.5312 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.4907 - accuracy: 0.0556 - val_loss: 0.5263 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.4841 - accuracy: 0.0556 - val_loss: 0.5213 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4775 - accuracy: 0.0556 - val_loss: 0.5163 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4708 - accuracy: 0.0556 - val_loss: 0.5112 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4641 - accuracy: 0.0556 - val_loss: 0.5060 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4572 - accuracy: 0.0556 - val_loss: 0.5008 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4504 - accuracy: 0.0556 - val_loss: 0.4955 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.4435 - accuracy: 0.0556 - val_loss: 0.4903 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4366 - accuracy: 0.0556 - val_loss: 0.4850 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4297 - accuracy: 0.0556 - val_loss: 0.4797 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4228 - accuracy: 0.0556 - val_loss: 0.4744 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4158 - accuracy: 0.0556 - val_loss: 0.4691 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4088 - accuracy: 0.11 - 0s 26ms/step - loss: 0.4088 - accuracy: 0.1111 - val_loss: 0.4638 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4019 - accuracy: 0.1111 - val_loss: 0.4586 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3949 - accuracy: 0.1111 - val_loss: 0.4533 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.3881 - accuracy: 0.1111 - val_loss: 0.4481 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3812 - accuracy: 0.1111 - val_loss: 0.4430 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3745 - accuracy: 0.1111 - val_loss: 0.4380 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3679 - accuracy: 0.1111 - val_loss: 0.4330 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3614 - accuracy: 0.1111 - val_loss: 0.4281 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3550 - accuracy: 0.1111 - val_loss: 0.4233 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3487 - accuracy: 0.1111 - val_loss: 0.4185 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3426 - accuracy: 0.1111 - val_loss: 0.4139 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3366 - accuracy: 0.1111 - val_loss: 0.4095 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3308 - accuracy: 0.1111 - val_loss: 0.4052 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3252 - accuracy: 0.1111 - val_loss: 0.4010 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3197 - accuracy: 0.1111 - val_loss: 0.3969 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3144 - accuracy: 0.1111 - val_loss: 0.3931 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3093 - accuracy: 0.1111 - val_loss: 0.3893 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3043 - accuracy: 0.1111 - val_loss: 0.3857 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2996 - accuracy: 0.1111 - val_loss: 0.3823 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2950 - accuracy: 0.1111 - val_loss: 0.3790 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2906 - accuracy: 0.1111 - val_loss: 0.3759 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.2865 - accuracy: 0.1111 - val_loss: 0.3729 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2825 - accuracy: 0.0556 - val_loss: 0.3700 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2786 - accuracy: 0.0556 - val_loss: 0.3673 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2750 - accuracy: 0.0556 - val_loss: 0.3648 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2715 - accuracy: 0.0556 - val_loss: 0.3624 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.2682 - accuracy: 0.0556 - val_loss: 0.3601 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.2651 - accuracy: 0.0556 - val_loss: 0.3580 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.2622 - accuracy: 0.0556 - val_loss: 0.3560 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.2593 - accuracy: 0.0556 - val_loss: 0.3541 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.2567 - accuracy: 0.0556 - val_loss: 0.3523 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.2542 - accuracy: 0.0556 - val_loss: 0.3507 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.2518 - accuracy: 0.0556 - val_loss: 0.3491 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.2496 - accuracy: 0.0556 - val_loss: 0.3477 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.2474 - accuracy: 0.0556 - val_loss: 0.3464 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.2454 - accuracy: 0.0556 - val_loss: 0.3451 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2435 - accuracy: 0.0556 - val_loss: 0.3440 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2418 - accuracy: 0.0556 - val_loss: 0.3429 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2401 - accuracy: 0.0556 - val_loss: 0.3419 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2385 - accuracy: 0.0556 - val_loss: 0.3410 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2370 - accuracy: 0.0556 - val_loss: 0.3402 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2356 - accuracy: 0.0556 - val_loss: 0.3394 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2343 - accuracy: 0.0556 - val_loss: 0.3387 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2331 - accuracy: 0.0556 - val_loss: 0.3380 - val_accuracy: 0.0000e+00\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2319 - accuracy: 0.0556 - val_loss: 0.3374 - val_accuracy: 0.0000e+00\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2308 - accuracy: 0.0556 - val_loss: 0.3369 - val_accuracy: 0.0000e+00\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2298 - accuracy: 0.0556 - val_loss: 0.3364 - val_accuracy: 0.0000e+00\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2288 - accuracy: 0.0556 - val_loss: 0.3359 - val_accuracy: 0.0000e+00\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2279 - accuracy: 0.0556 - val_loss: 0.3355 - val_accuracy: 0.0000e+00\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2270 - accuracy: 0.0556 - val_loss: 0.3351 - val_accuracy: 0.0000e+00\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2262 - accuracy: 0.0556 - val_loss: 0.3348 - val_accuracy: 0.0000e+00\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2254 - accuracy: 0.0556 - val_loss: 0.3345 - val_accuracy: 0.0000e+00\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2247 - accuracy: 0.0556 - val_loss: 0.3342 - val_accuracy: 0.0000e+00\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2240 - accuracy: 0.05 - 0s 26ms/step - loss: 0.2240 - accuracy: 0.0556 - val_loss: 0.3340 - val_accuracy: 0.0000e+00\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2234 - accuracy: 0.0556 - val_loss: 0.3338 - val_accuracy: 0.0000e+00\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2228 - accuracy: 0.0556 - val_loss: 0.3336 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.2222 - accuracy: 0.0556 - val_loss: 0.3334 - val_accuracy: 0.0000e+00\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2216 - accuracy: 0.0556 - val_loss: 0.3333 - val_accuracy: 0.0000e+00\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2211 - accuracy: 0.0556 - val_loss: 0.3331 - val_accuracy: 0.0000e+00\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2206 - accuracy: 0.0556 - val_loss: 0.3330 - val_accuracy: 0.0000e+00\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2201 - accuracy: 0.0556 - val_loss: 0.3329 - val_accuracy: 0.0000e+00\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2197 - accuracy: 0.0556 - val_loss: 0.3329 - val_accuracy: 0.0000e+00\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2193 - accuracy: 0.0556 - val_loss: 0.3328 - val_accuracy: 0.0000e+00\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2189 - accuracy: 0.0556 - val_loss: 0.3328 - val_accuracy: 0.0000e+00\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2185 - accuracy: 0.0556 - val_loss: 0.3327 - val_accuracy: 0.0000e+00\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2181 - accuracy: 0.0556 - val_loss: 0.3327 - val_accuracy: 0.0000e+00\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2178 - accuracy: 0.0556 - val_loss: 0.3327 - val_accuracy: 0.0000e+00\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2175 - accuracy: 0.0556 - val_loss: 0.3327 - val_accuracy: 0.0000e+00\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.2171 - accuracy: 0.0556 - val_loss: 0.3327 - val_accuracy: 0.0000e+00\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2168 - accuracy: 0.0556 - val_loss: 0.3327 - val_accuracy: 0.0000e+00\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2165 - accuracy: 0.0556 - val_loss: 0.3327 - val_accuracy: 0.0000e+00\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2163 - accuracy: 0.0556 - val_loss: 0.3327 - val_accuracy: 0.0000e+00\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2160 - accuracy: 0.0556 - val_loss: 0.3328 - val_accuracy: 0.0000e+00\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2158 - accuracy: 0.0556 - val_loss: 0.3328 - val_accuracy: 0.0000e+00\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2155 - accuracy: 0.0556 - val_loss: 0.3329 - val_accuracy: 0.0000e+00\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2153 - accuracy: 0.0556 - val_loss: 0.3329 - val_accuracy: 0.0000e+00\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2151 - accuracy: 0.0556 - val_loss: 0.3330 - val_accuracy: 0.0000e+00\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2148 - accuracy: 0.0556 - val_loss: 0.3330 - val_accuracy: 0.0000e+00\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2146 - accuracy: 0.0556 - val_loss: 0.3331 - val_accuracy: 0.0000e+00\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.2144 - accuracy: 0.0556 - val_loss: 0.3331 - val_accuracy: 0.0000e+00\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2142 - accuracy: 0.0556 - val_loss: 0.3332 - val_accuracy: 0.0000e+00\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2141 - accuracy: 0.0556 - val_loss: 0.3333 - val_accuracy: 0.0000e+00\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2139 - accuracy: 0.0556 - val_loss: 0.3334 - val_accuracy: 0.0000e+00\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2137 - accuracy: 0.0556 - val_loss: 0.3334 - val_accuracy: 0.0000e+00\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2135 - accuracy: 0.0556 - val_loss: 0.3335 - val_accuracy: 0.0000e+00\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2134 - accuracy: 0.0556 - val_loss: 0.3336 - val_accuracy: 0.0000e+00\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2132 - accuracy: 0.0556 - val_loss: 0.3337 - val_accuracy: 0.0000e+00\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2131 - accuracy: 0.0556 - val_loss: 0.3338 - val_accuracy: 0.0000e+00\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2129 - accuracy: 0.0556 - val_loss: 0.3339 - val_accuracy: 0.0000e+00\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2128 - accuracy: 0.0556 - val_loss: 0.3340 - val_accuracy: 0.0000e+00\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2126 - accuracy: 0.0556 - val_loss: 0.3340 - val_accuracy: 0.0000e+00\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2125 - accuracy: 0.0556 - val_loss: 0.3341 - val_accuracy: 0.0000e+00\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2124 - accuracy: 0.0556 - val_loss: 0.3342 - val_accuracy: 0.0000e+00\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2122 - accuracy: 0.0556 - val_loss: 0.3343 - val_accuracy: 0.0000e+00\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2121 - accuracy: 0.0556 - val_loss: 0.3344 - val_accuracy: 0.0000e+00\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2120 - accuracy: 0.0556 - val_loss: 0.3345 - val_accuracy: 0.0000e+00\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2119 - accuracy: 0.0556 - val_loss: 0.3346 - val_accuracy: 0.0000e+00\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2118 - accuracy: 0.0556 - val_loss: 0.3347 - val_accuracy: 0.0000e+00\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2117 - accuracy: 0.0556 - val_loss: 0.3348 - val_accuracy: 0.0000e+00\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2116 - accuracy: 0.0556 - val_loss: 0.3349 - val_accuracy: 0.0000e+00\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2114 - accuracy: 0.0556 - val_loss: 0.3350 - val_accuracy: 0.0000e+00\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2113 - accuracy: 0.0556 - val_loss: 0.3351 - val_accuracy: 0.0000e+00\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.2112 - accuracy: 0.0556 - val_loss: 0.3352 - val_accuracy: 0.0000e+00\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2111 - accuracy: 0.0556 - val_loss: 0.3353 - val_accuracy: 0.0000e+00\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2110 - accuracy: 0.0556 - val_loss: 0.3354 - val_accuracy: 0.0000e+00\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2110 - accuracy: 0.0556 - val_loss: 0.3355 - val_accuracy: 0.0000e+00\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2109 - accuracy: 0.0556 - val_loss: 0.3356 - val_accuracy: 0.0000e+00\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2108 - accuracy: 0.0556 - val_loss: 0.3357 - val_accuracy: 0.0000e+00\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2107 - accuracy: 0.0556 - val_loss: 0.3358 - val_accuracy: 0.0000e+00\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.2106 - accuracy: 0.0556 - val_loss: 0.3359 - val_accuracy: 0.0000e+00\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2105 - accuracy: 0.0556 - val_loss: 0.3360 - val_accuracy: 0.0000e+00\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2104 - accuracy: 0.0556 - val_loss: 0.3361 - val_accuracy: 0.0000e+00\n",
      "Epoch 169/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2104 - accuracy: 0.0556 - val_loss: 0.3361 - val_accuracy: 0.0000e+00\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2103 - accuracy: 0.0556 - val_loss: 0.3362 - val_accuracy: 0.0000e+00\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2102 - accuracy: 0.0556 - val_loss: 0.3363 - val_accuracy: 0.0000e+00\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2101 - accuracy: 0.0556 - val_loss: 0.3364 - val_accuracy: 0.0000e+00\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2101 - accuracy: 0.0556 - val_loss: 0.3365 - val_accuracy: 0.0000e+00\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2100 - accuracy: 0.0556 - val_loss: 0.3366 - val_accuracy: 0.0000e+00\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2099 - accuracy: 0.0556 - val_loss: 0.3367 - val_accuracy: 0.0000e+00\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2098 - accuracy: 0.0556 - val_loss: 0.3368 - val_accuracy: 0.0000e+00\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2098 - accuracy: 0.0556 - val_loss: 0.3369 - val_accuracy: 0.0000e+00\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2097 - accuracy: 0.0556 - val_loss: 0.3370 - val_accuracy: 0.0000e+00\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2096 - accuracy: 0.0556 - val_loss: 0.3371 - val_accuracy: 0.0000e+00\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2096 - accuracy: 0.0556 - val_loss: 0.3372 - val_accuracy: 0.0000e+00\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2095 - accuracy: 0.0556 - val_loss: 0.3373 - val_accuracy: 0.0000e+00\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2094 - accuracy: 0.0556 - val_loss: 0.3373 - val_accuracy: 0.0000e+00\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2094 - accuracy: 0.0556 - val_loss: 0.3374 - val_accuracy: 0.0000e+00\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2093 - accuracy: 0.0556 - val_loss: 0.3375 - val_accuracy: 0.0000e+00\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2093 - accuracy: 0.0556 - val_loss: 0.3376 - val_accuracy: 0.0000e+00\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2092 - accuracy: 0.0556 - val_loss: 0.3377 - val_accuracy: 0.0000e+00\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2091 - accuracy: 0.0556 - val_loss: 0.3378 - val_accuracy: 0.0000e+00\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2091 - accuracy: 0.0556 - val_loss: 0.3379 - val_accuracy: 0.0000e+00\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.2090 - accuracy: 0.0556 - val_loss: 0.3380 - val_accuracy: 0.0000e+00\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2090 - accuracy: 0.0556 - val_loss: 0.3380 - val_accuracy: 0.0000e+00\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.2089 - accuracy: 0.0556 - val_loss: 0.3381 - val_accuracy: 0.0000e+00\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2089 - accuracy: 0.0556 - val_loss: 0.3382 - val_accuracy: 0.0000e+00\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.2088 - accuracy: 0.0556 - val_loss: 0.3383 - val_accuracy: 0.0000e+00\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2088 - accuracy: 0.0556 - val_loss: 0.3384 - val_accuracy: 0.0000e+00\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2087 - accuracy: 0.0556 - val_loss: 0.3385 - val_accuracy: 0.0000e+00\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2087 - accuracy: 0.0556 - val_loss: 0.3385 - val_accuracy: 0.0000e+00\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2086 - accuracy: 0.0556 - val_loss: 0.3386 - val_accuracy: 0.0000e+00\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2086 - accuracy: 0.0556 - val_loss: 0.3387 - val_accuracy: 0.0000e+00\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2085 - accuracy: 0.0556 - val_loss: 0.3388 - val_accuracy: 0.0000e+00\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2085 - accuracy: 0.0556 - val_loss: 0.3389 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:6 out of the last 630 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000022DEC449E50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[5, 0]\n",
      "[0, 0]\n",
      "both b and c are zero\n",
      "[5, 0]\n",
      "[0, 0]\n",
      "both b and c are zero\n",
      "[1, 0]\n",
      "[4, 0]\n",
      "[1, 0]\n",
      "[4, 0]\n",
      "[4, 0]\n",
      "[1, 0]\n",
      "[4, 0]\n",
      "[0, 1]\n",
      "both b and c are zero\n",
      "[3, 1]\n",
      "[1, 0]\n",
      "[3, 1]\n",
      "[1, 0]\n",
      "[4, 1]\n",
      "[0, 0]\n",
      "[5, 0]\n",
      "[0, 0]\n",
      "both b and c are zero\n",
      "[4, 0]\n",
      "[1, 0]\n",
      "[4, 0]\n",
      "[1, 0]\n",
      "[3, 0]\n",
      "[1, 1]\n",
      "[3, 0]\n",
      "[1, 1]\n",
      "[3, 0]\n",
      "[1, 1]\n",
      "[3, 0]\n",
      "[1, 1]\n",
      "[5, 0]\n",
      "[0, 0]\n",
      "both b and c are zero\n",
      "[4, 1]\n",
      "[0, 0]\n",
      "[3, 0]\n",
      "[2, 0]\n",
      "[3, 0]\n",
      "[1, 1]\n",
      "[1, 0]\n",
      "[4, 0]\n",
      "[1, 0]\n",
      "[4, 0]\n",
      "[5, 0]\n",
      "[0, 0]\n",
      "both b and c are zero\n",
      "[5, 0]\n",
      "[0, 0]\n",
      "both b and c are zero\n",
      "[3, 0]\n",
      "[2, 0]\n",
      "[3, 0]\n",
      "[2, 0]\n",
      "[5, 0]\n",
      "[0, 0]\n",
      "both b and c are zero\n",
      "[5, 0]\n",
      "[0, 0]\n",
      "both b and c are zero\n",
      "[5, 0]\n",
      "[0, 0]\n",
      "both b and c are zero\n",
      "[5, 0]\n",
      "[0, 0]\n",
      "both b and c are zero\n",
      "[3, 1]\n",
      "[1, 0]\n",
      "[4, 0]\n",
      "[0, 1]\n",
      "both b and c are zero\n",
      "[4, 0]\n",
      "[1, 0]\n",
      "[4, 0]\n",
      "[0, 1]\n",
      "both b and c are zero\n",
      "[4, 0]\n",
      "[1, 0]\n",
      "[3, 1]\n",
      "[1, 0]\n",
      "[3, 0]\n",
      "[1, 1]\n",
      "[3, 0]\n",
      "[1, 1]\n",
      "------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds of MWPM:\n",
      "> Accuracy: 0.9494736842105264 (+- 0.02447101161285989)\n",
      "> F1: 0.6569624819624819(+- 0.14495380449650971)\n",
      "> Time: 0.0458878 (+- 0.004442331372151337)\n",
      "#####################################################################################\n",
      "Average scores for all folds of PLUT:\n",
      "> Accuracy: 0.9150877192982456 (+- 0.013616566955741967)\n",
      "> F1: 0.08080808080808081(+- 0.09979061846649269)\n",
      "> Time: 0.00019940000000000002 (+- 0.00039880000000000004)\n",
      "#####################################################################################\n",
      "Average scores for all folds of NN:\n",
      "> Accuracy: 0.14947368421052631 (+- 0.29894736842105263)\n",
      "> F1: 0.1038132080685272(+- 0.039520699799404056)\n",
      "> Time: 0.07457524 (+- 0.003045547863751283)\n",
      "#####################################################################################\n",
      "> AUC for class : 0.0 (+- 0.0)\n",
      "X^2 for MWPM and NN: 0\n",
      "X^2 for PLUT and NN: 0\n",
      "> AUC for class X00: 0.2 (+- 0.2)\n",
      "X^2 for MWPM and NN: 6.25\n",
      "X^2 for PLUT and NN: 6.25\n",
      "> AUC for class X01: 0.3 (+- 0.3)\n",
      "X^2 for MWPM and NN: 4.0\n",
      "X^2 for PLUT and NN: 0\n",
      "> AUC for class X02: 0.09999999999999998 (+- 0.09999999999999998)\n",
      "X^2 for MWPM and NN: 0.5\n",
      "X^2 for PLUT and NN: 0.5\n",
      "> AUC for class X10: 0.75 (+- 0.25)\n",
      "X^2 for MWPM and NN: 0.0\n",
      "X^2 for PLUT and NN: 0\n",
      "> AUC for class X11: 1.0 (+- 0.0)\n",
      "X^2 for MWPM and NN: 4.0\n",
      "X^2 for PLUT and NN: 4.0\n",
      "> AUC for class X12: 0.475 (+- 0.275)\n",
      "X^2 for MWPM and NN: 4.0\n",
      "X^2 for PLUT and NN: 4.0\n",
      "> AUC for class X20: 0.5 (+- 0.0)\n",
      "X^2 for MWPM and NN: 4.0\n",
      "X^2 for PLUT and NN: 4.0\n",
      "> AUC for class X21: 0.875 (+- 0.125)\n",
      "X^2 for MWPM and NN: 0\n",
      "X^2 for PLUT and NN: 0.0\n",
      "> AUC for class X22: 0.375 (+- 0.125)\n",
      "X^2 for MWPM and NN: 4.5\n",
      "X^2 for PLUT and NN: 4.0\n",
      "> AUC for class Z00: 0.2625 (+- 0.1375)\n",
      "X^2 for MWPM and NN: 6.25\n",
      "X^2 for PLUT and NN: 6.25\n",
      "> AUC for class Z01: 0.2625 (+- 0.1375)\n",
      "X^2 for MWPM and NN: 0\n",
      "X^2 for PLUT and NN: 0\n",
      "> AUC for class Z02: 0.39999999999999997 (+- 0.2)\n",
      "X^2 for MWPM and NN: 4.5\n",
      "X^2 for PLUT and NN: 4.5\n",
      "> AUC for class Z10: 0.9 (+- 0.09999999999999998)\n",
      "X^2 for MWPM and NN: 0\n",
      "X^2 for PLUT and NN: 0\n",
      "> AUC for class Z11: 1.0 (+- 0.0)\n",
      "X^2 for MWPM and NN: 0\n",
      "X^2 for PLUT and NN: 0\n",
      "> AUC for class Z12: 0.65 (+- 0.15000000000000002)\n",
      "X^2 for MWPM and NN: 0.5\n",
      "X^2 for PLUT and NN: 0\n",
      "> AUC for class Z20: 0.775 (+- 0.025000000000000022)\n",
      "X^2 for MWPM and NN: 4.0\n",
      "X^2 for PLUT and NN: 0\n",
      "> AUC for class Z21: 0.8 (+- 0.2)\n",
      "X^2 for MWPM and NN: 4.0\n",
      "X^2 for PLUT and NN: 0.5\n",
      "> AUC for class Z22: 0.2 (+- 0.2)\n",
      "X^2 for MWPM and NN: 4.0\n",
      "X^2 for PLUT and NN: 4.0\n",
      "###################################################################################\n",
      "TOTAL F1 NN: [0.05555555555555556, 0.14893617021276592, 0.06060606060606061, 0.1111111111111111, 0.14285714285714285]\n",
      "TOTAL F1 PLUT: [0.18181818181818182, 0.0, 0.0, 0.22222222222222224, 0.0]\n",
      "TOTAL F1 MWPM: [0.875, 0.4444444444444444, 0.5714285714285715, 0.7272727272727272, 0.6666666666666666]\n",
      "TOTAL ACC NN: [0.0, 0.0, 0.0, 0.0, 0.7473684210526316]\n",
      "TOTAL ACC PLUT: [0.9210526315789473, 0.9035087719298245, 0.9298245614035089, 0.9263157894736842, 0.8947368421052632]\n",
      "TOTAL ACC MWPM: [0.9824561403508771, 0.912280701754386, 0.9473684210526315, 0.968421052631579, 0.9368421052631579]\n",
      "TOTAL TIME NN: [0.071491, 0.0748421, 0.0747669, 0.0799691, 0.0718071]\n",
      "TOTAL TIME PLUT: [0.0, 0.0, 0.0, 0.0, 0.000997]\n",
      "TOTAL TIME MWPM: [0.0450112, 0.0468298, 0.0458784, 0.0388951, 0.052824499999999996]\n",
      "------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-31-8784861b343f>:134: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "  tprs.append(interp(mean_fpr, fpr, tpr))\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:806: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "inputs = x_d3.copy()\n",
    "inputs = np.array(inputs)\n",
    "inputs = np.stack(inputs)\n",
    "targets = y_d3.copy()\n",
    "targets = np.array([np.array(xi) for xi in targets])\n",
    "\n",
    "n_classes = 19\n",
    "\n",
    "acc_per_fold = []\n",
    "f1_per_fold = []\n",
    "time_per_fold = []\n",
    "\n",
    "\n",
    "acc_per_fold_plut = []\n",
    "f1_per_fold_plut = []\n",
    "time_per_fold_plut = []\n",
    "\n",
    "acc_per_fold_mwpm = []\n",
    "f1_per_fold_mwpm = []\n",
    "time_per_fold_mwpm = []\n",
    "\n",
    "\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "aucs_classes = {}\n",
    "for i in mlb_d3.classes_:\n",
    "    aucs_classes[i] = []\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "#for i in range(5):\n",
    "    \n",
    "    # K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "for i, (train, test) in enumerate(kfold.split(inputs, targets)):\n",
    "    i_train = inputs[train].copy()\n",
    "    i_test = inputs[test].copy()\n",
    "    x_test_d3 = i_test[:,:2]\n",
    "    inputs_train = i_train[:,3:]\n",
    "    inputs_test = i_test[:,3:]\n",
    "    ####################################################################################################\n",
    "    #test MWPM decoder for this fold\n",
    "    #labels = targets[train], features = inputs[train]\n",
    "    #x_test_d3 = translate_to_graph(testData_d3_MWPM, targets[test], mlb_d3)\n",
    "    decoding_d3, time_mwpm = do_new_decoding(x_test_d3, 3, .03)\n",
    "    decoding_d3['combine'] = decoding_d3[[0, 1]].values.tolist()\n",
    "    decoding_d3['combine'].apply(lambda x: x[0].extend(x[1]))\n",
    "    decoding_d3 = np.array(decoding_d3[0])\n",
    "\n",
    "    time_per_fold_mwpm.append(time_mwpm)\n",
    "\n",
    "    pred_mwpm = mlb_d3.transform(decoding_d3)\n",
    "    if fold_no < 5:\n",
    "        acc = partial_accuracy(targets[test], pred_mwpm)\n",
    "    else:\n",
    "        acc, contingency_mwpm = partial_accuracy_and_contingency(targets[test], pred_mwpm, mlb_d3)\n",
    "        \n",
    "    acc_per_fold_mwpm.append(acc)\n",
    "    f1_per_fold_mwpm.append(f1_score(targets[test], pred_mwpm, average='micro'))\n",
    "\n",
    "\n",
    "    #####################################################################################################\n",
    "    #test the plut decoder for this fold\n",
    "\n",
    "    lookup_d3 = lookup_decoder(3)\n",
    "\n",
    "    lookup_d3 = train_plut(lookup_d3, inputs_train, targets[train])\n",
    "\n",
    "    start = time.time_ns()\n",
    "    pred_plut_d3 = test_plut(lookup_d3, inputs_test)\n",
    "    end = time.time_ns() \n",
    "    time_per_fold_plut.append((end - start)/ (10 ** 9))\n",
    "    \n",
    "    if fold_no < 5:\n",
    "        acc = partial_accuracy(targets[test], pred_plut_d3)\n",
    "    else:\n",
    "        acc, contingency_plut = partial_accuracy_and_contingency(targets[test], pred_plut_d3, mlb_d3)\n",
    "\n",
    "    acc_per_fold_plut.append(acc)\n",
    "    f1_per_fold_plut.append(f1_score(targets[test], pred_plut_d3, average='micro'))\n",
    "\n",
    "    #####################################################################################################\n",
    "    #Test the NN decoder for this fold\n",
    "\n",
    "    model = compile_FFNN_model_DepthThree(3)\n",
    "    # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    # Fit data to model\n",
    "    \n",
    "    inputs_train = np.asarray(inputs_train).astype(np.int) #CHANGE (added)\n",
    "    targets[train] = np.asarray(targets[train]).astype(np.int) #CHANGE (added)\n",
    "    \n",
    "    history = model.fit(\n",
    "        inputs_train, targets[train],\n",
    "        validation_split=.2,\n",
    "        epochs=200,\n",
    "        verbose=1)\n",
    "\n",
    "   # Generate generalization metrics\n",
    "    inputs_test = np.asarray(inputs_test).astype(np.int) #CHANGE (added)\n",
    "    targets[test] = np.asarray(targets[test]).astype(np.int) #CHANGE (added)\n",
    "    \n",
    "    scores = model.evaluate(inputs_test, targets[test], verbose=0)\n",
    "\n",
    "    #get the time to predicting test\n",
    "    start = time.time_ns()\n",
    "    predictions_d3 = model.predict(inputs_test) #change here\n",
    "    end = time.time_ns()\n",
    "    time_per_fold.append((end - start)/ (10 ** 9))\n",
    "\n",
    "    #threshold based on previous tests with train_test_split in hyperparameter tuning\n",
    "    pred=predictions_d3.copy() #change here\n",
    "    pred[pred>=.1]=1 \n",
    "    pred[pred<.1]=0\n",
    "    \n",
    "    if fold_no <5:\n",
    "        acc = scores[1]\n",
    "    else:\n",
    "        acc, contingency_nn = partial_accuracy_and_contingency(targets[test], pred, mlb_d3)\n",
    "\n",
    "    acc_per_fold.append(acc)\n",
    "    f1_per_fold.append(f1_score(targets[test], pred, average='micro'))\n",
    "\n",
    "    #####################################################################################################\n",
    "    #comput ROC AUC for classes and the mircoaverage\n",
    "    fpr, tpr, thresholds = roc_curve(targets[test].ravel(), predictions_d3.ravel())\n",
    "    tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "    tprs[-1][0] = 0.0\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "    plt.plot(fpr, tpr, lw=1, alpha=0.3, label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
    "\n",
    "    #get the AUCs of each class, used to get average AUC of each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(targets[test][:, i], predictions_d3[:, i]) #change here\n",
    "        aucs_classes[mlb_d3.classes_[i]].append(auc(fpr[i], tpr[i]))\n",
    "        \n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n",
    "        \n",
    "#########################################################################################################\n",
    "#compute McNemar's statistic on results of last fold\n",
    "\n",
    "mcnemar_results_mwpm = {}\n",
    "mcnemar_results_plut = {}\n",
    "\n",
    "for class_ in mlb_d3.classes_:\n",
    "    \n",
    "    #compute the x^2 for NN and MWPM\n",
    "    mcnemar_results_mwpm[class_]=contingency_table_and_t(contingency_nn[class_], contingency_mwpm[class_])[1]    \n",
    "    #comput the x^2 for NN and PLUT\n",
    "    mcnemar_results_plut[class_] = contingency_table_and_t(contingency_nn[class_], contingency_plut[class_])[1]\n",
    "        \n",
    "    \n",
    "############print mean and stdev of AUC of each class#####################      \n",
    "    \n",
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds of MWPM:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold_mwpm)} (+- {np.std(acc_per_fold_mwpm)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold_mwpm)}(+- {np.std(f1_per_fold_mwpm)})')\n",
    "print(f'> Time: {np.mean(time_per_fold_mwpm)} (+- {np.std(time_per_fold_mwpm)})')\n",
    "print(\"#####################################################################################\")\n",
    "print('Average scores for all folds of PLUT:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold_plut)} (+- {np.std(acc_per_fold_plut)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold_plut)}(+- {np.std(f1_per_fold_plut)})')\n",
    "print(f'> Time: {np.mean(time_per_fold_plut)} (+- {np.std(time_per_fold_plut)})')\n",
    "print(\"#####################################################################################\")\n",
    "print('Average scores for all folds of NN:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold)}(+- {np.std(f1_per_fold)})')\n",
    "print(f'> Time: {np.mean(time_per_fold)} (+- {np.std(time_per_fold)})')\n",
    "print(\"#####################################################################################\")\n",
    "for key in aucs_classes:\n",
    "    cleanedList = [x for x in aucs_classes[key] if str(x) != 'nan']\n",
    "    print(f'> AUC for class {key}: {np.mean(cleanedList)} (+- {np.std(cleanedList)})') #this has to be a for loop\n",
    "    print(\"X^2 for MWPM and NN: \" + str(mcnemar_results_mwpm[key]))  \n",
    "    print(\"X^2 for PLUT and NN: \" + str(mcnemar_results_plut[key]))\n",
    "print(\"###################################################################################\")\n",
    "print(\"TOTAL F1 NN: \" + str(f1_per_fold))\n",
    "print(\"TOTAL F1 PLUT: \" + str(f1_per_fold_plut))\n",
    "print(\"TOTAL F1 MWPM: \" + str(f1_per_fold_mwpm))\n",
    "print(\"TOTAL ACC NN: \" + str(acc_per_fold))\n",
    "print(\"TOTAL ACC PLUT: \" + str(acc_per_fold_plut))\n",
    "print(\"TOTAL ACC MWPM: \" + str(acc_per_fold_mwpm))\n",
    "print(\"TOTAL TIME NN: \" + str(time_per_fold))\n",
    "print(\"TOTAL TIME PLUT: \" + str(time_per_fold_plut))\n",
    "print(\"TOTAL TIME MWPM: \" + str(time_per_fold_mwpm))\n",
    "print('------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "         label='Chance', alpha=.8)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "         label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "         lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                 label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "plt.xlim([-0.01, 1.01])\n",
    "plt.ylim([-0.01, 1.01])\n",
    "plt.xlabel('False Positive Rate',fontsize=18)\n",
    "plt.ylabel('True Positive Rate',fontsize=18)\n",
    "plt.title('Cross-Validation ROC of Depth 3 NN',fontsize=18)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "\n",
    "model = compile_FFNN_model_DepthThree(3)\n",
    "print(\"Fit model on training data\")\n",
    "history = model.fit(\n",
    "    x=x_train_d3.values,\n",
    "    y=Y_train_d3,\n",
    "    validation_split=.25,\n",
    "    epochs=200\n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Make a quick epoch vs. loss plot\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.grid()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Depth 3 Model Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (MSE)')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Make a quick epoch vs. accuracy plot\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.grid()\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Depth 3 Model Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "start = time.time()\n",
    "predictions_d3 = model.predict(x_test_d3.values)\n",
    "end = time.time()\n",
    "print(\"Time: \" + str(end - start))\n",
    "# predict\n",
    "\n",
    "thresholds=[0.1, .2, .3, .4, .5, .6, .7, .8, .9]\n",
    "for val in thresholds:\n",
    "    pred=predictions_d3.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "\n",
    "    precision = precision_score(Y_test_d3, pred, average='micro')\n",
    "    recall = recall_score(Y_test_d3, pred, average='micro')\n",
    "    f1 = f1_score(Y_test_d3, pred, average='micro')\n",
    "   \n",
    "    print(\"Micro-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "    print(\"Accuracy = \",accuracy_score(Y_test_d3, pred))\n",
    "    print(\"Partial Accuracy = \",partial_accuracy(Y_test_d3, pred))\n",
    "    print(\"\\n\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Epoch 1/800\n",
      "53/53 [==============================] - 1s 5ms/step - loss: 0.6582 - accuracy: 0.0060 - val_loss: 0.6105 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.4293 - accuracy: 0.0060 - val_loss: 0.2688 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1957 - accuracy: 0.0060 - val_loss: 0.2502 - val_accuracy: 0.0306\n",
      "Epoch 4/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1884 - accuracy: 0.0246 - val_loss: 0.2498 - val_accuracy: 0.0036\n",
      "Epoch 5/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1874 - accuracy: 0.0186 - val_loss: 0.2504 - val_accuracy: 0.0198\n",
      "Epoch 6/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1871 - accuracy: 0.0120 - val_loss: 0.2504 - val_accuracy: 0.0559\n",
      "Epoch 7/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1869 - accuracy: 0.0444 - val_loss: 0.2509 - val_accuracy: 0.0559\n",
      "Epoch 8/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1869 - accuracy: 0.0505 - val_loss: 0.2503 - val_accuracy: 0.0523\n",
      "Epoch 9/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1868 - accuracy: 0.0408 - val_loss: 0.2502 - val_accuracy: 0.0342\n",
      "Epoch 10/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1867 - accuracy: 0.0282 - val_loss: 0.2499 - val_accuracy: 0.0414\n",
      "Epoch 11/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1866 - accuracy: 0.0186 - val_loss: 0.2500 - val_accuracy: 0.0306\n",
      "Epoch 12/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1866 - accuracy: 0.0318 - val_loss: 0.2503 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1865 - accuracy: 0.0096 - val_loss: 0.2498 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1865 - accuracy: 0.0114 - val_loss: 0.2502 - val_accuracy: 0.0342\n",
      "Epoch 15/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1864 - accuracy: 0.0198 - val_loss: 0.2499 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1864 - accuracy: 0.0132 - val_loss: 0.2497 - val_accuracy: 0.0396\n",
      "Epoch 17/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1863 - accuracy: 0.0210 - val_loss: 0.2497 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1863 - accuracy: 0.0210 - val_loss: 0.2498 - val_accuracy: 0.0252\n",
      "Epoch 19/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1863 - accuracy: 0.0222 - val_loss: 0.2493 - val_accuracy: 0.0306\n",
      "Epoch 20/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1862 - accuracy: 0.0288 - val_loss: 0.2488 - val_accuracy: 0.0685\n",
      "Epoch 21/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1861 - accuracy: 0.0306 - val_loss: 0.2493 - val_accuracy: 0.0486\n",
      "Epoch 22/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1861 - accuracy: 0.0240 - val_loss: 0.2491 - val_accuracy: 0.0559\n",
      "Epoch 23/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1860 - accuracy: 0.0492 - val_loss: 0.2492 - val_accuracy: 0.0324\n",
      "Epoch 24/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1860 - accuracy: 0.0192 - val_loss: 0.2491 - val_accuracy: 0.0270\n",
      "Epoch 25/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1859 - accuracy: 0.0204 - val_loss: 0.2495 - val_accuracy: 0.0631\n",
      "Epoch 26/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1859 - accuracy: 0.0420 - val_loss: 0.2488 - val_accuracy: 0.0414\n",
      "Epoch 27/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1859 - accuracy: 0.0390 - val_loss: 0.2493 - val_accuracy: 0.0577\n",
      "Epoch 28/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1858 - accuracy: 0.0438 - val_loss: 0.2494 - val_accuracy: 0.0306\n",
      "Epoch 29/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1857 - accuracy: 0.0252 - val_loss: 0.2490 - val_accuracy: 0.0234\n",
      "Epoch 30/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1857 - accuracy: 0.0366 - val_loss: 0.2487 - val_accuracy: 0.0234\n",
      "Epoch 31/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1856 - accuracy: 0.0198 - val_loss: 0.2486 - val_accuracy: 0.0432\n",
      "Epoch 32/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1856 - accuracy: 0.0396 - val_loss: 0.2486 - val_accuracy: 0.0306\n",
      "Epoch 33/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1856 - accuracy: 0.0276 - val_loss: 0.2488 - val_accuracy: 0.0270\n",
      "Epoch 34/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1855 - accuracy: 0.0264 - val_loss: 0.2486 - val_accuracy: 0.0577\n",
      "Epoch 35/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1854 - accuracy: 0.0384 - val_loss: 0.2481 - val_accuracy: 0.0342\n",
      "Epoch 36/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1854 - accuracy: 0.0432 - val_loss: 0.2484 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1853 - accuracy: 0.0132 - val_loss: 0.2473 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1852 - accuracy: 0.0162 - val_loss: 0.2476 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1852 - accuracy: 0.0108 - val_loss: 0.2480 - val_accuracy: 0.0775\n",
      "Epoch 40/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1851 - accuracy: 0.0462 - val_loss: 0.2480 - val_accuracy: 0.0288\n",
      "Epoch 41/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1851 - accuracy: 0.0390 - val_loss: 0.2478 - val_accuracy: 0.0018\n",
      "Epoch 42/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1850 - accuracy: 0.0318 - val_loss: 0.2481 - val_accuracy: 0.0360\n",
      "Epoch 43/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1849 - accuracy: 0.0366 - val_loss: 0.2481 - val_accuracy: 0.0793\n",
      "Epoch 44/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1849 - accuracy: 0.0781 - val_loss: 0.2481 - val_accuracy: 0.0559\n",
      "Epoch 45/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1848 - accuracy: 0.0456 - val_loss: 0.2476 - val_accuracy: 0.0523\n",
      "Epoch 46/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1847 - accuracy: 0.0523 - val_loss: 0.2477 - val_accuracy: 0.0018\n",
      "Epoch 47/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1846 - accuracy: 0.0288 - val_loss: 0.2471 - val_accuracy: 0.0360\n",
      "Epoch 48/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1846 - accuracy: 0.0318 - val_loss: 0.2463 - val_accuracy: 0.0523\n",
      "Epoch 49/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1845 - accuracy: 0.0444 - val_loss: 0.2466 - val_accuracy: 0.0396\n",
      "Epoch 50/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1844 - accuracy: 0.0492 - val_loss: 0.2460 - val_accuracy: 0.0505\n",
      "Epoch 51/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1844 - accuracy: 0.0456 - val_loss: 0.2464 - val_accuracy: 0.0054\n",
      "Epoch 52/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1843 - accuracy: 0.0462 - val_loss: 0.2463 - val_accuracy: 0.0450\n",
      "Epoch 53/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1842 - accuracy: 0.0402 - val_loss: 0.2462 - val_accuracy: 0.0414\n",
      "Epoch 54/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1841 - accuracy: 0.0384 - val_loss: 0.2465 - val_accuracy: 0.0126\n",
      "Epoch 55/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1841 - accuracy: 0.0210 - val_loss: 0.2466 - val_accuracy: 0.0414\n",
      "Epoch 56/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1839 - accuracy: 0.0577 - val_loss: 0.2468 - val_accuracy: 0.0378\n",
      "Epoch 57/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1838 - accuracy: 0.0444 - val_loss: 0.2464 - val_accuracy: 0.0667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1837 - accuracy: 0.0432 - val_loss: 0.2463 - val_accuracy: 0.0631\n",
      "Epoch 59/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1837 - accuracy: 0.0691 - val_loss: 0.2457 - val_accuracy: 0.0378\n",
      "Epoch 60/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1836 - accuracy: 0.0667 - val_loss: 0.2457 - val_accuracy: 0.0559\n",
      "Epoch 61/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1835 - accuracy: 0.0757 - val_loss: 0.2457 - val_accuracy: 0.0613\n",
      "Epoch 62/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1833 - accuracy: 0.0619 - val_loss: 0.2452 - val_accuracy: 0.0901\n",
      "Epoch 63/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1832 - accuracy: 0.0931 - val_loss: 0.2451 - val_accuracy: 0.0360\n",
      "Epoch 64/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1831 - accuracy: 0.0631 - val_loss: 0.2449 - val_accuracy: 0.0036\n",
      "Epoch 65/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1831 - accuracy: 0.0366 - val_loss: 0.2447 - val_accuracy: 0.0432\n",
      "Epoch 66/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1829 - accuracy: 0.0583 - val_loss: 0.2447 - val_accuracy: 0.0505\n",
      "Epoch 67/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1828 - accuracy: 0.0847 - val_loss: 0.2448 - val_accuracy: 0.0162\n",
      "Epoch 68/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1826 - accuracy: 0.0450 - val_loss: 0.2441 - val_accuracy: 0.0667\n",
      "Epoch 69/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1825 - accuracy: 0.0679 - val_loss: 0.2438 - val_accuracy: 0.0468\n",
      "Epoch 70/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1824 - accuracy: 0.0703 - val_loss: 0.2436 - val_accuracy: 0.0703\n",
      "Epoch 71/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1822 - accuracy: 0.0823 - val_loss: 0.2435 - val_accuracy: 0.0667\n",
      "Epoch 72/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1821 - accuracy: 0.0781 - val_loss: 0.2433 - val_accuracy: 0.0775\n",
      "Epoch 73/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1820 - accuracy: 0.0919 - val_loss: 0.2425 - val_accuracy: 0.0865\n",
      "Epoch 74/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1818 - accuracy: 0.0973 - val_loss: 0.2426 - val_accuracy: 0.0829\n",
      "Epoch 75/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1816 - accuracy: 0.0775 - val_loss: 0.2431 - val_accuracy: 0.0685\n",
      "Epoch 76/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1814 - accuracy: 0.0799 - val_loss: 0.2427 - val_accuracy: 0.0613\n",
      "Epoch 77/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1813 - accuracy: 0.0823 - val_loss: 0.2420 - val_accuracy: 0.0378\n",
      "Epoch 78/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1811 - accuracy: 0.0541 - val_loss: 0.2414 - val_accuracy: 0.0775\n",
      "Epoch 79/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1809 - accuracy: 0.0841 - val_loss: 0.2421 - val_accuracy: 0.0973\n",
      "Epoch 80/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1807 - accuracy: 0.1105 - val_loss: 0.2412 - val_accuracy: 0.1297\n",
      "Epoch 81/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1805 - accuracy: 0.1153 - val_loss: 0.2417 - val_accuracy: 0.0937\n",
      "Epoch 82/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1803 - accuracy: 0.0925 - val_loss: 0.2411 - val_accuracy: 0.0486\n",
      "Epoch 83/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1801 - accuracy: 0.0679 - val_loss: 0.2401 - val_accuracy: 0.0811\n",
      "Epoch 84/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1799 - accuracy: 0.1069 - val_loss: 0.2403 - val_accuracy: 0.0973\n",
      "Epoch 85/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1796 - accuracy: 0.0955 - val_loss: 0.2400 - val_accuracy: 0.1225\n",
      "Epoch 86/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1794 - accuracy: 0.1039 - val_loss: 0.2389 - val_accuracy: 0.1315\n",
      "Epoch 87/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1791 - accuracy: 0.1099 - val_loss: 0.2388 - val_accuracy: 0.0667\n",
      "Epoch 88/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1789 - accuracy: 0.0955 - val_loss: 0.2380 - val_accuracy: 0.0721\n",
      "Epoch 89/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1786 - accuracy: 0.1033 - val_loss: 0.2377 - val_accuracy: 0.0793\n",
      "Epoch 90/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1784 - accuracy: 0.1081 - val_loss: 0.2380 - val_accuracy: 0.1171\n",
      "Epoch 91/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1781 - accuracy: 0.1057 - val_loss: 0.2377 - val_accuracy: 0.1117\n",
      "Epoch 92/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1777 - accuracy: 0.1135 - val_loss: 0.2369 - val_accuracy: 0.1117\n",
      "Epoch 93/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1775 - accuracy: 0.1141 - val_loss: 0.2355 - val_accuracy: 0.1027\n",
      "Epoch 94/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1771 - accuracy: 0.1135 - val_loss: 0.2358 - val_accuracy: 0.0973\n",
      "Epoch 95/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1767 - accuracy: 0.1117 - val_loss: 0.2354 - val_accuracy: 0.1081\n",
      "Epoch 96/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1764 - accuracy: 0.1201 - val_loss: 0.2353 - val_accuracy: 0.1495\n",
      "Epoch 97/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1760 - accuracy: 0.1417 - val_loss: 0.2345 - val_accuracy: 0.0901\n",
      "Epoch 98/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1756 - accuracy: 0.1309 - val_loss: 0.2336 - val_accuracy: 0.0955\n",
      "Epoch 99/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1752 - accuracy: 0.1183 - val_loss: 0.2326 - val_accuracy: 0.1261\n",
      "Epoch 100/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1748 - accuracy: 0.1207 - val_loss: 0.2331 - val_accuracy: 0.1297\n",
      "Epoch 101/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1743 - accuracy: 0.1225 - val_loss: 0.2317 - val_accuracy: 0.1225\n",
      "Epoch 102/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1739 - accuracy: 0.1195 - val_loss: 0.2316 - val_accuracy: 0.1099\n",
      "Epoch 103/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1735 - accuracy: 0.0997 - val_loss: 0.2305 - val_accuracy: 0.1477\n",
      "Epoch 104/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1729 - accuracy: 0.1399 - val_loss: 0.2301 - val_accuracy: 0.1441\n",
      "Epoch 105/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1725 - accuracy: 0.1453 - val_loss: 0.2295 - val_accuracy: 0.1261\n",
      "Epoch 106/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1720 - accuracy: 0.1237 - val_loss: 0.2289 - val_accuracy: 0.1099\n",
      "Epoch 107/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1715 - accuracy: 0.1093 - val_loss: 0.2278 - val_accuracy: 0.1495\n",
      "Epoch 108/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1709 - accuracy: 0.1381 - val_loss: 0.2272 - val_accuracy: 0.1243\n",
      "Epoch 109/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1704 - accuracy: 0.1411 - val_loss: 0.2263 - val_accuracy: 0.1351\n",
      "Epoch 110/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1700 - accuracy: 0.1423 - val_loss: 0.2252 - val_accuracy: 0.1225\n",
      "Epoch 111/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1694 - accuracy: 0.1279 - val_loss: 0.2249 - val_accuracy: 0.1225\n",
      "Epoch 112/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1688 - accuracy: 0.1279 - val_loss: 0.2240 - val_accuracy: 0.1640\n",
      "Epoch 113/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1682 - accuracy: 0.1477 - val_loss: 0.2232 - val_accuracy: 0.1550\n",
      "Epoch 114/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1677 - accuracy: 0.1393 - val_loss: 0.2222 - val_accuracy: 0.1622\n",
      "Epoch 115/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1671 - accuracy: 0.1459 - val_loss: 0.2223 - val_accuracy: 0.1423\n",
      "Epoch 116/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1665 - accuracy: 0.1544 - val_loss: 0.2213 - val_accuracy: 0.1477\n",
      "Epoch 117/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1659 - accuracy: 0.1405 - val_loss: 0.2203 - val_accuracy: 0.1387\n",
      "Epoch 118/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1654 - accuracy: 0.1423 - val_loss: 0.2196 - val_accuracy: 0.1279\n",
      "Epoch 119/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1648 - accuracy: 0.1453 - val_loss: 0.2186 - val_accuracy: 0.1369\n",
      "Epoch 120/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1643 - accuracy: 0.1495 - val_loss: 0.2183 - val_accuracy: 0.1495\n",
      "Epoch 121/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1637 - accuracy: 0.1495 - val_loss: 0.2175 - val_accuracy: 0.1514\n",
      "Epoch 122/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1631 - accuracy: 0.1495 - val_loss: 0.2170 - val_accuracy: 0.1423\n",
      "Epoch 123/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1625 - accuracy: 0.1483 - val_loss: 0.2157 - val_accuracy: 0.1369\n",
      "Epoch 124/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1620 - accuracy: 0.1526 - val_loss: 0.2153 - val_accuracy: 0.1225\n",
      "Epoch 125/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1614 - accuracy: 0.1562 - val_loss: 0.2148 - val_accuracy: 0.1604\n",
      "Epoch 126/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1610 - accuracy: 0.1658 - val_loss: 0.2142 - val_accuracy: 0.1225\n",
      "Epoch 127/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1604 - accuracy: 0.1550 - val_loss: 0.2133 - val_accuracy: 0.1514\n",
      "Epoch 128/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1598 - accuracy: 0.1622 - val_loss: 0.2126 - val_accuracy: 0.1297\n",
      "Epoch 129/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1593 - accuracy: 0.1520 - val_loss: 0.2123 - val_accuracy: 0.1315\n",
      "Epoch 130/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1588 - accuracy: 0.1622 - val_loss: 0.2116 - val_accuracy: 0.1856\n",
      "Epoch 131/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1583 - accuracy: 0.1748 - val_loss: 0.2108 - val_accuracy: 0.1604\n",
      "Epoch 132/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1578 - accuracy: 0.1676 - val_loss: 0.2106 - val_accuracy: 0.1766\n",
      "Epoch 133/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1572 - accuracy: 0.1658 - val_loss: 0.2099 - val_accuracy: 0.1405\n",
      "Epoch 134/800\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1567 - accuracy: 0.1664 - val_loss: 0.2099 - val_accuracy: 0.1459\n",
      "Epoch 135/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1563 - accuracy: 0.1652 - val_loss: 0.2091 - val_accuracy: 0.1459\n",
      "Epoch 136/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1558 - accuracy: 0.1646 - val_loss: 0.2090 - val_accuracy: 0.1694\n",
      "Epoch 137/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1553 - accuracy: 0.1748 - val_loss: 0.2076 - val_accuracy: 0.1514\n",
      "Epoch 138/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1548 - accuracy: 0.1754 - val_loss: 0.2077 - val_accuracy: 0.1459\n",
      "Epoch 139/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1543 - accuracy: 0.1550 - val_loss: 0.2061 - val_accuracy: 0.1766\n",
      "Epoch 140/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1539 - accuracy: 0.1766 - val_loss: 0.2060 - val_accuracy: 0.1441\n",
      "Epoch 141/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1534 - accuracy: 0.1718 - val_loss: 0.2051 - val_accuracy: 0.1712\n",
      "Epoch 142/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1530 - accuracy: 0.1736 - val_loss: 0.2064 - val_accuracy: 0.1604\n",
      "Epoch 143/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1526 - accuracy: 0.1796 - val_loss: 0.2054 - val_accuracy: 0.1459\n",
      "Epoch 144/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1521 - accuracy: 0.1568 - val_loss: 0.2041 - val_accuracy: 0.1405\n",
      "Epoch 145/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1517 - accuracy: 0.1742 - val_loss: 0.2040 - val_accuracy: 0.1694\n",
      "Epoch 146/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1512 - accuracy: 0.1760 - val_loss: 0.2031 - val_accuracy: 0.1297\n",
      "Epoch 147/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1508 - accuracy: 0.1700 - val_loss: 0.2021 - val_accuracy: 0.1658\n",
      "Epoch 148/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1502 - accuracy: 0.1700 - val_loss: 0.2023 - val_accuracy: 0.1676\n",
      "Epoch 149/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1499 - accuracy: 0.1754 - val_loss: 0.2015 - val_accuracy: 0.1477\n",
      "Epoch 150/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1494 - accuracy: 0.1766 - val_loss: 0.2030 - val_accuracy: 0.1802\n",
      "Epoch 151/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1491 - accuracy: 0.1826 - val_loss: 0.2016 - val_accuracy: 0.1820\n",
      "Epoch 152/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1487 - accuracy: 0.1958 - val_loss: 0.2014 - val_accuracy: 0.1405\n",
      "Epoch 153/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1483 - accuracy: 0.1700 - val_loss: 0.2006 - val_accuracy: 0.1495\n",
      "Epoch 154/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1480 - accuracy: 0.1814 - val_loss: 0.2012 - val_accuracy: 0.1730\n",
      "Epoch 155/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1475 - accuracy: 0.1838 - val_loss: 0.1995 - val_accuracy: 0.1568\n",
      "Epoch 156/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1471 - accuracy: 0.1850 - val_loss: 0.1994 - val_accuracy: 0.1892\n",
      "Epoch 157/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1468 - accuracy: 0.1784 - val_loss: 0.1985 - val_accuracy: 0.1550\n",
      "Epoch 158/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1464 - accuracy: 0.1832 - val_loss: 0.1995 - val_accuracy: 0.1532\n",
      "Epoch 159/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1461 - accuracy: 0.1814 - val_loss: 0.1982 - val_accuracy: 0.1748\n",
      "Epoch 160/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1457 - accuracy: 0.1826 - val_loss: 0.1994 - val_accuracy: 0.1387\n",
      "Epoch 161/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1453 - accuracy: 0.1832 - val_loss: 0.1966 - val_accuracy: 0.1477\n",
      "Epoch 162/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1449 - accuracy: 0.1844 - val_loss: 0.1957 - val_accuracy: 0.1477\n",
      "Epoch 163/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1447 - accuracy: 0.1856 - val_loss: 0.1966 - val_accuracy: 0.1748\n",
      "Epoch 164/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1442 - accuracy: 0.1910 - val_loss: 0.1965 - val_accuracy: 0.1315\n",
      "Epoch 165/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1439 - accuracy: 0.1784 - val_loss: 0.1962 - val_accuracy: 0.1423\n",
      "Epoch 166/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1436 - accuracy: 0.1904 - val_loss: 0.1954 - val_accuracy: 0.1604\n",
      "Epoch 167/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1432 - accuracy: 0.1910 - val_loss: 0.1955 - val_accuracy: 0.1495\n",
      "Epoch 168/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1430 - accuracy: 0.1970 - val_loss: 0.2005 - val_accuracy: 0.1261\n",
      "Epoch 169/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1428 - accuracy: 0.1892 - val_loss: 0.1953 - val_accuracy: 0.1477\n",
      "Epoch 170/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1423 - accuracy: 0.1874 - val_loss: 0.1952 - val_accuracy: 0.1514\n",
      "Epoch 171/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1420 - accuracy: 0.1874 - val_loss: 0.1936 - val_accuracy: 0.1784\n",
      "Epoch 172/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1416 - accuracy: 0.1958 - val_loss: 0.1929 - val_accuracy: 0.1477\n",
      "Epoch 173/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1413 - accuracy: 0.1844 - val_loss: 0.1933 - val_accuracy: 0.1604\n",
      "Epoch 174/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1409 - accuracy: 0.1940 - val_loss: 0.1924 - val_accuracy: 0.1730\n",
      "Epoch 175/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1406 - accuracy: 0.1910 - val_loss: 0.1929 - val_accuracy: 0.1712\n",
      "Epoch 176/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1403 - accuracy: 0.1976 - val_loss: 0.1910 - val_accuracy: 0.1568\n",
      "Epoch 177/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1400 - accuracy: 0.1892 - val_loss: 0.1911 - val_accuracy: 0.1622\n",
      "Epoch 178/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1397 - accuracy: 0.1934 - val_loss: 0.1912 - val_accuracy: 0.1766\n",
      "Epoch 179/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1394 - accuracy: 0.1988 - val_loss: 0.1925 - val_accuracy: 0.1459\n",
      "Epoch 180/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1392 - accuracy: 0.2060 - val_loss: 0.1898 - val_accuracy: 0.1676\n",
      "Epoch 181/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1388 - accuracy: 0.2024 - val_loss: 0.1910 - val_accuracy: 0.1495\n",
      "Epoch 182/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1386 - accuracy: 0.2030 - val_loss: 0.1889 - val_accuracy: 0.1640\n",
      "Epoch 183/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1382 - accuracy: 0.1886 - val_loss: 0.1904 - val_accuracy: 0.1730\n",
      "Epoch 184/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1380 - accuracy: 0.2030 - val_loss: 0.1922 - val_accuracy: 0.1856\n",
      "Epoch 185/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1376 - accuracy: 0.1988 - val_loss: 0.1889 - val_accuracy: 0.2072\n",
      "Epoch 186/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1373 - accuracy: 0.2114 - val_loss: 0.1889 - val_accuracy: 0.1802\n",
      "Epoch 187/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1370 - accuracy: 0.1922 - val_loss: 0.1871 - val_accuracy: 0.1477\n",
      "Epoch 188/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1367 - accuracy: 0.1982 - val_loss: 0.1874 - val_accuracy: 0.1459\n",
      "Epoch 189/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1365 - accuracy: 0.2018 - val_loss: 0.1871 - val_accuracy: 0.1802\n",
      "Epoch 190/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1361 - accuracy: 0.1976 - val_loss: 0.1862 - val_accuracy: 0.1622\n",
      "Epoch 191/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1357 - accuracy: 0.1940 - val_loss: 0.1858 - val_accuracy: 0.1568\n",
      "Epoch 192/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1355 - accuracy: 0.2036 - val_loss: 0.1865 - val_accuracy: 0.1874\n",
      "Epoch 193/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1352 - accuracy: 0.2114 - val_loss: 0.1860 - val_accuracy: 0.1622\n",
      "Epoch 194/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1348 - accuracy: 0.2078 - val_loss: 0.1862 - val_accuracy: 0.1730\n",
      "Epoch 195/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1346 - accuracy: 0.2084 - val_loss: 0.1859 - val_accuracy: 0.1802\n",
      "Epoch 196/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1343 - accuracy: 0.2120 - val_loss: 0.1847 - val_accuracy: 0.1694\n",
      "Epoch 197/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1339 - accuracy: 0.2150 - val_loss: 0.1849 - val_accuracy: 0.1676\n",
      "Epoch 198/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1337 - accuracy: 0.2078 - val_loss: 0.1858 - val_accuracy: 0.1784\n",
      "Epoch 199/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1334 - accuracy: 0.2204 - val_loss: 0.1855 - val_accuracy: 0.1459\n",
      "Epoch 200/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1332 - accuracy: 0.2138 - val_loss: 0.1838 - val_accuracy: 0.1874\n",
      "Epoch 201/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1328 - accuracy: 0.2078 - val_loss: 0.1829 - val_accuracy: 0.1387\n",
      "Epoch 202/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1325 - accuracy: 0.2096 - val_loss: 0.1852 - val_accuracy: 0.1568\n",
      "Epoch 203/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1323 - accuracy: 0.2042 - val_loss: 0.1839 - val_accuracy: 0.1640\n",
      "Epoch 204/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1318 - accuracy: 0.2036 - val_loss: 0.1810 - val_accuracy: 0.1712\n",
      "Epoch 205/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1314 - accuracy: 0.2156 - val_loss: 0.1813 - val_accuracy: 0.1658\n",
      "Epoch 206/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1311 - accuracy: 0.2078 - val_loss: 0.1815 - val_accuracy: 0.1586\n",
      "Epoch 207/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1308 - accuracy: 0.2072 - val_loss: 0.1806 - val_accuracy: 0.1495\n",
      "Epoch 208/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1303 - accuracy: 0.2180 - val_loss: 0.1800 - val_accuracy: 0.1405\n",
      "Epoch 209/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1302 - accuracy: 0.2138 - val_loss: 0.1798 - val_accuracy: 0.1838\n",
      "Epoch 210/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1298 - accuracy: 0.2114 - val_loss: 0.1795 - val_accuracy: 0.1405\n",
      "Epoch 211/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1295 - accuracy: 0.2138 - val_loss: 0.1791 - val_accuracy: 0.1802\n",
      "Epoch 212/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1290 - accuracy: 0.2192 - val_loss: 0.1798 - val_accuracy: 0.1982\n",
      "Epoch 213/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1288 - accuracy: 0.2216 - val_loss: 0.1790 - val_accuracy: 0.1658\n",
      "Epoch 214/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1283 - accuracy: 0.2198 - val_loss: 0.1782 - val_accuracy: 0.1568\n",
      "Epoch 215/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1281 - accuracy: 0.2174 - val_loss: 0.1775 - val_accuracy: 0.1748\n",
      "Epoch 216/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1276 - accuracy: 0.2174 - val_loss: 0.1848 - val_accuracy: 0.2072\n",
      "Epoch 217/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1277 - accuracy: 0.2234 - val_loss: 0.1760 - val_accuracy: 0.1604\n",
      "Epoch 218/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1269 - accuracy: 0.2234 - val_loss: 0.1769 - val_accuracy: 0.1586\n",
      "Epoch 219/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1267 - accuracy: 0.2084 - val_loss: 0.1765 - val_accuracy: 0.1694\n",
      "Epoch 220/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1264 - accuracy: 0.2216 - val_loss: 0.1758 - val_accuracy: 0.1568\n",
      "Epoch 221/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1260 - accuracy: 0.2198 - val_loss: 0.1783 - val_accuracy: 0.1135\n",
      "Epoch 222/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1258 - accuracy: 0.2138 - val_loss: 0.1750 - val_accuracy: 0.1622\n",
      "Epoch 223/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1253 - accuracy: 0.2252 - val_loss: 0.1736 - val_accuracy: 0.1910\n",
      "Epoch 224/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1250 - accuracy: 0.2198 - val_loss: 0.1731 - val_accuracy: 0.1441\n",
      "Epoch 225/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1247 - accuracy: 0.2162 - val_loss: 0.1741 - val_accuracy: 0.1712\n",
      "Epoch 226/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1244 - accuracy: 0.2222 - val_loss: 0.1737 - val_accuracy: 0.1838\n",
      "Epoch 227/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1240 - accuracy: 0.2228 - val_loss: 0.1726 - val_accuracy: 0.1423\n",
      "Epoch 228/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1237 - accuracy: 0.2234 - val_loss: 0.1728 - val_accuracy: 0.2018\n",
      "Epoch 229/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1234 - accuracy: 0.2270 - val_loss: 0.1723 - val_accuracy: 0.1712\n",
      "Epoch 230/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1230 - accuracy: 0.2288 - val_loss: 0.1719 - val_accuracy: 0.1748\n",
      "Epoch 231/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1226 - accuracy: 0.2258 - val_loss: 0.1705 - val_accuracy: 0.1676\n",
      "Epoch 232/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1221 - accuracy: 0.2198 - val_loss: 0.1716 - val_accuracy: 0.1712\n",
      "Epoch 233/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1221 - accuracy: 0.2378 - val_loss: 0.1699 - val_accuracy: 0.1550\n",
      "Epoch 234/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1216 - accuracy: 0.2276 - val_loss: 0.1724 - val_accuracy: 0.1658\n",
      "Epoch 235/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1215 - accuracy: 0.2276 - val_loss: 0.1700 - val_accuracy: 0.1730\n",
      "Epoch 236/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1210 - accuracy: 0.2258 - val_loss: 0.1697 - val_accuracy: 0.1946\n",
      "Epoch 237/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1206 - accuracy: 0.2348 - val_loss: 0.1689 - val_accuracy: 0.1622\n",
      "Epoch 238/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1203 - accuracy: 0.2204 - val_loss: 0.1682 - val_accuracy: 0.1802\n",
      "Epoch 239/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1201 - accuracy: 0.2318 - val_loss: 0.1692 - val_accuracy: 0.1694\n",
      "Epoch 240/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1198 - accuracy: 0.2270 - val_loss: 0.1696 - val_accuracy: 0.2054\n",
      "Epoch 241/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1194 - accuracy: 0.2348 - val_loss: 0.1739 - val_accuracy: 0.2306\n",
      "Epoch 242/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1194 - accuracy: 0.2264 - val_loss: 0.1693 - val_accuracy: 0.2180\n",
      "Epoch 243/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1190 - accuracy: 0.2426 - val_loss: 0.1661 - val_accuracy: 0.1766\n",
      "Epoch 244/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1186 - accuracy: 0.2216 - val_loss: 0.1658 - val_accuracy: 0.1982\n",
      "Epoch 245/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1183 - accuracy: 0.2372 - val_loss: 0.1657 - val_accuracy: 0.1712\n",
      "Epoch 246/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1180 - accuracy: 0.2342 - val_loss: 0.1659 - val_accuracy: 0.1784\n",
      "Epoch 247/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1176 - accuracy: 0.2390 - val_loss: 0.1665 - val_accuracy: 0.1946\n",
      "Epoch 248/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1174 - accuracy: 0.2306 - val_loss: 0.1658 - val_accuracy: 0.1405\n",
      "Epoch 249/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1173 - accuracy: 0.2330 - val_loss: 0.1650 - val_accuracy: 0.2144\n",
      "Epoch 250/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1168 - accuracy: 0.2360 - val_loss: 0.1652 - val_accuracy: 0.1910\n",
      "Epoch 251/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1165 - accuracy: 0.2420 - val_loss: 0.1656 - val_accuracy: 0.1351\n",
      "Epoch 252/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1164 - accuracy: 0.2336 - val_loss: 0.1641 - val_accuracy: 0.1604\n",
      "Epoch 253/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1161 - accuracy: 0.2444 - val_loss: 0.1650 - val_accuracy: 0.2144\n",
      "Epoch 254/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1158 - accuracy: 0.2390 - val_loss: 0.1673 - val_accuracy: 0.2234\n",
      "Epoch 255/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1156 - accuracy: 0.2270 - val_loss: 0.1626 - val_accuracy: 0.1784\n",
      "Epoch 256/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1151 - accuracy: 0.2324 - val_loss: 0.1623 - val_accuracy: 0.1694\n",
      "Epoch 257/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1150 - accuracy: 0.2324 - val_loss: 0.1624 - val_accuracy: 0.2216\n",
      "Epoch 258/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1147 - accuracy: 0.2390 - val_loss: 0.1627 - val_accuracy: 0.1964\n",
      "Epoch 259/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1143 - accuracy: 0.2456 - val_loss: 0.1615 - val_accuracy: 0.1910\n",
      "Epoch 260/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1141 - accuracy: 0.2294 - val_loss: 0.1625 - val_accuracy: 0.1766\n",
      "Epoch 261/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1139 - accuracy: 0.2396 - val_loss: 0.1694 - val_accuracy: 0.1189\n",
      "Epoch 262/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1143 - accuracy: 0.2288 - val_loss: 0.1702 - val_accuracy: 0.1423\n",
      "Epoch 263/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1140 - accuracy: 0.2354 - val_loss: 0.1599 - val_accuracy: 0.1892\n",
      "Epoch 264/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1132 - accuracy: 0.2300 - val_loss: 0.1614 - val_accuracy: 0.1604\n",
      "Epoch 265/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1131 - accuracy: 0.2336 - val_loss: 0.1604 - val_accuracy: 0.2072\n",
      "Epoch 266/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1128 - accuracy: 0.2462 - val_loss: 0.1613 - val_accuracy: 0.2306\n",
      "Epoch 267/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1126 - accuracy: 0.2456 - val_loss: 0.1595 - val_accuracy: 0.2090\n",
      "Epoch 268/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1124 - accuracy: 0.2378 - val_loss: 0.1606 - val_accuracy: 0.1964\n",
      "Epoch 269/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1121 - accuracy: 0.2438 - val_loss: 0.1597 - val_accuracy: 0.2288\n",
      "Epoch 270/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1119 - accuracy: 0.2486 - val_loss: 0.1585 - val_accuracy: 0.2054\n",
      "Epoch 271/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1116 - accuracy: 0.2408 - val_loss: 0.1586 - val_accuracy: 0.2144\n",
      "Epoch 272/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1115 - accuracy: 0.2432 - val_loss: 0.1613 - val_accuracy: 0.2306\n",
      "Epoch 273/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1112 - accuracy: 0.2462 - val_loss: 0.1573 - val_accuracy: 0.1946\n",
      "Epoch 274/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1112 - accuracy: 0.2420 - val_loss: 0.1602 - val_accuracy: 0.1910\n",
      "Epoch 275/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1109 - accuracy: 0.2438 - val_loss: 0.1591 - val_accuracy: 0.2450\n",
      "Epoch 276/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1106 - accuracy: 0.2553 - val_loss: 0.1681 - val_accuracy: 0.2018\n",
      "Epoch 277/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1109 - accuracy: 0.2372 - val_loss: 0.1575 - val_accuracy: 0.2216\n",
      "Epoch 278/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1102 - accuracy: 0.2396 - val_loss: 0.1584 - val_accuracy: 0.2360\n",
      "Epoch 279/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1101 - accuracy: 0.2505 - val_loss: 0.1619 - val_accuracy: 0.1784\n",
      "Epoch 280/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1101 - accuracy: 0.2354 - val_loss: 0.1563 - val_accuracy: 0.2072\n",
      "Epoch 281/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1097 - accuracy: 0.2450 - val_loss: 0.1569 - val_accuracy: 0.2450\n",
      "Epoch 282/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1093 - accuracy: 0.2420 - val_loss: 0.1617 - val_accuracy: 0.2541\n",
      "Epoch 283/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1096 - accuracy: 0.2462 - val_loss: 0.1566 - val_accuracy: 0.2234\n",
      "Epoch 284/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1091 - accuracy: 0.2474 - val_loss: 0.1575 - val_accuracy: 0.2234\n",
      "Epoch 285/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1089 - accuracy: 0.2420 - val_loss: 0.1566 - val_accuracy: 0.2414\n",
      "Epoch 286/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1087 - accuracy: 0.2565 - val_loss: 0.1564 - val_accuracy: 0.2324\n",
      "Epoch 287/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1085 - accuracy: 0.2492 - val_loss: 0.1560 - val_accuracy: 0.2162\n",
      "Epoch 288/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1085 - accuracy: 0.2414 - val_loss: 0.1568 - val_accuracy: 0.2450\n",
      "Epoch 289/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1083 - accuracy: 0.2480 - val_loss: 0.1575 - val_accuracy: 0.2234\n",
      "Epoch 290/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1081 - accuracy: 0.2523 - val_loss: 0.1586 - val_accuracy: 0.2631\n",
      "Epoch 291/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1078 - accuracy: 0.2565 - val_loss: 0.1551 - val_accuracy: 0.2324\n",
      "Epoch 292/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1076 - accuracy: 0.2505 - val_loss: 0.1541 - val_accuracy: 0.2216\n",
      "Epoch 293/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1075 - accuracy: 0.2523 - val_loss: 0.1547 - val_accuracy: 0.2270\n",
      "Epoch 294/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1072 - accuracy: 0.2517 - val_loss: 0.1544 - val_accuracy: 0.2541\n",
      "Epoch 295/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1071 - accuracy: 0.2523 - val_loss: 0.1550 - val_accuracy: 0.2180\n",
      "Epoch 296/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1070 - accuracy: 0.2523 - val_loss: 0.1542 - val_accuracy: 0.2342\n",
      "Epoch 297/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1069 - accuracy: 0.2486 - val_loss: 0.1544 - val_accuracy: 0.2505\n",
      "Epoch 298/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1066 - accuracy: 0.2529 - val_loss: 0.1552 - val_accuracy: 0.2198\n",
      "Epoch 299/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1066 - accuracy: 0.2547 - val_loss: 0.1548 - val_accuracy: 0.2486\n",
      "Epoch 300/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1063 - accuracy: 0.2535 - val_loss: 0.1549 - val_accuracy: 0.2577\n",
      "Epoch 301/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1063 - accuracy: 0.2498 - val_loss: 0.1539 - val_accuracy: 0.2414\n",
      "Epoch 302/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1061 - accuracy: 0.2511 - val_loss: 0.1542 - val_accuracy: 0.2450\n",
      "Epoch 303/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1060 - accuracy: 0.2498 - val_loss: 0.1538 - val_accuracy: 0.2342\n",
      "Epoch 304/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1058 - accuracy: 0.2571 - val_loss: 0.1533 - val_accuracy: 0.2613\n",
      "Epoch 305/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1055 - accuracy: 0.2511 - val_loss: 0.1532 - val_accuracy: 0.2126\n",
      "Epoch 306/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1054 - accuracy: 0.2480 - val_loss: 0.1537 - val_accuracy: 0.2342\n",
      "Epoch 307/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1052 - accuracy: 0.2565 - val_loss: 0.1533 - val_accuracy: 0.2018\n",
      "Epoch 308/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1051 - accuracy: 0.2583 - val_loss: 0.1533 - val_accuracy: 0.2721\n",
      "Epoch 309/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1049 - accuracy: 0.2577 - val_loss: 0.1547 - val_accuracy: 0.2252\n",
      "Epoch 310/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1048 - accuracy: 0.2474 - val_loss: 0.1515 - val_accuracy: 0.1820\n",
      "Epoch 311/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1045 - accuracy: 0.2444 - val_loss: 0.1605 - val_accuracy: 0.3081\n",
      "Epoch 312/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1046 - accuracy: 0.2637 - val_loss: 0.1532 - val_accuracy: 0.2757\n",
      "Epoch 313/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1042 - accuracy: 0.2637 - val_loss: 0.1520 - val_accuracy: 0.2378\n",
      "Epoch 314/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1040 - accuracy: 0.2480 - val_loss: 0.1549 - val_accuracy: 0.2162\n",
      "Epoch 315/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1043 - accuracy: 0.2535 - val_loss: 0.1502 - val_accuracy: 0.2324\n",
      "Epoch 316/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1036 - accuracy: 0.2547 - val_loss: 0.1509 - val_accuracy: 0.2198\n",
      "Epoch 317/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1034 - accuracy: 0.2547 - val_loss: 0.1506 - val_accuracy: 0.2505\n",
      "Epoch 318/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1033 - accuracy: 0.2547 - val_loss: 0.1524 - val_accuracy: 0.2523\n",
      "Epoch 319/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1031 - accuracy: 0.2583 - val_loss: 0.1511 - val_accuracy: 0.2288\n",
      "Epoch 320/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1030 - accuracy: 0.2529 - val_loss: 0.1513 - val_accuracy: 0.2523\n",
      "Epoch 321/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1027 - accuracy: 0.2529 - val_loss: 0.1507 - val_accuracy: 0.2450\n",
      "Epoch 322/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1026 - accuracy: 0.2565 - val_loss: 0.1502 - val_accuracy: 0.2541\n",
      "Epoch 323/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1025 - accuracy: 0.2637 - val_loss: 0.1537 - val_accuracy: 0.2721\n",
      "Epoch 324/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1024 - accuracy: 0.2583 - val_loss: 0.1488 - val_accuracy: 0.2450\n",
      "Epoch 325/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1021 - accuracy: 0.2559 - val_loss: 0.1551 - val_accuracy: 0.2054\n",
      "Epoch 326/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1024 - accuracy: 0.2523 - val_loss: 0.1489 - val_accuracy: 0.2559\n",
      "Epoch 327/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1017 - accuracy: 0.2601 - val_loss: 0.1590 - val_accuracy: 0.1820\n",
      "Epoch 328/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1022 - accuracy: 0.2631 - val_loss: 0.1481 - val_accuracy: 0.2703\n",
      "Epoch 329/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1015 - accuracy: 0.2571 - val_loss: 0.1491 - val_accuracy: 0.2631\n",
      "Epoch 330/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1011 - accuracy: 0.2667 - val_loss: 0.1503 - val_accuracy: 0.2703\n",
      "Epoch 331/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1011 - accuracy: 0.2649 - val_loss: 0.1598 - val_accuracy: 0.2649\n",
      "Epoch 332/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1013 - accuracy: 0.2619 - val_loss: 0.1502 - val_accuracy: 0.2901\n",
      "Epoch 333/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1006 - accuracy: 0.2565 - val_loss: 0.1593 - val_accuracy: 0.2721\n",
      "Epoch 334/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1010 - accuracy: 0.2655 - val_loss: 0.1497 - val_accuracy: 0.2595\n",
      "Epoch 335/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1004 - accuracy: 0.2643 - val_loss: 0.1481 - val_accuracy: 0.2396\n",
      "Epoch 336/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1002 - accuracy: 0.2661 - val_loss: 0.1497 - val_accuracy: 0.2486\n",
      "Epoch 337/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1000 - accuracy: 0.2619 - val_loss: 0.1483 - val_accuracy: 0.2559\n",
      "Epoch 338/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0998 - accuracy: 0.2631 - val_loss: 0.1478 - val_accuracy: 0.2523\n",
      "Epoch 339/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0995 - accuracy: 0.2601 - val_loss: 0.1514 - val_accuracy: 0.2865\n",
      "Epoch 340/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0996 - accuracy: 0.2697 - val_loss: 0.1467 - val_accuracy: 0.2649\n",
      "Epoch 341/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0992 - accuracy: 0.2607 - val_loss: 0.1471 - val_accuracy: 0.2685\n",
      "Epoch 342/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0990 - accuracy: 0.2655 - val_loss: 0.1473 - val_accuracy: 0.2324\n",
      "Epoch 343/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0989 - accuracy: 0.2721 - val_loss: 0.1474 - val_accuracy: 0.2847\n",
      "Epoch 344/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0985 - accuracy: 0.2691 - val_loss: 0.1496 - val_accuracy: 0.2577\n",
      "Epoch 345/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0988 - accuracy: 0.2625 - val_loss: 0.1461 - val_accuracy: 0.2775\n",
      "Epoch 346/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0982 - accuracy: 0.2703 - val_loss: 0.1463 - val_accuracy: 0.2595\n",
      "Epoch 347/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0980 - accuracy: 0.2715 - val_loss: 0.1483 - val_accuracy: 0.2811\n",
      "Epoch 348/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0979 - accuracy: 0.2667 - val_loss: 0.1450 - val_accuracy: 0.2505\n",
      "Epoch 349/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0977 - accuracy: 0.2649 - val_loss: 0.1447 - val_accuracy: 0.2577\n",
      "Epoch 350/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0975 - accuracy: 0.2619 - val_loss: 0.1465 - val_accuracy: 0.2685\n",
      "Epoch 351/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0973 - accuracy: 0.2583 - val_loss: 0.1463 - val_accuracy: 0.2721\n",
      "Epoch 352/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0972 - accuracy: 0.2757 - val_loss: 0.1466 - val_accuracy: 0.2829\n",
      "Epoch 353/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0970 - accuracy: 0.2631 - val_loss: 0.1457 - val_accuracy: 0.2811\n",
      "Epoch 354/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0967 - accuracy: 0.2667 - val_loss: 0.1471 - val_accuracy: 0.2829\n",
      "Epoch 355/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0965 - accuracy: 0.2649 - val_loss: 0.1441 - val_accuracy: 0.2649\n",
      "Epoch 356/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0963 - accuracy: 0.2691 - val_loss: 0.1474 - val_accuracy: 0.2829\n",
      "Epoch 357/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0962 - accuracy: 0.2667 - val_loss: 0.1532 - val_accuracy: 0.3063\n",
      "Epoch 358/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0963 - accuracy: 0.2703 - val_loss: 0.1461 - val_accuracy: 0.2685\n",
      "Epoch 359/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0957 - accuracy: 0.2673 - val_loss: 0.1515 - val_accuracy: 0.2811\n",
      "Epoch 360/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0958 - accuracy: 0.2631 - val_loss: 0.1450 - val_accuracy: 0.2937\n",
      "Epoch 361/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0954 - accuracy: 0.2703 - val_loss: 0.1458 - val_accuracy: 0.3063\n",
      "Epoch 362/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0952 - accuracy: 0.2769 - val_loss: 0.1502 - val_accuracy: 0.2252\n",
      "Epoch 363/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0952 - accuracy: 0.2715 - val_loss: 0.1437 - val_accuracy: 0.3009\n",
      "Epoch 364/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0947 - accuracy: 0.2745 - val_loss: 0.1440 - val_accuracy: 0.2667\n",
      "Epoch 365/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0946 - accuracy: 0.2697 - val_loss: 0.1449 - val_accuracy: 0.3135\n",
      "Epoch 366/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0944 - accuracy: 0.2739 - val_loss: 0.1451 - val_accuracy: 0.2739\n",
      "Epoch 367/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0942 - accuracy: 0.2733 - val_loss: 0.1588 - val_accuracy: 0.2054\n",
      "Epoch 368/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0948 - accuracy: 0.2643 - val_loss: 0.1436 - val_accuracy: 0.2919\n",
      "Epoch 369/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0938 - accuracy: 0.2781 - val_loss: 0.1438 - val_accuracy: 0.2811\n",
      "Epoch 370/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0934 - accuracy: 0.2733 - val_loss: 0.1430 - val_accuracy: 0.2486\n",
      "Epoch 371/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0936 - accuracy: 0.2643 - val_loss: 0.1431 - val_accuracy: 0.2685\n",
      "Epoch 372/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0931 - accuracy: 0.2661 - val_loss: 0.1440 - val_accuracy: 0.2396\n",
      "Epoch 373/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0931 - accuracy: 0.2673 - val_loss: 0.1445 - val_accuracy: 0.3153\n",
      "Epoch 374/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0927 - accuracy: 0.2745 - val_loss: 0.1439 - val_accuracy: 0.2739\n",
      "Epoch 375/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0926 - accuracy: 0.2673 - val_loss: 0.1440 - val_accuracy: 0.3063\n",
      "Epoch 376/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0924 - accuracy: 0.2757 - val_loss: 0.1430 - val_accuracy: 0.2829\n",
      "Epoch 377/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0919 - accuracy: 0.2799 - val_loss: 0.1440 - val_accuracy: 0.3153\n",
      "Epoch 378/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0919 - accuracy: 0.2811 - val_loss: 0.1547 - val_accuracy: 0.3225\n",
      "Epoch 379/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0923 - accuracy: 0.2745 - val_loss: 0.1416 - val_accuracy: 0.2667\n",
      "Epoch 380/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0916 - accuracy: 0.2727 - val_loss: 0.1608 - val_accuracy: 0.2847\n",
      "Epoch 381/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0922 - accuracy: 0.2721 - val_loss: 0.1413 - val_accuracy: 0.3045\n",
      "Epoch 382/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0912 - accuracy: 0.2757 - val_loss: 0.1401 - val_accuracy: 0.2955\n",
      "Epoch 383/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0912 - accuracy: 0.2751 - val_loss: 0.1408 - val_accuracy: 0.2865\n",
      "Epoch 384/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0908 - accuracy: 0.2853 - val_loss: 0.1418 - val_accuracy: 0.3117\n",
      "Epoch 385/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0907 - accuracy: 0.2757 - val_loss: 0.1412 - val_accuracy: 0.2757\n",
      "Epoch 386/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0906 - accuracy: 0.2685 - val_loss: 0.1413 - val_accuracy: 0.3171\n",
      "Epoch 387/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0902 - accuracy: 0.2835 - val_loss: 0.1402 - val_accuracy: 0.2883\n",
      "Epoch 388/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0903 - accuracy: 0.2751 - val_loss: 0.1428 - val_accuracy: 0.2991\n",
      "Epoch 389/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0901 - accuracy: 0.2775 - val_loss: 0.1400 - val_accuracy: 0.3099\n",
      "Epoch 390/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0897 - accuracy: 0.2811 - val_loss: 0.1397 - val_accuracy: 0.2829\n",
      "Epoch 391/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0896 - accuracy: 0.2769 - val_loss: 0.1528 - val_accuracy: 0.2054\n",
      "Epoch 392/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0901 - accuracy: 0.2769 - val_loss: 0.1409 - val_accuracy: 0.2559\n",
      "Epoch 393/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0893 - accuracy: 0.2715 - val_loss: 0.1397 - val_accuracy: 0.2901\n",
      "Epoch 394/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0891 - accuracy: 0.2769 - val_loss: 0.1408 - val_accuracy: 0.3063\n",
      "Epoch 395/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0889 - accuracy: 0.2835 - val_loss: 0.1418 - val_accuracy: 0.2793\n",
      "Epoch 396/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0887 - accuracy: 0.2805 - val_loss: 0.1393 - val_accuracy: 0.2847\n",
      "Epoch 397/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0886 - accuracy: 0.2841 - val_loss: 0.1403 - val_accuracy: 0.2793\n",
      "Epoch 398/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0886 - accuracy: 0.2769 - val_loss: 0.1422 - val_accuracy: 0.3081\n",
      "Epoch 399/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0883 - accuracy: 0.2823 - val_loss: 0.1389 - val_accuracy: 0.2757\n",
      "Epoch 400/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0881 - accuracy: 0.2745 - val_loss: 0.1452 - val_accuracy: 0.2883\n",
      "Epoch 401/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0883 - accuracy: 0.2799 - val_loss: 0.1480 - val_accuracy: 0.2829\n",
      "Epoch 402/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0884 - accuracy: 0.2835 - val_loss: 0.1400 - val_accuracy: 0.2577\n",
      "Epoch 403/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0876 - accuracy: 0.2799 - val_loss: 0.1394 - val_accuracy: 0.2811\n",
      "Epoch 404/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0875 - accuracy: 0.2763 - val_loss: 0.1412 - val_accuracy: 0.2685\n",
      "Epoch 405/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0874 - accuracy: 0.2769 - val_loss: 0.1385 - val_accuracy: 0.3009\n",
      "Epoch 406/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0873 - accuracy: 0.2727 - val_loss: 0.1400 - val_accuracy: 0.3027\n",
      "Epoch 407/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0870 - accuracy: 0.2829 - val_loss: 0.1379 - val_accuracy: 0.2775\n",
      "Epoch 408/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0869 - accuracy: 0.2823 - val_loss: 0.1384 - val_accuracy: 0.2901\n",
      "Epoch 409/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0868 - accuracy: 0.2865 - val_loss: 0.1397 - val_accuracy: 0.2757\n",
      "Epoch 410/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0869 - accuracy: 0.2709 - val_loss: 0.1416 - val_accuracy: 0.2721\n",
      "Epoch 411/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0867 - accuracy: 0.2859 - val_loss: 0.1358 - val_accuracy: 0.2649\n",
      "Epoch 412/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0864 - accuracy: 0.2799 - val_loss: 0.1402 - val_accuracy: 0.3081\n",
      "Epoch 413/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0864 - accuracy: 0.2805 - val_loss: 0.1383 - val_accuracy: 0.2631\n",
      "Epoch 414/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0863 - accuracy: 0.2751 - val_loss: 0.1413 - val_accuracy: 0.3027\n",
      "Epoch 415/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0860 - accuracy: 0.2793 - val_loss: 0.1377 - val_accuracy: 0.2667\n",
      "Epoch 416/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0858 - accuracy: 0.2805 - val_loss: 0.1393 - val_accuracy: 0.3027\n",
      "Epoch 417/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0859 - accuracy: 0.2865 - val_loss: 0.1375 - val_accuracy: 0.2739\n",
      "Epoch 418/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0857 - accuracy: 0.2853 - val_loss: 0.1390 - val_accuracy: 0.2991\n",
      "Epoch 419/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0856 - accuracy: 0.2799 - val_loss: 0.1382 - val_accuracy: 0.2991\n",
      "Epoch 420/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0855 - accuracy: 0.2907 - val_loss: 0.1355 - val_accuracy: 0.2757\n",
      "Epoch 421/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0853 - accuracy: 0.2853 - val_loss: 0.1378 - val_accuracy: 0.2739\n",
      "Epoch 422/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0852 - accuracy: 0.2919 - val_loss: 0.1464 - val_accuracy: 0.2288\n",
      "Epoch 423/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0858 - accuracy: 0.2793 - val_loss: 0.1371 - val_accuracy: 0.2450\n",
      "Epoch 424/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0850 - accuracy: 0.2829 - val_loss: 0.1377 - val_accuracy: 0.2847\n",
      "Epoch 425/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0846 - accuracy: 0.2883 - val_loss: 0.1394 - val_accuracy: 0.2991\n",
      "Epoch 426/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0848 - accuracy: 0.3027 - val_loss: 0.1404 - val_accuracy: 0.2865\n",
      "Epoch 427/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0847 - accuracy: 0.2907 - val_loss: 0.1394 - val_accuracy: 0.2811\n",
      "Epoch 428/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0844 - accuracy: 0.2925 - val_loss: 0.1377 - val_accuracy: 0.2973\n",
      "Epoch 429/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0844 - accuracy: 0.2835 - val_loss: 0.1371 - val_accuracy: 0.2631\n",
      "Epoch 430/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0842 - accuracy: 0.2853 - val_loss: 0.1387 - val_accuracy: 0.3243\n",
      "Epoch 431/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0841 - accuracy: 0.2871 - val_loss: 0.1330 - val_accuracy: 0.2523\n",
      "Epoch 432/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0840 - accuracy: 0.2883 - val_loss: 0.1367 - val_accuracy: 0.2955\n",
      "Epoch 433/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0838 - accuracy: 0.2853 - val_loss: 0.1414 - val_accuracy: 0.2937\n",
      "Epoch 434/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0840 - accuracy: 0.2883 - val_loss: 0.1355 - val_accuracy: 0.2559\n",
      "Epoch 435/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0837 - accuracy: 0.2853 - val_loss: 0.1380 - val_accuracy: 0.2811\n",
      "Epoch 436/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0835 - accuracy: 0.2907 - val_loss: 0.1371 - val_accuracy: 0.2793\n",
      "Epoch 437/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0834 - accuracy: 0.2907 - val_loss: 0.1393 - val_accuracy: 0.2721\n",
      "Epoch 438/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0834 - accuracy: 0.2889 - val_loss: 0.1466 - val_accuracy: 0.2955\n",
      "Epoch 439/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0837 - accuracy: 0.2895 - val_loss: 0.1354 - val_accuracy: 0.2793\n",
      "Epoch 440/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0831 - accuracy: 0.2871 - val_loss: 0.1371 - val_accuracy: 0.2198\n",
      "Epoch 441/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0831 - accuracy: 0.2841 - val_loss: 0.1360 - val_accuracy: 0.2991\n",
      "Epoch 442/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0829 - accuracy: 0.2859 - val_loss: 0.1374 - val_accuracy: 0.2649\n",
      "Epoch 443/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0829 - accuracy: 0.2847 - val_loss: 0.1385 - val_accuracy: 0.2919\n",
      "Epoch 444/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0827 - accuracy: 0.2919 - val_loss: 0.1346 - val_accuracy: 0.2685\n",
      "Epoch 445/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0826 - accuracy: 0.2841 - val_loss: 0.1382 - val_accuracy: 0.2883\n",
      "Epoch 446/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0824 - accuracy: 0.2943 - val_loss: 0.1410 - val_accuracy: 0.2721\n",
      "Epoch 447/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0825 - accuracy: 0.2931 - val_loss: 0.1362 - val_accuracy: 0.2793\n",
      "Epoch 448/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0822 - accuracy: 0.2871 - val_loss: 0.1372 - val_accuracy: 0.2865\n",
      "Epoch 449/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0821 - accuracy: 0.2871 - val_loss: 0.1361 - val_accuracy: 0.2847\n",
      "Epoch 450/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0821 - accuracy: 0.2883 - val_loss: 0.1365 - val_accuracy: 0.2829\n",
      "Epoch 451/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0819 - accuracy: 0.2877 - val_loss: 0.1451 - val_accuracy: 0.3117\n",
      "Epoch 452/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0824 - accuracy: 0.2937 - val_loss: 0.1395 - val_accuracy: 0.2775\n",
      "Epoch 453/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0816 - accuracy: 0.2919 - val_loss: 0.1371 - val_accuracy: 0.2757\n",
      "Epoch 454/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0817 - accuracy: 0.2865 - val_loss: 0.1367 - val_accuracy: 0.2919\n",
      "Epoch 455/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0815 - accuracy: 0.2901 - val_loss: 0.1355 - val_accuracy: 0.2865\n",
      "Epoch 456/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0816 - accuracy: 0.2895 - val_loss: 0.1400 - val_accuracy: 0.3315\n",
      "Epoch 457/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0816 - accuracy: 0.2967 - val_loss: 0.1370 - val_accuracy: 0.2811\n",
      "Epoch 458/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0813 - accuracy: 0.2889 - val_loss: 0.1347 - val_accuracy: 0.2865\n",
      "Epoch 459/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0814 - accuracy: 0.2907 - val_loss: 0.1358 - val_accuracy: 0.2847\n",
      "Epoch 460/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0811 - accuracy: 0.2937 - val_loss: 0.1363 - val_accuracy: 0.2919\n",
      "Epoch 461/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0810 - accuracy: 0.2907 - val_loss: 0.1363 - val_accuracy: 0.2775\n",
      "Epoch 462/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0813 - accuracy: 0.2973 - val_loss: 0.1345 - val_accuracy: 0.2919\n",
      "Epoch 463/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0808 - accuracy: 0.2961 - val_loss: 0.1367 - val_accuracy: 0.3009\n",
      "Epoch 464/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0808 - accuracy: 0.2943 - val_loss: 0.1366 - val_accuracy: 0.2865\n",
      "Epoch 465/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0811 - accuracy: 0.2949 - val_loss: 0.1341 - val_accuracy: 0.2829\n",
      "Epoch 466/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0807 - accuracy: 0.2913 - val_loss: 0.1351 - val_accuracy: 0.2901\n",
      "Epoch 467/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0805 - accuracy: 0.2925 - val_loss: 0.1363 - val_accuracy: 0.2685\n",
      "Epoch 468/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0805 - accuracy: 0.2895 - val_loss: 0.1446 - val_accuracy: 0.2991\n",
      "Epoch 469/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0809 - accuracy: 0.2985 - val_loss: 0.1366 - val_accuracy: 0.2757\n",
      "Epoch 470/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0804 - accuracy: 0.2967 - val_loss: 0.1351 - val_accuracy: 0.2847\n",
      "Epoch 471/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0805 - accuracy: 0.2871 - val_loss: 0.1472 - val_accuracy: 0.3099\n",
      "Epoch 472/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0807 - accuracy: 0.2895 - val_loss: 0.1345 - val_accuracy: 0.2649\n",
      "Epoch 473/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0802 - accuracy: 0.2787 - val_loss: 0.1346 - val_accuracy: 0.2865\n",
      "Epoch 474/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0799 - accuracy: 0.2955 - val_loss: 0.1351 - val_accuracy: 0.2847\n",
      "Epoch 475/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0798 - accuracy: 0.2991 - val_loss: 0.1387 - val_accuracy: 0.2901\n",
      "Epoch 476/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0800 - accuracy: 0.2853 - val_loss: 0.1368 - val_accuracy: 0.2775\n",
      "Epoch 477/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0796 - accuracy: 0.2943 - val_loss: 0.1335 - val_accuracy: 0.2829\n",
      "Epoch 478/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0795 - accuracy: 0.2925 - val_loss: 0.1348 - val_accuracy: 0.2955\n",
      "Epoch 479/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0794 - accuracy: 0.2931 - val_loss: 0.1361 - val_accuracy: 0.2919\n",
      "Epoch 480/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0794 - accuracy: 0.2907 - val_loss: 0.1348 - val_accuracy: 0.3081\n",
      "Epoch 481/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0793 - accuracy: 0.2949 - val_loss: 0.1411 - val_accuracy: 0.3207\n",
      "Epoch 482/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0795 - accuracy: 0.3021 - val_loss: 0.1343 - val_accuracy: 0.2937\n",
      "Epoch 483/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0793 - accuracy: 0.2937 - val_loss: 0.1404 - val_accuracy: 0.2847\n",
      "Epoch 484/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0794 - accuracy: 0.2907 - val_loss: 0.1358 - val_accuracy: 0.2847\n",
      "Epoch 485/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0790 - accuracy: 0.2925 - val_loss: 0.1402 - val_accuracy: 0.3117\n",
      "Epoch 486/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0789 - accuracy: 0.2967 - val_loss: 0.1360 - val_accuracy: 0.2739\n",
      "Epoch 487/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0789 - accuracy: 0.2937 - val_loss: 0.1342 - val_accuracy: 0.2703\n",
      "Epoch 488/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0788 - accuracy: 0.2955 - val_loss: 0.1359 - val_accuracy: 0.2685\n",
      "Epoch 489/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0788 - accuracy: 0.2955 - val_loss: 0.1349 - val_accuracy: 0.3045\n",
      "Epoch 490/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0785 - accuracy: 0.2931 - val_loss: 0.1319 - val_accuracy: 0.2468\n",
      "Epoch 491/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0787 - accuracy: 0.2871 - val_loss: 0.1344 - val_accuracy: 0.2919\n",
      "Epoch 492/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0783 - accuracy: 0.2943 - val_loss: 0.1340 - val_accuracy: 0.2739\n",
      "Epoch 493/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0784 - accuracy: 0.2997 - val_loss: 0.1367 - val_accuracy: 0.2396\n",
      "Epoch 494/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0785 - accuracy: 0.2889 - val_loss: 0.1334 - val_accuracy: 0.2757\n",
      "Epoch 495/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0782 - accuracy: 0.2829 - val_loss: 0.1541 - val_accuracy: 0.3099\n",
      "Epoch 496/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0788 - accuracy: 0.2997 - val_loss: 0.1353 - val_accuracy: 0.2937\n",
      "Epoch 497/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0782 - accuracy: 0.2985 - val_loss: 0.1327 - val_accuracy: 0.2973\n",
      "Epoch 498/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0779 - accuracy: 0.2967 - val_loss: 0.1330 - val_accuracy: 0.2919\n",
      "Epoch 499/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0779 - accuracy: 0.2961 - val_loss: 0.1391 - val_accuracy: 0.2973\n",
      "Epoch 500/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0779 - accuracy: 0.2973 - val_loss: 0.1313 - val_accuracy: 0.2757\n",
      "Epoch 501/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0780 - accuracy: 0.2979 - val_loss: 0.1337 - val_accuracy: 0.2901\n",
      "Epoch 502/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0778 - accuracy: 0.2901 - val_loss: 0.1333 - val_accuracy: 0.2973\n",
      "Epoch 503/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0774 - accuracy: 0.2919 - val_loss: 0.1345 - val_accuracy: 0.3027\n",
      "Epoch 504/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0776 - accuracy: 0.2985 - val_loss: 0.1349 - val_accuracy: 0.2955\n",
      "Epoch 505/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0775 - accuracy: 0.2973 - val_loss: 0.1324 - val_accuracy: 0.2883\n",
      "Epoch 506/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0772 - accuracy: 0.2919 - val_loss: 0.1354 - val_accuracy: 0.3189\n",
      "Epoch 507/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0775 - accuracy: 0.3021 - val_loss: 0.1326 - val_accuracy: 0.2991\n",
      "Epoch 508/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0770 - accuracy: 0.2937 - val_loss: 0.1384 - val_accuracy: 0.2901\n",
      "Epoch 509/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0773 - accuracy: 0.2979 - val_loss: 0.1346 - val_accuracy: 0.2955\n",
      "Epoch 510/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0771 - accuracy: 0.2883 - val_loss: 0.1319 - val_accuracy: 0.2649\n",
      "Epoch 511/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0769 - accuracy: 0.3003 - val_loss: 0.1373 - val_accuracy: 0.2505\n",
      "Epoch 512/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0773 - accuracy: 0.2979 - val_loss: 0.1356 - val_accuracy: 0.2991\n",
      "Epoch 513/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0768 - accuracy: 0.3021 - val_loss: 0.1358 - val_accuracy: 0.2775\n",
      "Epoch 514/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0768 - accuracy: 0.2943 - val_loss: 0.1331 - val_accuracy: 0.2847\n",
      "Epoch 515/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0765 - accuracy: 0.2925 - val_loss: 0.1352 - val_accuracy: 0.2883\n",
      "Epoch 516/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0766 - accuracy: 0.3009 - val_loss: 0.1338 - val_accuracy: 0.2829\n",
      "Epoch 517/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0766 - accuracy: 0.2895 - val_loss: 0.1330 - val_accuracy: 0.3009\n",
      "Epoch 518/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0764 - accuracy: 0.3015 - val_loss: 0.1346 - val_accuracy: 0.2450\n",
      "Epoch 519/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0765 - accuracy: 0.2913 - val_loss: 0.1324 - val_accuracy: 0.2901\n",
      "Epoch 520/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0763 - accuracy: 0.2979 - val_loss: 0.1322 - val_accuracy: 0.2667\n",
      "Epoch 521/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0762 - accuracy: 0.2925 - val_loss: 0.1331 - val_accuracy: 0.2793\n",
      "Epoch 522/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0761 - accuracy: 0.3027 - val_loss: 0.1441 - val_accuracy: 0.3225\n",
      "Epoch 523/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0765 - accuracy: 0.3027 - val_loss: 0.1520 - val_accuracy: 0.2829\n",
      "Epoch 524/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0769 - accuracy: 0.2991 - val_loss: 0.1348 - val_accuracy: 0.2973\n",
      "Epoch 525/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0759 - accuracy: 0.2913 - val_loss: 0.1336 - val_accuracy: 0.3063\n",
      "Epoch 526/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0760 - accuracy: 0.2961 - val_loss: 0.1370 - val_accuracy: 0.3189\n",
      "Epoch 527/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0760 - accuracy: 0.2943 - val_loss: 0.1321 - val_accuracy: 0.2793\n",
      "Epoch 528/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0757 - accuracy: 0.2973 - val_loss: 0.1322 - val_accuracy: 0.2757\n",
      "Epoch 529/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0756 - accuracy: 0.3039 - val_loss: 0.1324 - val_accuracy: 0.2919\n",
      "Epoch 530/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0757 - accuracy: 0.3021 - val_loss: 0.1339 - val_accuracy: 0.2667\n",
      "Epoch 531/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0754 - accuracy: 0.3015 - val_loss: 0.1314 - val_accuracy: 0.3009\n",
      "Epoch 532/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0753 - accuracy: 0.2991 - val_loss: 0.1337 - val_accuracy: 0.2685\n",
      "Epoch 533/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0755 - accuracy: 0.2937 - val_loss: 0.1375 - val_accuracy: 0.2847\n",
      "Epoch 534/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0754 - accuracy: 0.3027 - val_loss: 0.1392 - val_accuracy: 0.2865\n",
      "Epoch 535/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0754 - accuracy: 0.3021 - val_loss: 0.1335 - val_accuracy: 0.2486\n",
      "Epoch 536/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0755 - accuracy: 0.2985 - val_loss: 0.1324 - val_accuracy: 0.3081\n",
      "Epoch 537/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0748 - accuracy: 0.3021 - val_loss: 0.1327 - val_accuracy: 0.2649\n",
      "Epoch 538/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0749 - accuracy: 0.2991 - val_loss: 0.1327 - val_accuracy: 0.3009\n",
      "Epoch 539/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0749 - accuracy: 0.3009 - val_loss: 0.1311 - val_accuracy: 0.2829\n",
      "Epoch 540/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0749 - accuracy: 0.2937 - val_loss: 0.1379 - val_accuracy: 0.3117\n",
      "Epoch 541/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0752 - accuracy: 0.3087 - val_loss: 0.1508 - val_accuracy: 0.2450\n",
      "Epoch 542/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0757 - accuracy: 0.2991 - val_loss: 0.1320 - val_accuracy: 0.2775\n",
      "Epoch 543/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0745 - accuracy: 0.3057 - val_loss: 0.1331 - val_accuracy: 0.2829\n",
      "Epoch 544/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0745 - accuracy: 0.2979 - val_loss: 0.1324 - val_accuracy: 0.2865\n",
      "Epoch 545/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0747 - accuracy: 0.2973 - val_loss: 0.1339 - val_accuracy: 0.3063\n",
      "Epoch 546/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0745 - accuracy: 0.3033 - val_loss: 0.1298 - val_accuracy: 0.3009\n",
      "Epoch 547/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0742 - accuracy: 0.2991 - val_loss: 0.1310 - val_accuracy: 0.3171\n",
      "Epoch 548/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0742 - accuracy: 0.3105 - val_loss: 0.1382 - val_accuracy: 0.3189\n",
      "Epoch 549/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0745 - accuracy: 0.3027 - val_loss: 0.1286 - val_accuracy: 0.2523\n",
      "Epoch 550/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0743 - accuracy: 0.2997 - val_loss: 0.1331 - val_accuracy: 0.2757\n",
      "Epoch 551/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0740 - accuracy: 0.3057 - val_loss: 0.1330 - val_accuracy: 0.2901\n",
      "Epoch 552/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0739 - accuracy: 0.2955 - val_loss: 0.1405 - val_accuracy: 0.2901\n",
      "Epoch 553/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0744 - accuracy: 0.3081 - val_loss: 0.1323 - val_accuracy: 0.2973\n",
      "Epoch 554/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0738 - accuracy: 0.2997 - val_loss: 0.1394 - val_accuracy: 0.3279\n",
      "Epoch 555/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0740 - accuracy: 0.3129 - val_loss: 0.1683 - val_accuracy: 0.2703\n",
      "Epoch 556/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0749 - accuracy: 0.2967 - val_loss: 0.1297 - val_accuracy: 0.2883\n",
      "Epoch 557/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0736 - accuracy: 0.3051 - val_loss: 0.1314 - val_accuracy: 0.2775\n",
      "Epoch 558/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0736 - accuracy: 0.3003 - val_loss: 0.1316 - val_accuracy: 0.2793\n",
      "Epoch 559/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0736 - accuracy: 0.3039 - val_loss: 0.1329 - val_accuracy: 0.3099\n",
      "Epoch 560/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0736 - accuracy: 0.3099 - val_loss: 0.1280 - val_accuracy: 0.2775\n",
      "Epoch 561/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0736 - accuracy: 0.3087 - val_loss: 0.1297 - val_accuracy: 0.2883\n",
      "Epoch 562/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0732 - accuracy: 0.3129 - val_loss: 0.1374 - val_accuracy: 0.3135\n",
      "Epoch 563/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0735 - accuracy: 0.3195 - val_loss: 0.1306 - val_accuracy: 0.3099\n",
      "Epoch 564/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0732 - accuracy: 0.3027 - val_loss: 0.1314 - val_accuracy: 0.2991\n",
      "Epoch 565/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0732 - accuracy: 0.3093 - val_loss: 0.1333 - val_accuracy: 0.3027\n",
      "Epoch 566/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0730 - accuracy: 0.3111 - val_loss: 0.1322 - val_accuracy: 0.2811\n",
      "Epoch 567/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0729 - accuracy: 0.3087 - val_loss: 0.1392 - val_accuracy: 0.2523\n",
      "Epoch 568/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0734 - accuracy: 0.2997 - val_loss: 0.1318 - val_accuracy: 0.2991\n",
      "Epoch 569/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0728 - accuracy: 0.3045 - val_loss: 0.1328 - val_accuracy: 0.2937\n",
      "Epoch 570/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0727 - accuracy: 0.3081 - val_loss: 0.1371 - val_accuracy: 0.2757\n",
      "Epoch 571/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0727 - accuracy: 0.3075 - val_loss: 0.1299 - val_accuracy: 0.3189\n",
      "Epoch 572/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0726 - accuracy: 0.3099 - val_loss: 0.1287 - val_accuracy: 0.2811\n",
      "Epoch 573/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0727 - accuracy: 0.3039 - val_loss: 0.1290 - val_accuracy: 0.2937\n",
      "Epoch 574/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0725 - accuracy: 0.3021 - val_loss: 0.1303 - val_accuracy: 0.2793\n",
      "Epoch 575/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0724 - accuracy: 0.3099 - val_loss: 0.1289 - val_accuracy: 0.2991\n",
      "Epoch 576/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0724 - accuracy: 0.3051 - val_loss: 0.1602 - val_accuracy: 0.3009\n",
      "Epoch 577/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0737 - accuracy: 0.3039 - val_loss: 0.1378 - val_accuracy: 0.3153\n",
      "Epoch 578/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0725 - accuracy: 0.3147 - val_loss: 0.1372 - val_accuracy: 0.2739\n",
      "Epoch 579/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0723 - accuracy: 0.3105 - val_loss: 0.1343 - val_accuracy: 0.2757\n",
      "Epoch 580/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0722 - accuracy: 0.3087 - val_loss: 0.1291 - val_accuracy: 0.2649\n",
      "Epoch 581/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0719 - accuracy: 0.3069 - val_loss: 0.1295 - val_accuracy: 0.2937\n",
      "Epoch 582/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0719 - accuracy: 0.3081 - val_loss: 0.1297 - val_accuracy: 0.3009\n",
      "Epoch 583/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0717 - accuracy: 0.3117 - val_loss: 0.1298 - val_accuracy: 0.2991\n",
      "Epoch 584/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0718 - accuracy: 0.3087 - val_loss: 0.1352 - val_accuracy: 0.2901\n",
      "Epoch 585/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0718 - accuracy: 0.3099 - val_loss: 0.1298 - val_accuracy: 0.2883\n",
      "Epoch 586/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0715 - accuracy: 0.3099 - val_loss: 0.1325 - val_accuracy: 0.2883\n",
      "Epoch 587/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0716 - accuracy: 0.3123 - val_loss: 0.1368 - val_accuracy: 0.3045\n",
      "Epoch 588/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0716 - accuracy: 0.3165 - val_loss: 0.1272 - val_accuracy: 0.2811\n",
      "Epoch 589/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0714 - accuracy: 0.3093 - val_loss: 0.1338 - val_accuracy: 0.2901\n",
      "Epoch 590/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0714 - accuracy: 0.3135 - val_loss: 0.1297 - val_accuracy: 0.2937\n",
      "Epoch 591/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0714 - accuracy: 0.3087 - val_loss: 0.1336 - val_accuracy: 0.2901\n",
      "Epoch 592/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0714 - accuracy: 0.3213 - val_loss: 0.1322 - val_accuracy: 0.2667\n",
      "Epoch 593/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0713 - accuracy: 0.3021 - val_loss: 0.1302 - val_accuracy: 0.2901\n",
      "Epoch 594/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0712 - accuracy: 0.3135 - val_loss: 0.1311 - val_accuracy: 0.2883\n",
      "Epoch 595/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0710 - accuracy: 0.3159 - val_loss: 0.1909 - val_accuracy: 0.1586\n",
      "Epoch 596/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0729 - accuracy: 0.2991 - val_loss: 0.1331 - val_accuracy: 0.2595\n",
      "Epoch 597/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0713 - accuracy: 0.3099 - val_loss: 0.1294 - val_accuracy: 0.3099\n",
      "Epoch 598/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0707 - accuracy: 0.3117 - val_loss: 0.1321 - val_accuracy: 0.3045\n",
      "Epoch 599/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0708 - accuracy: 0.3129 - val_loss: 0.1283 - val_accuracy: 0.3063\n",
      "Epoch 600/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0705 - accuracy: 0.3165 - val_loss: 0.1390 - val_accuracy: 0.3243\n",
      "Epoch 601/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0712 - accuracy: 0.3087 - val_loss: 0.1333 - val_accuracy: 0.2919\n",
      "Epoch 602/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0706 - accuracy: 0.3117 - val_loss: 0.1297 - val_accuracy: 0.2919\n",
      "Epoch 603/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0704 - accuracy: 0.3081 - val_loss: 0.1291 - val_accuracy: 0.2865\n",
      "Epoch 604/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0703 - accuracy: 0.3123 - val_loss: 0.1310 - val_accuracy: 0.2919\n",
      "Epoch 605/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0702 - accuracy: 0.3159 - val_loss: 0.1297 - val_accuracy: 0.2955\n",
      "Epoch 606/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0703 - accuracy: 0.3159 - val_loss: 0.1292 - val_accuracy: 0.3099\n",
      "Epoch 607/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0700 - accuracy: 0.3177 - val_loss: 0.1277 - val_accuracy: 0.2955\n",
      "Epoch 608/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0701 - accuracy: 0.3111 - val_loss: 0.1310 - val_accuracy: 0.3117\n",
      "Epoch 609/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0701 - accuracy: 0.3237 - val_loss: 0.1311 - val_accuracy: 0.3081\n",
      "Epoch 610/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0701 - accuracy: 0.3159 - val_loss: 0.1358 - val_accuracy: 0.3153\n",
      "Epoch 611/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0701 - accuracy: 0.3165 - val_loss: 0.1318 - val_accuracy: 0.3027\n",
      "Epoch 612/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0698 - accuracy: 0.3147 - val_loss: 0.1315 - val_accuracy: 0.3207\n",
      "Epoch 613/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0699 - accuracy: 0.3153 - val_loss: 0.1278 - val_accuracy: 0.3027\n",
      "Epoch 614/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0696 - accuracy: 0.3153 - val_loss: 0.1362 - val_accuracy: 0.2811\n",
      "Epoch 615/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0701 - accuracy: 0.3195 - val_loss: 0.1277 - val_accuracy: 0.2883\n",
      "Epoch 616/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0694 - accuracy: 0.3081 - val_loss: 0.1398 - val_accuracy: 0.3153\n",
      "Epoch 617/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0702 - accuracy: 0.3135 - val_loss: 0.1287 - val_accuracy: 0.2973\n",
      "Epoch 618/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0696 - accuracy: 0.3141 - val_loss: 0.1395 - val_accuracy: 0.2721\n",
      "Epoch 619/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0703 - accuracy: 0.3153 - val_loss: 0.1488 - val_accuracy: 0.2432\n",
      "Epoch 620/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0702 - accuracy: 0.3117 - val_loss: 0.1305 - val_accuracy: 0.2559\n",
      "Epoch 621/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0695 - accuracy: 0.3159 - val_loss: 0.1273 - val_accuracy: 0.3009\n",
      "Epoch 622/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0695 - accuracy: 0.3153 - val_loss: 0.1303 - val_accuracy: 0.2865\n",
      "Epoch 623/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0691 - accuracy: 0.3225 - val_loss: 0.1281 - val_accuracy: 0.3351\n",
      "Epoch 624/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0695 - accuracy: 0.3159 - val_loss: 0.1286 - val_accuracy: 0.3225\n",
      "Epoch 625/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0692 - accuracy: 0.3189 - val_loss: 0.1293 - val_accuracy: 0.3063\n",
      "Epoch 626/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0690 - accuracy: 0.3153 - val_loss: 0.1282 - val_accuracy: 0.3081\n",
      "Epoch 627/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0689 - accuracy: 0.3219 - val_loss: 0.1267 - val_accuracy: 0.3099\n",
      "Epoch 628/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0689 - accuracy: 0.3165 - val_loss: 0.1286 - val_accuracy: 0.3099\n",
      "Epoch 629/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0689 - accuracy: 0.3201 - val_loss: 0.1268 - val_accuracy: 0.2937\n",
      "Epoch 630/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0688 - accuracy: 0.3195 - val_loss: 0.1294 - val_accuracy: 0.2883\n",
      "Epoch 631/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0688 - accuracy: 0.3207 - val_loss: 0.1300 - val_accuracy: 0.2829\n",
      "Epoch 632/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0688 - accuracy: 0.3135 - val_loss: 0.1288 - val_accuracy: 0.3045\n",
      "Epoch 633/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0685 - accuracy: 0.3231 - val_loss: 0.1487 - val_accuracy: 0.1495\n",
      "Epoch 634/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0695 - accuracy: 0.3171 - val_loss: 0.1289 - val_accuracy: 0.3261\n",
      "Epoch 635/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0684 - accuracy: 0.3207 - val_loss: 0.1286 - val_accuracy: 0.3153\n",
      "Epoch 636/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0682 - accuracy: 0.3219 - val_loss: 0.1286 - val_accuracy: 0.3045\n",
      "Epoch 637/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0683 - accuracy: 0.3195 - val_loss: 0.1373 - val_accuracy: 0.2829\n",
      "Epoch 638/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0685 - accuracy: 0.3195 - val_loss: 0.1345 - val_accuracy: 0.2414\n",
      "Epoch 639/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0687 - accuracy: 0.3117 - val_loss: 0.1276 - val_accuracy: 0.2865\n",
      "Epoch 640/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0680 - accuracy: 0.3231 - val_loss: 0.1335 - val_accuracy: 0.3315\n",
      "Epoch 641/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0683 - accuracy: 0.3183 - val_loss: 0.1268 - val_accuracy: 0.3027\n",
      "Epoch 642/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0678 - accuracy: 0.3213 - val_loss: 0.1474 - val_accuracy: 0.3261\n",
      "Epoch 643/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0690 - accuracy: 0.3123 - val_loss: 0.1276 - val_accuracy: 0.3261\n",
      "Epoch 644/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0678 - accuracy: 0.3231 - val_loss: 0.1292 - val_accuracy: 0.3081\n",
      "Epoch 645/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0678 - accuracy: 0.3231 - val_loss: 0.1289 - val_accuracy: 0.3117\n",
      "Epoch 646/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0677 - accuracy: 0.3177 - val_loss: 0.1280 - val_accuracy: 0.3027\n",
      "Epoch 647/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0675 - accuracy: 0.3237 - val_loss: 0.1352 - val_accuracy: 0.2721\n",
      "Epoch 648/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0682 - accuracy: 0.3135 - val_loss: 0.1276 - val_accuracy: 0.3081\n",
      "Epoch 649/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0674 - accuracy: 0.3207 - val_loss: 0.1353 - val_accuracy: 0.2775\n",
      "Epoch 650/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0678 - accuracy: 0.3219 - val_loss: 0.1444 - val_accuracy: 0.1964\n",
      "Epoch 651/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0686 - accuracy: 0.3153 - val_loss: 0.1289 - val_accuracy: 0.3171\n",
      "Epoch 652/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0675 - accuracy: 0.3243 - val_loss: 0.1281 - val_accuracy: 0.3009\n",
      "Epoch 653/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0674 - accuracy: 0.3219 - val_loss: 0.1272 - val_accuracy: 0.3171\n",
      "Epoch 654/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0672 - accuracy: 0.3309 - val_loss: 0.1326 - val_accuracy: 0.2973\n",
      "Epoch 655/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0673 - accuracy: 0.3255 - val_loss: 0.1263 - val_accuracy: 0.3171\n",
      "Epoch 656/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0671 - accuracy: 0.3255 - val_loss: 0.1282 - val_accuracy: 0.3171\n",
      "Epoch 657/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0672 - accuracy: 0.3273 - val_loss: 0.1263 - val_accuracy: 0.3027\n",
      "Epoch 658/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0671 - accuracy: 0.3135 - val_loss: 0.1254 - val_accuracy: 0.2973\n",
      "Epoch 659/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0668 - accuracy: 0.3225 - val_loss: 0.1271 - val_accuracy: 0.3261\n",
      "Epoch 660/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0668 - accuracy: 0.3207 - val_loss: 0.1338 - val_accuracy: 0.3099\n",
      "Epoch 661/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0671 - accuracy: 0.3207 - val_loss: 0.1253 - val_accuracy: 0.3027\n",
      "Epoch 662/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0668 - accuracy: 0.3237 - val_loss: 0.1299 - val_accuracy: 0.3117\n",
      "Epoch 663/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0669 - accuracy: 0.3321 - val_loss: 0.1294 - val_accuracy: 0.3189\n",
      "Epoch 664/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0667 - accuracy: 0.3273 - val_loss: 0.1247 - val_accuracy: 0.3135\n",
      "Epoch 665/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0666 - accuracy: 0.3207 - val_loss: 0.1282 - val_accuracy: 0.3207\n",
      "Epoch 666/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0666 - accuracy: 0.3183 - val_loss: 0.1255 - val_accuracy: 0.3315\n",
      "Epoch 667/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0664 - accuracy: 0.3267 - val_loss: 0.1332 - val_accuracy: 0.3351\n",
      "Epoch 668/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0670 - accuracy: 0.3243 - val_loss: 0.1266 - val_accuracy: 0.2486\n",
      "Epoch 669/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0666 - accuracy: 0.3123 - val_loss: 0.1245 - val_accuracy: 0.3207\n",
      "Epoch 670/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0664 - accuracy: 0.3327 - val_loss: 0.1267 - val_accuracy: 0.3027\n",
      "Epoch 671/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0662 - accuracy: 0.3261 - val_loss: 0.1376 - val_accuracy: 0.3315\n",
      "Epoch 672/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0667 - accuracy: 0.3207 - val_loss: 0.1255 - val_accuracy: 0.3009\n",
      "Epoch 673/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0660 - accuracy: 0.3225 - val_loss: 0.1412 - val_accuracy: 0.3099\n",
      "Epoch 674/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0668 - accuracy: 0.3219 - val_loss: 0.1261 - val_accuracy: 0.3351\n",
      "Epoch 675/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0658 - accuracy: 0.3249 - val_loss: 0.1269 - val_accuracy: 0.3135\n",
      "Epoch 676/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0659 - accuracy: 0.3297 - val_loss: 0.1251 - val_accuracy: 0.3171\n",
      "Epoch 677/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0657 - accuracy: 0.3231 - val_loss: 0.1244 - val_accuracy: 0.3351\n",
      "Epoch 678/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0659 - accuracy: 0.3273 - val_loss: 0.1292 - val_accuracy: 0.3279\n",
      "Epoch 679/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0659 - accuracy: 0.3231 - val_loss: 0.1278 - val_accuracy: 0.3207\n",
      "Epoch 680/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0655 - accuracy: 0.3321 - val_loss: 0.1264 - val_accuracy: 0.3153\n",
      "Epoch 681/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0658 - accuracy: 0.3261 - val_loss: 0.1308 - val_accuracy: 0.3351\n",
      "Epoch 682/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0657 - accuracy: 0.3285 - val_loss: 0.1284 - val_accuracy: 0.3171\n",
      "Epoch 683/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0655 - accuracy: 0.3165 - val_loss: 0.1253 - val_accuracy: 0.3009\n",
      "Epoch 684/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0656 - accuracy: 0.3111 - val_loss: 0.1458 - val_accuracy: 0.2090\n",
      "Epoch 685/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0663 - accuracy: 0.3195 - val_loss: 0.1284 - val_accuracy: 0.3225\n",
      "Epoch 686/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0654 - accuracy: 0.3315 - val_loss: 0.1413 - val_accuracy: 0.3117\n",
      "Epoch 687/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0661 - accuracy: 0.3237 - val_loss: 0.1321 - val_accuracy: 0.2739\n",
      "Epoch 688/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0653 - accuracy: 0.3273 - val_loss: 0.1273 - val_accuracy: 0.3387\n",
      "Epoch 689/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0652 - accuracy: 0.3189 - val_loss: 0.1242 - val_accuracy: 0.3207\n",
      "Epoch 690/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0652 - accuracy: 0.3363 - val_loss: 0.1471 - val_accuracy: 0.2721\n",
      "Epoch 691/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0664 - accuracy: 0.3243 - val_loss: 0.1303 - val_accuracy: 0.3279\n",
      "Epoch 692/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0651 - accuracy: 0.3297 - val_loss: 0.1587 - val_accuracy: 0.2955\n",
      "Epoch 693/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0668 - accuracy: 0.3357 - val_loss: 0.1359 - val_accuracy: 0.2955\n",
      "Epoch 694/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0652 - accuracy: 0.3285 - val_loss: 0.1292 - val_accuracy: 0.3009\n",
      "Epoch 695/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0650 - accuracy: 0.3201 - val_loss: 0.1284 - val_accuracy: 0.3369\n",
      "Epoch 696/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0650 - accuracy: 0.3237 - val_loss: 0.1454 - val_accuracy: 0.3009\n",
      "Epoch 697/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0661 - accuracy: 0.3243 - val_loss: 0.1282 - val_accuracy: 0.3315\n",
      "Epoch 698/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0648 - accuracy: 0.3219 - val_loss: 0.1258 - val_accuracy: 0.3315\n",
      "Epoch 699/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0647 - accuracy: 0.3279 - val_loss: 0.1320 - val_accuracy: 0.2973\n",
      "Epoch 700/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0655 - accuracy: 0.3249 - val_loss: 0.1266 - val_accuracy: 0.2937\n",
      "Epoch 701/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0649 - accuracy: 0.3291 - val_loss: 0.1251 - val_accuracy: 0.3297\n",
      "Epoch 702/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0645 - accuracy: 0.3309 - val_loss: 0.1237 - val_accuracy: 0.3189\n",
      "Epoch 703/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0643 - accuracy: 0.3321 - val_loss: 0.1244 - val_accuracy: 0.3261\n",
      "Epoch 704/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0643 - accuracy: 0.3225 - val_loss: 0.1391 - val_accuracy: 0.2505\n",
      "Epoch 705/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0650 - accuracy: 0.3243 - val_loss: 0.1257 - val_accuracy: 0.3081\n",
      "Epoch 706/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0642 - accuracy: 0.3201 - val_loss: 0.1274 - val_accuracy: 0.3153\n",
      "Epoch 707/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0643 - accuracy: 0.3261 - val_loss: 0.1340 - val_accuracy: 0.2793\n",
      "Epoch 708/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0647 - accuracy: 0.3315 - val_loss: 0.1243 - val_accuracy: 0.2847\n",
      "Epoch 709/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0641 - accuracy: 0.3195 - val_loss: 0.1245 - val_accuracy: 0.3225\n",
      "Epoch 710/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0641 - accuracy: 0.3273 - val_loss: 0.1239 - val_accuracy: 0.3117\n",
      "Epoch 711/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0640 - accuracy: 0.3309 - val_loss: 0.1275 - val_accuracy: 0.3297\n",
      "Epoch 712/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0640 - accuracy: 0.3333 - val_loss: 0.1236 - val_accuracy: 0.3117\n",
      "Epoch 713/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0642 - accuracy: 0.3225 - val_loss: 0.1238 - val_accuracy: 0.3207\n",
      "Epoch 714/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0637 - accuracy: 0.3273 - val_loss: 0.1266 - val_accuracy: 0.3315\n",
      "Epoch 715/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0637 - accuracy: 0.3327 - val_loss: 0.1247 - val_accuracy: 0.3171\n",
      "Epoch 716/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0636 - accuracy: 0.3399 - val_loss: 0.1243 - val_accuracy: 0.3315\n",
      "Epoch 717/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0637 - accuracy: 0.3261 - val_loss: 0.1206 - val_accuracy: 0.3063\n",
      "Epoch 718/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0636 - accuracy: 0.3309 - val_loss: 0.1224 - val_accuracy: 0.3207\n",
      "Epoch 719/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0634 - accuracy: 0.3279 - val_loss: 0.1245 - val_accuracy: 0.3027\n",
      "Epoch 720/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0633 - accuracy: 0.3309 - val_loss: 0.1240 - val_accuracy: 0.3045\n",
      "Epoch 721/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0635 - accuracy: 0.3273 - val_loss: 0.1253 - val_accuracy: 0.3153\n",
      "Epoch 722/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0633 - accuracy: 0.3327 - val_loss: 0.1276 - val_accuracy: 0.3153\n",
      "Epoch 723/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0634 - accuracy: 0.3201 - val_loss: 0.1248 - val_accuracy: 0.3423\n",
      "Epoch 724/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0631 - accuracy: 0.3321 - val_loss: 0.1257 - val_accuracy: 0.3189\n",
      "Epoch 725/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0630 - accuracy: 0.3267 - val_loss: 0.1254 - val_accuracy: 0.2703\n",
      "Epoch 726/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0632 - accuracy: 0.3285 - val_loss: 0.1279 - val_accuracy: 0.3351\n",
      "Epoch 727/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0636 - accuracy: 0.3429 - val_loss: 0.1262 - val_accuracy: 0.2991\n",
      "Epoch 728/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0630 - accuracy: 0.3213 - val_loss: 0.1228 - val_accuracy: 0.3369\n",
      "Epoch 729/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0631 - accuracy: 0.3363 - val_loss: 0.1268 - val_accuracy: 0.3189\n",
      "Epoch 730/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0629 - accuracy: 0.3357 - val_loss: 0.1225 - val_accuracy: 0.3261\n",
      "Epoch 731/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0626 - accuracy: 0.3315 - val_loss: 0.1546 - val_accuracy: 0.3946\n",
      "Epoch 732/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0645 - accuracy: 0.3273 - val_loss: 0.1257 - val_accuracy: 0.3207\n",
      "Epoch 733/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0626 - accuracy: 0.3273 - val_loss: 0.1284 - val_accuracy: 0.3387\n",
      "Epoch 734/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0629 - accuracy: 0.3423 - val_loss: 0.1255 - val_accuracy: 0.3171\n",
      "Epoch 735/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0628 - accuracy: 0.3333 - val_loss: 0.1250 - val_accuracy: 0.3099\n",
      "Epoch 736/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0626 - accuracy: 0.3255 - val_loss: 0.1256 - val_accuracy: 0.3243\n",
      "Epoch 737/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0624 - accuracy: 0.3261 - val_loss: 0.1257 - val_accuracy: 0.3243\n",
      "Epoch 738/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0628 - accuracy: 0.3327 - val_loss: 0.1272 - val_accuracy: 0.2883\n",
      "Epoch 739/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0626 - accuracy: 0.3357 - val_loss: 0.1249 - val_accuracy: 0.3153\n",
      "Epoch 740/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0625 - accuracy: 0.3297 - val_loss: 0.1387 - val_accuracy: 0.2865\n",
      "Epoch 741/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0633 - accuracy: 0.3309 - val_loss: 0.1243 - val_accuracy: 0.3243\n",
      "Epoch 742/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0622 - accuracy: 0.3309 - val_loss: 0.1234 - val_accuracy: 0.3207\n",
      "Epoch 743/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0621 - accuracy: 0.3255 - val_loss: 0.1228 - val_accuracy: 0.3261\n",
      "Epoch 744/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0622 - accuracy: 0.3243 - val_loss: 0.1235 - val_accuracy: 0.3225\n",
      "Epoch 745/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0623 - accuracy: 0.3309 - val_loss: 0.1243 - val_accuracy: 0.3351\n",
      "Epoch 746/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0623 - accuracy: 0.3315 - val_loss: 0.1224 - val_accuracy: 0.3279\n",
      "Epoch 747/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0621 - accuracy: 0.3237 - val_loss: 0.1239 - val_accuracy: 0.3225\n",
      "Epoch 748/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0621 - accuracy: 0.3333 - val_loss: 0.1262 - val_accuracy: 0.3189\n",
      "Epoch 749/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0618 - accuracy: 0.3345 - val_loss: 0.1250 - val_accuracy: 0.3207\n",
      "Epoch 750/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0621 - accuracy: 0.3363 - val_loss: 0.1279 - val_accuracy: 0.3063\n",
      "Epoch 751/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0621 - accuracy: 0.3285 - val_loss: 0.1505 - val_accuracy: 0.2937\n",
      "Epoch 752/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0632 - accuracy: 0.3309 - val_loss: 0.1261 - val_accuracy: 0.3189\n",
      "Epoch 753/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0618 - accuracy: 0.3279 - val_loss: 0.1232 - val_accuracy: 0.3189\n",
      "Epoch 754/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0616 - accuracy: 0.3345 - val_loss: 0.1234 - val_accuracy: 0.3117\n",
      "Epoch 755/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0614 - accuracy: 0.3315 - val_loss: 0.1295 - val_accuracy: 0.3441\n",
      "Epoch 756/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0619 - accuracy: 0.3327 - val_loss: 0.1239 - val_accuracy: 0.3243\n",
      "Epoch 757/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0615 - accuracy: 0.3303 - val_loss: 0.1264 - val_accuracy: 0.3243\n",
      "Epoch 758/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0616 - accuracy: 0.3285 - val_loss: 0.1263 - val_accuracy: 0.3063\n",
      "Epoch 759/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0619 - accuracy: 0.3261 - val_loss: 0.1256 - val_accuracy: 0.3045\n",
      "Epoch 760/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0615 - accuracy: 0.3297 - val_loss: 0.1206 - val_accuracy: 0.3261\n",
      "Epoch 761/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0616 - accuracy: 0.3285 - val_loss: 0.1227 - val_accuracy: 0.3171\n",
      "Epoch 762/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0612 - accuracy: 0.3279 - val_loss: 0.1258 - val_accuracy: 0.3207\n",
      "Epoch 763/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0614 - accuracy: 0.3429 - val_loss: 0.1251 - val_accuracy: 0.3189\n",
      "Epoch 764/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0612 - accuracy: 0.3327 - val_loss: 0.1257 - val_accuracy: 0.3207\n",
      "Epoch 765/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0616 - accuracy: 0.3387 - val_loss: 0.1232 - val_accuracy: 0.3081\n",
      "Epoch 766/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0614 - accuracy: 0.3315 - val_loss: 0.1229 - val_accuracy: 0.3243\n",
      "Epoch 767/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0610 - accuracy: 0.3303 - val_loss: 0.1270 - val_accuracy: 0.3387\n",
      "Epoch 768/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0613 - accuracy: 0.3279 - val_loss: 0.1271 - val_accuracy: 0.3405\n",
      "Epoch 769/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0610 - accuracy: 0.3327 - val_loss: 0.1209 - val_accuracy: 0.3207\n",
      "Epoch 770/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0610 - accuracy: 0.3363 - val_loss: 0.1250 - val_accuracy: 0.3207\n",
      "Epoch 771/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0610 - accuracy: 0.3315 - val_loss: 0.1250 - val_accuracy: 0.3171\n",
      "Epoch 772/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0611 - accuracy: 0.3369 - val_loss: 0.1241 - val_accuracy: 0.3063\n",
      "Epoch 773/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0610 - accuracy: 0.3303 - val_loss: 0.1343 - val_accuracy: 0.2523\n",
      "Epoch 774/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0614 - accuracy: 0.3213 - val_loss: 0.1304 - val_accuracy: 0.2649\n",
      "Epoch 775/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0611 - accuracy: 0.3243 - val_loss: 0.1244 - val_accuracy: 0.3009\n",
      "Epoch 776/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0607 - accuracy: 0.3219 - val_loss: 0.1243 - val_accuracy: 0.3477\n",
      "Epoch 777/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0608 - accuracy: 0.3255 - val_loss: 0.1267 - val_accuracy: 0.3243\n",
      "Epoch 778/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0606 - accuracy: 0.3357 - val_loss: 0.1269 - val_accuracy: 0.3279\n",
      "Epoch 779/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0605 - accuracy: 0.3375 - val_loss: 0.1291 - val_accuracy: 0.2991\n",
      "Epoch 780/800\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0608 - accuracy: 0.3315 - val_loss: 0.1500 - val_accuracy: 0.1820\n",
      "Epoch 781/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0617 - accuracy: 0.3309 - val_loss: 0.1396 - val_accuracy: 0.3369\n",
      "Epoch 782/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0612 - accuracy: 0.3303 - val_loss: 0.1230 - val_accuracy: 0.3279\n",
      "Epoch 783/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0603 - accuracy: 0.3435 - val_loss: 0.1261 - val_accuracy: 0.3099\n",
      "Epoch 784/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0606 - accuracy: 0.3351 - val_loss: 0.1323 - val_accuracy: 0.3640\n",
      "Epoch 785/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0604 - accuracy: 0.3417 - val_loss: 0.1290 - val_accuracy: 0.3063\n",
      "Epoch 786/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0605 - accuracy: 0.3249 - val_loss: 0.1209 - val_accuracy: 0.3135\n",
      "Epoch 787/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0600 - accuracy: 0.3381 - val_loss: 0.1392 - val_accuracy: 0.3658\n",
      "Epoch 788/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0609 - accuracy: 0.3411 - val_loss: 0.1236 - val_accuracy: 0.3387\n",
      "Epoch 789/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0601 - accuracy: 0.3357 - val_loss: 0.1242 - val_accuracy: 0.3063\n",
      "Epoch 790/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0602 - accuracy: 0.3333 - val_loss: 0.1223 - val_accuracy: 0.2937\n",
      "Epoch 791/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0599 - accuracy: 0.3363 - val_loss: 0.1290 - val_accuracy: 0.2919\n",
      "Epoch 792/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0604 - accuracy: 0.3345 - val_loss: 0.1267 - val_accuracy: 0.3225\n",
      "Epoch 793/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0600 - accuracy: 0.3327 - val_loss: 0.1273 - val_accuracy: 0.3135\n",
      "Epoch 794/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0600 - accuracy: 0.3315 - val_loss: 0.1224 - val_accuracy: 0.3261\n",
      "Epoch 795/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0597 - accuracy: 0.3363 - val_loss: 0.1261 - val_accuracy: 0.2811\n",
      "Epoch 796/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0603 - accuracy: 0.3327 - val_loss: 0.1246 - val_accuracy: 0.2901\n",
      "Epoch 797/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0599 - accuracy: 0.3357 - val_loss: 0.1243 - val_accuracy: 0.3351\n",
      "Epoch 798/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0594 - accuracy: 0.3363 - val_loss: 0.1311 - val_accuracy: 0.3027\n",
      "Epoch 799/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0602 - accuracy: 0.3267 - val_loss: 0.1254 - val_accuracy: 0.3351\n",
      "Epoch 800/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0597 - accuracy: 0.3393 - val_loss: 0.1223 - val_accuracy: 0.3009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-32-f01cf9afd9ce>:131: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "  tprs.append(interp(mean_fpr, fpr, tpr))\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:806: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Epoch 1/800\n",
      "53/53 [==============================] - 1s 6ms/step - loss: 0.6354 - accuracy: 0.0306 - val_loss: 0.5378 - val_accuracy: 0.0432\n",
      "Epoch 2/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.3014 - accuracy: 0.0348 - val_loss: 0.2540 - val_accuracy: 0.0090\n",
      "Epoch 3/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1906 - accuracy: 0.0312 - val_loss: 0.2510 - val_accuracy: 0.0594\n",
      "Epoch 4/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1878 - accuracy: 0.0535 - val_loss: 0.2498 - val_accuracy: 0.0342\n",
      "Epoch 5/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1870 - accuracy: 0.0366 - val_loss: 0.2492 - val_accuracy: 0.0414\n",
      "Epoch 6/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1867 - accuracy: 0.0234 - val_loss: 0.2487 - val_accuracy: 0.0396\n",
      "Epoch 7/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1865 - accuracy: 0.0312 - val_loss: 0.2489 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1864 - accuracy: 0.0150 - val_loss: 0.2492 - val_accuracy: 0.0414\n",
      "Epoch 9/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1863 - accuracy: 0.0150 - val_loss: 0.2490 - val_accuracy: 0.0324\n",
      "Epoch 10/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1863 - accuracy: 0.0324 - val_loss: 0.2486 - val_accuracy: 0.0432\n",
      "Epoch 11/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1862 - accuracy: 0.0384 - val_loss: 0.2484 - val_accuracy: 0.0450\n",
      "Epoch 12/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1862 - accuracy: 0.0384 - val_loss: 0.2486 - val_accuracy: 0.0432\n",
      "Epoch 13/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1861 - accuracy: 0.0420 - val_loss: 0.2484 - val_accuracy: 0.0288\n",
      "Epoch 14/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1859 - accuracy: 0.0204 - val_loss: 0.2479 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1858 - accuracy: 0.0180 - val_loss: 0.2480 - val_accuracy: 0.0342\n",
      "Epoch 16/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1858 - accuracy: 0.0336 - val_loss: 0.2483 - val_accuracy: 0.0414\n",
      "Epoch 17/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1858 - accuracy: 0.0306 - val_loss: 0.2484 - val_accuracy: 0.0468\n",
      "Epoch 18/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1857 - accuracy: 0.0523 - val_loss: 0.2484 - val_accuracy: 0.0396\n",
      "Epoch 19/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1856 - accuracy: 0.0426 - val_loss: 0.2480 - val_accuracy: 0.0755\n",
      "Epoch 20/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1856 - accuracy: 0.0625 - val_loss: 0.2476 - val_accuracy: 0.0018\n",
      "Epoch 21/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1855 - accuracy: 0.0517 - val_loss: 0.2471 - val_accuracy: 0.0270\n",
      "Epoch 22/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1854 - accuracy: 0.0306 - val_loss: 0.2475 - val_accuracy: 0.0755\n",
      "Epoch 23/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1854 - accuracy: 0.0438 - val_loss: 0.2476 - val_accuracy: 0.0683\n",
      "Epoch 24/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1853 - accuracy: 0.0589 - val_loss: 0.2473 - val_accuracy: 0.0306\n",
      "Epoch 25/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1852 - accuracy: 0.0408 - val_loss: 0.2475 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1851 - accuracy: 0.0216 - val_loss: 0.2471 - val_accuracy: 0.0288\n",
      "Epoch 27/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1851 - accuracy: 0.0456 - val_loss: 0.2469 - val_accuracy: 0.0558\n",
      "Epoch 28/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1850 - accuracy: 0.0553 - val_loss: 0.2471 - val_accuracy: 0.0144\n",
      "Epoch 29/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1849 - accuracy: 0.0330 - val_loss: 0.2464 - val_accuracy: 0.0665\n",
      "Epoch 30/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1849 - accuracy: 0.0565 - val_loss: 0.2468 - val_accuracy: 0.0072\n",
      "Epoch 31/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1848 - accuracy: 0.0366 - val_loss: 0.2466 - val_accuracy: 0.0594\n",
      "Epoch 32/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1847 - accuracy: 0.0486 - val_loss: 0.2467 - val_accuracy: 0.0522\n",
      "Epoch 33/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1847 - accuracy: 0.0450 - val_loss: 0.2461 - val_accuracy: 0.0935\n",
      "Epoch 34/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1845 - accuracy: 0.0859 - val_loss: 0.2463 - val_accuracy: 0.0396\n",
      "Epoch 35/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1845 - accuracy: 0.0565 - val_loss: 0.2463 - val_accuracy: 0.0486\n",
      "Epoch 36/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1844 - accuracy: 0.0498 - val_loss: 0.2461 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1843 - accuracy: 0.0336 - val_loss: 0.2457 - val_accuracy: 0.0378\n",
      "Epoch 38/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1842 - accuracy: 0.0462 - val_loss: 0.2461 - val_accuracy: 0.0486\n",
      "Epoch 39/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1841 - accuracy: 0.0703 - val_loss: 0.2454 - val_accuracy: 0.0504\n",
      "Epoch 40/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1840 - accuracy: 0.0793 - val_loss: 0.2454 - val_accuracy: 0.0288\n",
      "Epoch 41/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1839 - accuracy: 0.0523 - val_loss: 0.2458 - val_accuracy: 0.0288\n",
      "Epoch 42/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1838 - accuracy: 0.0324 - val_loss: 0.2453 - val_accuracy: 0.0360\n",
      "Epoch 43/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1838 - accuracy: 0.0535 - val_loss: 0.2454 - val_accuracy: 0.0432\n",
      "Epoch 44/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1837 - accuracy: 0.0697 - val_loss: 0.2455 - val_accuracy: 0.0432\n",
      "Epoch 45/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1836 - accuracy: 0.0673 - val_loss: 0.2453 - val_accuracy: 0.0450\n",
      "Epoch 46/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1834 - accuracy: 0.0685 - val_loss: 0.2451 - val_accuracy: 0.0288\n",
      "Epoch 47/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1833 - accuracy: 0.0703 - val_loss: 0.2442 - val_accuracy: 0.0360\n",
      "Epoch 48/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1832 - accuracy: 0.0763 - val_loss: 0.2446 - val_accuracy: 0.0036\n",
      "Epoch 49/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1831 - accuracy: 0.0480 - val_loss: 0.2439 - val_accuracy: 0.0737\n",
      "Epoch 50/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1829 - accuracy: 0.0865 - val_loss: 0.2436 - val_accuracy: 0.0450\n",
      "Epoch 51/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1829 - accuracy: 0.0553 - val_loss: 0.2441 - val_accuracy: 0.0701\n",
      "Epoch 52/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1828 - accuracy: 0.0781 - val_loss: 0.2434 - val_accuracy: 0.0306\n",
      "Epoch 53/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1826 - accuracy: 0.1057 - val_loss: 0.2435 - val_accuracy: 0.0468\n",
      "Epoch 54/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1825 - accuracy: 0.0895 - val_loss: 0.2429 - val_accuracy: 0.0522\n",
      "Epoch 55/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1824 - accuracy: 0.0907 - val_loss: 0.2432 - val_accuracy: 0.0144\n",
      "Epoch 56/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1822 - accuracy: 0.0727 - val_loss: 0.2426 - val_accuracy: 0.0414\n",
      "Epoch 57/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1821 - accuracy: 0.1015 - val_loss: 0.2427 - val_accuracy: 0.0306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1820 - accuracy: 0.0727 - val_loss: 0.2420 - val_accuracy: 0.0342\n",
      "Epoch 59/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1818 - accuracy: 0.0835 - val_loss: 0.2424 - val_accuracy: 0.0935\n",
      "Epoch 60/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1817 - accuracy: 0.1147 - val_loss: 0.2419 - val_accuracy: 0.0036\n",
      "Epoch 61/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1815 - accuracy: 0.0817 - val_loss: 0.2417 - val_accuracy: 0.0522\n",
      "Epoch 62/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1814 - accuracy: 0.0859 - val_loss: 0.2418 - val_accuracy: 0.0306\n",
      "Epoch 63/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1812 - accuracy: 0.0913 - val_loss: 0.2419 - val_accuracy: 0.0737\n",
      "Epoch 64/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1810 - accuracy: 0.1021 - val_loss: 0.2417 - val_accuracy: 0.0234\n",
      "Epoch 65/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1808 - accuracy: 0.0937 - val_loss: 0.2409 - val_accuracy: 0.0971\n",
      "Epoch 66/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1807 - accuracy: 0.1279 - val_loss: 0.2403 - val_accuracy: 0.0845\n",
      "Epoch 67/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1805 - accuracy: 0.1255 - val_loss: 0.2403 - val_accuracy: 0.0827\n",
      "Epoch 68/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1802 - accuracy: 0.1375 - val_loss: 0.2396 - val_accuracy: 0.2248\n",
      "Epoch 69/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1800 - accuracy: 0.1562 - val_loss: 0.2400 - val_accuracy: 0.0773\n",
      "Epoch 70/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1799 - accuracy: 0.1033 - val_loss: 0.2392 - val_accuracy: 0.0917\n",
      "Epoch 71/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1796 - accuracy: 0.1159 - val_loss: 0.2387 - val_accuracy: 0.0809\n",
      "Epoch 72/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1794 - accuracy: 0.1345 - val_loss: 0.2382 - val_accuracy: 0.1133\n",
      "Epoch 73/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1792 - accuracy: 0.1532 - val_loss: 0.2382 - val_accuracy: 0.0935\n",
      "Epoch 74/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1790 - accuracy: 0.1279 - val_loss: 0.2383 - val_accuracy: 0.1349\n",
      "Epoch 75/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1786 - accuracy: 0.1459 - val_loss: 0.2380 - val_accuracy: 0.1025\n",
      "Epoch 76/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1784 - accuracy: 0.1435 - val_loss: 0.2377 - val_accuracy: 0.0989\n",
      "Epoch 77/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1782 - accuracy: 0.1423 - val_loss: 0.2364 - val_accuracy: 0.1079\n",
      "Epoch 78/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1779 - accuracy: 0.1309 - val_loss: 0.2363 - val_accuracy: 0.1565\n",
      "Epoch 79/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1776 - accuracy: 0.1622 - val_loss: 0.2360 - val_accuracy: 0.1439\n",
      "Epoch 80/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1773 - accuracy: 0.1526 - val_loss: 0.2353 - val_accuracy: 0.1475\n",
      "Epoch 81/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1770 - accuracy: 0.1520 - val_loss: 0.2345 - val_accuracy: 0.1367\n",
      "Epoch 82/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1766 - accuracy: 0.1477 - val_loss: 0.2340 - val_accuracy: 0.0773\n",
      "Epoch 83/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1763 - accuracy: 0.1273 - val_loss: 0.2341 - val_accuracy: 0.1115\n",
      "Epoch 84/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1759 - accuracy: 0.1417 - val_loss: 0.2341 - val_accuracy: 0.0881\n",
      "Epoch 85/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1755 - accuracy: 0.1255 - val_loss: 0.2329 - val_accuracy: 0.1637\n",
      "Epoch 86/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1750 - accuracy: 0.1405 - val_loss: 0.2327 - val_accuracy: 0.1547\n",
      "Epoch 87/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1747 - accuracy: 0.1538 - val_loss: 0.2314 - val_accuracy: 0.1583\n",
      "Epoch 88/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1742 - accuracy: 0.1514 - val_loss: 0.2315 - val_accuracy: 0.1421\n",
      "Epoch 89/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1738 - accuracy: 0.1502 - val_loss: 0.2302 - val_accuracy: 0.1475\n",
      "Epoch 90/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1733 - accuracy: 0.1387 - val_loss: 0.2293 - val_accuracy: 0.1906\n",
      "Epoch 91/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1728 - accuracy: 0.1676 - val_loss: 0.2285 - val_accuracy: 0.1475\n",
      "Epoch 92/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1723 - accuracy: 0.1441 - val_loss: 0.2281 - val_accuracy: 0.2014\n",
      "Epoch 93/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1718 - accuracy: 0.1634 - val_loss: 0.2273 - val_accuracy: 0.2158\n",
      "Epoch 94/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1713 - accuracy: 0.1742 - val_loss: 0.2264 - val_accuracy: 0.1151\n",
      "Epoch 95/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1707 - accuracy: 0.1477 - val_loss: 0.2254 - val_accuracy: 0.2068\n",
      "Epoch 96/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1701 - accuracy: 0.1856 - val_loss: 0.2253 - val_accuracy: 0.1835\n",
      "Epoch 97/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1695 - accuracy: 0.1664 - val_loss: 0.2240 - val_accuracy: 0.1727\n",
      "Epoch 98/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1688 - accuracy: 0.1640 - val_loss: 0.2237 - val_accuracy: 0.1583\n",
      "Epoch 99/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1682 - accuracy: 0.1562 - val_loss: 0.2211 - val_accuracy: 0.2248\n",
      "Epoch 100/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1676 - accuracy: 0.1814 - val_loss: 0.2216 - val_accuracy: 0.1709\n",
      "Epoch 101/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1670 - accuracy: 0.1820 - val_loss: 0.2201 - val_accuracy: 0.1673\n",
      "Epoch 102/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1663 - accuracy: 0.1634 - val_loss: 0.2197 - val_accuracy: 0.1888\n",
      "Epoch 103/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1655 - accuracy: 0.1694 - val_loss: 0.2186 - val_accuracy: 0.1421\n",
      "Epoch 104/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1649 - accuracy: 0.1760 - val_loss: 0.2169 - val_accuracy: 0.2050\n",
      "Epoch 105/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1641 - accuracy: 0.1868 - val_loss: 0.2169 - val_accuracy: 0.1763\n",
      "Epoch 106/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1635 - accuracy: 0.1796 - val_loss: 0.2158 - val_accuracy: 0.2158\n",
      "Epoch 107/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1626 - accuracy: 0.1778 - val_loss: 0.2137 - val_accuracy: 0.2482\n",
      "Epoch 108/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1619 - accuracy: 0.1982 - val_loss: 0.2136 - val_accuracy: 0.2032\n",
      "Epoch 109/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1612 - accuracy: 0.1988 - val_loss: 0.2122 - val_accuracy: 0.2248\n",
      "Epoch 110/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1603 - accuracy: 0.1988 - val_loss: 0.2111 - val_accuracy: 0.2536\n",
      "Epoch 111/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1595 - accuracy: 0.1904 - val_loss: 0.2099 - val_accuracy: 0.2356\n",
      "Epoch 112/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1588 - accuracy: 0.1916 - val_loss: 0.2086 - val_accuracy: 0.1960\n",
      "Epoch 113/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1579 - accuracy: 0.1946 - val_loss: 0.2082 - val_accuracy: 0.2338\n",
      "Epoch 114/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1573 - accuracy: 0.1964 - val_loss: 0.2071 - val_accuracy: 0.2284\n",
      "Epoch 115/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1564 - accuracy: 0.1982 - val_loss: 0.2049 - val_accuracy: 0.2392\n",
      "Epoch 116/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1556 - accuracy: 0.2060 - val_loss: 0.2056 - val_accuracy: 0.2176\n",
      "Epoch 117/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1549 - accuracy: 0.2054 - val_loss: 0.2030 - val_accuracy: 0.2338\n",
      "Epoch 118/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1541 - accuracy: 0.2108 - val_loss: 0.2022 - val_accuracy: 0.2410\n",
      "Epoch 119/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1533 - accuracy: 0.2114 - val_loss: 0.2012 - val_accuracy: 0.2266\n",
      "Epoch 120/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1525 - accuracy: 0.2012 - val_loss: 0.2001 - val_accuracy: 0.2212\n",
      "Epoch 121/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1518 - accuracy: 0.2048 - val_loss: 0.1997 - val_accuracy: 0.2338\n",
      "Epoch 122/800\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1511 - accuracy: 0.2246 - val_loss: 0.1974 - val_accuracy: 0.2644\n",
      "Epoch 123/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1503 - accuracy: 0.2192 - val_loss: 0.1970 - val_accuracy: 0.2950\n",
      "Epoch 124/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1496 - accuracy: 0.2144 - val_loss: 0.1959 - val_accuracy: 0.3183\n",
      "Epoch 125/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1488 - accuracy: 0.2282 - val_loss: 0.1936 - val_accuracy: 0.2734\n",
      "Epoch 126/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1482 - accuracy: 0.2108 - val_loss: 0.1936 - val_accuracy: 0.2320\n",
      "Epoch 127/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1475 - accuracy: 0.2210 - val_loss: 0.1925 - val_accuracy: 0.2842\n",
      "Epoch 128/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1469 - accuracy: 0.2162 - val_loss: 0.1912 - val_accuracy: 0.2284\n",
      "Epoch 129/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1461 - accuracy: 0.2234 - val_loss: 0.1915 - val_accuracy: 0.2626\n",
      "Epoch 130/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1453 - accuracy: 0.2204 - val_loss: 0.1908 - val_accuracy: 0.2392\n",
      "Epoch 131/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1448 - accuracy: 0.2090 - val_loss: 0.1888 - val_accuracy: 0.2860\n",
      "Epoch 132/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1440 - accuracy: 0.2192 - val_loss: 0.1891 - val_accuracy: 0.2698\n",
      "Epoch 133/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1434 - accuracy: 0.2168 - val_loss: 0.1896 - val_accuracy: 0.2518\n",
      "Epoch 134/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1429 - accuracy: 0.2198 - val_loss: 0.1861 - val_accuracy: 0.2842\n",
      "Epoch 135/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1422 - accuracy: 0.2330 - val_loss: 0.1848 - val_accuracy: 0.3345\n",
      "Epoch 136/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1415 - accuracy: 0.2438 - val_loss: 0.1888 - val_accuracy: 0.2284\n",
      "Epoch 137/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1410 - accuracy: 0.2288 - val_loss: 0.1838 - val_accuracy: 0.3129\n",
      "Epoch 138/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1403 - accuracy: 0.2306 - val_loss: 0.1824 - val_accuracy: 0.2824\n",
      "Epoch 139/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1396 - accuracy: 0.2505 - val_loss: 0.1817 - val_accuracy: 0.2698\n",
      "Epoch 140/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1391 - accuracy: 0.2324 - val_loss: 0.1813 - val_accuracy: 0.2788\n",
      "Epoch 141/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1384 - accuracy: 0.2312 - val_loss: 0.1807 - val_accuracy: 0.3309\n",
      "Epoch 142/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1380 - accuracy: 0.2372 - val_loss: 0.1795 - val_accuracy: 0.2806\n",
      "Epoch 143/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1373 - accuracy: 0.2306 - val_loss: 0.1787 - val_accuracy: 0.3004\n",
      "Epoch 144/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1366 - accuracy: 0.2330 - val_loss: 0.1795 - val_accuracy: 0.2770\n",
      "Epoch 145/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1361 - accuracy: 0.2288 - val_loss: 0.1786 - val_accuracy: 0.3004\n",
      "Epoch 146/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1356 - accuracy: 0.2378 - val_loss: 0.1767 - val_accuracy: 0.3237\n",
      "Epoch 147/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1350 - accuracy: 0.2450 - val_loss: 0.1757 - val_accuracy: 0.3040\n",
      "Epoch 148/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1345 - accuracy: 0.2384 - val_loss: 0.1732 - val_accuracy: 0.3129\n",
      "Epoch 149/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1339 - accuracy: 0.2492 - val_loss: 0.1733 - val_accuracy: 0.3363\n",
      "Epoch 150/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1333 - accuracy: 0.2541 - val_loss: 0.1742 - val_accuracy: 0.2986\n",
      "Epoch 151/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1328 - accuracy: 0.2390 - val_loss: 0.1757 - val_accuracy: 0.2428\n",
      "Epoch 152/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1325 - accuracy: 0.2366 - val_loss: 0.1714 - val_accuracy: 0.3076\n",
      "Epoch 153/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1317 - accuracy: 0.2492 - val_loss: 0.1715 - val_accuracy: 0.3399\n",
      "Epoch 154/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1311 - accuracy: 0.2450 - val_loss: 0.1709 - val_accuracy: 0.3201\n",
      "Epoch 155/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1306 - accuracy: 0.2553 - val_loss: 0.1699 - val_accuracy: 0.3363\n",
      "Epoch 156/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1301 - accuracy: 0.2529 - val_loss: 0.1691 - val_accuracy: 0.3147\n",
      "Epoch 157/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1296 - accuracy: 0.2468 - val_loss: 0.1705 - val_accuracy: 0.3183\n",
      "Epoch 158/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1293 - accuracy: 0.2649 - val_loss: 0.1688 - val_accuracy: 0.3094\n",
      "Epoch 159/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1286 - accuracy: 0.2517 - val_loss: 0.1688 - val_accuracy: 0.3040\n",
      "Epoch 160/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1282 - accuracy: 0.2571 - val_loss: 0.1660 - val_accuracy: 0.3255\n",
      "Epoch 161/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1275 - accuracy: 0.2583 - val_loss: 0.1650 - val_accuracy: 0.3363\n",
      "Epoch 162/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1272 - accuracy: 0.2667 - val_loss: 0.1656 - val_accuracy: 0.3561\n",
      "Epoch 163/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1268 - accuracy: 0.2727 - val_loss: 0.1657 - val_accuracy: 0.3597\n",
      "Epoch 164/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1262 - accuracy: 0.2697 - val_loss: 0.1647 - val_accuracy: 0.3022\n",
      "Epoch 165/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1258 - accuracy: 0.2691 - val_loss: 0.1642 - val_accuracy: 0.3543\n",
      "Epoch 166/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1253 - accuracy: 0.2637 - val_loss: 0.1696 - val_accuracy: 0.3058\n",
      "Epoch 167/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1250 - accuracy: 0.2595 - val_loss: 0.1625 - val_accuracy: 0.3363\n",
      "Epoch 168/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1244 - accuracy: 0.2679 - val_loss: 0.1636 - val_accuracy: 0.3273\n",
      "Epoch 169/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1241 - accuracy: 0.2607 - val_loss: 0.1624 - val_accuracy: 0.3183\n",
      "Epoch 170/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1237 - accuracy: 0.2709 - val_loss: 0.1609 - val_accuracy: 0.3363\n",
      "Epoch 171/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1232 - accuracy: 0.2697 - val_loss: 0.1615 - val_accuracy: 0.3345\n",
      "Epoch 172/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1228 - accuracy: 0.2811 - val_loss: 0.1639 - val_accuracy: 0.3273\n",
      "Epoch 173/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1224 - accuracy: 0.2775 - val_loss: 0.1603 - val_accuracy: 0.3795\n",
      "Epoch 174/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1220 - accuracy: 0.2781 - val_loss: 0.1605 - val_accuracy: 0.3687\n",
      "Epoch 175/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1216 - accuracy: 0.2691 - val_loss: 0.1601 - val_accuracy: 0.3471\n",
      "Epoch 176/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1213 - accuracy: 0.2757 - val_loss: 0.1588 - val_accuracy: 0.3345\n",
      "Epoch 177/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1210 - accuracy: 0.2739 - val_loss: 0.1588 - val_accuracy: 0.3381\n",
      "Epoch 178/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1206 - accuracy: 0.2727 - val_loss: 0.1589 - val_accuracy: 0.3651\n",
      "Epoch 179/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1203 - accuracy: 0.2793 - val_loss: 0.1582 - val_accuracy: 0.3705\n",
      "Epoch 180/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1200 - accuracy: 0.2751 - val_loss: 0.1592 - val_accuracy: 0.3147\n",
      "Epoch 181/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1196 - accuracy: 0.2799 - val_loss: 0.1576 - val_accuracy: 0.3471\n",
      "Epoch 182/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1193 - accuracy: 0.2847 - val_loss: 0.1571 - val_accuracy: 0.3255\n",
      "Epoch 183/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1191 - accuracy: 0.2799 - val_loss: 0.1590 - val_accuracy: 0.3399\n",
      "Epoch 184/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1189 - accuracy: 0.2871 - val_loss: 0.1586 - val_accuracy: 0.3615\n",
      "Epoch 185/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1186 - accuracy: 0.2763 - val_loss: 0.1588 - val_accuracy: 0.3597\n",
      "Epoch 186/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1182 - accuracy: 0.2841 - val_loss: 0.1553 - val_accuracy: 0.3525\n",
      "Epoch 187/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1178 - accuracy: 0.2835 - val_loss: 0.1558 - val_accuracy: 0.3741\n",
      "Epoch 188/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1176 - accuracy: 0.2757 - val_loss: 0.1576 - val_accuracy: 0.3633\n",
      "Epoch 189/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1174 - accuracy: 0.2895 - val_loss: 0.1544 - val_accuracy: 0.3417\n",
      "Epoch 190/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1170 - accuracy: 0.2871 - val_loss: 0.1602 - val_accuracy: 0.2770\n",
      "Epoch 191/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1171 - accuracy: 0.2805 - val_loss: 0.1552 - val_accuracy: 0.3651\n",
      "Epoch 192/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1167 - accuracy: 0.2787 - val_loss: 0.1642 - val_accuracy: 0.3076\n",
      "Epoch 193/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1168 - accuracy: 0.2817 - val_loss: 0.1555 - val_accuracy: 0.3723\n",
      "Epoch 194/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1161 - accuracy: 0.2865 - val_loss: 0.1551 - val_accuracy: 0.3399\n",
      "Epoch 195/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1159 - accuracy: 0.2853 - val_loss: 0.1559 - val_accuracy: 0.3669\n",
      "Epoch 196/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1156 - accuracy: 0.2829 - val_loss: 0.1534 - val_accuracy: 0.3633\n",
      "Epoch 197/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1153 - accuracy: 0.2829 - val_loss: 0.1534 - val_accuracy: 0.3651\n",
      "Epoch 198/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1151 - accuracy: 0.2835 - val_loss: 0.1530 - val_accuracy: 0.3741\n",
      "Epoch 199/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1148 - accuracy: 0.2841 - val_loss: 0.1540 - val_accuracy: 0.3129\n",
      "Epoch 200/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1147 - accuracy: 0.2835 - val_loss: 0.1516 - val_accuracy: 0.3489\n",
      "Epoch 201/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1144 - accuracy: 0.2823 - val_loss: 0.1540 - val_accuracy: 0.3885\n",
      "Epoch 202/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1142 - accuracy: 0.2913 - val_loss: 0.1525 - val_accuracy: 0.3417\n",
      "Epoch 203/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1140 - accuracy: 0.2829 - val_loss: 0.1565 - val_accuracy: 0.3669\n",
      "Epoch 204/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1139 - accuracy: 0.2745 - val_loss: 0.1553 - val_accuracy: 0.3813\n",
      "Epoch 205/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1137 - accuracy: 0.2787 - val_loss: 0.1521 - val_accuracy: 0.3507\n",
      "Epoch 206/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1134 - accuracy: 0.2847 - val_loss: 0.1529 - val_accuracy: 0.3489\n",
      "Epoch 207/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1132 - accuracy: 0.2937 - val_loss: 0.1581 - val_accuracy: 0.3004\n",
      "Epoch 208/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1133 - accuracy: 0.2799 - val_loss: 0.1520 - val_accuracy: 0.3327\n",
      "Epoch 209/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1128 - accuracy: 0.2877 - val_loss: 0.1513 - val_accuracy: 0.3687\n",
      "Epoch 210/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1125 - accuracy: 0.2949 - val_loss: 0.1509 - val_accuracy: 0.3453\n",
      "Epoch 211/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1122 - accuracy: 0.2853 - val_loss: 0.1532 - val_accuracy: 0.3579\n",
      "Epoch 212/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1123 - accuracy: 0.2781 - val_loss: 0.1498 - val_accuracy: 0.3489\n",
      "Epoch 213/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1118 - accuracy: 0.2895 - val_loss: 0.1522 - val_accuracy: 0.3399\n",
      "Epoch 214/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1117 - accuracy: 0.2811 - val_loss: 0.1501 - val_accuracy: 0.3381\n",
      "Epoch 215/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1114 - accuracy: 0.2805 - val_loss: 0.1581 - val_accuracy: 0.3237\n",
      "Epoch 216/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1117 - accuracy: 0.2913 - val_loss: 0.1498 - val_accuracy: 0.3525\n",
      "Epoch 217/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1110 - accuracy: 0.2847 - val_loss: 0.1609 - val_accuracy: 0.3399\n",
      "Epoch 218/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1116 - accuracy: 0.2853 - val_loss: 0.1495 - val_accuracy: 0.3489\n",
      "Epoch 219/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1106 - accuracy: 0.2823 - val_loss: 0.1499 - val_accuracy: 0.3453\n",
      "Epoch 220/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1105 - accuracy: 0.2799 - val_loss: 0.1495 - val_accuracy: 0.3561\n",
      "Epoch 221/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1102 - accuracy: 0.2919 - val_loss: 0.1491 - val_accuracy: 0.3615\n",
      "Epoch 222/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1099 - accuracy: 0.2871 - val_loss: 0.1501 - val_accuracy: 0.3561\n",
      "Epoch 223/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1099 - accuracy: 0.2961 - val_loss: 0.1509 - val_accuracy: 0.3543\n",
      "Epoch 224/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1097 - accuracy: 0.2841 - val_loss: 0.1501 - val_accuracy: 0.3471\n",
      "Epoch 225/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1095 - accuracy: 0.2847 - val_loss: 0.1514 - val_accuracy: 0.3219\n",
      "Epoch 226/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1095 - accuracy: 0.2793 - val_loss: 0.1566 - val_accuracy: 0.3219\n",
      "Epoch 227/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1095 - accuracy: 0.2841 - val_loss: 0.1491 - val_accuracy: 0.3291\n",
      "Epoch 228/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1088 - accuracy: 0.2823 - val_loss: 0.1472 - val_accuracy: 0.3435\n",
      "Epoch 229/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1085 - accuracy: 0.2883 - val_loss: 0.1500 - val_accuracy: 0.3381\n",
      "Epoch 230/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1086 - accuracy: 0.2859 - val_loss: 0.1486 - val_accuracy: 0.3669\n",
      "Epoch 231/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1083 - accuracy: 0.3051 - val_loss: 0.1477 - val_accuracy: 0.3327\n",
      "Epoch 232/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1080 - accuracy: 0.2937 - val_loss: 0.1520 - val_accuracy: 0.3489\n",
      "Epoch 233/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1081 - accuracy: 0.2925 - val_loss: 0.1480 - val_accuracy: 0.3417\n",
      "Epoch 234/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1077 - accuracy: 0.2913 - val_loss: 0.1471 - val_accuracy: 0.3309\n",
      "Epoch 235/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1074 - accuracy: 0.2961 - val_loss: 0.1464 - val_accuracy: 0.3453\n",
      "Epoch 236/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1071 - accuracy: 0.2919 - val_loss: 0.1467 - val_accuracy: 0.3327\n",
      "Epoch 237/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1071 - accuracy: 0.2925 - val_loss: 0.1465 - val_accuracy: 0.3255\n",
      "Epoch 238/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1069 - accuracy: 0.2919 - val_loss: 0.1469 - val_accuracy: 0.3345\n",
      "Epoch 239/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1066 - accuracy: 0.2973 - val_loss: 0.1467 - val_accuracy: 0.3291\n",
      "Epoch 240/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1065 - accuracy: 0.2931 - val_loss: 0.1473 - val_accuracy: 0.3507\n",
      "Epoch 241/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1063 - accuracy: 0.2835 - val_loss: 0.1493 - val_accuracy: 0.3309\n",
      "Epoch 242/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1063 - accuracy: 0.3009 - val_loss: 0.1458 - val_accuracy: 0.3022\n",
      "Epoch 243/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1058 - accuracy: 0.2949 - val_loss: 0.1464 - val_accuracy: 0.3633\n",
      "Epoch 244/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1056 - accuracy: 0.2955 - val_loss: 0.1451 - val_accuracy: 0.3507\n",
      "Epoch 245/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1054 - accuracy: 0.3003 - val_loss: 0.1463 - val_accuracy: 0.3363\n",
      "Epoch 246/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1052 - accuracy: 0.2961 - val_loss: 0.1449 - val_accuracy: 0.3345\n",
      "Epoch 247/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1049 - accuracy: 0.2919 - val_loss: 0.1448 - val_accuracy: 0.3255\n",
      "Epoch 248/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1048 - accuracy: 0.2949 - val_loss: 0.1461 - val_accuracy: 0.3561\n",
      "Epoch 249/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1047 - accuracy: 0.3063 - val_loss: 0.1458 - val_accuracy: 0.3525\n",
      "Epoch 250/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1044 - accuracy: 0.3039 - val_loss: 0.1468 - val_accuracy: 0.3435\n",
      "Epoch 251/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1042 - accuracy: 0.3009 - val_loss: 0.1453 - val_accuracy: 0.3525\n",
      "Epoch 252/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1039 - accuracy: 0.2955 - val_loss: 0.1435 - val_accuracy: 0.3219\n",
      "Epoch 253/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1038 - accuracy: 0.2859 - val_loss: 0.1529 - val_accuracy: 0.3489\n",
      "Epoch 254/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1039 - accuracy: 0.2937 - val_loss: 0.1438 - val_accuracy: 0.3543\n",
      "Epoch 255/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1033 - accuracy: 0.2997 - val_loss: 0.1448 - val_accuracy: 0.3561\n",
      "Epoch 256/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1031 - accuracy: 0.2949 - val_loss: 0.1431 - val_accuracy: 0.3219\n",
      "Epoch 257/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1029 - accuracy: 0.2955 - val_loss: 0.1436 - val_accuracy: 0.3112\n",
      "Epoch 258/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1028 - accuracy: 0.2991 - val_loss: 0.1423 - val_accuracy: 0.3381\n",
      "Epoch 259/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1023 - accuracy: 0.2985 - val_loss: 0.1429 - val_accuracy: 0.3507\n",
      "Epoch 260/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1023 - accuracy: 0.3015 - val_loss: 0.1424 - val_accuracy: 0.3381\n",
      "Epoch 261/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1020 - accuracy: 0.2991 - val_loss: 0.1421 - val_accuracy: 0.3273\n",
      "Epoch 262/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1018 - accuracy: 0.2985 - val_loss: 0.1419 - val_accuracy: 0.3363\n",
      "Epoch 263/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1015 - accuracy: 0.2937 - val_loss: 0.1432 - val_accuracy: 0.3507\n",
      "Epoch 264/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1014 - accuracy: 0.2937 - val_loss: 0.1430 - val_accuracy: 0.3363\n",
      "Epoch 265/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1012 - accuracy: 0.2919 - val_loss: 0.1431 - val_accuracy: 0.3507\n",
      "Epoch 266/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1010 - accuracy: 0.2883 - val_loss: 0.1422 - val_accuracy: 0.3633\n",
      "Epoch 267/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1009 - accuracy: 0.2907 - val_loss: 0.1404 - val_accuracy: 0.3435\n",
      "Epoch 268/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1005 - accuracy: 0.2973 - val_loss: 0.1420 - val_accuracy: 0.3435\n",
      "Epoch 269/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1003 - accuracy: 0.3009 - val_loss: 0.1412 - val_accuracy: 0.3291\n",
      "Epoch 270/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1002 - accuracy: 0.2883 - val_loss: 0.1404 - val_accuracy: 0.3399\n",
      "Epoch 271/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1000 - accuracy: 0.2985 - val_loss: 0.1465 - val_accuracy: 0.2986\n",
      "Epoch 272/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1003 - accuracy: 0.2943 - val_loss: 0.1414 - val_accuracy: 0.3579\n",
      "Epoch 273/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0996 - accuracy: 0.2907 - val_loss: 0.1478 - val_accuracy: 0.2392\n",
      "Epoch 274/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0999 - accuracy: 0.2925 - val_loss: 0.1414 - val_accuracy: 0.3201\n",
      "Epoch 275/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0992 - accuracy: 0.2979 - val_loss: 0.1418 - val_accuracy: 0.3453\n",
      "Epoch 276/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0989 - accuracy: 0.2955 - val_loss: 0.1404 - val_accuracy: 0.3345\n",
      "Epoch 277/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0987 - accuracy: 0.2937 - val_loss: 0.1453 - val_accuracy: 0.2608\n",
      "Epoch 278/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0990 - accuracy: 0.2901 - val_loss: 0.1416 - val_accuracy: 0.3471\n",
      "Epoch 279/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0985 - accuracy: 0.3051 - val_loss: 0.1393 - val_accuracy: 0.3309\n",
      "Epoch 280/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0981 - accuracy: 0.2943 - val_loss: 0.1387 - val_accuracy: 0.3165\n",
      "Epoch 281/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0978 - accuracy: 0.3027 - val_loss: 0.1396 - val_accuracy: 0.3417\n",
      "Epoch 282/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0976 - accuracy: 0.2967 - val_loss: 0.1381 - val_accuracy: 0.3219\n",
      "Epoch 283/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0973 - accuracy: 0.2955 - val_loss: 0.1377 - val_accuracy: 0.3471\n",
      "Epoch 284/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0973 - accuracy: 0.2901 - val_loss: 0.1388 - val_accuracy: 0.3112\n",
      "Epoch 285/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0972 - accuracy: 0.2931 - val_loss: 0.1383 - val_accuracy: 0.3525\n",
      "Epoch 286/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0968 - accuracy: 0.3069 - val_loss: 0.1378 - val_accuracy: 0.3345\n",
      "Epoch 287/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0966 - accuracy: 0.2967 - val_loss: 0.1380 - val_accuracy: 0.3363\n",
      "Epoch 288/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0965 - accuracy: 0.2955 - val_loss: 0.1368 - val_accuracy: 0.3435\n",
      "Epoch 289/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0962 - accuracy: 0.3051 - val_loss: 0.1380 - val_accuracy: 0.3291\n",
      "Epoch 290/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0960 - accuracy: 0.3087 - val_loss: 0.1448 - val_accuracy: 0.3255\n",
      "Epoch 291/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0964 - accuracy: 0.3033 - val_loss: 0.1377 - val_accuracy: 0.3345\n",
      "Epoch 292/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0956 - accuracy: 0.2949 - val_loss: 0.1372 - val_accuracy: 0.3291\n",
      "Epoch 293/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0955 - accuracy: 0.2943 - val_loss: 0.1361 - val_accuracy: 0.3327\n",
      "Epoch 294/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0953 - accuracy: 0.2967 - val_loss: 0.1353 - val_accuracy: 0.3094\n",
      "Epoch 295/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0951 - accuracy: 0.2937 - val_loss: 0.1434 - val_accuracy: 0.3147\n",
      "Epoch 296/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0953 - accuracy: 0.2901 - val_loss: 0.1373 - val_accuracy: 0.3471\n",
      "Epoch 297/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0948 - accuracy: 0.2985 - val_loss: 0.1369 - val_accuracy: 0.3345\n",
      "Epoch 298/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0946 - accuracy: 0.3039 - val_loss: 0.1360 - val_accuracy: 0.3633\n",
      "Epoch 299/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0943 - accuracy: 0.3033 - val_loss: 0.1354 - val_accuracy: 0.3435\n",
      "Epoch 300/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0940 - accuracy: 0.3003 - val_loss: 0.1350 - val_accuracy: 0.3471\n",
      "Epoch 301/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0937 - accuracy: 0.3045 - val_loss: 0.1361 - val_accuracy: 0.3399\n",
      "Epoch 302/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0937 - accuracy: 0.3027 - val_loss: 0.1348 - val_accuracy: 0.3489\n",
      "Epoch 303/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0935 - accuracy: 0.3087 - val_loss: 0.1358 - val_accuracy: 0.2986\n",
      "Epoch 304/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0933 - accuracy: 0.2985 - val_loss: 0.1471 - val_accuracy: 0.2608\n",
      "Epoch 305/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0938 - accuracy: 0.2967 - val_loss: 0.1360 - val_accuracy: 0.3543\n",
      "Epoch 306/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0931 - accuracy: 0.3105 - val_loss: 0.1423 - val_accuracy: 0.3058\n",
      "Epoch 307/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0933 - accuracy: 0.2997 - val_loss: 0.1338 - val_accuracy: 0.3255\n",
      "Epoch 308/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0925 - accuracy: 0.3045 - val_loss: 0.1362 - val_accuracy: 0.3327\n",
      "Epoch 309/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0925 - accuracy: 0.3081 - val_loss: 0.1361 - val_accuracy: 0.3273\n",
      "Epoch 310/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0925 - accuracy: 0.2997 - val_loss: 0.1383 - val_accuracy: 0.3399\n",
      "Epoch 311/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0923 - accuracy: 0.3051 - val_loss: 0.1362 - val_accuracy: 0.3453\n",
      "Epoch 312/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0921 - accuracy: 0.3009 - val_loss: 0.1351 - val_accuracy: 0.3399\n",
      "Epoch 313/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0918 - accuracy: 0.3039 - val_loss: 0.1355 - val_accuracy: 0.3651\n",
      "Epoch 314/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0916 - accuracy: 0.3075 - val_loss: 0.1353 - val_accuracy: 0.3201\n",
      "Epoch 315/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0914 - accuracy: 0.3057 - val_loss: 0.1352 - val_accuracy: 0.3255\n",
      "Epoch 316/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0913 - accuracy: 0.3105 - val_loss: 0.1344 - val_accuracy: 0.3669\n",
      "Epoch 317/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0911 - accuracy: 0.3093 - val_loss: 0.1351 - val_accuracy: 0.3219\n",
      "Epoch 318/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0910 - accuracy: 0.3057 - val_loss: 0.1405 - val_accuracy: 0.3363\n",
      "Epoch 319/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0912 - accuracy: 0.3069 - val_loss: 0.1327 - val_accuracy: 0.3040\n",
      "Epoch 320/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0908 - accuracy: 0.3087 - val_loss: 0.1350 - val_accuracy: 0.3633\n",
      "Epoch 321/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0905 - accuracy: 0.3105 - val_loss: 0.1324 - val_accuracy: 0.3327\n",
      "Epoch 322/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0904 - accuracy: 0.3075 - val_loss: 0.1336 - val_accuracy: 0.3273\n",
      "Epoch 323/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0901 - accuracy: 0.3063 - val_loss: 0.1345 - val_accuracy: 0.3471\n",
      "Epoch 324/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0901 - accuracy: 0.3081 - val_loss: 0.1415 - val_accuracy: 0.3309\n",
      "Epoch 325/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0902 - accuracy: 0.3117 - val_loss: 0.1332 - val_accuracy: 0.3363\n",
      "Epoch 326/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0898 - accuracy: 0.3051 - val_loss: 0.1340 - val_accuracy: 0.3597\n",
      "Epoch 327/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0895 - accuracy: 0.3087 - val_loss: 0.1333 - val_accuracy: 0.3183\n",
      "Epoch 328/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0893 - accuracy: 0.2991 - val_loss: 0.1318 - val_accuracy: 0.3453\n",
      "Epoch 329/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0891 - accuracy: 0.3033 - val_loss: 0.1327 - val_accuracy: 0.3417\n",
      "Epoch 330/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0891 - accuracy: 0.3111 - val_loss: 0.1324 - val_accuracy: 0.3399\n",
      "Epoch 331/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0888 - accuracy: 0.3117 - val_loss: 0.1320 - val_accuracy: 0.3633\n",
      "Epoch 332/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0886 - accuracy: 0.3147 - val_loss: 0.1340 - val_accuracy: 0.3076\n",
      "Epoch 333/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0888 - accuracy: 0.3039 - val_loss: 0.1346 - val_accuracy: 0.3525\n",
      "Epoch 334/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0884 - accuracy: 0.3129 - val_loss: 0.1335 - val_accuracy: 0.3615\n",
      "Epoch 335/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0883 - accuracy: 0.3147 - val_loss: 0.1320 - val_accuracy: 0.3543\n",
      "Epoch 336/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0881 - accuracy: 0.3153 - val_loss: 0.1309 - val_accuracy: 0.3417\n",
      "Epoch 337/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0879 - accuracy: 0.3117 - val_loss: 0.1328 - val_accuracy: 0.3561\n",
      "Epoch 338/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0878 - accuracy: 0.3081 - val_loss: 0.1330 - val_accuracy: 0.3417\n",
      "Epoch 339/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0879 - accuracy: 0.3177 - val_loss: 0.1304 - val_accuracy: 0.3525\n",
      "Epoch 340/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0874 - accuracy: 0.3069 - val_loss: 0.1304 - val_accuracy: 0.3237\n",
      "Epoch 341/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0873 - accuracy: 0.3171 - val_loss: 0.1301 - val_accuracy: 0.3309\n",
      "Epoch 342/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0873 - accuracy: 0.3063 - val_loss: 0.1299 - val_accuracy: 0.3363\n",
      "Epoch 343/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0870 - accuracy: 0.3093 - val_loss: 0.1310 - val_accuracy: 0.3094\n",
      "Epoch 344/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0870 - accuracy: 0.3141 - val_loss: 0.1382 - val_accuracy: 0.3094\n",
      "Epoch 345/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0873 - accuracy: 0.3081 - val_loss: 0.1300 - val_accuracy: 0.3417\n",
      "Epoch 346/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0866 - accuracy: 0.3117 - val_loss: 0.1304 - val_accuracy: 0.3399\n",
      "Epoch 347/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0866 - accuracy: 0.3105 - val_loss: 0.1305 - val_accuracy: 0.3327\n",
      "Epoch 348/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0865 - accuracy: 0.3135 - val_loss: 0.1311 - val_accuracy: 0.3345\n",
      "Epoch 349/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0863 - accuracy: 0.3141 - val_loss: 0.1364 - val_accuracy: 0.3273\n",
      "Epoch 350/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0864 - accuracy: 0.3069 - val_loss: 0.1302 - val_accuracy: 0.3165\n",
      "Epoch 351/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0862 - accuracy: 0.3147 - val_loss: 0.1308 - val_accuracy: 0.3489\n",
      "Epoch 352/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0859 - accuracy: 0.3117 - val_loss: 0.1306 - val_accuracy: 0.3399\n",
      "Epoch 353/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0858 - accuracy: 0.3117 - val_loss: 0.1302 - val_accuracy: 0.3183\n",
      "Epoch 354/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0857 - accuracy: 0.3147 - val_loss: 0.1355 - val_accuracy: 0.3112\n",
      "Epoch 355/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0859 - accuracy: 0.3081 - val_loss: 0.1293 - val_accuracy: 0.3345\n",
      "Epoch 356/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0852 - accuracy: 0.3129 - val_loss: 0.1301 - val_accuracy: 0.3489\n",
      "Epoch 357/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0851 - accuracy: 0.3183 - val_loss: 0.1291 - val_accuracy: 0.3525\n",
      "Epoch 358/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0852 - accuracy: 0.3135 - val_loss: 0.1313 - val_accuracy: 0.3507\n",
      "Epoch 359/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0849 - accuracy: 0.3225 - val_loss: 0.1305 - val_accuracy: 0.3525\n",
      "Epoch 360/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0847 - accuracy: 0.3195 - val_loss: 0.1309 - val_accuracy: 0.3309\n",
      "Epoch 361/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0847 - accuracy: 0.3189 - val_loss: 0.1536 - val_accuracy: 0.2248\n",
      "Epoch 362/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0857 - accuracy: 0.3159 - val_loss: 0.1268 - val_accuracy: 0.3004\n",
      "Epoch 363/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0845 - accuracy: 0.3147 - val_loss: 0.1275 - val_accuracy: 0.3417\n",
      "Epoch 364/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0842 - accuracy: 0.3279 - val_loss: 0.1289 - val_accuracy: 0.3417\n",
      "Epoch 365/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0842 - accuracy: 0.3141 - val_loss: 0.1278 - val_accuracy: 0.3345\n",
      "Epoch 366/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0841 - accuracy: 0.3207 - val_loss: 0.1288 - val_accuracy: 0.3615\n",
      "Epoch 367/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0838 - accuracy: 0.3267 - val_loss: 0.1296 - val_accuracy: 0.3363\n",
      "Epoch 368/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0838 - accuracy: 0.3207 - val_loss: 0.1276 - val_accuracy: 0.3345\n",
      "Epoch 369/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0835 - accuracy: 0.3183 - val_loss: 0.1308 - val_accuracy: 0.3813\n",
      "Epoch 370/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0837 - accuracy: 0.3189 - val_loss: 0.1316 - val_accuracy: 0.3327\n",
      "Epoch 371/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0836 - accuracy: 0.3219 - val_loss: 0.1297 - val_accuracy: 0.3255\n",
      "Epoch 372/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0837 - accuracy: 0.3225 - val_loss: 0.1306 - val_accuracy: 0.3381\n",
      "Epoch 373/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0834 - accuracy: 0.3201 - val_loss: 0.1280 - val_accuracy: 0.3453\n",
      "Epoch 374/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0831 - accuracy: 0.3165 - val_loss: 0.1284 - val_accuracy: 0.3417\n",
      "Epoch 375/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0830 - accuracy: 0.3291 - val_loss: 0.1282 - val_accuracy: 0.3507\n",
      "Epoch 376/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0829 - accuracy: 0.3231 - val_loss: 0.1291 - val_accuracy: 0.3507\n",
      "Epoch 377/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0831 - accuracy: 0.3159 - val_loss: 0.1340 - val_accuracy: 0.3669\n",
      "Epoch 378/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0830 - accuracy: 0.3309 - val_loss: 0.1281 - val_accuracy: 0.3165\n",
      "Epoch 379/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0826 - accuracy: 0.3177 - val_loss: 0.1278 - val_accuracy: 0.3669\n",
      "Epoch 380/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0826 - accuracy: 0.3267 - val_loss: 0.1287 - val_accuracy: 0.3597\n",
      "Epoch 381/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0824 - accuracy: 0.3237 - val_loss: 0.1264 - val_accuracy: 0.3525\n",
      "Epoch 382/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0822 - accuracy: 0.3207 - val_loss: 0.1513 - val_accuracy: 0.3076\n",
      "Epoch 383/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0832 - accuracy: 0.3279 - val_loss: 0.1256 - val_accuracy: 0.3543\n",
      "Epoch 384/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0820 - accuracy: 0.3255 - val_loss: 0.1270 - val_accuracy: 0.3363\n",
      "Epoch 385/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0820 - accuracy: 0.3177 - val_loss: 0.1288 - val_accuracy: 0.3507\n",
      "Epoch 386/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0818 - accuracy: 0.3249 - val_loss: 0.1285 - val_accuracy: 0.3489\n",
      "Epoch 387/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0818 - accuracy: 0.3261 - val_loss: 0.1264 - val_accuracy: 0.3399\n",
      "Epoch 388/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0815 - accuracy: 0.3219 - val_loss: 0.1273 - val_accuracy: 0.3471\n",
      "Epoch 389/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0814 - accuracy: 0.3195 - val_loss: 0.1264 - val_accuracy: 0.3381\n",
      "Epoch 390/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0812 - accuracy: 0.3279 - val_loss: 0.1270 - val_accuracy: 0.3417\n",
      "Epoch 391/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0810 - accuracy: 0.3225 - val_loss: 0.1285 - val_accuracy: 0.3255\n",
      "Epoch 392/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0811 - accuracy: 0.3249 - val_loss: 0.1263 - val_accuracy: 0.3417\n",
      "Epoch 393/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0808 - accuracy: 0.3345 - val_loss: 0.1297 - val_accuracy: 0.3291\n",
      "Epoch 394/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0808 - accuracy: 0.3267 - val_loss: 0.1265 - val_accuracy: 0.3381\n",
      "Epoch 395/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0807 - accuracy: 0.3255 - val_loss: 0.1254 - val_accuracy: 0.3471\n",
      "Epoch 396/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0808 - accuracy: 0.3303 - val_loss: 0.1264 - val_accuracy: 0.3489\n",
      "Epoch 397/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0807 - accuracy: 0.3219 - val_loss: 0.1279 - val_accuracy: 0.3201\n",
      "Epoch 398/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0806 - accuracy: 0.3315 - val_loss: 0.1247 - val_accuracy: 0.3417\n",
      "Epoch 399/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0801 - accuracy: 0.3225 - val_loss: 0.1267 - val_accuracy: 0.3669\n",
      "Epoch 400/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0801 - accuracy: 0.3255 - val_loss: 0.1252 - val_accuracy: 0.3471\n",
      "Epoch 401/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0799 - accuracy: 0.3243 - val_loss: 0.1254 - val_accuracy: 0.3381\n",
      "Epoch 402/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0800 - accuracy: 0.3237 - val_loss: 0.1253 - val_accuracy: 0.3237\n",
      "Epoch 403/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0799 - accuracy: 0.3255 - val_loss: 0.1259 - val_accuracy: 0.3579\n",
      "Epoch 404/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0797 - accuracy: 0.3273 - val_loss: 0.1281 - val_accuracy: 0.3417\n",
      "Epoch 405/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0798 - accuracy: 0.3327 - val_loss: 0.1248 - val_accuracy: 0.3579\n",
      "Epoch 406/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0794 - accuracy: 0.3315 - val_loss: 0.1232 - val_accuracy: 0.3363\n",
      "Epoch 407/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0794 - accuracy: 0.3189 - val_loss: 0.1315 - val_accuracy: 0.3237\n",
      "Epoch 408/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0797 - accuracy: 0.3237 - val_loss: 0.1260 - val_accuracy: 0.3147\n",
      "Epoch 409/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0792 - accuracy: 0.3267 - val_loss: 0.1271 - val_accuracy: 0.3651\n",
      "Epoch 410/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0793 - accuracy: 0.3321 - val_loss: 0.1258 - val_accuracy: 0.3345\n",
      "Epoch 411/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0791 - accuracy: 0.3291 - val_loss: 0.1244 - val_accuracy: 0.3291\n",
      "Epoch 412/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0789 - accuracy: 0.3303 - val_loss: 0.1245 - val_accuracy: 0.3381\n",
      "Epoch 413/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0788 - accuracy: 0.3321 - val_loss: 0.1239 - val_accuracy: 0.3327\n",
      "Epoch 414/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0787 - accuracy: 0.3321 - val_loss: 0.1257 - val_accuracy: 0.3543\n",
      "Epoch 415/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0786 - accuracy: 0.3285 - val_loss: 0.1237 - val_accuracy: 0.3345\n",
      "Epoch 416/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0786 - accuracy: 0.3201 - val_loss: 0.1251 - val_accuracy: 0.3543\n",
      "Epoch 417/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0783 - accuracy: 0.3291 - val_loss: 0.1244 - val_accuracy: 0.3417\n",
      "Epoch 418/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0783 - accuracy: 0.3357 - val_loss: 0.1254 - val_accuracy: 0.3453\n",
      "Epoch 419/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0783 - accuracy: 0.3309 - val_loss: 0.1416 - val_accuracy: 0.3219\n",
      "Epoch 420/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0792 - accuracy: 0.3201 - val_loss: 0.1274 - val_accuracy: 0.3309\n",
      "Epoch 421/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0782 - accuracy: 0.3207 - val_loss: 0.1253 - val_accuracy: 0.3561\n",
      "Epoch 422/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0781 - accuracy: 0.3303 - val_loss: 0.1266 - val_accuracy: 0.3381\n",
      "Epoch 423/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0780 - accuracy: 0.3255 - val_loss: 0.1255 - val_accuracy: 0.3453\n",
      "Epoch 424/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0779 - accuracy: 0.3375 - val_loss: 0.1246 - val_accuracy: 0.3435\n",
      "Epoch 425/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0777 - accuracy: 0.3303 - val_loss: 0.1248 - val_accuracy: 0.3669\n",
      "Epoch 426/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0775 - accuracy: 0.3315 - val_loss: 0.1301 - val_accuracy: 0.3777\n",
      "Epoch 427/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0777 - accuracy: 0.3321 - val_loss: 0.1266 - val_accuracy: 0.3507\n",
      "Epoch 428/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0776 - accuracy: 0.3303 - val_loss: 0.1238 - val_accuracy: 0.3435\n",
      "Epoch 429/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0772 - accuracy: 0.3315 - val_loss: 0.1235 - val_accuracy: 0.3561\n",
      "Epoch 430/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0771 - accuracy: 0.3363 - val_loss: 0.1230 - val_accuracy: 0.3561\n",
      "Epoch 431/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0769 - accuracy: 0.3351 - val_loss: 0.1220 - val_accuracy: 0.3453\n",
      "Epoch 432/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0770 - accuracy: 0.3261 - val_loss: 0.1244 - val_accuracy: 0.3687\n",
      "Epoch 433/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0769 - accuracy: 0.3291 - val_loss: 0.1237 - val_accuracy: 0.3597\n",
      "Epoch 434/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0768 - accuracy: 0.3261 - val_loss: 0.1235 - val_accuracy: 0.3381\n",
      "Epoch 435/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0768 - accuracy: 0.3351 - val_loss: 0.1249 - val_accuracy: 0.3255\n",
      "Epoch 436/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0767 - accuracy: 0.3303 - val_loss: 0.1236 - val_accuracy: 0.3597\n",
      "Epoch 437/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0766 - accuracy: 0.3255 - val_loss: 0.1216 - val_accuracy: 0.3435\n",
      "Epoch 438/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0765 - accuracy: 0.3237 - val_loss: 0.1244 - val_accuracy: 0.3561\n",
      "Epoch 439/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0765 - accuracy: 0.3309 - val_loss: 0.1234 - val_accuracy: 0.3417\n",
      "Epoch 440/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0763 - accuracy: 0.3345 - val_loss: 0.1260 - val_accuracy: 0.4029\n",
      "Epoch 441/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0763 - accuracy: 0.3381 - val_loss: 0.1245 - val_accuracy: 0.3597\n",
      "Epoch 442/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0760 - accuracy: 0.3369 - val_loss: 0.1250 - val_accuracy: 0.3597\n",
      "Epoch 443/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0761 - accuracy: 0.3363 - val_loss: 0.1238 - val_accuracy: 0.3525\n",
      "Epoch 444/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0762 - accuracy: 0.3249 - val_loss: 0.1325 - val_accuracy: 0.3633\n",
      "Epoch 445/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0761 - accuracy: 0.3363 - val_loss: 0.1219 - val_accuracy: 0.3813\n",
      "Epoch 446/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0757 - accuracy: 0.3357 - val_loss: 0.1213 - val_accuracy: 0.3669\n",
      "Epoch 447/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0758 - accuracy: 0.3285 - val_loss: 0.1223 - val_accuracy: 0.3849\n",
      "Epoch 448/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0755 - accuracy: 0.3369 - val_loss: 0.1295 - val_accuracy: 0.3795\n",
      "Epoch 449/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0759 - accuracy: 0.3297 - val_loss: 0.1220 - val_accuracy: 0.3705\n",
      "Epoch 450/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0752 - accuracy: 0.3375 - val_loss: 0.1203 - val_accuracy: 0.3669\n",
      "Epoch 451/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0753 - accuracy: 0.3285 - val_loss: 0.1223 - val_accuracy: 0.3921\n",
      "Epoch 452/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0753 - accuracy: 0.3339 - val_loss: 0.1208 - val_accuracy: 0.3327\n",
      "Epoch 453/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0751 - accuracy: 0.3291 - val_loss: 0.1252 - val_accuracy: 0.3363\n",
      "Epoch 454/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0750 - accuracy: 0.3315 - val_loss: 0.1488 - val_accuracy: 0.3417\n",
      "Epoch 455/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0762 - accuracy: 0.3213 - val_loss: 0.1212 - val_accuracy: 0.3561\n",
      "Epoch 456/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0749 - accuracy: 0.3279 - val_loss: 0.1297 - val_accuracy: 0.3309\n",
      "Epoch 457/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0753 - accuracy: 0.3303 - val_loss: 0.1212 - val_accuracy: 0.3561\n",
      "Epoch 458/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0748 - accuracy: 0.3375 - val_loss: 0.1218 - val_accuracy: 0.3489\n",
      "Epoch 459/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0746 - accuracy: 0.3423 - val_loss: 0.1205 - val_accuracy: 0.3543\n",
      "Epoch 460/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0744 - accuracy: 0.3375 - val_loss: 0.1230 - val_accuracy: 0.3831\n",
      "Epoch 461/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0745 - accuracy: 0.3339 - val_loss: 0.1218 - val_accuracy: 0.3471\n",
      "Epoch 462/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0744 - accuracy: 0.3315 - val_loss: 0.1216 - val_accuracy: 0.3687\n",
      "Epoch 463/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0744 - accuracy: 0.3237 - val_loss: 0.1213 - val_accuracy: 0.3705\n",
      "Epoch 464/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0742 - accuracy: 0.3309 - val_loss: 0.1349 - val_accuracy: 0.3327\n",
      "Epoch 465/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0748 - accuracy: 0.3285 - val_loss: 0.1292 - val_accuracy: 0.3615\n",
      "Epoch 466/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0746 - accuracy: 0.3261 - val_loss: 0.1214 - val_accuracy: 0.3831\n",
      "Epoch 467/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0739 - accuracy: 0.3327 - val_loss: 0.1227 - val_accuracy: 0.3687\n",
      "Epoch 468/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0740 - accuracy: 0.3261 - val_loss: 0.1224 - val_accuracy: 0.3687\n",
      "Epoch 469/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0738 - accuracy: 0.3219 - val_loss: 0.1221 - val_accuracy: 0.3597\n",
      "Epoch 470/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0739 - accuracy: 0.3309 - val_loss: 0.1229 - val_accuracy: 0.3201\n",
      "Epoch 471/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0739 - accuracy: 0.3249 - val_loss: 0.1214 - val_accuracy: 0.3813\n",
      "Epoch 472/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0737 - accuracy: 0.3363 - val_loss: 0.1199 - val_accuracy: 0.3885\n",
      "Epoch 473/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0735 - accuracy: 0.3423 - val_loss: 0.1220 - val_accuracy: 0.4047\n",
      "Epoch 474/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0735 - accuracy: 0.3405 - val_loss: 0.1219 - val_accuracy: 0.3579\n",
      "Epoch 475/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0736 - accuracy: 0.3327 - val_loss: 0.1226 - val_accuracy: 0.3615\n",
      "Epoch 476/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0732 - accuracy: 0.3315 - val_loss: 0.1232 - val_accuracy: 0.3669\n",
      "Epoch 477/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0733 - accuracy: 0.3333 - val_loss: 0.1353 - val_accuracy: 0.3345\n",
      "Epoch 478/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0739 - accuracy: 0.3237 - val_loss: 0.1210 - val_accuracy: 0.3777\n",
      "Epoch 479/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0731 - accuracy: 0.3291 - val_loss: 0.1388 - val_accuracy: 0.2428\n",
      "Epoch 480/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0739 - accuracy: 0.3279 - val_loss: 0.1217 - val_accuracy: 0.3813\n",
      "Epoch 481/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0731 - accuracy: 0.3339 - val_loss: 0.1193 - val_accuracy: 0.3669\n",
      "Epoch 482/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0728 - accuracy: 0.3357 - val_loss: 0.1186 - val_accuracy: 0.3651\n",
      "Epoch 483/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0728 - accuracy: 0.3327 - val_loss: 0.1194 - val_accuracy: 0.3939\n",
      "Epoch 484/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0728 - accuracy: 0.3423 - val_loss: 0.1199 - val_accuracy: 0.3489\n",
      "Epoch 485/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0726 - accuracy: 0.3321 - val_loss: 0.1201 - val_accuracy: 0.3561\n",
      "Epoch 486/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0724 - accuracy: 0.3351 - val_loss: 0.1264 - val_accuracy: 0.3633\n",
      "Epoch 487/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0726 - accuracy: 0.3333 - val_loss: 0.1211 - val_accuracy: 0.3777\n",
      "Epoch 488/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0723 - accuracy: 0.3357 - val_loss: 0.1205 - val_accuracy: 0.3345\n",
      "Epoch 489/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0724 - accuracy: 0.3357 - val_loss: 0.1208 - val_accuracy: 0.3921\n",
      "Epoch 490/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0722 - accuracy: 0.3363 - val_loss: 0.1183 - val_accuracy: 0.3597\n",
      "Epoch 491/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0723 - accuracy: 0.3237 - val_loss: 0.1207 - val_accuracy: 0.3525\n",
      "Epoch 492/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0721 - accuracy: 0.3297 - val_loss: 0.1191 - val_accuracy: 0.3777\n",
      "Epoch 493/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0719 - accuracy: 0.3381 - val_loss: 0.1197 - val_accuracy: 0.3921\n",
      "Epoch 494/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0719 - accuracy: 0.3339 - val_loss: 0.1195 - val_accuracy: 0.3543\n",
      "Epoch 495/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0719 - accuracy: 0.3393 - val_loss: 0.1349 - val_accuracy: 0.2518\n",
      "Epoch 496/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0727 - accuracy: 0.3261 - val_loss: 0.1189 - val_accuracy: 0.3777\n",
      "Epoch 497/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0716 - accuracy: 0.3315 - val_loss: 0.1411 - val_accuracy: 0.3291\n",
      "Epoch 498/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0730 - accuracy: 0.3321 - val_loss: 0.1202 - val_accuracy: 0.3867\n",
      "Epoch 499/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0716 - accuracy: 0.3321 - val_loss: 0.1221 - val_accuracy: 0.4083\n",
      "Epoch 500/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0716 - accuracy: 0.3381 - val_loss: 0.1230 - val_accuracy: 0.3885\n",
      "Epoch 501/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0715 - accuracy: 0.3303 - val_loss: 0.1190 - val_accuracy: 0.3615\n",
      "Epoch 502/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0713 - accuracy: 0.3381 - val_loss: 0.1186 - val_accuracy: 0.3525\n",
      "Epoch 503/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0713 - accuracy: 0.3219 - val_loss: 0.1254 - val_accuracy: 0.3759\n",
      "Epoch 504/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0718 - accuracy: 0.3261 - val_loss: 0.1177 - val_accuracy: 0.3867\n",
      "Epoch 505/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0711 - accuracy: 0.3297 - val_loss: 0.1202 - val_accuracy: 0.3723\n",
      "Epoch 506/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0709 - accuracy: 0.3351 - val_loss: 0.1187 - val_accuracy: 0.4029\n",
      "Epoch 507/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0709 - accuracy: 0.3309 - val_loss: 0.1179 - val_accuracy: 0.3975\n",
      "Epoch 508/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0709 - accuracy: 0.3417 - val_loss: 0.1246 - val_accuracy: 0.3543\n",
      "Epoch 509/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0712 - accuracy: 0.3357 - val_loss: 0.1173 - val_accuracy: 0.3831\n",
      "Epoch 510/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0707 - accuracy: 0.3375 - val_loss: 0.1195 - val_accuracy: 0.3687\n",
      "Epoch 511/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0708 - accuracy: 0.3303 - val_loss: 0.1290 - val_accuracy: 0.3741\n",
      "Epoch 512/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0712 - accuracy: 0.3339 - val_loss: 0.1203 - val_accuracy: 0.3795\n",
      "Epoch 513/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0707 - accuracy: 0.3333 - val_loss: 0.1411 - val_accuracy: 0.3453\n",
      "Epoch 514/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0717 - accuracy: 0.3363 - val_loss: 0.1191 - val_accuracy: 0.3813\n",
      "Epoch 515/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0704 - accuracy: 0.3327 - val_loss: 0.1197 - val_accuracy: 0.3831\n",
      "Epoch 516/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0702 - accuracy: 0.3351 - val_loss: 0.1221 - val_accuracy: 0.3777\n",
      "Epoch 517/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0705 - accuracy: 0.3315 - val_loss: 0.1248 - val_accuracy: 0.3885\n",
      "Epoch 518/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0706 - accuracy: 0.3423 - val_loss: 0.1187 - val_accuracy: 0.3795\n",
      "Epoch 519/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0702 - accuracy: 0.3351 - val_loss: 0.1187 - val_accuracy: 0.3975\n",
      "Epoch 520/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0702 - accuracy: 0.3339 - val_loss: 0.1183 - val_accuracy: 0.3435\n",
      "Epoch 521/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0701 - accuracy: 0.3249 - val_loss: 0.1173 - val_accuracy: 0.3903\n",
      "Epoch 522/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0700 - accuracy: 0.3327 - val_loss: 0.1179 - val_accuracy: 0.3921\n",
      "Epoch 523/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0700 - accuracy: 0.3333 - val_loss: 0.1281 - val_accuracy: 0.3759\n",
      "Epoch 524/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0705 - accuracy: 0.3303 - val_loss: 0.1200 - val_accuracy: 0.3597\n",
      "Epoch 525/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0700 - accuracy: 0.3291 - val_loss: 0.1165 - val_accuracy: 0.3777\n",
      "Epoch 526/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0699 - accuracy: 0.3309 - val_loss: 0.1177 - val_accuracy: 0.3705\n",
      "Epoch 527/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0700 - accuracy: 0.3363 - val_loss: 0.1462 - val_accuracy: 0.2554\n",
      "Epoch 528/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0709 - accuracy: 0.3255 - val_loss: 0.1215 - val_accuracy: 0.3579\n",
      "Epoch 529/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0701 - accuracy: 0.3333 - val_loss: 0.1178 - val_accuracy: 0.3705\n",
      "Epoch 530/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0695 - accuracy: 0.3333 - val_loss: 0.1198 - val_accuracy: 0.3651\n",
      "Epoch 531/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0695 - accuracy: 0.3327 - val_loss: 0.1206 - val_accuracy: 0.3525\n",
      "Epoch 532/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0696 - accuracy: 0.3255 - val_loss: 0.1195 - val_accuracy: 0.3741\n",
      "Epoch 533/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0692 - accuracy: 0.3273 - val_loss: 0.1220 - val_accuracy: 0.3813\n",
      "Epoch 534/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0692 - accuracy: 0.3399 - val_loss: 0.1205 - val_accuracy: 0.3885\n",
      "Epoch 535/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0694 - accuracy: 0.3405 - val_loss: 0.1216 - val_accuracy: 0.3633\n",
      "Epoch 536/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0694 - accuracy: 0.3339 - val_loss: 0.1235 - val_accuracy: 0.3921\n",
      "Epoch 537/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0692 - accuracy: 0.3369 - val_loss: 0.1205 - val_accuracy: 0.3507\n",
      "Epoch 538/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0690 - accuracy: 0.3327 - val_loss: 0.1195 - val_accuracy: 0.3777\n",
      "Epoch 539/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0690 - accuracy: 0.3327 - val_loss: 0.1179 - val_accuracy: 0.3705\n",
      "Epoch 540/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0689 - accuracy: 0.3309 - val_loss: 0.1682 - val_accuracy: 0.3165\n",
      "Epoch 541/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0715 - accuracy: 0.3195 - val_loss: 0.1355 - val_accuracy: 0.2482\n",
      "Epoch 542/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0699 - accuracy: 0.3267 - val_loss: 0.1326 - val_accuracy: 0.3489\n",
      "Epoch 543/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0693 - accuracy: 0.3375 - val_loss: 0.1175 - val_accuracy: 0.3849\n",
      "Epoch 544/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0687 - accuracy: 0.3315 - val_loss: 0.1176 - val_accuracy: 0.3777\n",
      "Epoch 545/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0685 - accuracy: 0.3297 - val_loss: 0.1177 - val_accuracy: 0.3705\n",
      "Epoch 546/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0684 - accuracy: 0.3309 - val_loss: 0.1184 - val_accuracy: 0.3993\n",
      "Epoch 547/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0684 - accuracy: 0.3381 - val_loss: 0.1171 - val_accuracy: 0.3867\n",
      "Epoch 548/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0683 - accuracy: 0.3363 - val_loss: 0.1180 - val_accuracy: 0.3579\n",
      "Epoch 549/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0682 - accuracy: 0.3261 - val_loss: 0.1183 - val_accuracy: 0.3903\n",
      "Epoch 550/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0682 - accuracy: 0.3315 - val_loss: 0.1179 - val_accuracy: 0.3939\n",
      "Epoch 551/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0683 - accuracy: 0.3375 - val_loss: 0.1172 - val_accuracy: 0.3921\n",
      "Epoch 552/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0680 - accuracy: 0.3345 - val_loss: 0.1180 - val_accuracy: 0.3903\n",
      "Epoch 553/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0680 - accuracy: 0.3309 - val_loss: 0.1294 - val_accuracy: 0.3777\n",
      "Epoch 554/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0686 - accuracy: 0.3321 - val_loss: 0.1237 - val_accuracy: 0.3921\n",
      "Epoch 555/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0680 - accuracy: 0.3327 - val_loss: 0.1185 - val_accuracy: 0.3921\n",
      "Epoch 556/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0681 - accuracy: 0.3339 - val_loss: 0.1174 - val_accuracy: 0.3813\n",
      "Epoch 557/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0678 - accuracy: 0.3363 - val_loss: 0.1171 - val_accuracy: 0.3813\n",
      "Epoch 558/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0677 - accuracy: 0.3309 - val_loss: 0.1182 - val_accuracy: 0.3525\n",
      "Epoch 559/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0676 - accuracy: 0.3339 - val_loss: 0.1177 - val_accuracy: 0.3705\n",
      "Epoch 560/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0676 - accuracy: 0.3249 - val_loss: 0.1203 - val_accuracy: 0.3777\n",
      "Epoch 561/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0676 - accuracy: 0.3351 - val_loss: 0.1213 - val_accuracy: 0.3957\n",
      "Epoch 562/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0676 - accuracy: 0.3327 - val_loss: 0.1182 - val_accuracy: 0.3669\n",
      "Epoch 563/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0675 - accuracy: 0.3297 - val_loss: 0.1222 - val_accuracy: 0.3615\n",
      "Epoch 564/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0676 - accuracy: 0.3405 - val_loss: 0.1163 - val_accuracy: 0.3795\n",
      "Epoch 565/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0673 - accuracy: 0.3363 - val_loss: 0.1288 - val_accuracy: 0.3273\n",
      "Epoch 566/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0679 - accuracy: 0.3225 - val_loss: 0.1193 - val_accuracy: 0.3849\n",
      "Epoch 567/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0674 - accuracy: 0.3339 - val_loss: 0.1184 - val_accuracy: 0.3849\n",
      "Epoch 568/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0670 - accuracy: 0.3339 - val_loss: 0.1182 - val_accuracy: 0.3741\n",
      "Epoch 569/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0671 - accuracy: 0.3285 - val_loss: 0.1189 - val_accuracy: 0.3633\n",
      "Epoch 570/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0675 - accuracy: 0.3339 - val_loss: 0.1159 - val_accuracy: 0.3777\n",
      "Epoch 571/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0671 - accuracy: 0.3273 - val_loss: 0.1157 - val_accuracy: 0.3615\n",
      "Epoch 572/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0670 - accuracy: 0.3339 - val_loss: 0.1173 - val_accuracy: 0.4047\n",
      "Epoch 573/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0668 - accuracy: 0.3393 - val_loss: 0.1212 - val_accuracy: 0.3939\n",
      "Epoch 574/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0671 - accuracy: 0.3291 - val_loss: 0.1171 - val_accuracy: 0.3417\n",
      "Epoch 575/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0668 - accuracy: 0.3291 - val_loss: 0.1185 - val_accuracy: 0.3939\n",
      "Epoch 576/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0667 - accuracy: 0.3375 - val_loss: 0.1170 - val_accuracy: 0.3975\n",
      "Epoch 577/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0665 - accuracy: 0.3417 - val_loss: 0.1186 - val_accuracy: 0.3813\n",
      "Epoch 578/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0667 - accuracy: 0.3369 - val_loss: 0.1170 - val_accuracy: 0.3957\n",
      "Epoch 579/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0665 - accuracy: 0.3357 - val_loss: 0.1152 - val_accuracy: 0.3777\n",
      "Epoch 580/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0667 - accuracy: 0.3321 - val_loss: 0.1155 - val_accuracy: 0.4083\n",
      "Epoch 581/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0663 - accuracy: 0.3417 - val_loss: 0.1320 - val_accuracy: 0.2554\n",
      "Epoch 582/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0674 - accuracy: 0.3333 - val_loss: 0.1319 - val_accuracy: 0.3741\n",
      "Epoch 583/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0670 - accuracy: 0.3315 - val_loss: 0.1201 - val_accuracy: 0.3417\n",
      "Epoch 584/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0668 - accuracy: 0.3273 - val_loss: 0.1195 - val_accuracy: 0.4137\n",
      "Epoch 585/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0665 - accuracy: 0.3441 - val_loss: 0.1194 - val_accuracy: 0.4137\n",
      "Epoch 586/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0662 - accuracy: 0.3363 - val_loss: 0.1142 - val_accuracy: 0.4119\n",
      "Epoch 587/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0661 - accuracy: 0.3417 - val_loss: 0.1166 - val_accuracy: 0.4065\n",
      "Epoch 588/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0660 - accuracy: 0.3303 - val_loss: 0.1171 - val_accuracy: 0.3975\n",
      "Epoch 589/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0661 - accuracy: 0.3357 - val_loss: 0.1156 - val_accuracy: 0.3723\n",
      "Epoch 590/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0660 - accuracy: 0.3345 - val_loss: 0.1423 - val_accuracy: 0.2356\n",
      "Epoch 591/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0676 - accuracy: 0.3225 - val_loss: 0.1177 - val_accuracy: 0.3849\n",
      "Epoch 592/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0662 - accuracy: 0.3417 - val_loss: 0.1162 - val_accuracy: 0.3849\n",
      "Epoch 593/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0659 - accuracy: 0.3363 - val_loss: 0.1189 - val_accuracy: 0.3921\n",
      "Epoch 594/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0657 - accuracy: 0.3279 - val_loss: 0.1155 - val_accuracy: 0.3777\n",
      "Epoch 595/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0657 - accuracy: 0.3303 - val_loss: 0.1152 - val_accuracy: 0.3813\n",
      "Epoch 596/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0656 - accuracy: 0.3291 - val_loss: 0.1217 - val_accuracy: 0.4029\n",
      "Epoch 597/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0661 - accuracy: 0.3285 - val_loss: 0.1175 - val_accuracy: 0.3705\n",
      "Epoch 598/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0656 - accuracy: 0.3339 - val_loss: 0.1165 - val_accuracy: 0.3669\n",
      "Epoch 599/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0657 - accuracy: 0.3267 - val_loss: 0.1181 - val_accuracy: 0.3975\n",
      "Epoch 600/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0657 - accuracy: 0.3309 - val_loss: 0.1146 - val_accuracy: 0.3957\n",
      "Epoch 601/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0654 - accuracy: 0.3375 - val_loss: 0.1321 - val_accuracy: 0.3112\n",
      "Epoch 602/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0664 - accuracy: 0.3321 - val_loss: 0.1174 - val_accuracy: 0.3669\n",
      "Epoch 603/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0653 - accuracy: 0.3351 - val_loss: 0.1142 - val_accuracy: 0.3831\n",
      "Epoch 604/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0652 - accuracy: 0.3339 - val_loss: 0.1165 - val_accuracy: 0.3975\n",
      "Epoch 605/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0652 - accuracy: 0.3387 - val_loss: 0.1152 - val_accuracy: 0.3975\n",
      "Epoch 606/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0652 - accuracy: 0.3411 - val_loss: 0.1159 - val_accuracy: 0.3507\n",
      "Epoch 607/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0651 - accuracy: 0.3351 - val_loss: 0.1333 - val_accuracy: 0.3040\n",
      "Epoch 608/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0660 - accuracy: 0.3243 - val_loss: 0.1162 - val_accuracy: 0.3885\n",
      "Epoch 609/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0648 - accuracy: 0.3417 - val_loss: 0.1149 - val_accuracy: 0.3831\n",
      "Epoch 610/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0650 - accuracy: 0.3363 - val_loss: 0.1198 - val_accuracy: 0.3849\n",
      "Epoch 611/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0649 - accuracy: 0.3351 - val_loss: 0.1153 - val_accuracy: 0.3777\n",
      "Epoch 612/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0647 - accuracy: 0.3393 - val_loss: 0.1175 - val_accuracy: 0.4029\n",
      "Epoch 613/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0648 - accuracy: 0.3417 - val_loss: 0.1187 - val_accuracy: 0.3813\n",
      "Epoch 614/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0648 - accuracy: 0.3363 - val_loss: 0.1173 - val_accuracy: 0.3633\n",
      "Epoch 615/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0647 - accuracy: 0.3387 - val_loss: 0.1234 - val_accuracy: 0.3561\n",
      "Epoch 616/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0650 - accuracy: 0.3315 - val_loss: 0.1162 - val_accuracy: 0.3669\n",
      "Epoch 617/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0645 - accuracy: 0.3309 - val_loss: 0.1188 - val_accuracy: 0.3957\n",
      "Epoch 618/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0647 - accuracy: 0.3309 - val_loss: 0.1187 - val_accuracy: 0.4029\n",
      "Epoch 619/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0645 - accuracy: 0.3387 - val_loss: 0.1238 - val_accuracy: 0.4083\n",
      "Epoch 620/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0647 - accuracy: 0.3417 - val_loss: 0.1148 - val_accuracy: 0.3849\n",
      "Epoch 621/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0642 - accuracy: 0.3357 - val_loss: 0.1167 - val_accuracy: 0.3939\n",
      "Epoch 622/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0644 - accuracy: 0.3297 - val_loss: 0.1235 - val_accuracy: 0.3705\n",
      "Epoch 623/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0647 - accuracy: 0.3291 - val_loss: 0.1153 - val_accuracy: 0.3813\n",
      "Epoch 624/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0640 - accuracy: 0.3297 - val_loss: 0.1149 - val_accuracy: 0.3723\n",
      "Epoch 625/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0640 - accuracy: 0.3321 - val_loss: 0.1151 - val_accuracy: 0.3795\n",
      "Epoch 626/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0641 - accuracy: 0.3327 - val_loss: 0.1169 - val_accuracy: 0.3939\n",
      "Epoch 627/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0640 - accuracy: 0.3369 - val_loss: 0.1170 - val_accuracy: 0.3741\n",
      "Epoch 628/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0639 - accuracy: 0.3381 - val_loss: 0.1151 - val_accuracy: 0.3723\n",
      "Epoch 629/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0640 - accuracy: 0.3429 - val_loss: 0.1133 - val_accuracy: 0.3885\n",
      "Epoch 630/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0638 - accuracy: 0.3333 - val_loss: 0.1186 - val_accuracy: 0.3867\n",
      "Epoch 631/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0638 - accuracy: 0.3333 - val_loss: 0.1213 - val_accuracy: 0.3903\n",
      "Epoch 632/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0642 - accuracy: 0.3303 - val_loss: 0.1164 - val_accuracy: 0.3849\n",
      "Epoch 633/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0637 - accuracy: 0.3297 - val_loss: 0.1229 - val_accuracy: 0.3885\n",
      "Epoch 634/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0640 - accuracy: 0.3285 - val_loss: 0.1146 - val_accuracy: 0.3615\n",
      "Epoch 635/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0638 - accuracy: 0.3285 - val_loss: 0.1149 - val_accuracy: 0.3867\n",
      "Epoch 636/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0634 - accuracy: 0.3345 - val_loss: 0.1156 - val_accuracy: 0.3885\n",
      "Epoch 637/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0636 - accuracy: 0.3267 - val_loss: 0.1158 - val_accuracy: 0.3939\n",
      "Epoch 638/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0633 - accuracy: 0.3357 - val_loss: 0.1166 - val_accuracy: 0.3777\n",
      "Epoch 639/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0637 - accuracy: 0.3411 - val_loss: 0.1156 - val_accuracy: 0.3705\n",
      "Epoch 640/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0634 - accuracy: 0.3267 - val_loss: 0.1152 - val_accuracy: 0.4083\n",
      "Epoch 641/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0633 - accuracy: 0.3327 - val_loss: 0.1142 - val_accuracy: 0.3975\n",
      "Epoch 642/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0632 - accuracy: 0.3375 - val_loss: 0.1149 - val_accuracy: 0.3849\n",
      "Epoch 643/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0636 - accuracy: 0.3291 - val_loss: 0.1130 - val_accuracy: 0.3777\n",
      "Epoch 644/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0632 - accuracy: 0.3243 - val_loss: 0.1145 - val_accuracy: 0.3849\n",
      "Epoch 645/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0630 - accuracy: 0.3285 - val_loss: 0.1151 - val_accuracy: 0.3849\n",
      "Epoch 646/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0631 - accuracy: 0.3261 - val_loss: 0.1208 - val_accuracy: 0.3759\n",
      "Epoch 647/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0637 - accuracy: 0.3345 - val_loss: 0.1343 - val_accuracy: 0.2608\n",
      "Epoch 648/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0641 - accuracy: 0.3189 - val_loss: 0.1134 - val_accuracy: 0.3993\n",
      "Epoch 649/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0628 - accuracy: 0.3261 - val_loss: 0.1144 - val_accuracy: 0.3939\n",
      "Epoch 650/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0627 - accuracy: 0.3315 - val_loss: 0.1143 - val_accuracy: 0.3957\n",
      "Epoch 651/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0629 - accuracy: 0.3387 - val_loss: 0.1160 - val_accuracy: 0.3849\n",
      "Epoch 652/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0627 - accuracy: 0.3327 - val_loss: 0.1142 - val_accuracy: 0.3867\n",
      "Epoch 653/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0629 - accuracy: 0.3375 - val_loss: 0.1166 - val_accuracy: 0.3885\n",
      "Epoch 654/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0628 - accuracy: 0.3429 - val_loss: 0.1139 - val_accuracy: 0.3939\n",
      "Epoch 655/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0626 - accuracy: 0.3363 - val_loss: 0.1149 - val_accuracy: 0.4065\n",
      "Epoch 656/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0629 - accuracy: 0.3291 - val_loss: 0.1165 - val_accuracy: 0.3813\n",
      "Epoch 657/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0626 - accuracy: 0.3357 - val_loss: 0.1254 - val_accuracy: 0.3669\n",
      "Epoch 658/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0632 - accuracy: 0.3393 - val_loss: 0.1162 - val_accuracy: 0.3813\n",
      "Epoch 659/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0624 - accuracy: 0.3345 - val_loss: 0.1214 - val_accuracy: 0.3489\n",
      "Epoch 660/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0628 - accuracy: 0.3315 - val_loss: 0.1145 - val_accuracy: 0.3957\n",
      "Epoch 661/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0625 - accuracy: 0.3333 - val_loss: 0.1165 - val_accuracy: 0.3831\n",
      "Epoch 662/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0624 - accuracy: 0.3345 - val_loss: 0.1155 - val_accuracy: 0.3975\n",
      "Epoch 663/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0623 - accuracy: 0.3321 - val_loss: 0.1153 - val_accuracy: 0.3669\n",
      "Epoch 664/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0622 - accuracy: 0.3321 - val_loss: 0.1140 - val_accuracy: 0.3813\n",
      "Epoch 665/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0624 - accuracy: 0.3327 - val_loss: 0.1148 - val_accuracy: 0.3813\n",
      "Epoch 666/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0621 - accuracy: 0.3279 - val_loss: 0.1185 - val_accuracy: 0.4011\n",
      "Epoch 667/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0623 - accuracy: 0.3339 - val_loss: 0.1152 - val_accuracy: 0.3903\n",
      "Epoch 668/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0618 - accuracy: 0.3357 - val_loss: 0.1140 - val_accuracy: 0.3615\n",
      "Epoch 669/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0622 - accuracy: 0.3327 - val_loss: 0.1121 - val_accuracy: 0.3921\n",
      "Epoch 670/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0620 - accuracy: 0.3363 - val_loss: 0.1164 - val_accuracy: 0.3813\n",
      "Epoch 671/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0622 - accuracy: 0.3333 - val_loss: 0.1146 - val_accuracy: 0.3903\n",
      "Epoch 672/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0618 - accuracy: 0.3375 - val_loss: 0.1180 - val_accuracy: 0.3705\n",
      "Epoch 673/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0621 - accuracy: 0.3363 - val_loss: 0.1191 - val_accuracy: 0.3435\n",
      "Epoch 674/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0620 - accuracy: 0.3417 - val_loss: 0.1158 - val_accuracy: 0.3795\n",
      "Epoch 675/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0616 - accuracy: 0.3351 - val_loss: 0.1133 - val_accuracy: 0.3993\n",
      "Epoch 676/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0618 - accuracy: 0.3231 - val_loss: 0.1154 - val_accuracy: 0.3831\n",
      "Epoch 677/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0617 - accuracy: 0.3345 - val_loss: 0.1135 - val_accuracy: 0.4011\n",
      "Epoch 678/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0615 - accuracy: 0.3375 - val_loss: 0.1149 - val_accuracy: 0.3993\n",
      "Epoch 679/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0615 - accuracy: 0.3297 - val_loss: 0.1158 - val_accuracy: 0.3921\n",
      "Epoch 680/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0613 - accuracy: 0.3315 - val_loss: 0.1167 - val_accuracy: 0.4065\n",
      "Epoch 681/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0620 - accuracy: 0.3315 - val_loss: 0.1148 - val_accuracy: 0.3939\n",
      "Epoch 682/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0616 - accuracy: 0.3345 - val_loss: 0.1145 - val_accuracy: 0.3867\n",
      "Epoch 683/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0613 - accuracy: 0.3327 - val_loss: 0.1181 - val_accuracy: 0.3903\n",
      "Epoch 684/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0614 - accuracy: 0.3345 - val_loss: 0.1127 - val_accuracy: 0.3903\n",
      "Epoch 685/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0615 - accuracy: 0.3303 - val_loss: 0.1157 - val_accuracy: 0.4011\n",
      "Epoch 686/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0611 - accuracy: 0.3399 - val_loss: 0.1138 - val_accuracy: 0.4083\n",
      "Epoch 687/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0612 - accuracy: 0.3321 - val_loss: 0.1139 - val_accuracy: 0.3975\n",
      "Epoch 688/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0610 - accuracy: 0.3291 - val_loss: 0.1133 - val_accuracy: 0.3867\n",
      "Epoch 689/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0610 - accuracy: 0.3321 - val_loss: 0.1131 - val_accuracy: 0.4047\n",
      "Epoch 690/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0609 - accuracy: 0.3321 - val_loss: 0.1161 - val_accuracy: 0.3795\n",
      "Epoch 691/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0612 - accuracy: 0.3291 - val_loss: 0.1147 - val_accuracy: 0.3651\n",
      "Epoch 692/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0610 - accuracy: 0.3303 - val_loss: 0.1180 - val_accuracy: 0.3777\n",
      "Epoch 693/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0612 - accuracy: 0.3333 - val_loss: 0.1155 - val_accuracy: 0.3903\n",
      "Epoch 694/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0607 - accuracy: 0.3333 - val_loss: 0.1138 - val_accuracy: 0.3795\n",
      "Epoch 695/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0610 - accuracy: 0.3291 - val_loss: 0.1148 - val_accuracy: 0.4083\n",
      "Epoch 696/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0609 - accuracy: 0.3339 - val_loss: 0.1165 - val_accuracy: 0.4191\n",
      "Epoch 697/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0608 - accuracy: 0.3339 - val_loss: 0.1129 - val_accuracy: 0.3903\n",
      "Epoch 698/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0605 - accuracy: 0.3387 - val_loss: 0.1131 - val_accuracy: 0.4011\n",
      "Epoch 699/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0604 - accuracy: 0.3405 - val_loss: 0.1130 - val_accuracy: 0.3867\n",
      "Epoch 700/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0606 - accuracy: 0.3315 - val_loss: 0.1170 - val_accuracy: 0.3543\n",
      "Epoch 701/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0610 - accuracy: 0.3249 - val_loss: 0.1224 - val_accuracy: 0.4047\n",
      "Epoch 702/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0611 - accuracy: 0.3321 - val_loss: 0.1140 - val_accuracy: 0.3831\n",
      "Epoch 703/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0605 - accuracy: 0.3279 - val_loss: 0.1138 - val_accuracy: 0.3633\n",
      "Epoch 704/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0604 - accuracy: 0.3267 - val_loss: 0.1221 - val_accuracy: 0.4137\n",
      "Epoch 705/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0610 - accuracy: 0.3291 - val_loss: 0.1197 - val_accuracy: 0.3813\n",
      "Epoch 706/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0609 - accuracy: 0.3363 - val_loss: 0.1119 - val_accuracy: 0.4029\n",
      "Epoch 707/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0604 - accuracy: 0.3291 - val_loss: 0.1132 - val_accuracy: 0.3921\n",
      "Epoch 708/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0602 - accuracy: 0.3291 - val_loss: 0.1139 - val_accuracy: 0.4029\n",
      "Epoch 709/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0602 - accuracy: 0.3237 - val_loss: 0.1138 - val_accuracy: 0.3579\n",
      "Epoch 710/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0601 - accuracy: 0.3285 - val_loss: 0.1610 - val_accuracy: 0.3058\n",
      "Epoch 711/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0622 - accuracy: 0.3333 - val_loss: 0.1136 - val_accuracy: 0.3849\n",
      "Epoch 712/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0601 - accuracy: 0.3309 - val_loss: 0.1173 - val_accuracy: 0.3903\n",
      "Epoch 713/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0602 - accuracy: 0.3297 - val_loss: 0.1136 - val_accuracy: 0.3903\n",
      "Epoch 714/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0600 - accuracy: 0.3333 - val_loss: 0.1195 - val_accuracy: 0.3129\n",
      "Epoch 715/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0601 - accuracy: 0.3249 - val_loss: 0.1132 - val_accuracy: 0.3957\n",
      "Epoch 716/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0600 - accuracy: 0.3303 - val_loss: 0.1344 - val_accuracy: 0.2950\n",
      "Epoch 717/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0609 - accuracy: 0.3351 - val_loss: 0.1153 - val_accuracy: 0.3993\n",
      "Epoch 718/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0597 - accuracy: 0.3327 - val_loss: 0.1145 - val_accuracy: 0.3813\n",
      "Epoch 719/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0599 - accuracy: 0.3345 - val_loss: 0.1111 - val_accuracy: 0.3831\n",
      "Epoch 720/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0596 - accuracy: 0.3279 - val_loss: 0.1153 - val_accuracy: 0.3723\n",
      "Epoch 721/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0597 - accuracy: 0.3363 - val_loss: 0.1129 - val_accuracy: 0.3993\n",
      "Epoch 722/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0596 - accuracy: 0.3345 - val_loss: 0.1168 - val_accuracy: 0.3453\n",
      "Epoch 723/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0600 - accuracy: 0.3363 - val_loss: 0.1275 - val_accuracy: 0.4137\n",
      "Epoch 724/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0606 - accuracy: 0.3315 - val_loss: 0.1148 - val_accuracy: 0.4029\n",
      "Epoch 725/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0597 - accuracy: 0.3333 - val_loss: 0.1148 - val_accuracy: 0.3993\n",
      "Epoch 726/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0596 - accuracy: 0.3339 - val_loss: 0.1139 - val_accuracy: 0.4191\n",
      "Epoch 727/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0596 - accuracy: 0.3423 - val_loss: 0.1146 - val_accuracy: 0.3975\n",
      "Epoch 728/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0595 - accuracy: 0.3249 - val_loss: 0.1408 - val_accuracy: 0.3040\n",
      "Epoch 729/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0608 - accuracy: 0.3381 - val_loss: 0.1154 - val_accuracy: 0.4047\n",
      "Epoch 730/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0595 - accuracy: 0.3441 - val_loss: 0.1138 - val_accuracy: 0.4155\n",
      "Epoch 731/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0593 - accuracy: 0.3321 - val_loss: 0.1124 - val_accuracy: 0.3723\n",
      "Epoch 732/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0592 - accuracy: 0.3381 - val_loss: 0.1127 - val_accuracy: 0.4011\n",
      "Epoch 733/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0590 - accuracy: 0.3423 - val_loss: 0.1359 - val_accuracy: 0.2878\n",
      "Epoch 734/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0606 - accuracy: 0.3279 - val_loss: 0.1122 - val_accuracy: 0.4101\n",
      "Epoch 735/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0590 - accuracy: 0.3363 - val_loss: 0.1196 - val_accuracy: 0.3957\n",
      "Epoch 736/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0593 - accuracy: 0.3273 - val_loss: 0.1142 - val_accuracy: 0.3957\n",
      "Epoch 737/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0589 - accuracy: 0.3333 - val_loss: 0.1143 - val_accuracy: 0.4029\n",
      "Epoch 738/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0593 - accuracy: 0.3387 - val_loss: 0.1153 - val_accuracy: 0.3867\n",
      "Epoch 739/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0589 - accuracy: 0.3375 - val_loss: 0.1156 - val_accuracy: 0.4119\n",
      "Epoch 740/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0590 - accuracy: 0.3369 - val_loss: 0.1134 - val_accuracy: 0.3939\n",
      "Epoch 741/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0588 - accuracy: 0.3255 - val_loss: 0.1138 - val_accuracy: 0.4119\n",
      "Epoch 742/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0587 - accuracy: 0.3387 - val_loss: 0.1123 - val_accuracy: 0.4101\n",
      "Epoch 743/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0587 - accuracy: 0.3357 - val_loss: 0.1142 - val_accuracy: 0.3921\n",
      "Epoch 744/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0588 - accuracy: 0.3321 - val_loss: 0.1139 - val_accuracy: 0.3813\n",
      "Epoch 745/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0587 - accuracy: 0.3393 - val_loss: 0.1111 - val_accuracy: 0.4011\n",
      "Epoch 746/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0588 - accuracy: 0.3279 - val_loss: 0.1115 - val_accuracy: 0.3867\n",
      "Epoch 747/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0588 - accuracy: 0.3387 - val_loss: 0.1120 - val_accuracy: 0.3939\n",
      "Epoch 748/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0585 - accuracy: 0.3363 - val_loss: 0.1126 - val_accuracy: 0.3993\n",
      "Epoch 749/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0587 - accuracy: 0.3351 - val_loss: 0.1132 - val_accuracy: 0.4083\n",
      "Epoch 750/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0584 - accuracy: 0.3387 - val_loss: 0.1147 - val_accuracy: 0.3975\n",
      "Epoch 751/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0585 - accuracy: 0.3339 - val_loss: 0.1139 - val_accuracy: 0.3669\n",
      "Epoch 752/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0585 - accuracy: 0.3315 - val_loss: 0.1125 - val_accuracy: 0.4011\n",
      "Epoch 753/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0582 - accuracy: 0.3351 - val_loss: 0.1142 - val_accuracy: 0.3975\n",
      "Epoch 754/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0582 - accuracy: 0.3315 - val_loss: 0.1144 - val_accuracy: 0.3939\n",
      "Epoch 755/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0582 - accuracy: 0.3345 - val_loss: 0.1129 - val_accuracy: 0.3687\n",
      "Epoch 756/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0583 - accuracy: 0.3213 - val_loss: 0.1287 - val_accuracy: 0.3363\n",
      "Epoch 757/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0591 - accuracy: 0.3417 - val_loss: 0.1124 - val_accuracy: 0.4029\n",
      "Epoch 758/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0581 - accuracy: 0.3309 - val_loss: 0.1152 - val_accuracy: 0.4047\n",
      "Epoch 759/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0581 - accuracy: 0.3429 - val_loss: 0.1127 - val_accuracy: 0.3867\n",
      "Epoch 760/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0582 - accuracy: 0.3327 - val_loss: 0.1133 - val_accuracy: 0.4029\n",
      "Epoch 761/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0581 - accuracy: 0.3339 - val_loss: 0.1144 - val_accuracy: 0.3849\n",
      "Epoch 762/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0580 - accuracy: 0.3327 - val_loss: 0.1252 - val_accuracy: 0.3921\n",
      "Epoch 763/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0586 - accuracy: 0.3393 - val_loss: 0.1112 - val_accuracy: 0.3669\n",
      "Epoch 764/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0579 - accuracy: 0.3345 - val_loss: 0.1134 - val_accuracy: 0.3813\n",
      "Epoch 765/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0579 - accuracy: 0.3405 - val_loss: 0.1119 - val_accuracy: 0.3849\n",
      "Epoch 766/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0578 - accuracy: 0.3261 - val_loss: 0.1168 - val_accuracy: 0.4227\n",
      "Epoch 767/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0581 - accuracy: 0.3393 - val_loss: 0.1136 - val_accuracy: 0.4119\n",
      "Epoch 768/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0579 - accuracy: 0.3375 - val_loss: 0.1127 - val_accuracy: 0.3993\n",
      "Epoch 769/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0578 - accuracy: 0.3351 - val_loss: 0.1141 - val_accuracy: 0.4011\n",
      "Epoch 770/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0578 - accuracy: 0.3315 - val_loss: 0.1236 - val_accuracy: 0.3543\n",
      "Epoch 771/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0585 - accuracy: 0.3297 - val_loss: 0.1151 - val_accuracy: 0.3939\n",
      "Epoch 772/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0577 - accuracy: 0.3339 - val_loss: 0.1139 - val_accuracy: 0.4083\n",
      "Epoch 773/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0576 - accuracy: 0.3345 - val_loss: 0.1146 - val_accuracy: 0.3777\n",
      "Epoch 774/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0577 - accuracy: 0.3369 - val_loss: 0.1164 - val_accuracy: 0.3831\n",
      "Epoch 775/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0577 - accuracy: 0.3339 - val_loss: 0.1134 - val_accuracy: 0.3957\n",
      "Epoch 776/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0574 - accuracy: 0.3345 - val_loss: 0.1187 - val_accuracy: 0.3867\n",
      "Epoch 777/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0576 - accuracy: 0.3387 - val_loss: 0.1143 - val_accuracy: 0.4137\n",
      "Epoch 778/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0574 - accuracy: 0.3417 - val_loss: 0.1136 - val_accuracy: 0.3849\n",
      "Epoch 779/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0574 - accuracy: 0.3315 - val_loss: 0.1155 - val_accuracy: 0.3993\n",
      "Epoch 780/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0581 - accuracy: 0.3303 - val_loss: 0.1215 - val_accuracy: 0.4173\n",
      "Epoch 781/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0581 - accuracy: 0.3339 - val_loss: 0.1122 - val_accuracy: 0.4119\n",
      "Epoch 782/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0573 - accuracy: 0.3315 - val_loss: 0.1136 - val_accuracy: 0.3939\n",
      "Epoch 783/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0575 - accuracy: 0.3411 - val_loss: 0.1153 - val_accuracy: 0.4442\n",
      "Epoch 784/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0573 - accuracy: 0.3387 - val_loss: 0.1118 - val_accuracy: 0.3939\n",
      "Epoch 785/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0572 - accuracy: 0.3369 - val_loss: 0.1121 - val_accuracy: 0.4083\n",
      "Epoch 786/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0570 - accuracy: 0.3447 - val_loss: 0.1279 - val_accuracy: 0.4119\n",
      "Epoch 787/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0577 - accuracy: 0.3399 - val_loss: 0.1169 - val_accuracy: 0.3795\n",
      "Epoch 788/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0573 - accuracy: 0.3339 - val_loss: 0.1109 - val_accuracy: 0.4119\n",
      "Epoch 789/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0569 - accuracy: 0.3369 - val_loss: 0.1311 - val_accuracy: 0.2824\n",
      "Epoch 790/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0580 - accuracy: 0.3339 - val_loss: 0.1183 - val_accuracy: 0.4155\n",
      "Epoch 791/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0571 - accuracy: 0.3303 - val_loss: 0.1114 - val_accuracy: 0.3867\n",
      "Epoch 792/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0569 - accuracy: 0.3411 - val_loss: 0.1129 - val_accuracy: 0.4047\n",
      "Epoch 793/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0569 - accuracy: 0.3399 - val_loss: 0.1128 - val_accuracy: 0.3885\n",
      "Epoch 794/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0568 - accuracy: 0.3285 - val_loss: 0.1123 - val_accuracy: 0.4101\n",
      "Epoch 795/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0566 - accuracy: 0.3369 - val_loss: 0.1170 - val_accuracy: 0.3723\n",
      "Epoch 796/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0571 - accuracy: 0.3411 - val_loss: 0.1131 - val_accuracy: 0.4047\n",
      "Epoch 797/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0566 - accuracy: 0.3339 - val_loss: 0.1147 - val_accuracy: 0.4065\n",
      "Epoch 798/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0567 - accuracy: 0.3387 - val_loss: 0.1239 - val_accuracy: 0.3885\n",
      "Epoch 799/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0570 - accuracy: 0.3249 - val_loss: 0.1116 - val_accuracy: 0.3939\n",
      "Epoch 800/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0565 - accuracy: 0.3339 - val_loss: 0.1196 - val_accuracy: 0.4191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-32-f01cf9afd9ce>:131: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "  tprs.append(interp(mean_fpr, fpr, tpr))\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:806: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Epoch 1/800\n",
      "53/53 [==============================] - 1s 6ms/step - loss: 0.6530 - accuracy: 0.0222 - val_loss: 0.5957 - val_accuracy: 0.0414\n",
      "Epoch 2/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.3903 - accuracy: 0.0390 - val_loss: 0.2599 - val_accuracy: 0.0396\n",
      "Epoch 3/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1925 - accuracy: 0.0318 - val_loss: 0.2514 - val_accuracy: 0.0018\n",
      "Epoch 4/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1877 - accuracy: 0.0216 - val_loss: 0.2503 - val_accuracy: 0.0144\n",
      "Epoch 5/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1867 - accuracy: 0.0174 - val_loss: 0.2500 - val_accuracy: 0.0468\n",
      "Epoch 6/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1865 - accuracy: 0.0354 - val_loss: 0.2496 - val_accuracy: 0.0252\n",
      "Epoch 7/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1863 - accuracy: 0.0216 - val_loss: 0.2497 - val_accuracy: 0.0036\n",
      "Epoch 8/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1863 - accuracy: 0.0192 - val_loss: 0.2505 - val_accuracy: 0.0360\n",
      "Epoch 9/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1862 - accuracy: 0.0192 - val_loss: 0.2497 - val_accuracy: 0.0612\n",
      "Epoch 10/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1861 - accuracy: 0.0498 - val_loss: 0.2500 - val_accuracy: 0.0468\n",
      "Epoch 11/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1860 - accuracy: 0.0384 - val_loss: 0.2496 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1860 - accuracy: 0.0096 - val_loss: 0.2499 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1859 - accuracy: 0.0198 - val_loss: 0.2497 - val_accuracy: 0.0324\n",
      "Epoch 14/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1859 - accuracy: 0.0216 - val_loss: 0.2501 - val_accuracy: 0.0216\n",
      "Epoch 15/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1858 - accuracy: 0.0240 - val_loss: 0.2491 - val_accuracy: 0.0324\n",
      "Epoch 16/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1858 - accuracy: 0.0276 - val_loss: 0.2495 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1857 - accuracy: 0.0276 - val_loss: 0.2491 - val_accuracy: 0.0468\n",
      "Epoch 18/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1857 - accuracy: 0.0318 - val_loss: 0.2499 - val_accuracy: 0.0036\n",
      "Epoch 19/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1856 - accuracy: 0.0144 - val_loss: 0.2492 - val_accuracy: 0.0432\n",
      "Epoch 20/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1856 - accuracy: 0.0354 - val_loss: 0.2492 - val_accuracy: 0.0378\n",
      "Epoch 21/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1855 - accuracy: 0.0474 - val_loss: 0.2490 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1854 - accuracy: 0.0114 - val_loss: 0.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1854 - accuracy: 0.0096 - val_loss: 0.2486 - val_accuracy: 0.0306\n",
      "Epoch 24/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1854 - accuracy: 0.0288 - val_loss: 0.2486 - val_accuracy: 0.0486\n",
      "Epoch 25/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1853 - accuracy: 0.0360 - val_loss: 0.2486 - val_accuracy: 0.0432\n",
      "Epoch 26/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1852 - accuracy: 0.0366 - val_loss: 0.2488 - val_accuracy: 0.0432\n",
      "Epoch 27/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1851 - accuracy: 0.0312 - val_loss: 0.2487 - val_accuracy: 0.0414\n",
      "Epoch 28/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1851 - accuracy: 0.0444 - val_loss: 0.2482 - val_accuracy: 0.0324\n",
      "Epoch 29/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1850 - accuracy: 0.0288 - val_loss: 0.2481 - val_accuracy: 0.0252\n",
      "Epoch 30/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1849 - accuracy: 0.0360 - val_loss: 0.2478 - val_accuracy: 0.0378\n",
      "Epoch 31/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1849 - accuracy: 0.0258 - val_loss: 0.2480 - val_accuracy: 0.0432\n",
      "Epoch 32/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1848 - accuracy: 0.0438 - val_loss: 0.2485 - val_accuracy: 0.0198\n",
      "Epoch 33/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1847 - accuracy: 0.0462 - val_loss: 0.2482 - val_accuracy: 0.0018\n",
      "Epoch 34/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1847 - accuracy: 0.0198 - val_loss: 0.2476 - val_accuracy: 0.0594\n",
      "Epoch 35/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1846 - accuracy: 0.0396 - val_loss: 0.2475 - val_accuracy: 0.0737\n",
      "Epoch 36/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1846 - accuracy: 0.0751 - val_loss: 0.2473 - val_accuracy: 0.0396\n",
      "Epoch 37/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1845 - accuracy: 0.0312 - val_loss: 0.2472 - val_accuracy: 0.0378\n",
      "Epoch 38/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1844 - accuracy: 0.0450 - val_loss: 0.2475 - val_accuracy: 0.0396\n",
      "Epoch 39/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1844 - accuracy: 0.0294 - val_loss: 0.2475 - val_accuracy: 0.0612\n",
      "Epoch 40/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1843 - accuracy: 0.0559 - val_loss: 0.2480 - val_accuracy: 0.0665\n",
      "Epoch 41/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1842 - accuracy: 0.0685 - val_loss: 0.2473 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1842 - accuracy: 0.0162 - val_loss: 0.2470 - val_accuracy: 0.0360\n",
      "Epoch 43/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1841 - accuracy: 0.0390 - val_loss: 0.2467 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1840 - accuracy: 0.0288 - val_loss: 0.2469 - val_accuracy: 0.0162\n",
      "Epoch 45/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1840 - accuracy: 0.0474 - val_loss: 0.2471 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1839 - accuracy: 0.0330 - val_loss: 0.2465 - val_accuracy: 0.0719\n",
      "Epoch 47/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1838 - accuracy: 0.0643 - val_loss: 0.2466 - val_accuracy: 0.0612\n",
      "Epoch 48/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1837 - accuracy: 0.0583 - val_loss: 0.2467 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1836 - accuracy: 0.0222 - val_loss: 0.2460 - val_accuracy: 0.0612\n",
      "Epoch 50/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1836 - accuracy: 0.0559 - val_loss: 0.2461 - val_accuracy: 0.0360\n",
      "Epoch 51/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1835 - accuracy: 0.0420 - val_loss: 0.2458 - val_accuracy: 0.0360\n",
      "Epoch 52/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1834 - accuracy: 0.0486 - val_loss: 0.2458 - val_accuracy: 0.0360\n",
      "Epoch 53/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1834 - accuracy: 0.0414 - val_loss: 0.2458 - val_accuracy: 0.0558\n",
      "Epoch 54/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1832 - accuracy: 0.0474 - val_loss: 0.2459 - val_accuracy: 0.0540\n",
      "Epoch 55/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1831 - accuracy: 0.0613 - val_loss: 0.2453 - val_accuracy: 0.0629\n",
      "Epoch 56/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1830 - accuracy: 0.0679 - val_loss: 0.2453 - val_accuracy: 0.0378\n",
      "Epoch 57/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1829 - accuracy: 0.0517 - val_loss: 0.2454 - val_accuracy: 0.0288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1828 - accuracy: 0.0492 - val_loss: 0.2454 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1827 - accuracy: 0.0294 - val_loss: 0.2452 - val_accuracy: 0.0432\n",
      "Epoch 60/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1826 - accuracy: 0.0685 - val_loss: 0.2454 - val_accuracy: 0.0198\n",
      "Epoch 61/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1825 - accuracy: 0.0745 - val_loss: 0.2447 - val_accuracy: 0.0773\n",
      "Epoch 62/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1824 - accuracy: 0.0835 - val_loss: 0.2442 - val_accuracy: 0.0827\n",
      "Epoch 63/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1823 - accuracy: 0.0715 - val_loss: 0.2448 - val_accuracy: 0.0827\n",
      "Epoch 64/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1822 - accuracy: 0.0847 - val_loss: 0.2440 - val_accuracy: 0.0324\n",
      "Epoch 65/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1820 - accuracy: 0.0631 - val_loss: 0.2440 - val_accuracy: 0.0360\n",
      "Epoch 66/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1819 - accuracy: 0.0709 - val_loss: 0.2441 - val_accuracy: 0.0090\n",
      "Epoch 67/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1818 - accuracy: 0.0835 - val_loss: 0.2437 - val_accuracy: 0.0054\n",
      "Epoch 68/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1817 - accuracy: 0.0841 - val_loss: 0.2430 - val_accuracy: 0.0432\n",
      "Epoch 69/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1816 - accuracy: 0.0793 - val_loss: 0.2435 - val_accuracy: 0.0126\n",
      "Epoch 70/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1814 - accuracy: 0.0871 - val_loss: 0.2429 - val_accuracy: 0.0522\n",
      "Epoch 71/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1812 - accuracy: 0.0937 - val_loss: 0.2425 - val_accuracy: 0.0719\n",
      "Epoch 72/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1811 - accuracy: 0.0961 - val_loss: 0.2426 - val_accuracy: 0.0576\n",
      "Epoch 73/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1809 - accuracy: 0.0961 - val_loss: 0.2426 - val_accuracy: 0.0719\n",
      "Epoch 74/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1808 - accuracy: 0.0805 - val_loss: 0.2425 - val_accuracy: 0.0594\n",
      "Epoch 75/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1806 - accuracy: 0.0991 - val_loss: 0.2420 - val_accuracy: 0.0845\n",
      "Epoch 76/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1804 - accuracy: 0.1147 - val_loss: 0.2418 - val_accuracy: 0.0594\n",
      "Epoch 77/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1802 - accuracy: 0.0919 - val_loss: 0.2416 - val_accuracy: 0.0432\n",
      "Epoch 78/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1800 - accuracy: 0.0829 - val_loss: 0.2414 - val_accuracy: 0.1079\n",
      "Epoch 79/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1799 - accuracy: 0.0961 - val_loss: 0.2402 - val_accuracy: 0.0612\n",
      "Epoch 80/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1797 - accuracy: 0.0883 - val_loss: 0.2401 - val_accuracy: 0.0935\n",
      "Epoch 81/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1795 - accuracy: 0.1393 - val_loss: 0.2403 - val_accuracy: 0.1025\n",
      "Epoch 82/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1792 - accuracy: 0.1291 - val_loss: 0.2399 - val_accuracy: 0.0360\n",
      "Epoch 83/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1790 - accuracy: 0.1213 - val_loss: 0.2393 - val_accuracy: 0.0360\n",
      "Epoch 84/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1788 - accuracy: 0.0709 - val_loss: 0.2389 - val_accuracy: 0.1205\n",
      "Epoch 85/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1785 - accuracy: 0.1417 - val_loss: 0.2385 - val_accuracy: 0.0629\n",
      "Epoch 86/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1783 - accuracy: 0.1009 - val_loss: 0.2386 - val_accuracy: 0.0683\n",
      "Epoch 87/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1780 - accuracy: 0.1063 - val_loss: 0.2377 - val_accuracy: 0.0342\n",
      "Epoch 88/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1777 - accuracy: 0.0967 - val_loss: 0.2378 - val_accuracy: 0.0378\n",
      "Epoch 89/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1775 - accuracy: 0.1033 - val_loss: 0.2372 - val_accuracy: 0.0594\n",
      "Epoch 90/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1772 - accuracy: 0.0919 - val_loss: 0.2367 - val_accuracy: 0.0342\n",
      "Epoch 91/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1769 - accuracy: 0.0691 - val_loss: 0.2368 - val_accuracy: 0.0414\n",
      "Epoch 92/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1766 - accuracy: 0.0997 - val_loss: 0.2357 - val_accuracy: 0.1115\n",
      "Epoch 93/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1763 - accuracy: 0.1381 - val_loss: 0.2358 - val_accuracy: 0.1007\n",
      "Epoch 94/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1759 - accuracy: 0.1129 - val_loss: 0.2344 - val_accuracy: 0.0953\n",
      "Epoch 95/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1756 - accuracy: 0.1303 - val_loss: 0.2350 - val_accuracy: 0.0737\n",
      "Epoch 96/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1752 - accuracy: 0.1153 - val_loss: 0.2343 - val_accuracy: 0.0576\n",
      "Epoch 97/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1749 - accuracy: 0.1195 - val_loss: 0.2331 - val_accuracy: 0.0612\n",
      "Epoch 98/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1745 - accuracy: 0.1219 - val_loss: 0.2336 - val_accuracy: 0.0701\n",
      "Epoch 99/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1741 - accuracy: 0.1075 - val_loss: 0.2323 - val_accuracy: 0.0917\n",
      "Epoch 100/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1737 - accuracy: 0.1459 - val_loss: 0.2316 - val_accuracy: 0.0576\n",
      "Epoch 101/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1733 - accuracy: 0.1297 - val_loss: 0.2310 - val_accuracy: 0.0342\n",
      "Epoch 102/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1730 - accuracy: 0.1069 - val_loss: 0.2306 - val_accuracy: 0.0953\n",
      "Epoch 103/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1725 - accuracy: 0.1628 - val_loss: 0.2307 - val_accuracy: 0.0773\n",
      "Epoch 104/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1721 - accuracy: 0.1538 - val_loss: 0.2299 - val_accuracy: 0.0522\n",
      "Epoch 105/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1716 - accuracy: 0.1237 - val_loss: 0.2292 - val_accuracy: 0.1475\n",
      "Epoch 106/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1712 - accuracy: 0.1658 - val_loss: 0.2281 - val_accuracy: 0.1133\n",
      "Epoch 107/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1708 - accuracy: 0.1441 - val_loss: 0.2271 - val_accuracy: 0.1205\n",
      "Epoch 108/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1704 - accuracy: 0.1694 - val_loss: 0.2270 - val_accuracy: 0.1151\n",
      "Epoch 109/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1699 - accuracy: 0.1514 - val_loss: 0.2266 - val_accuracy: 0.1385\n",
      "Epoch 110/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1696 - accuracy: 0.1592 - val_loss: 0.2261 - val_accuracy: 0.1151\n",
      "Epoch 111/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1691 - accuracy: 0.1550 - val_loss: 0.2257 - val_accuracy: 0.1223\n",
      "Epoch 112/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1687 - accuracy: 0.1532 - val_loss: 0.2246 - val_accuracy: 0.1259\n",
      "Epoch 113/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1682 - accuracy: 0.1712 - val_loss: 0.2244 - val_accuracy: 0.1277\n",
      "Epoch 114/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1678 - accuracy: 0.1622 - val_loss: 0.2244 - val_accuracy: 0.0989\n",
      "Epoch 115/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1673 - accuracy: 0.1459 - val_loss: 0.2229 - val_accuracy: 0.1349\n",
      "Epoch 116/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1668 - accuracy: 0.1742 - val_loss: 0.2225 - val_accuracy: 0.0773\n",
      "Epoch 117/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1665 - accuracy: 0.1550 - val_loss: 0.2216 - val_accuracy: 0.1367\n",
      "Epoch 118/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1659 - accuracy: 0.1964 - val_loss: 0.2212 - val_accuracy: 0.1187\n",
      "Epoch 119/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1655 - accuracy: 0.1688 - val_loss: 0.2207 - val_accuracy: 0.1475\n",
      "Epoch 120/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1650 - accuracy: 0.1898 - val_loss: 0.2199 - val_accuracy: 0.1313\n",
      "Epoch 121/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1645 - accuracy: 0.1880 - val_loss: 0.2191 - val_accuracy: 0.1403\n",
      "Epoch 122/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1641 - accuracy: 0.1892 - val_loss: 0.2193 - val_accuracy: 0.0953\n",
      "Epoch 123/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1636 - accuracy: 0.1718 - val_loss: 0.2187 - val_accuracy: 0.1385\n",
      "Epoch 124/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1631 - accuracy: 0.2030 - val_loss: 0.2188 - val_accuracy: 0.1313\n",
      "Epoch 125/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1627 - accuracy: 0.1682 - val_loss: 0.2175 - val_accuracy: 0.1385\n",
      "Epoch 126/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1620 - accuracy: 0.1790 - val_loss: 0.2155 - val_accuracy: 0.1457\n",
      "Epoch 127/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1615 - accuracy: 0.1970 - val_loss: 0.2154 - val_accuracy: 0.1457\n",
      "Epoch 128/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1610 - accuracy: 0.1994 - val_loss: 0.2146 - val_accuracy: 0.1709\n",
      "Epoch 129/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1605 - accuracy: 0.2012 - val_loss: 0.2143 - val_accuracy: 0.1637\n",
      "Epoch 130/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1600 - accuracy: 0.1910 - val_loss: 0.2141 - val_accuracy: 0.1673\n",
      "Epoch 131/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1595 - accuracy: 0.1940 - val_loss: 0.2128 - val_accuracy: 0.1331\n",
      "Epoch 132/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1588 - accuracy: 0.1802 - val_loss: 0.2116 - val_accuracy: 0.1457\n",
      "Epoch 133/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1582 - accuracy: 0.1916 - val_loss: 0.2105 - val_accuracy: 0.1511\n",
      "Epoch 134/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1577 - accuracy: 0.1952 - val_loss: 0.2103 - val_accuracy: 0.1727\n",
      "Epoch 135/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1571 - accuracy: 0.2120 - val_loss: 0.2088 - val_accuracy: 0.1799\n",
      "Epoch 136/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1565 - accuracy: 0.2234 - val_loss: 0.2088 - val_accuracy: 0.1835\n",
      "Epoch 137/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1559 - accuracy: 0.2120 - val_loss: 0.2073 - val_accuracy: 0.1691\n",
      "Epoch 138/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1553 - accuracy: 0.2144 - val_loss: 0.2066 - val_accuracy: 0.1601\n",
      "Epoch 139/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1547 - accuracy: 0.2000 - val_loss: 0.2061 - val_accuracy: 0.1799\n",
      "Epoch 140/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1541 - accuracy: 0.2096 - val_loss: 0.2086 - val_accuracy: 0.1960\n",
      "Epoch 141/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1536 - accuracy: 0.2012 - val_loss: 0.2047 - val_accuracy: 0.2212\n",
      "Epoch 142/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1528 - accuracy: 0.2138 - val_loss: 0.2029 - val_accuracy: 0.1888\n",
      "Epoch 143/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1521 - accuracy: 0.2222 - val_loss: 0.2016 - val_accuracy: 0.2212\n",
      "Epoch 144/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1515 - accuracy: 0.2246 - val_loss: 0.2016 - val_accuracy: 0.1942\n",
      "Epoch 145/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1509 - accuracy: 0.2264 - val_loss: 0.2008 - val_accuracy: 0.1565\n",
      "Epoch 146/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1503 - accuracy: 0.2306 - val_loss: 0.2004 - val_accuracy: 0.2194\n",
      "Epoch 147/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1496 - accuracy: 0.2360 - val_loss: 0.1988 - val_accuracy: 0.2068\n",
      "Epoch 148/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1490 - accuracy: 0.2240 - val_loss: 0.1977 - val_accuracy: 0.1978\n",
      "Epoch 149/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1484 - accuracy: 0.2378 - val_loss: 0.1975 - val_accuracy: 0.1996\n",
      "Epoch 150/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1477 - accuracy: 0.2312 - val_loss: 0.1963 - val_accuracy: 0.1960\n",
      "Epoch 151/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1472 - accuracy: 0.2210 - val_loss: 0.1962 - val_accuracy: 0.2086\n",
      "Epoch 152/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1465 - accuracy: 0.2402 - val_loss: 0.1952 - val_accuracy: 0.2194\n",
      "Epoch 153/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1459 - accuracy: 0.2432 - val_loss: 0.1942 - val_accuracy: 0.1960\n",
      "Epoch 154/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1453 - accuracy: 0.2390 - val_loss: 0.1932 - val_accuracy: 0.1763\n",
      "Epoch 155/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1446 - accuracy: 0.2426 - val_loss: 0.1932 - val_accuracy: 0.1835\n",
      "Epoch 156/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1441 - accuracy: 0.2360 - val_loss: 0.1919 - val_accuracy: 0.2104\n",
      "Epoch 157/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1435 - accuracy: 0.2246 - val_loss: 0.1914 - val_accuracy: 0.1996\n",
      "Epoch 158/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1430 - accuracy: 0.2420 - val_loss: 0.1898 - val_accuracy: 0.1924\n",
      "Epoch 159/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1423 - accuracy: 0.2384 - val_loss: 0.1902 - val_accuracy: 0.2158\n",
      "Epoch 160/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1416 - accuracy: 0.2318 - val_loss: 0.1886 - val_accuracy: 0.2230\n",
      "Epoch 161/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1411 - accuracy: 0.2486 - val_loss: 0.1885 - val_accuracy: 0.2068\n",
      "Epoch 162/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1406 - accuracy: 0.2414 - val_loss: 0.1880 - val_accuracy: 0.2212\n",
      "Epoch 163/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1401 - accuracy: 0.2486 - val_loss: 0.1859 - val_accuracy: 0.2248\n",
      "Epoch 164/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1393 - accuracy: 0.2498 - val_loss: 0.1865 - val_accuracy: 0.2338\n",
      "Epoch 165/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1388 - accuracy: 0.2517 - val_loss: 0.1877 - val_accuracy: 0.1637\n",
      "Epoch 166/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1384 - accuracy: 0.2444 - val_loss: 0.1845 - val_accuracy: 0.2356\n",
      "Epoch 167/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1377 - accuracy: 0.2517 - val_loss: 0.1849 - val_accuracy: 0.1799\n",
      "Epoch 168/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1371 - accuracy: 0.2517 - val_loss: 0.1833 - val_accuracy: 0.2014\n",
      "Epoch 169/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1365 - accuracy: 0.2474 - val_loss: 0.1833 - val_accuracy: 0.2212\n",
      "Epoch 170/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1361 - accuracy: 0.2559 - val_loss: 0.1815 - val_accuracy: 0.2104\n",
      "Epoch 171/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1356 - accuracy: 0.2462 - val_loss: 0.1818 - val_accuracy: 0.2248\n",
      "Epoch 172/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1350 - accuracy: 0.2529 - val_loss: 0.1813 - val_accuracy: 0.1996\n",
      "Epoch 173/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1346 - accuracy: 0.2420 - val_loss: 0.1806 - val_accuracy: 0.2158\n",
      "Epoch 174/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1341 - accuracy: 0.2517 - val_loss: 0.1802 - val_accuracy: 0.2068\n",
      "Epoch 175/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1336 - accuracy: 0.2571 - val_loss: 0.1801 - val_accuracy: 0.2572\n",
      "Epoch 176/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1331 - accuracy: 0.2577 - val_loss: 0.1796 - val_accuracy: 0.2050\n",
      "Epoch 177/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1327 - accuracy: 0.2541 - val_loss: 0.1788 - val_accuracy: 0.2140\n",
      "Epoch 178/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1321 - accuracy: 0.2541 - val_loss: 0.1778 - val_accuracy: 0.2212\n",
      "Epoch 179/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1316 - accuracy: 0.2541 - val_loss: 0.1779 - val_accuracy: 0.2374\n",
      "Epoch 180/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1312 - accuracy: 0.2625 - val_loss: 0.1771 - val_accuracy: 0.1978\n",
      "Epoch 181/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1307 - accuracy: 0.2511 - val_loss: 0.1762 - val_accuracy: 0.2230\n",
      "Epoch 182/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1304 - accuracy: 0.2565 - val_loss: 0.1752 - val_accuracy: 0.2140\n",
      "Epoch 183/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1299 - accuracy: 0.2517 - val_loss: 0.1767 - val_accuracy: 0.1799\n",
      "Epoch 184/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1296 - accuracy: 0.2444 - val_loss: 0.1755 - val_accuracy: 0.2446\n",
      "Epoch 185/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1291 - accuracy: 0.2535 - val_loss: 0.1799 - val_accuracy: 0.1745\n",
      "Epoch 186/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1290 - accuracy: 0.2523 - val_loss: 0.1742 - val_accuracy: 0.1853\n",
      "Epoch 187/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1283 - accuracy: 0.2511 - val_loss: 0.1758 - val_accuracy: 0.2248\n",
      "Epoch 188/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1281 - accuracy: 0.2577 - val_loss: 0.1742 - val_accuracy: 0.2428\n",
      "Epoch 189/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1275 - accuracy: 0.2559 - val_loss: 0.1727 - val_accuracy: 0.2284\n",
      "Epoch 190/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1272 - accuracy: 0.2631 - val_loss: 0.1728 - val_accuracy: 0.2086\n",
      "Epoch 191/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1267 - accuracy: 0.2679 - val_loss: 0.1717 - val_accuracy: 0.2194\n",
      "Epoch 192/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1264 - accuracy: 0.2583 - val_loss: 0.1715 - val_accuracy: 0.2050\n",
      "Epoch 193/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1261 - accuracy: 0.2577 - val_loss: 0.1731 - val_accuracy: 0.2086\n",
      "Epoch 194/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1258 - accuracy: 0.2643 - val_loss: 0.1719 - val_accuracy: 0.2320\n",
      "Epoch 195/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1255 - accuracy: 0.2649 - val_loss: 0.1708 - val_accuracy: 0.2302\n",
      "Epoch 196/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1250 - accuracy: 0.2655 - val_loss: 0.1699 - val_accuracy: 0.2176\n",
      "Epoch 197/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1246 - accuracy: 0.2505 - val_loss: 0.1709 - val_accuracy: 0.2032\n",
      "Epoch 198/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1244 - accuracy: 0.2661 - val_loss: 0.1745 - val_accuracy: 0.2248\n",
      "Epoch 199/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1243 - accuracy: 0.2625 - val_loss: 0.1699 - val_accuracy: 0.2302\n",
      "Epoch 200/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1237 - accuracy: 0.2595 - val_loss: 0.1694 - val_accuracy: 0.2122\n",
      "Epoch 201/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1234 - accuracy: 0.2619 - val_loss: 0.1778 - val_accuracy: 0.2428\n",
      "Epoch 202/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1235 - accuracy: 0.2637 - val_loss: 0.1692 - val_accuracy: 0.2428\n",
      "Epoch 203/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1227 - accuracy: 0.2685 - val_loss: 0.1682 - val_accuracy: 0.2320\n",
      "Epoch 204/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1225 - accuracy: 0.2577 - val_loss: 0.1696 - val_accuracy: 0.2086\n",
      "Epoch 205/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1222 - accuracy: 0.2643 - val_loss: 0.1704 - val_accuracy: 0.2014\n",
      "Epoch 206/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1219 - accuracy: 0.2595 - val_loss: 0.1677 - val_accuracy: 0.2032\n",
      "Epoch 207/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1215 - accuracy: 0.2571 - val_loss: 0.1680 - val_accuracy: 0.2212\n",
      "Epoch 208/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1213 - accuracy: 0.2709 - val_loss: 0.1669 - val_accuracy: 0.2410\n",
      "Epoch 209/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1209 - accuracy: 0.2649 - val_loss: 0.1670 - val_accuracy: 0.2518\n",
      "Epoch 210/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1206 - accuracy: 0.2607 - val_loss: 0.1666 - val_accuracy: 0.2302\n",
      "Epoch 211/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1203 - accuracy: 0.2649 - val_loss: 0.1661 - val_accuracy: 0.1960\n",
      "Epoch 212/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1201 - accuracy: 0.2583 - val_loss: 0.1671 - val_accuracy: 0.2770\n",
      "Epoch 213/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1199 - accuracy: 0.2637 - val_loss: 0.1653 - val_accuracy: 0.2374\n",
      "Epoch 214/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1194 - accuracy: 0.2655 - val_loss: 0.1654 - val_accuracy: 0.2410\n",
      "Epoch 215/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1192 - accuracy: 0.2697 - val_loss: 0.1668 - val_accuracy: 0.2212\n",
      "Epoch 216/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1191 - accuracy: 0.2727 - val_loss: 0.1650 - val_accuracy: 0.2428\n",
      "Epoch 217/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1186 - accuracy: 0.2613 - val_loss: 0.1654 - val_accuracy: 0.2554\n",
      "Epoch 218/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1185 - accuracy: 0.2679 - val_loss: 0.1641 - val_accuracy: 0.2338\n",
      "Epoch 219/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1180 - accuracy: 0.2637 - val_loss: 0.1643 - val_accuracy: 0.2662\n",
      "Epoch 220/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1179 - accuracy: 0.2613 - val_loss: 0.1654 - val_accuracy: 0.2248\n",
      "Epoch 221/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1177 - accuracy: 0.2691 - val_loss: 0.1633 - val_accuracy: 0.2536\n",
      "Epoch 222/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1173 - accuracy: 0.2769 - val_loss: 0.1646 - val_accuracy: 0.2086\n",
      "Epoch 223/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1173 - accuracy: 0.2679 - val_loss: 0.1638 - val_accuracy: 0.2446\n",
      "Epoch 224/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1169 - accuracy: 0.2769 - val_loss: 0.1633 - val_accuracy: 0.2572\n",
      "Epoch 225/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1166 - accuracy: 0.2709 - val_loss: 0.1624 - val_accuracy: 0.2338\n",
      "Epoch 226/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1164 - accuracy: 0.2613 - val_loss: 0.1624 - val_accuracy: 0.2248\n",
      "Epoch 227/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1162 - accuracy: 0.2733 - val_loss: 0.1624 - val_accuracy: 0.2608\n",
      "Epoch 228/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1159 - accuracy: 0.2697 - val_loss: 0.1627 - val_accuracy: 0.2500\n",
      "Epoch 229/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1156 - accuracy: 0.2763 - val_loss: 0.1623 - val_accuracy: 0.2320\n",
      "Epoch 230/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1153 - accuracy: 0.2727 - val_loss: 0.1618 - val_accuracy: 0.2662\n",
      "Epoch 231/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1152 - accuracy: 0.2721 - val_loss: 0.1712 - val_accuracy: 0.2104\n",
      "Epoch 232/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1153 - accuracy: 0.2703 - val_loss: 0.1603 - val_accuracy: 0.2140\n",
      "Epoch 233/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1147 - accuracy: 0.2715 - val_loss: 0.1610 - val_accuracy: 0.2050\n",
      "Epoch 234/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1144 - accuracy: 0.2709 - val_loss: 0.1606 - val_accuracy: 0.2428\n",
      "Epoch 235/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1142 - accuracy: 0.2775 - val_loss: 0.1598 - val_accuracy: 0.2374\n",
      "Epoch 236/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1139 - accuracy: 0.2673 - val_loss: 0.1596 - val_accuracy: 0.2428\n",
      "Epoch 237/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1137 - accuracy: 0.2739 - val_loss: 0.1675 - val_accuracy: 0.2860\n",
      "Epoch 238/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1139 - accuracy: 0.2751 - val_loss: 0.1614 - val_accuracy: 0.2428\n",
      "Epoch 239/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1134 - accuracy: 0.2685 - val_loss: 0.1595 - val_accuracy: 0.2536\n",
      "Epoch 240/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1130 - accuracy: 0.2775 - val_loss: 0.1600 - val_accuracy: 0.2446\n",
      "Epoch 241/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1128 - accuracy: 0.2709 - val_loss: 0.1578 - val_accuracy: 0.2392\n",
      "Epoch 242/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1126 - accuracy: 0.2661 - val_loss: 0.1585 - val_accuracy: 0.2536\n",
      "Epoch 243/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1123 - accuracy: 0.2769 - val_loss: 0.1600 - val_accuracy: 0.2284\n",
      "Epoch 244/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1121 - accuracy: 0.2763 - val_loss: 0.1583 - val_accuracy: 0.2410\n",
      "Epoch 245/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1118 - accuracy: 0.2745 - val_loss: 0.1631 - val_accuracy: 0.2320\n",
      "Epoch 246/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1119 - accuracy: 0.2763 - val_loss: 0.1581 - val_accuracy: 0.2320\n",
      "Epoch 247/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1114 - accuracy: 0.2721 - val_loss: 0.1576 - val_accuracy: 0.2572\n",
      "Epoch 248/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1112 - accuracy: 0.2739 - val_loss: 0.1571 - val_accuracy: 0.2266\n",
      "Epoch 249/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1111 - accuracy: 0.2721 - val_loss: 0.1620 - val_accuracy: 0.2176\n",
      "Epoch 250/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1111 - accuracy: 0.2745 - val_loss: 0.1567 - val_accuracy: 0.2680\n",
      "Epoch 251/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1107 - accuracy: 0.2745 - val_loss: 0.1576 - val_accuracy: 0.2626\n",
      "Epoch 252/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1105 - accuracy: 0.2661 - val_loss: 0.1563 - val_accuracy: 0.2482\n",
      "Epoch 253/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1101 - accuracy: 0.2673 - val_loss: 0.1565 - val_accuracy: 0.2500\n",
      "Epoch 254/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1100 - accuracy: 0.2805 - val_loss: 0.1550 - val_accuracy: 0.2500\n",
      "Epoch 255/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1097 - accuracy: 0.2643 - val_loss: 0.1565 - val_accuracy: 0.2842\n",
      "Epoch 256/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1096 - accuracy: 0.2739 - val_loss: 0.1557 - val_accuracy: 0.2464\n",
      "Epoch 257/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1093 - accuracy: 0.2727 - val_loss: 0.1557 - val_accuracy: 0.2608\n",
      "Epoch 258/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1092 - accuracy: 0.2643 - val_loss: 0.1575 - val_accuracy: 0.2572\n",
      "Epoch 259/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1089 - accuracy: 0.2787 - val_loss: 0.1547 - val_accuracy: 0.2338\n",
      "Epoch 260/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1087 - accuracy: 0.2685 - val_loss: 0.1560 - val_accuracy: 0.2500\n",
      "Epoch 261/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1085 - accuracy: 0.2697 - val_loss: 0.1546 - val_accuracy: 0.2572\n",
      "Epoch 262/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1082 - accuracy: 0.2697 - val_loss: 0.1549 - val_accuracy: 0.2356\n",
      "Epoch 263/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1080 - accuracy: 0.2697 - val_loss: 0.1553 - val_accuracy: 0.2482\n",
      "Epoch 264/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1078 - accuracy: 0.2763 - val_loss: 0.1542 - val_accuracy: 0.2248\n",
      "Epoch 265/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1078 - accuracy: 0.2673 - val_loss: 0.1541 - val_accuracy: 0.2590\n",
      "Epoch 266/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1073 - accuracy: 0.2757 - val_loss: 0.1543 - val_accuracy: 0.2446\n",
      "Epoch 267/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1072 - accuracy: 0.2757 - val_loss: 0.1551 - val_accuracy: 0.2644\n",
      "Epoch 268/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1071 - accuracy: 0.2745 - val_loss: 0.1534 - val_accuracy: 0.2554\n",
      "Epoch 269/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1067 - accuracy: 0.2745 - val_loss: 0.1526 - val_accuracy: 0.2572\n",
      "Epoch 270/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1066 - accuracy: 0.2703 - val_loss: 0.1519 - val_accuracy: 0.2356\n",
      "Epoch 271/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1063 - accuracy: 0.2679 - val_loss: 0.1534 - val_accuracy: 0.2536\n",
      "Epoch 272/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1062 - accuracy: 0.2721 - val_loss: 0.1532 - val_accuracy: 0.2518\n",
      "Epoch 273/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1060 - accuracy: 0.2709 - val_loss: 0.1531 - val_accuracy: 0.2518\n",
      "Epoch 274/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1057 - accuracy: 0.2715 - val_loss: 0.1524 - val_accuracy: 0.2446\n",
      "Epoch 275/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1054 - accuracy: 0.2721 - val_loss: 0.1535 - val_accuracy: 0.2662\n",
      "Epoch 276/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1052 - accuracy: 0.2781 - val_loss: 0.1523 - val_accuracy: 0.2590\n",
      "Epoch 277/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1051 - accuracy: 0.2727 - val_loss: 0.1525 - val_accuracy: 0.2752\n",
      "Epoch 278/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1049 - accuracy: 0.2811 - val_loss: 0.1529 - val_accuracy: 0.2266\n",
      "Epoch 279/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1048 - accuracy: 0.2757 - val_loss: 0.1520 - val_accuracy: 0.2464\n",
      "Epoch 280/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1045 - accuracy: 0.2685 - val_loss: 0.1531 - val_accuracy: 0.2698\n",
      "Epoch 281/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1043 - accuracy: 0.2769 - val_loss: 0.1511 - val_accuracy: 0.2788\n",
      "Epoch 282/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1039 - accuracy: 0.2751 - val_loss: 0.1512 - val_accuracy: 0.2770\n",
      "Epoch 283/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1038 - accuracy: 0.2781 - val_loss: 0.1503 - val_accuracy: 0.2608\n",
      "Epoch 284/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1037 - accuracy: 0.2739 - val_loss: 0.1514 - val_accuracy: 0.2860\n",
      "Epoch 285/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1034 - accuracy: 0.2811 - val_loss: 0.1517 - val_accuracy: 0.2734\n",
      "Epoch 286/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1032 - accuracy: 0.2763 - val_loss: 0.1501 - val_accuracy: 0.2752\n",
      "Epoch 287/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1030 - accuracy: 0.2835 - val_loss: 0.1505 - val_accuracy: 0.2482\n",
      "Epoch 288/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1029 - accuracy: 0.2811 - val_loss: 0.1532 - val_accuracy: 0.2698\n",
      "Epoch 289/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1027 - accuracy: 0.2817 - val_loss: 0.1523 - val_accuracy: 0.2464\n",
      "Epoch 290/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1025 - accuracy: 0.2859 - val_loss: 0.1501 - val_accuracy: 0.2806\n",
      "Epoch 291/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1021 - accuracy: 0.2799 - val_loss: 0.1500 - val_accuracy: 0.2608\n",
      "Epoch 292/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1020 - accuracy: 0.2859 - val_loss: 0.1510 - val_accuracy: 0.2662\n",
      "Epoch 293/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1018 - accuracy: 0.2823 - val_loss: 0.1493 - val_accuracy: 0.2572\n",
      "Epoch 294/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1016 - accuracy: 0.2745 - val_loss: 0.1489 - val_accuracy: 0.2752\n",
      "Epoch 295/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1013 - accuracy: 0.2793 - val_loss: 0.1489 - val_accuracy: 0.2986\n",
      "Epoch 296/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1012 - accuracy: 0.2913 - val_loss: 0.1483 - val_accuracy: 0.2644\n",
      "Epoch 297/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1009 - accuracy: 0.2787 - val_loss: 0.1477 - val_accuracy: 0.2608\n",
      "Epoch 298/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1007 - accuracy: 0.2787 - val_loss: 0.1468 - val_accuracy: 0.2500\n",
      "Epoch 299/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1004 - accuracy: 0.2829 - val_loss: 0.1495 - val_accuracy: 0.2644\n",
      "Epoch 300/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1004 - accuracy: 0.2829 - val_loss: 0.1477 - val_accuracy: 0.2680\n",
      "Epoch 301/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1001 - accuracy: 0.2877 - val_loss: 0.1481 - val_accuracy: 0.2788\n",
      "Epoch 302/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1000 - accuracy: 0.2829 - val_loss: 0.1475 - val_accuracy: 0.2554\n",
      "Epoch 303/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0995 - accuracy: 0.2835 - val_loss: 0.1471 - val_accuracy: 0.2608\n",
      "Epoch 304/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0993 - accuracy: 0.2853 - val_loss: 0.1505 - val_accuracy: 0.1960\n",
      "Epoch 305/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0996 - accuracy: 0.2781 - val_loss: 0.1483 - val_accuracy: 0.2734\n",
      "Epoch 306/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0991 - accuracy: 0.2853 - val_loss: 0.1505 - val_accuracy: 0.3004\n",
      "Epoch 307/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0991 - accuracy: 0.2955 - val_loss: 0.1459 - val_accuracy: 0.2680\n",
      "Epoch 308/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0986 - accuracy: 0.2859 - val_loss: 0.1475 - val_accuracy: 0.2608\n",
      "Epoch 309/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0984 - accuracy: 0.2853 - val_loss: 0.1490 - val_accuracy: 0.2842\n",
      "Epoch 310/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0982 - accuracy: 0.2943 - val_loss: 0.1477 - val_accuracy: 0.2662\n",
      "Epoch 311/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0981 - accuracy: 0.2985 - val_loss: 0.1460 - val_accuracy: 0.2554\n",
      "Epoch 312/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0978 - accuracy: 0.2889 - val_loss: 0.1571 - val_accuracy: 0.1996\n",
      "Epoch 313/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0982 - accuracy: 0.2805 - val_loss: 0.1450 - val_accuracy: 0.2518\n",
      "Epoch 314/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0974 - accuracy: 0.2859 - val_loss: 0.1454 - val_accuracy: 0.2320\n",
      "Epoch 315/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0973 - accuracy: 0.2847 - val_loss: 0.1456 - val_accuracy: 0.2662\n",
      "Epoch 316/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0970 - accuracy: 0.2865 - val_loss: 0.1473 - val_accuracy: 0.2536\n",
      "Epoch 317/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0969 - accuracy: 0.2877 - val_loss: 0.1453 - val_accuracy: 0.2716\n",
      "Epoch 318/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0966 - accuracy: 0.2907 - val_loss: 0.1456 - val_accuracy: 0.2608\n",
      "Epoch 319/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0963 - accuracy: 0.2943 - val_loss: 0.1451 - val_accuracy: 0.2626\n",
      "Epoch 320/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0961 - accuracy: 0.2913 - val_loss: 0.1471 - val_accuracy: 0.2770\n",
      "Epoch 321/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0959 - accuracy: 0.2895 - val_loss: 0.1448 - val_accuracy: 0.2464\n",
      "Epoch 322/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0958 - accuracy: 0.2943 - val_loss: 0.1450 - val_accuracy: 0.2644\n",
      "Epoch 323/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0956 - accuracy: 0.2925 - val_loss: 0.1442 - val_accuracy: 0.2734\n",
      "Epoch 324/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0952 - accuracy: 0.2877 - val_loss: 0.1464 - val_accuracy: 0.2536\n",
      "Epoch 325/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0951 - accuracy: 0.2925 - val_loss: 0.1453 - val_accuracy: 0.2896\n",
      "Epoch 326/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0951 - accuracy: 0.2871 - val_loss: 0.1439 - val_accuracy: 0.2752\n",
      "Epoch 327/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0947 - accuracy: 0.2925 - val_loss: 0.1432 - val_accuracy: 0.2536\n",
      "Epoch 328/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0945 - accuracy: 0.2883 - val_loss: 0.1446 - val_accuracy: 0.2482\n",
      "Epoch 329/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0945 - accuracy: 0.2949 - val_loss: 0.1457 - val_accuracy: 0.2680\n",
      "Epoch 330/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0942 - accuracy: 0.2925 - val_loss: 0.1450 - val_accuracy: 0.2590\n",
      "Epoch 331/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0940 - accuracy: 0.2931 - val_loss: 0.1436 - val_accuracy: 0.2788\n",
      "Epoch 332/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0937 - accuracy: 0.2937 - val_loss: 0.1429 - val_accuracy: 0.2734\n",
      "Epoch 333/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0937 - accuracy: 0.2937 - val_loss: 0.1424 - val_accuracy: 0.2788\n",
      "Epoch 334/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0933 - accuracy: 0.2991 - val_loss: 0.1422 - val_accuracy: 0.2680\n",
      "Epoch 335/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0932 - accuracy: 0.2931 - val_loss: 0.1432 - val_accuracy: 0.2932\n",
      "Epoch 336/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0929 - accuracy: 0.3021 - val_loss: 0.1416 - val_accuracy: 0.2806\n",
      "Epoch 337/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0927 - accuracy: 0.3045 - val_loss: 0.1425 - val_accuracy: 0.2572\n",
      "Epoch 338/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0927 - accuracy: 0.2913 - val_loss: 0.1451 - val_accuracy: 0.2770\n",
      "Epoch 339/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0925 - accuracy: 0.2991 - val_loss: 0.1429 - val_accuracy: 0.2914\n",
      "Epoch 340/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0922 - accuracy: 0.3027 - val_loss: 0.1428 - val_accuracy: 0.2806\n",
      "Epoch 341/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0920 - accuracy: 0.3015 - val_loss: 0.1437 - val_accuracy: 0.2806\n",
      "Epoch 342/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0919 - accuracy: 0.2955 - val_loss: 0.1421 - val_accuracy: 0.2860\n",
      "Epoch 343/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0916 - accuracy: 0.2949 - val_loss: 0.1417 - val_accuracy: 0.2914\n",
      "Epoch 344/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0914 - accuracy: 0.3015 - val_loss: 0.1400 - val_accuracy: 0.2554\n",
      "Epoch 345/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0913 - accuracy: 0.2943 - val_loss: 0.1397 - val_accuracy: 0.2716\n",
      "Epoch 346/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0911 - accuracy: 0.2931 - val_loss: 0.1405 - val_accuracy: 0.2806\n",
      "Epoch 347/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0909 - accuracy: 0.2889 - val_loss: 0.1406 - val_accuracy: 0.2896\n",
      "Epoch 348/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0906 - accuracy: 0.2919 - val_loss: 0.1457 - val_accuracy: 0.3147\n",
      "Epoch 349/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0908 - accuracy: 0.2979 - val_loss: 0.1411 - val_accuracy: 0.3004\n",
      "Epoch 350/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0902 - accuracy: 0.3033 - val_loss: 0.1416 - val_accuracy: 0.2788\n",
      "Epoch 351/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0902 - accuracy: 0.2985 - val_loss: 0.1399 - val_accuracy: 0.2806\n",
      "Epoch 352/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0899 - accuracy: 0.2967 - val_loss: 0.1391 - val_accuracy: 0.2896\n",
      "Epoch 353/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0897 - accuracy: 0.2961 - val_loss: 0.1389 - val_accuracy: 0.2968\n",
      "Epoch 354/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0897 - accuracy: 0.3003 - val_loss: 0.1398 - val_accuracy: 0.2860\n",
      "Epoch 355/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0896 - accuracy: 0.2991 - val_loss: 0.1395 - val_accuracy: 0.2806\n",
      "Epoch 356/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0893 - accuracy: 0.2967 - val_loss: 0.1532 - val_accuracy: 0.2446\n",
      "Epoch 357/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0897 - accuracy: 0.2991 - val_loss: 0.1388 - val_accuracy: 0.3094\n",
      "Epoch 358/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0891 - accuracy: 0.3051 - val_loss: 0.1401 - val_accuracy: 0.2626\n",
      "Epoch 359/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0889 - accuracy: 0.2955 - val_loss: 0.1442 - val_accuracy: 0.2878\n",
      "Epoch 360/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0888 - accuracy: 0.2979 - val_loss: 0.1386 - val_accuracy: 0.3022\n",
      "Epoch 361/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0886 - accuracy: 0.3045 - val_loss: 0.1385 - val_accuracy: 0.2860\n",
      "Epoch 362/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0883 - accuracy: 0.2991 - val_loss: 0.1380 - val_accuracy: 0.2968\n",
      "Epoch 363/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0882 - accuracy: 0.3021 - val_loss: 0.1411 - val_accuracy: 0.2608\n",
      "Epoch 364/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0881 - accuracy: 0.3027 - val_loss: 0.1386 - val_accuracy: 0.2806\n",
      "Epoch 365/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0877 - accuracy: 0.3015 - val_loss: 0.1378 - val_accuracy: 0.2716\n",
      "Epoch 366/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0877 - accuracy: 0.2925 - val_loss: 0.1358 - val_accuracy: 0.2896\n",
      "Epoch 367/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0875 - accuracy: 0.2979 - val_loss: 0.1385 - val_accuracy: 0.2410\n",
      "Epoch 368/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0876 - accuracy: 0.2997 - val_loss: 0.1369 - val_accuracy: 0.2878\n",
      "Epoch 369/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0871 - accuracy: 0.3045 - val_loss: 0.1364 - val_accuracy: 0.2950\n",
      "Epoch 370/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0870 - accuracy: 0.3045 - val_loss: 0.1379 - val_accuracy: 0.2824\n",
      "Epoch 371/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0870 - accuracy: 0.3087 - val_loss: 0.1387 - val_accuracy: 0.2806\n",
      "Epoch 372/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0867 - accuracy: 0.3093 - val_loss: 0.1374 - val_accuracy: 0.2662\n",
      "Epoch 373/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0867 - accuracy: 0.3021 - val_loss: 0.1359 - val_accuracy: 0.2788\n",
      "Epoch 374/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0865 - accuracy: 0.3045 - val_loss: 0.1462 - val_accuracy: 0.2680\n",
      "Epoch 375/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0866 - accuracy: 0.3153 - val_loss: 0.1372 - val_accuracy: 0.2878\n",
      "Epoch 376/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0864 - accuracy: 0.3045 - val_loss: 0.1378 - val_accuracy: 0.2626\n",
      "Epoch 377/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0861 - accuracy: 0.3039 - val_loss: 0.1377 - val_accuracy: 0.2842\n",
      "Epoch 378/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0860 - accuracy: 0.2955 - val_loss: 0.1383 - val_accuracy: 0.3129\n",
      "Epoch 379/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0858 - accuracy: 0.3045 - val_loss: 0.1350 - val_accuracy: 0.2878\n",
      "Epoch 380/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0856 - accuracy: 0.3063 - val_loss: 0.1390 - val_accuracy: 0.2518\n",
      "Epoch 381/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0855 - accuracy: 0.2931 - val_loss: 0.1358 - val_accuracy: 0.2770\n",
      "Epoch 382/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0852 - accuracy: 0.2985 - val_loss: 0.1357 - val_accuracy: 0.2878\n",
      "Epoch 383/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0851 - accuracy: 0.3003 - val_loss: 0.1352 - val_accuracy: 0.2950\n",
      "Epoch 384/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0850 - accuracy: 0.3003 - val_loss: 0.1360 - val_accuracy: 0.2608\n",
      "Epoch 385/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0849 - accuracy: 0.3081 - val_loss: 0.1373 - val_accuracy: 0.2968\n",
      "Epoch 386/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0848 - accuracy: 0.3141 - val_loss: 0.1356 - val_accuracy: 0.2734\n",
      "Epoch 387/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0847 - accuracy: 0.2979 - val_loss: 0.1339 - val_accuracy: 0.2770\n",
      "Epoch 388/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0845 - accuracy: 0.3003 - val_loss: 0.1365 - val_accuracy: 0.2842\n",
      "Epoch 389/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0844 - accuracy: 0.3147 - val_loss: 0.1339 - val_accuracy: 0.2734\n",
      "Epoch 390/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0842 - accuracy: 0.3039 - val_loss: 0.1447 - val_accuracy: 0.3291\n",
      "Epoch 391/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0845 - accuracy: 0.3057 - val_loss: 0.1390 - val_accuracy: 0.3291\n",
      "Epoch 392/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0842 - accuracy: 0.3123 - val_loss: 0.1364 - val_accuracy: 0.2932\n",
      "Epoch 393/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0840 - accuracy: 0.3045 - val_loss: 0.1432 - val_accuracy: 0.2572\n",
      "Epoch 394/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0841 - accuracy: 0.3111 - val_loss: 0.1358 - val_accuracy: 0.2932\n",
      "Epoch 395/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0835 - accuracy: 0.3009 - val_loss: 0.1343 - val_accuracy: 0.2680\n",
      "Epoch 396/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0838 - accuracy: 0.3003 - val_loss: 0.1343 - val_accuracy: 0.2860\n",
      "Epoch 397/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0835 - accuracy: 0.3123 - val_loss: 0.1374 - val_accuracy: 0.2860\n",
      "Epoch 398/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0834 - accuracy: 0.3105 - val_loss: 0.1351 - val_accuracy: 0.2932\n",
      "Epoch 399/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0831 - accuracy: 0.3069 - val_loss: 0.1352 - val_accuracy: 0.2662\n",
      "Epoch 400/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0831 - accuracy: 0.3033 - val_loss: 0.1344 - val_accuracy: 0.2842\n",
      "Epoch 401/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0829 - accuracy: 0.3033 - val_loss: 0.1372 - val_accuracy: 0.2806\n",
      "Epoch 402/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0830 - accuracy: 0.3087 - val_loss: 0.1334 - val_accuracy: 0.3022\n",
      "Epoch 403/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0826 - accuracy: 0.3081 - val_loss: 0.1341 - val_accuracy: 0.2482\n",
      "Epoch 404/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0827 - accuracy: 0.3027 - val_loss: 0.1379 - val_accuracy: 0.2878\n",
      "Epoch 405/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0829 - accuracy: 0.3099 - val_loss: 0.1348 - val_accuracy: 0.2788\n",
      "Epoch 406/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0825 - accuracy: 0.3081 - val_loss: 0.1325 - val_accuracy: 0.2842\n",
      "Epoch 407/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0822 - accuracy: 0.3069 - val_loss: 0.1458 - val_accuracy: 0.2356\n",
      "Epoch 408/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0829 - accuracy: 0.3111 - val_loss: 0.1450 - val_accuracy: 0.2698\n",
      "Epoch 409/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0826 - accuracy: 0.3087 - val_loss: 0.1328 - val_accuracy: 0.2860\n",
      "Epoch 410/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0821 - accuracy: 0.3057 - val_loss: 0.1363 - val_accuracy: 0.2842\n",
      "Epoch 411/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0820 - accuracy: 0.3069 - val_loss: 0.1338 - val_accuracy: 0.2824\n",
      "Epoch 412/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0818 - accuracy: 0.3045 - val_loss: 0.1324 - val_accuracy: 0.3040\n",
      "Epoch 413/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0816 - accuracy: 0.3117 - val_loss: 0.1383 - val_accuracy: 0.2824\n",
      "Epoch 414/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0817 - accuracy: 0.3069 - val_loss: 0.1370 - val_accuracy: 0.2788\n",
      "Epoch 415/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0817 - accuracy: 0.3117 - val_loss: 0.1327 - val_accuracy: 0.2842\n",
      "Epoch 416/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0812 - accuracy: 0.3039 - val_loss: 0.1332 - val_accuracy: 0.3165\n",
      "Epoch 417/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0812 - accuracy: 0.3063 - val_loss: 0.1326 - val_accuracy: 0.3076\n",
      "Epoch 418/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0812 - accuracy: 0.3069 - val_loss: 0.1354 - val_accuracy: 0.2986\n",
      "Epoch 419/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0811 - accuracy: 0.3063 - val_loss: 0.1335 - val_accuracy: 0.3004\n",
      "Epoch 420/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0809 - accuracy: 0.3111 - val_loss: 0.1328 - val_accuracy: 0.2824\n",
      "Epoch 421/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0808 - accuracy: 0.3147 - val_loss: 0.1349 - val_accuracy: 0.2518\n",
      "Epoch 422/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0809 - accuracy: 0.3039 - val_loss: 0.1328 - val_accuracy: 0.3094\n",
      "Epoch 423/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0806 - accuracy: 0.3111 - val_loss: 0.1336 - val_accuracy: 0.2788\n",
      "Epoch 424/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0805 - accuracy: 0.3081 - val_loss: 0.1323 - val_accuracy: 0.2716\n",
      "Epoch 425/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0807 - accuracy: 0.3039 - val_loss: 0.1322 - val_accuracy: 0.2914\n",
      "Epoch 426/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0803 - accuracy: 0.3051 - val_loss: 0.1330 - val_accuracy: 0.2914\n",
      "Epoch 427/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0803 - accuracy: 0.3105 - val_loss: 0.1316 - val_accuracy: 0.2788\n",
      "Epoch 428/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0801 - accuracy: 0.3045 - val_loss: 0.1325 - val_accuracy: 0.3094\n",
      "Epoch 429/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0801 - accuracy: 0.3069 - val_loss: 0.1320 - val_accuracy: 0.2716\n",
      "Epoch 430/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0800 - accuracy: 0.3015 - val_loss: 0.1319 - val_accuracy: 0.2950\n",
      "Epoch 431/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0798 - accuracy: 0.3033 - val_loss: 0.1308 - val_accuracy: 0.2878\n",
      "Epoch 432/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0797 - accuracy: 0.3069 - val_loss: 0.1311 - val_accuracy: 0.2896\n",
      "Epoch 433/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0795 - accuracy: 0.3135 - val_loss: 0.1311 - val_accuracy: 0.2914\n",
      "Epoch 434/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0794 - accuracy: 0.3057 - val_loss: 0.1301 - val_accuracy: 0.2662\n",
      "Epoch 435/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0793 - accuracy: 0.3123 - val_loss: 0.1419 - val_accuracy: 0.2788\n",
      "Epoch 436/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0798 - accuracy: 0.3123 - val_loss: 0.1342 - val_accuracy: 0.2986\n",
      "Epoch 437/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0793 - accuracy: 0.3105 - val_loss: 0.1349 - val_accuracy: 0.2788\n",
      "Epoch 438/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0793 - accuracy: 0.3039 - val_loss: 0.1305 - val_accuracy: 0.3094\n",
      "Epoch 439/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0791 - accuracy: 0.3069 - val_loss: 0.1299 - val_accuracy: 0.2824\n",
      "Epoch 440/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0790 - accuracy: 0.3171 - val_loss: 0.1355 - val_accuracy: 0.2788\n",
      "Epoch 441/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0791 - accuracy: 0.3171 - val_loss: 0.1328 - val_accuracy: 0.2644\n",
      "Epoch 442/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0791 - accuracy: 0.3093 - val_loss: 0.1317 - val_accuracy: 0.3076\n",
      "Epoch 443/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0788 - accuracy: 0.3075 - val_loss: 0.1381 - val_accuracy: 0.3076\n",
      "Epoch 444/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0791 - accuracy: 0.3093 - val_loss: 0.1328 - val_accuracy: 0.3004\n",
      "Epoch 445/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0785 - accuracy: 0.3075 - val_loss: 0.1308 - val_accuracy: 0.2590\n",
      "Epoch 446/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0785 - accuracy: 0.3153 - val_loss: 0.1413 - val_accuracy: 0.2734\n",
      "Epoch 447/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0788 - accuracy: 0.3147 - val_loss: 0.1314 - val_accuracy: 0.2770\n",
      "Epoch 448/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0783 - accuracy: 0.3009 - val_loss: 0.1338 - val_accuracy: 0.3273\n",
      "Epoch 449/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0783 - accuracy: 0.3147 - val_loss: 0.1342 - val_accuracy: 0.3094\n",
      "Epoch 450/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0781 - accuracy: 0.3117 - val_loss: 0.1313 - val_accuracy: 0.2950\n",
      "Epoch 451/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0781 - accuracy: 0.3135 - val_loss: 0.1306 - val_accuracy: 0.2950\n",
      "Epoch 452/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0779 - accuracy: 0.3063 - val_loss: 0.1343 - val_accuracy: 0.2986\n",
      "Epoch 453/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0779 - accuracy: 0.3153 - val_loss: 0.1311 - val_accuracy: 0.2788\n",
      "Epoch 454/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0778 - accuracy: 0.3063 - val_loss: 0.1313 - val_accuracy: 0.2932\n",
      "Epoch 455/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0776 - accuracy: 0.3105 - val_loss: 0.1309 - val_accuracy: 0.2734\n",
      "Epoch 456/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0777 - accuracy: 0.3105 - val_loss: 0.1318 - val_accuracy: 0.3076\n",
      "Epoch 457/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0774 - accuracy: 0.3129 - val_loss: 0.1336 - val_accuracy: 0.3058\n",
      "Epoch 458/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0777 - accuracy: 0.3099 - val_loss: 0.1299 - val_accuracy: 0.2914\n",
      "Epoch 459/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0774 - accuracy: 0.3093 - val_loss: 0.1289 - val_accuracy: 0.2680\n",
      "Epoch 460/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0774 - accuracy: 0.3069 - val_loss: 0.1293 - val_accuracy: 0.2914\n",
      "Epoch 461/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0769 - accuracy: 0.3129 - val_loss: 0.1306 - val_accuracy: 0.3147\n",
      "Epoch 462/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0773 - accuracy: 0.3141 - val_loss: 0.1293 - val_accuracy: 0.2608\n",
      "Epoch 463/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0770 - accuracy: 0.3189 - val_loss: 0.1289 - val_accuracy: 0.2968\n",
      "Epoch 464/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0768 - accuracy: 0.3069 - val_loss: 0.1289 - val_accuracy: 0.3112\n",
      "Epoch 465/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0768 - accuracy: 0.3099 - val_loss: 0.1292 - val_accuracy: 0.3022\n",
      "Epoch 466/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0767 - accuracy: 0.3087 - val_loss: 0.1298 - val_accuracy: 0.2896\n",
      "Epoch 467/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0767 - accuracy: 0.3129 - val_loss: 0.1324 - val_accuracy: 0.3291\n",
      "Epoch 468/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0766 - accuracy: 0.3153 - val_loss: 0.1273 - val_accuracy: 0.2968\n",
      "Epoch 469/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0765 - accuracy: 0.3117 - val_loss: 0.1294 - val_accuracy: 0.3040\n",
      "Epoch 470/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0765 - accuracy: 0.3147 - val_loss: 0.1289 - val_accuracy: 0.2860\n",
      "Epoch 471/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0764 - accuracy: 0.3111 - val_loss: 0.1288 - val_accuracy: 0.2896\n",
      "Epoch 472/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0761 - accuracy: 0.3177 - val_loss: 0.1377 - val_accuracy: 0.3183\n",
      "Epoch 473/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0763 - accuracy: 0.3093 - val_loss: 0.1289 - val_accuracy: 0.2770\n",
      "Epoch 474/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0762 - accuracy: 0.3081 - val_loss: 0.1280 - val_accuracy: 0.2860\n",
      "Epoch 475/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0760 - accuracy: 0.3075 - val_loss: 0.1286 - val_accuracy: 0.3094\n",
      "Epoch 476/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0759 - accuracy: 0.3117 - val_loss: 0.1291 - val_accuracy: 0.2932\n",
      "Epoch 477/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0758 - accuracy: 0.3141 - val_loss: 0.1298 - val_accuracy: 0.3112\n",
      "Epoch 478/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0758 - accuracy: 0.3135 - val_loss: 0.1318 - val_accuracy: 0.3165\n",
      "Epoch 479/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0757 - accuracy: 0.3141 - val_loss: 0.1298 - val_accuracy: 0.3022\n",
      "Epoch 480/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0755 - accuracy: 0.3111 - val_loss: 0.1285 - val_accuracy: 0.2932\n",
      "Epoch 481/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0754 - accuracy: 0.3153 - val_loss: 0.1294 - val_accuracy: 0.3183\n",
      "Epoch 482/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0756 - accuracy: 0.3051 - val_loss: 0.1268 - val_accuracy: 0.2986\n",
      "Epoch 483/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0753 - accuracy: 0.3117 - val_loss: 0.1273 - val_accuracy: 0.2914\n",
      "Epoch 484/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0754 - accuracy: 0.3123 - val_loss: 0.1307 - val_accuracy: 0.3201\n",
      "Epoch 485/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0752 - accuracy: 0.3147 - val_loss: 0.1293 - val_accuracy: 0.2914\n",
      "Epoch 486/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0752 - accuracy: 0.3111 - val_loss: 0.1278 - val_accuracy: 0.2932\n",
      "Epoch 487/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0750 - accuracy: 0.3147 - val_loss: 0.1271 - val_accuracy: 0.2788\n",
      "Epoch 488/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0751 - accuracy: 0.3141 - val_loss: 0.1278 - val_accuracy: 0.2950\n",
      "Epoch 489/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0748 - accuracy: 0.3141 - val_loss: 0.1258 - val_accuracy: 0.2896\n",
      "Epoch 490/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0749 - accuracy: 0.3195 - val_loss: 0.1280 - val_accuracy: 0.2878\n",
      "Epoch 491/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0747 - accuracy: 0.3123 - val_loss: 0.1297 - val_accuracy: 0.2878\n",
      "Epoch 492/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0747 - accuracy: 0.3189 - val_loss: 0.1269 - val_accuracy: 0.2968\n",
      "Epoch 493/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0744 - accuracy: 0.3135 - val_loss: 0.1287 - val_accuracy: 0.2986\n",
      "Epoch 494/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0745 - accuracy: 0.3105 - val_loss: 0.1286 - val_accuracy: 0.3076\n",
      "Epoch 495/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0744 - accuracy: 0.3225 - val_loss: 0.1311 - val_accuracy: 0.2950\n",
      "Epoch 496/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0745 - accuracy: 0.3063 - val_loss: 0.1329 - val_accuracy: 0.3076\n",
      "Epoch 497/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0745 - accuracy: 0.3177 - val_loss: 0.1274 - val_accuracy: 0.3058\n",
      "Epoch 498/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0742 - accuracy: 0.3159 - val_loss: 0.1287 - val_accuracy: 0.2932\n",
      "Epoch 499/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0741 - accuracy: 0.3057 - val_loss: 0.1275 - val_accuracy: 0.2698\n",
      "Epoch 500/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0740 - accuracy: 0.3141 - val_loss: 0.1337 - val_accuracy: 0.2860\n",
      "Epoch 501/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0743 - accuracy: 0.3207 - val_loss: 0.1256 - val_accuracy: 0.3094\n",
      "Epoch 502/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0738 - accuracy: 0.3111 - val_loss: 0.1288 - val_accuracy: 0.3112\n",
      "Epoch 503/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0736 - accuracy: 0.3213 - val_loss: 0.1278 - val_accuracy: 0.3004\n",
      "Epoch 504/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0737 - accuracy: 0.3195 - val_loss: 0.1318 - val_accuracy: 0.3076\n",
      "Epoch 505/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0738 - accuracy: 0.3183 - val_loss: 0.1263 - val_accuracy: 0.2914\n",
      "Epoch 506/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0736 - accuracy: 0.3153 - val_loss: 0.1260 - val_accuracy: 0.2968\n",
      "Epoch 507/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0733 - accuracy: 0.3069 - val_loss: 0.1289 - val_accuracy: 0.3022\n",
      "Epoch 508/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0735 - accuracy: 0.3171 - val_loss: 0.1277 - val_accuracy: 0.3112\n",
      "Epoch 509/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0734 - accuracy: 0.3141 - val_loss: 0.1275 - val_accuracy: 0.3094\n",
      "Epoch 510/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0732 - accuracy: 0.3105 - val_loss: 0.1298 - val_accuracy: 0.2806\n",
      "Epoch 511/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0734 - accuracy: 0.3231 - val_loss: 0.1280 - val_accuracy: 0.3058\n",
      "Epoch 512/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0731 - accuracy: 0.3219 - val_loss: 0.1270 - val_accuracy: 0.3076\n",
      "Epoch 513/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0729 - accuracy: 0.3171 - val_loss: 0.1276 - val_accuracy: 0.2986\n",
      "Epoch 514/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0728 - accuracy: 0.3171 - val_loss: 0.1256 - val_accuracy: 0.3219\n",
      "Epoch 515/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0730 - accuracy: 0.3159 - val_loss: 0.1282 - val_accuracy: 0.3004\n",
      "Epoch 516/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0729 - accuracy: 0.3105 - val_loss: 0.1282 - val_accuracy: 0.3183\n",
      "Epoch 517/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0728 - accuracy: 0.3183 - val_loss: 0.1289 - val_accuracy: 0.2932\n",
      "Epoch 518/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0728 - accuracy: 0.3189 - val_loss: 0.1276 - val_accuracy: 0.3147\n",
      "Epoch 519/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0725 - accuracy: 0.3195 - val_loss: 0.1282 - val_accuracy: 0.2950\n",
      "Epoch 520/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0725 - accuracy: 0.3075 - val_loss: 0.1284 - val_accuracy: 0.2896\n",
      "Epoch 521/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0726 - accuracy: 0.3273 - val_loss: 0.1290 - val_accuracy: 0.3076\n",
      "Epoch 522/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0724 - accuracy: 0.3255 - val_loss: 0.1273 - val_accuracy: 0.3094\n",
      "Epoch 523/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0723 - accuracy: 0.3135 - val_loss: 0.1275 - val_accuracy: 0.2806\n",
      "Epoch 524/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0721 - accuracy: 0.3129 - val_loss: 0.1286 - val_accuracy: 0.3112\n",
      "Epoch 525/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0724 - accuracy: 0.3153 - val_loss: 0.1329 - val_accuracy: 0.3219\n",
      "Epoch 526/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0726 - accuracy: 0.3261 - val_loss: 0.1269 - val_accuracy: 0.3004\n",
      "Epoch 527/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0720 - accuracy: 0.3177 - val_loss: 0.1254 - val_accuracy: 0.2932\n",
      "Epoch 528/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0718 - accuracy: 0.3135 - val_loss: 0.1267 - val_accuracy: 0.2986\n",
      "Epoch 529/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0718 - accuracy: 0.3159 - val_loss: 0.1266 - val_accuracy: 0.3112\n",
      "Epoch 530/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0719 - accuracy: 0.3147 - val_loss: 0.1444 - val_accuracy: 0.3273\n",
      "Epoch 531/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0725 - accuracy: 0.3117 - val_loss: 0.1285 - val_accuracy: 0.3094\n",
      "Epoch 532/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0717 - accuracy: 0.3153 - val_loss: 0.1279 - val_accuracy: 0.3129\n",
      "Epoch 533/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0716 - accuracy: 0.3153 - val_loss: 0.1252 - val_accuracy: 0.3094\n",
      "Epoch 534/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0714 - accuracy: 0.3171 - val_loss: 0.1282 - val_accuracy: 0.3022\n",
      "Epoch 535/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0718 - accuracy: 0.3159 - val_loss: 0.1293 - val_accuracy: 0.3040\n",
      "Epoch 536/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0716 - accuracy: 0.3165 - val_loss: 0.1264 - val_accuracy: 0.2932\n",
      "Epoch 537/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0714 - accuracy: 0.3177 - val_loss: 0.1317 - val_accuracy: 0.2896\n",
      "Epoch 538/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0715 - accuracy: 0.3147 - val_loss: 0.1339 - val_accuracy: 0.2986\n",
      "Epoch 539/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0716 - accuracy: 0.3183 - val_loss: 0.1354 - val_accuracy: 0.2860\n",
      "Epoch 540/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0718 - accuracy: 0.3165 - val_loss: 0.1271 - val_accuracy: 0.3147\n",
      "Epoch 541/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0710 - accuracy: 0.3183 - val_loss: 0.1392 - val_accuracy: 0.3004\n",
      "Epoch 542/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0718 - accuracy: 0.3201 - val_loss: 0.1252 - val_accuracy: 0.2914\n",
      "Epoch 543/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0707 - accuracy: 0.3135 - val_loss: 0.1275 - val_accuracy: 0.3094\n",
      "Epoch 544/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0707 - accuracy: 0.3129 - val_loss: 0.1261 - val_accuracy: 0.3094\n",
      "Epoch 545/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0705 - accuracy: 0.3201 - val_loss: 0.1261 - val_accuracy: 0.3094\n",
      "Epoch 546/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0707 - accuracy: 0.3099 - val_loss: 0.1281 - val_accuracy: 0.2986\n",
      "Epoch 547/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0707 - accuracy: 0.3159 - val_loss: 0.1458 - val_accuracy: 0.2428\n",
      "Epoch 548/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0716 - accuracy: 0.3099 - val_loss: 0.1366 - val_accuracy: 0.2806\n",
      "Epoch 549/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0707 - accuracy: 0.3063 - val_loss: 0.1324 - val_accuracy: 0.3058\n",
      "Epoch 550/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0705 - accuracy: 0.3123 - val_loss: 0.1281 - val_accuracy: 0.3022\n",
      "Epoch 551/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0704 - accuracy: 0.3129 - val_loss: 0.1292 - val_accuracy: 0.3112\n",
      "Epoch 552/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0701 - accuracy: 0.3141 - val_loss: 0.1253 - val_accuracy: 0.3183\n",
      "Epoch 553/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0700 - accuracy: 0.3201 - val_loss: 0.1352 - val_accuracy: 0.3004\n",
      "Epoch 554/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0704 - accuracy: 0.3219 - val_loss: 0.1239 - val_accuracy: 0.3004\n",
      "Epoch 555/800\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0698 - accuracy: 0.3219 - val_loss: 0.1250 - val_accuracy: 0.3022\n",
      "Epoch 556/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0698 - accuracy: 0.3141 - val_loss: 0.1393 - val_accuracy: 0.2392\n",
      "Epoch 557/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0707 - accuracy: 0.3105 - val_loss: 0.1348 - val_accuracy: 0.3040\n",
      "Epoch 558/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0702 - accuracy: 0.3105 - val_loss: 0.1276 - val_accuracy: 0.3022\n",
      "Epoch 559/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0699 - accuracy: 0.3105 - val_loss: 0.1260 - val_accuracy: 0.3129\n",
      "Epoch 560/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0695 - accuracy: 0.3189 - val_loss: 0.1264 - val_accuracy: 0.3165\n",
      "Epoch 561/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0694 - accuracy: 0.3171 - val_loss: 0.1258 - val_accuracy: 0.3273\n",
      "Epoch 562/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0695 - accuracy: 0.3213 - val_loss: 0.1244 - val_accuracy: 0.3076\n",
      "Epoch 563/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0695 - accuracy: 0.3201 - val_loss: 0.1257 - val_accuracy: 0.3076\n",
      "Epoch 564/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0694 - accuracy: 0.3153 - val_loss: 0.1248 - val_accuracy: 0.3201\n",
      "Epoch 565/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0693 - accuracy: 0.3093 - val_loss: 0.1253 - val_accuracy: 0.3076\n",
      "Epoch 566/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0690 - accuracy: 0.3207 - val_loss: 0.1246 - val_accuracy: 0.3129\n",
      "Epoch 567/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0690 - accuracy: 0.3183 - val_loss: 0.1247 - val_accuracy: 0.2968\n",
      "Epoch 568/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0691 - accuracy: 0.3141 - val_loss: 0.1367 - val_accuracy: 0.3741\n",
      "Epoch 569/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0696 - accuracy: 0.3219 - val_loss: 0.1265 - val_accuracy: 0.3201\n",
      "Epoch 570/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0690 - accuracy: 0.3153 - val_loss: 0.1247 - val_accuracy: 0.3112\n",
      "Epoch 571/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0690 - accuracy: 0.3177 - val_loss: 0.1241 - val_accuracy: 0.3291\n",
      "Epoch 572/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0687 - accuracy: 0.3171 - val_loss: 0.1238 - val_accuracy: 0.3129\n",
      "Epoch 573/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0686 - accuracy: 0.3207 - val_loss: 0.1359 - val_accuracy: 0.3543\n",
      "Epoch 574/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0689 - accuracy: 0.3105 - val_loss: 0.1245 - val_accuracy: 0.3022\n",
      "Epoch 575/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0685 - accuracy: 0.3117 - val_loss: 0.1244 - val_accuracy: 0.3165\n",
      "Epoch 576/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0684 - accuracy: 0.3105 - val_loss: 0.1237 - val_accuracy: 0.3309\n",
      "Epoch 577/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0684 - accuracy: 0.3255 - val_loss: 0.1221 - val_accuracy: 0.3147\n",
      "Epoch 578/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0683 - accuracy: 0.3207 - val_loss: 0.1232 - val_accuracy: 0.3201\n",
      "Epoch 579/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0682 - accuracy: 0.3165 - val_loss: 0.1271 - val_accuracy: 0.2950\n",
      "Epoch 580/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0683 - accuracy: 0.3135 - val_loss: 0.1292 - val_accuracy: 0.3058\n",
      "Epoch 581/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0684 - accuracy: 0.3153 - val_loss: 0.1308 - val_accuracy: 0.2590\n",
      "Epoch 582/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0686 - accuracy: 0.3087 - val_loss: 0.1282 - val_accuracy: 0.3291\n",
      "Epoch 583/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0680 - accuracy: 0.3147 - val_loss: 0.1248 - val_accuracy: 0.3165\n",
      "Epoch 584/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0679 - accuracy: 0.3201 - val_loss: 0.1248 - val_accuracy: 0.3273\n",
      "Epoch 585/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0679 - accuracy: 0.3135 - val_loss: 0.1247 - val_accuracy: 0.3040\n",
      "Epoch 586/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0679 - accuracy: 0.3105 - val_loss: 0.1241 - val_accuracy: 0.3345\n",
      "Epoch 587/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0677 - accuracy: 0.3153 - val_loss: 0.1260 - val_accuracy: 0.3345\n",
      "Epoch 588/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0676 - accuracy: 0.3243 - val_loss: 0.1259 - val_accuracy: 0.3165\n",
      "Epoch 589/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0675 - accuracy: 0.3165 - val_loss: 0.1252 - val_accuracy: 0.3112\n",
      "Epoch 590/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0674 - accuracy: 0.3147 - val_loss: 0.1245 - val_accuracy: 0.3345\n",
      "Epoch 591/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0674 - accuracy: 0.3135 - val_loss: 0.1326 - val_accuracy: 0.3129\n",
      "Epoch 592/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0678 - accuracy: 0.3183 - val_loss: 0.1260 - val_accuracy: 0.2932\n",
      "Epoch 593/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0676 - accuracy: 0.3075 - val_loss: 0.1237 - val_accuracy: 0.3129\n",
      "Epoch 594/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0671 - accuracy: 0.3195 - val_loss: 0.1401 - val_accuracy: 0.2392\n",
      "Epoch 595/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0682 - accuracy: 0.3159 - val_loss: 0.1229 - val_accuracy: 0.2968\n",
      "Epoch 596/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0670 - accuracy: 0.3105 - val_loss: 0.1242 - val_accuracy: 0.3291\n",
      "Epoch 597/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0673 - accuracy: 0.3189 - val_loss: 0.1234 - val_accuracy: 0.3219\n",
      "Epoch 598/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0671 - accuracy: 0.3153 - val_loss: 0.1244 - val_accuracy: 0.3058\n",
      "Epoch 599/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0670 - accuracy: 0.3087 - val_loss: 0.1230 - val_accuracy: 0.3327\n",
      "Epoch 600/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0670 - accuracy: 0.3165 - val_loss: 0.1222 - val_accuracy: 0.3022\n",
      "Epoch 601/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0668 - accuracy: 0.3225 - val_loss: 0.1311 - val_accuracy: 0.3040\n",
      "Epoch 602/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0672 - accuracy: 0.3159 - val_loss: 0.1230 - val_accuracy: 0.3112\n",
      "Epoch 603/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0666 - accuracy: 0.3129 - val_loss: 0.1216 - val_accuracy: 0.3147\n",
      "Epoch 604/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0665 - accuracy: 0.3165 - val_loss: 0.1242 - val_accuracy: 0.3129\n",
      "Epoch 605/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0665 - accuracy: 0.3081 - val_loss: 0.1299 - val_accuracy: 0.3435\n",
      "Epoch 606/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0668 - accuracy: 0.3201 - val_loss: 0.1266 - val_accuracy: 0.3363\n",
      "Epoch 607/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0666 - accuracy: 0.3135 - val_loss: 0.1249 - val_accuracy: 0.3309\n",
      "Epoch 608/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0662 - accuracy: 0.3159 - val_loss: 0.1229 - val_accuracy: 0.3058\n",
      "Epoch 609/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0663 - accuracy: 0.3195 - val_loss: 0.1218 - val_accuracy: 0.3094\n",
      "Epoch 610/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0660 - accuracy: 0.3087 - val_loss: 0.1232 - val_accuracy: 0.3004\n",
      "Epoch 611/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0663 - accuracy: 0.3141 - val_loss: 0.1223 - val_accuracy: 0.3112\n",
      "Epoch 612/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0660 - accuracy: 0.3195 - val_loss: 0.1249 - val_accuracy: 0.3291\n",
      "Epoch 613/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0661 - accuracy: 0.3219 - val_loss: 0.1272 - val_accuracy: 0.3561\n",
      "Epoch 614/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0660 - accuracy: 0.3147 - val_loss: 0.1236 - val_accuracy: 0.3237\n",
      "Epoch 615/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0658 - accuracy: 0.3105 - val_loss: 0.1564 - val_accuracy: 0.3076\n",
      "Epoch 616/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0675 - accuracy: 0.3141 - val_loss: 0.1235 - val_accuracy: 0.3022\n",
      "Epoch 617/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0662 - accuracy: 0.3171 - val_loss: 0.1252 - val_accuracy: 0.3129\n",
      "Epoch 618/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0660 - accuracy: 0.3069 - val_loss: 0.1217 - val_accuracy: 0.3273\n",
      "Epoch 619/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0654 - accuracy: 0.3123 - val_loss: 0.1225 - val_accuracy: 0.3076\n",
      "Epoch 620/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0656 - accuracy: 0.3177 - val_loss: 0.1242 - val_accuracy: 0.3201\n",
      "Epoch 621/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0656 - accuracy: 0.3117 - val_loss: 0.1254 - val_accuracy: 0.3219\n",
      "Epoch 622/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0655 - accuracy: 0.3177 - val_loss: 0.1234 - val_accuracy: 0.3237\n",
      "Epoch 623/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0654 - accuracy: 0.3075 - val_loss: 0.1217 - val_accuracy: 0.3004\n",
      "Epoch 624/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0655 - accuracy: 0.3177 - val_loss: 0.1212 - val_accuracy: 0.3345\n",
      "Epoch 625/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0651 - accuracy: 0.3147 - val_loss: 0.1222 - val_accuracy: 0.3291\n",
      "Epoch 626/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0652 - accuracy: 0.3171 - val_loss: 0.1232 - val_accuracy: 0.3165\n",
      "Epoch 627/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0657 - accuracy: 0.3087 - val_loss: 0.1236 - val_accuracy: 0.3147\n",
      "Epoch 628/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0652 - accuracy: 0.3201 - val_loss: 0.1265 - val_accuracy: 0.3219\n",
      "Epoch 629/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0652 - accuracy: 0.3141 - val_loss: 0.1255 - val_accuracy: 0.2986\n",
      "Epoch 630/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0652 - accuracy: 0.3255 - val_loss: 0.1229 - val_accuracy: 0.3129\n",
      "Epoch 631/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0650 - accuracy: 0.3111 - val_loss: 0.1255 - val_accuracy: 0.3201\n",
      "Epoch 632/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0649 - accuracy: 0.3135 - val_loss: 0.1285 - val_accuracy: 0.3147\n",
      "Epoch 633/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0653 - accuracy: 0.3153 - val_loss: 0.1214 - val_accuracy: 0.3219\n",
      "Epoch 634/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0646 - accuracy: 0.3171 - val_loss: 0.1223 - val_accuracy: 0.2986\n",
      "Epoch 635/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0648 - accuracy: 0.3159 - val_loss: 0.1422 - val_accuracy: 0.2662\n",
      "Epoch 636/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0658 - accuracy: 0.3123 - val_loss: 0.1216 - val_accuracy: 0.3219\n",
      "Epoch 637/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0646 - accuracy: 0.3141 - val_loss: 0.1232 - val_accuracy: 0.3165\n",
      "Epoch 638/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0647 - accuracy: 0.3153 - val_loss: 0.1251 - val_accuracy: 0.3112\n",
      "Epoch 639/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0647 - accuracy: 0.3135 - val_loss: 0.1386 - val_accuracy: 0.3579\n",
      "Epoch 640/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0652 - accuracy: 0.3261 - val_loss: 0.1222 - val_accuracy: 0.3112\n",
      "Epoch 641/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0644 - accuracy: 0.3147 - val_loss: 0.1379 - val_accuracy: 0.2158\n",
      "Epoch 642/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0653 - accuracy: 0.3111 - val_loss: 0.1197 - val_accuracy: 0.3004\n",
      "Epoch 643/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0644 - accuracy: 0.3135 - val_loss: 0.1201 - val_accuracy: 0.3094\n",
      "Epoch 644/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0644 - accuracy: 0.3171 - val_loss: 0.1221 - val_accuracy: 0.3327\n",
      "Epoch 645/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0641 - accuracy: 0.3165 - val_loss: 0.1244 - val_accuracy: 0.3255\n",
      "Epoch 646/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0640 - accuracy: 0.3177 - val_loss: 0.1234 - val_accuracy: 0.3399\n",
      "Epoch 647/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0640 - accuracy: 0.3183 - val_loss: 0.1219 - val_accuracy: 0.3345\n",
      "Epoch 648/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0638 - accuracy: 0.3153 - val_loss: 0.1376 - val_accuracy: 0.2824\n",
      "Epoch 649/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0649 - accuracy: 0.3207 - val_loss: 0.1211 - val_accuracy: 0.3345\n",
      "Epoch 650/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0638 - accuracy: 0.3147 - val_loss: 0.1280 - val_accuracy: 0.2968\n",
      "Epoch 651/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0642 - accuracy: 0.3153 - val_loss: 0.1236 - val_accuracy: 0.3129\n",
      "Epoch 652/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0638 - accuracy: 0.3105 - val_loss: 0.1254 - val_accuracy: 0.2914\n",
      "Epoch 653/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0637 - accuracy: 0.3201 - val_loss: 0.1220 - val_accuracy: 0.3309\n",
      "Epoch 654/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0639 - accuracy: 0.3183 - val_loss: 0.1215 - val_accuracy: 0.3273\n",
      "Epoch 655/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0636 - accuracy: 0.3141 - val_loss: 0.1238 - val_accuracy: 0.3363\n",
      "Epoch 656/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0637 - accuracy: 0.3129 - val_loss: 0.1240 - val_accuracy: 0.3112\n",
      "Epoch 657/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0636 - accuracy: 0.3099 - val_loss: 0.1326 - val_accuracy: 0.2932\n",
      "Epoch 658/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0644 - accuracy: 0.3135 - val_loss: 0.1202 - val_accuracy: 0.3291\n",
      "Epoch 659/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0632 - accuracy: 0.3111 - val_loss: 0.1225 - val_accuracy: 0.3183\n",
      "Epoch 660/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0632 - accuracy: 0.3159 - val_loss: 0.1228 - val_accuracy: 0.3219\n",
      "Epoch 661/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0636 - accuracy: 0.3153 - val_loss: 0.1210 - val_accuracy: 0.3381\n",
      "Epoch 662/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0632 - accuracy: 0.3195 - val_loss: 0.1219 - val_accuracy: 0.3273\n",
      "Epoch 663/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0630 - accuracy: 0.3171 - val_loss: 0.1212 - val_accuracy: 0.3165\n",
      "Epoch 664/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0632 - accuracy: 0.3135 - val_loss: 0.1207 - val_accuracy: 0.3237\n",
      "Epoch 665/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0630 - accuracy: 0.3183 - val_loss: 0.1255 - val_accuracy: 0.3291\n",
      "Epoch 666/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0632 - accuracy: 0.3201 - val_loss: 0.1189 - val_accuracy: 0.3435\n",
      "Epoch 667/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0631 - accuracy: 0.3165 - val_loss: 0.1204 - val_accuracy: 0.3435\n",
      "Epoch 668/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0630 - accuracy: 0.3219 - val_loss: 0.1226 - val_accuracy: 0.3165\n",
      "Epoch 669/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0632 - accuracy: 0.3213 - val_loss: 0.1206 - val_accuracy: 0.3363\n",
      "Epoch 670/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0627 - accuracy: 0.3165 - val_loss: 0.1233 - val_accuracy: 0.3183\n",
      "Epoch 671/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0629 - accuracy: 0.3009 - val_loss: 0.1300 - val_accuracy: 0.3112\n",
      "Epoch 672/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0631 - accuracy: 0.3171 - val_loss: 0.1221 - val_accuracy: 0.3327\n",
      "Epoch 673/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0625 - accuracy: 0.3117 - val_loss: 0.1223 - val_accuracy: 0.3381\n",
      "Epoch 674/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0624 - accuracy: 0.3105 - val_loss: 0.1227 - val_accuracy: 0.3309\n",
      "Epoch 675/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0624 - accuracy: 0.3177 - val_loss: 0.1422 - val_accuracy: 0.2680\n",
      "Epoch 676/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0638 - accuracy: 0.3153 - val_loss: 0.1233 - val_accuracy: 0.3309\n",
      "Epoch 677/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0625 - accuracy: 0.3123 - val_loss: 0.1322 - val_accuracy: 0.3291\n",
      "Epoch 678/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0630 - accuracy: 0.3159 - val_loss: 0.1217 - val_accuracy: 0.3094\n",
      "Epoch 679/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0623 - accuracy: 0.3171 - val_loss: 0.1228 - val_accuracy: 0.3273\n",
      "Epoch 680/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0623 - accuracy: 0.3117 - val_loss: 0.1224 - val_accuracy: 0.3309\n",
      "Epoch 681/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0623 - accuracy: 0.3243 - val_loss: 0.1239 - val_accuracy: 0.3147\n",
      "Epoch 682/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0622 - accuracy: 0.3249 - val_loss: 0.1213 - val_accuracy: 0.3309\n",
      "Epoch 683/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0622 - accuracy: 0.3207 - val_loss: 0.1231 - val_accuracy: 0.3309\n",
      "Epoch 684/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0621 - accuracy: 0.3135 - val_loss: 0.1233 - val_accuracy: 0.3201\n",
      "Epoch 685/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0620 - accuracy: 0.3117 - val_loss: 0.1196 - val_accuracy: 0.3201\n",
      "Epoch 686/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0620 - accuracy: 0.3231 - val_loss: 0.1407 - val_accuracy: 0.2392\n",
      "Epoch 687/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0630 - accuracy: 0.3129 - val_loss: 0.1243 - val_accuracy: 0.3417\n",
      "Epoch 688/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0618 - accuracy: 0.3141 - val_loss: 0.1213 - val_accuracy: 0.3237\n",
      "Epoch 689/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0617 - accuracy: 0.3207 - val_loss: 0.1202 - val_accuracy: 0.3435\n",
      "Epoch 690/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0618 - accuracy: 0.3165 - val_loss: 0.1188 - val_accuracy: 0.3273\n",
      "Epoch 691/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0615 - accuracy: 0.3123 - val_loss: 0.1226 - val_accuracy: 0.3201\n",
      "Epoch 692/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0617 - accuracy: 0.3165 - val_loss: 0.1202 - val_accuracy: 0.3201\n",
      "Epoch 693/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0617 - accuracy: 0.3189 - val_loss: 0.1209 - val_accuracy: 0.3219\n",
      "Epoch 694/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0613 - accuracy: 0.3189 - val_loss: 0.1249 - val_accuracy: 0.3561\n",
      "Epoch 695/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0620 - accuracy: 0.3153 - val_loss: 0.1186 - val_accuracy: 0.3201\n",
      "Epoch 696/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0613 - accuracy: 0.3177 - val_loss: 0.1214 - val_accuracy: 0.3219\n",
      "Epoch 697/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0612 - accuracy: 0.3141 - val_loss: 0.1200 - val_accuracy: 0.3363\n",
      "Epoch 698/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0614 - accuracy: 0.3141 - val_loss: 0.1407 - val_accuracy: 0.3939\n",
      "Epoch 699/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0621 - accuracy: 0.3255 - val_loss: 0.1253 - val_accuracy: 0.3129\n",
      "Epoch 700/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0617 - accuracy: 0.3105 - val_loss: 0.1189 - val_accuracy: 0.3022\n",
      "Epoch 701/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0611 - accuracy: 0.3081 - val_loss: 0.1338 - val_accuracy: 0.3129\n",
      "Epoch 702/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0620 - accuracy: 0.3033 - val_loss: 0.1230 - val_accuracy: 0.3291\n",
      "Epoch 703/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0611 - accuracy: 0.3201 - val_loss: 0.1202 - val_accuracy: 0.3291\n",
      "Epoch 704/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0608 - accuracy: 0.3183 - val_loss: 0.1202 - val_accuracy: 0.3129\n",
      "Epoch 705/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0607 - accuracy: 0.3159 - val_loss: 0.1215 - val_accuracy: 0.3363\n",
      "Epoch 706/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0608 - accuracy: 0.3117 - val_loss: 0.1240 - val_accuracy: 0.3237\n",
      "Epoch 707/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0611 - accuracy: 0.3129 - val_loss: 0.1236 - val_accuracy: 0.3183\n",
      "Epoch 708/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0611 - accuracy: 0.3177 - val_loss: 0.1213 - val_accuracy: 0.3273\n",
      "Epoch 709/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0606 - accuracy: 0.3183 - val_loss: 0.1230 - val_accuracy: 0.3147\n",
      "Epoch 710/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0607 - accuracy: 0.3129 - val_loss: 0.1245 - val_accuracy: 0.3399\n",
      "Epoch 711/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0607 - accuracy: 0.3177 - val_loss: 0.1214 - val_accuracy: 0.3004\n",
      "Epoch 712/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0605 - accuracy: 0.3171 - val_loss: 0.1211 - val_accuracy: 0.3255\n",
      "Epoch 713/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0606 - accuracy: 0.3225 - val_loss: 0.1190 - val_accuracy: 0.3237\n",
      "Epoch 714/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0605 - accuracy: 0.3213 - val_loss: 0.1211 - val_accuracy: 0.3219\n",
      "Epoch 715/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0604 - accuracy: 0.3099 - val_loss: 0.1262 - val_accuracy: 0.3597\n",
      "Epoch 716/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0610 - accuracy: 0.3291 - val_loss: 0.1309 - val_accuracy: 0.3525\n",
      "Epoch 717/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0609 - accuracy: 0.3081 - val_loss: 0.1212 - val_accuracy: 0.3525\n",
      "Epoch 718/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0604 - accuracy: 0.3303 - val_loss: 0.1224 - val_accuracy: 0.3273\n",
      "Epoch 719/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0602 - accuracy: 0.3171 - val_loss: 0.1203 - val_accuracy: 0.3255\n",
      "Epoch 720/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0602 - accuracy: 0.3153 - val_loss: 0.1228 - val_accuracy: 0.3633\n",
      "Epoch 721/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0603 - accuracy: 0.3183 - val_loss: 0.1226 - val_accuracy: 0.3237\n",
      "Epoch 722/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0603 - accuracy: 0.3171 - val_loss: 0.1184 - val_accuracy: 0.3399\n",
      "Epoch 723/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0600 - accuracy: 0.3165 - val_loss: 0.1216 - val_accuracy: 0.3219\n",
      "Epoch 724/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0600 - accuracy: 0.3159 - val_loss: 0.1191 - val_accuracy: 0.3435\n",
      "Epoch 725/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0603 - accuracy: 0.3177 - val_loss: 0.1222 - val_accuracy: 0.3309\n",
      "Epoch 726/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0599 - accuracy: 0.3177 - val_loss: 0.1187 - val_accuracy: 0.3381\n",
      "Epoch 727/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0598 - accuracy: 0.3099 - val_loss: 0.1220 - val_accuracy: 0.3201\n",
      "Epoch 728/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0599 - accuracy: 0.3135 - val_loss: 0.1200 - val_accuracy: 0.3237\n",
      "Epoch 729/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0597 - accuracy: 0.3147 - val_loss: 0.1367 - val_accuracy: 0.3525\n",
      "Epoch 730/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0606 - accuracy: 0.3033 - val_loss: 0.1207 - val_accuracy: 0.3201\n",
      "Epoch 731/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0597 - accuracy: 0.3171 - val_loss: 0.1289 - val_accuracy: 0.3076\n",
      "Epoch 732/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0601 - accuracy: 0.3117 - val_loss: 0.1375 - val_accuracy: 0.3705\n",
      "Epoch 733/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0604 - accuracy: 0.3183 - val_loss: 0.1205 - val_accuracy: 0.3327\n",
      "Epoch 734/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0596 - accuracy: 0.3153 - val_loss: 0.1200 - val_accuracy: 0.3345\n",
      "Epoch 735/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0594 - accuracy: 0.3195 - val_loss: 0.1189 - val_accuracy: 0.3417\n",
      "Epoch 736/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0595 - accuracy: 0.3195 - val_loss: 0.1249 - val_accuracy: 0.3453\n",
      "Epoch 737/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0596 - accuracy: 0.3147 - val_loss: 0.1210 - val_accuracy: 0.3417\n",
      "Epoch 738/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0594 - accuracy: 0.3177 - val_loss: 0.1195 - val_accuracy: 0.3453\n",
      "Epoch 739/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0593 - accuracy: 0.3009 - val_loss: 0.1185 - val_accuracy: 0.3237\n",
      "Epoch 740/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0594 - accuracy: 0.3087 - val_loss: 0.1203 - val_accuracy: 0.2968\n",
      "Epoch 741/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0593 - accuracy: 0.3177 - val_loss: 0.1206 - val_accuracy: 0.3255\n",
      "Epoch 742/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0591 - accuracy: 0.3147 - val_loss: 0.1194 - val_accuracy: 0.3345\n",
      "Epoch 743/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0590 - accuracy: 0.3135 - val_loss: 0.1205 - val_accuracy: 0.3435\n",
      "Epoch 744/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0591 - accuracy: 0.3219 - val_loss: 0.1276 - val_accuracy: 0.3004\n",
      "Epoch 745/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0598 - accuracy: 0.3159 - val_loss: 0.1231 - val_accuracy: 0.3040\n",
      "Epoch 746/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0593 - accuracy: 0.3159 - val_loss: 0.1201 - val_accuracy: 0.3255\n",
      "Epoch 747/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0589 - accuracy: 0.3105 - val_loss: 0.1547 - val_accuracy: 0.2788\n",
      "Epoch 748/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0607 - accuracy: 0.3009 - val_loss: 0.1198 - val_accuracy: 0.3112\n",
      "Epoch 749/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0589 - accuracy: 0.3159 - val_loss: 0.1236 - val_accuracy: 0.3201\n",
      "Epoch 750/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0591 - accuracy: 0.3153 - val_loss: 0.1207 - val_accuracy: 0.3273\n",
      "Epoch 751/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0593 - accuracy: 0.3165 - val_loss: 0.1476 - val_accuracy: 0.2932\n",
      "Epoch 752/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0606 - accuracy: 0.3219 - val_loss: 0.1243 - val_accuracy: 0.3435\n",
      "Epoch 753/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0590 - accuracy: 0.3111 - val_loss: 0.1619 - val_accuracy: 0.1583\n",
      "Epoch 754/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0610 - accuracy: 0.3171 - val_loss: 0.1213 - val_accuracy: 0.3327\n",
      "Epoch 755/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0589 - accuracy: 0.3225 - val_loss: 0.1205 - val_accuracy: 0.3471\n",
      "Epoch 756/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0587 - accuracy: 0.3153 - val_loss: 0.1227 - val_accuracy: 0.3129\n",
      "Epoch 757/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0589 - accuracy: 0.3135 - val_loss: 0.1192 - val_accuracy: 0.3273\n",
      "Epoch 758/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0585 - accuracy: 0.3147 - val_loss: 0.1209 - val_accuracy: 0.3471\n",
      "Epoch 759/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0585 - accuracy: 0.3201 - val_loss: 0.1365 - val_accuracy: 0.2608\n",
      "Epoch 760/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0595 - accuracy: 0.3123 - val_loss: 0.1271 - val_accuracy: 0.3507\n",
      "Epoch 761/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0587 - accuracy: 0.3237 - val_loss: 0.1250 - val_accuracy: 0.2968\n",
      "Epoch 762/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0594 - accuracy: 0.3063 - val_loss: 0.1201 - val_accuracy: 0.3273\n",
      "Epoch 763/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0582 - accuracy: 0.3153 - val_loss: 0.1188 - val_accuracy: 0.3327\n",
      "Epoch 764/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0582 - accuracy: 0.3201 - val_loss: 0.1220 - val_accuracy: 0.3273\n",
      "Epoch 765/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0581 - accuracy: 0.3183 - val_loss: 0.1191 - val_accuracy: 0.3381\n",
      "Epoch 766/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0583 - accuracy: 0.3225 - val_loss: 0.1306 - val_accuracy: 0.2896\n",
      "Epoch 767/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0593 - accuracy: 0.3111 - val_loss: 0.1204 - val_accuracy: 0.3183\n",
      "Epoch 768/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0583 - accuracy: 0.3141 - val_loss: 0.1251 - val_accuracy: 0.2986\n",
      "Epoch 769/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0582 - accuracy: 0.3141 - val_loss: 0.1231 - val_accuracy: 0.3795\n",
      "Epoch 770/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0581 - accuracy: 0.3081 - val_loss: 0.1179 - val_accuracy: 0.3309\n",
      "Epoch 771/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0579 - accuracy: 0.3135 - val_loss: 0.1389 - val_accuracy: 0.3112\n",
      "Epoch 772/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0591 - accuracy: 0.3141 - val_loss: 0.1190 - val_accuracy: 0.3561\n",
      "Epoch 773/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0577 - accuracy: 0.3153 - val_loss: 0.1189 - val_accuracy: 0.3345\n",
      "Epoch 774/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0579 - accuracy: 0.3159 - val_loss: 0.1189 - val_accuracy: 0.3597\n",
      "Epoch 775/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0578 - accuracy: 0.3231 - val_loss: 0.1178 - val_accuracy: 0.3112\n",
      "Epoch 776/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0577 - accuracy: 0.3159 - val_loss: 0.1224 - val_accuracy: 0.3291\n",
      "Epoch 777/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0579 - accuracy: 0.3189 - val_loss: 0.1181 - val_accuracy: 0.3219\n",
      "Epoch 778/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0575 - accuracy: 0.3159 - val_loss: 0.1192 - val_accuracy: 0.3291\n",
      "Epoch 779/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0578 - accuracy: 0.3111 - val_loss: 0.1544 - val_accuracy: 0.2680\n",
      "Epoch 780/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0592 - accuracy: 0.3087 - val_loss: 0.1198 - val_accuracy: 0.3489\n",
      "Epoch 781/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0574 - accuracy: 0.3195 - val_loss: 0.1192 - val_accuracy: 0.3363\n",
      "Epoch 782/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0575 - accuracy: 0.3111 - val_loss: 0.1227 - val_accuracy: 0.3273\n",
      "Epoch 783/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0575 - accuracy: 0.3159 - val_loss: 0.1170 - val_accuracy: 0.3201\n",
      "Epoch 784/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0576 - accuracy: 0.3141 - val_loss: 0.1209 - val_accuracy: 0.3165\n",
      "Epoch 785/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0574 - accuracy: 0.3117 - val_loss: 0.1188 - val_accuracy: 0.3435\n",
      "Epoch 786/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0573 - accuracy: 0.3213 - val_loss: 0.1169 - val_accuracy: 0.3327\n",
      "Epoch 787/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0574 - accuracy: 0.3159 - val_loss: 0.1174 - val_accuracy: 0.3345\n",
      "Epoch 788/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0572 - accuracy: 0.3117 - val_loss: 0.1227 - val_accuracy: 0.3076\n",
      "Epoch 789/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0573 - accuracy: 0.3141 - val_loss: 0.1199 - val_accuracy: 0.3543\n",
      "Epoch 790/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0578 - accuracy: 0.3195 - val_loss: 0.1204 - val_accuracy: 0.3435\n",
      "Epoch 791/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0571 - accuracy: 0.3117 - val_loss: 0.1181 - val_accuracy: 0.3255\n",
      "Epoch 792/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0570 - accuracy: 0.3141 - val_loss: 0.1186 - val_accuracy: 0.3237\n",
      "Epoch 793/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0570 - accuracy: 0.3075 - val_loss: 0.1200 - val_accuracy: 0.3327\n",
      "Epoch 794/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0571 - accuracy: 0.3159 - val_loss: 0.1218 - val_accuracy: 0.3112\n",
      "Epoch 795/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0571 - accuracy: 0.3135 - val_loss: 0.1219 - val_accuracy: 0.3795\n",
      "Epoch 796/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0570 - accuracy: 0.3195 - val_loss: 0.1218 - val_accuracy: 0.3201\n",
      "Epoch 797/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0570 - accuracy: 0.3159 - val_loss: 0.1207 - val_accuracy: 0.3201\n",
      "Epoch 798/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0569 - accuracy: 0.3159 - val_loss: 0.1296 - val_accuracy: 0.3273\n",
      "Epoch 799/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0575 - accuracy: 0.3231 - val_loss: 0.1255 - val_accuracy: 0.3219\n",
      "Epoch 800/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0570 - accuracy: 0.3201 - val_loss: 0.1187 - val_accuracy: 0.2896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-32-f01cf9afd9ce>:131: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "  tprs.append(interp(mean_fpr, fpr, tpr))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Epoch 1/800\n",
      "53/53 [==============================] - 1s 5ms/step - loss: 0.6324 - accuracy: 0.0258 - val_loss: 0.5361 - val_accuracy: 0.0288\n",
      "Epoch 2/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.3034 - accuracy: 0.0276 - val_loss: 0.2554 - val_accuracy: 0.0252\n",
      "Epoch 3/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1908 - accuracy: 0.0294 - val_loss: 0.2516 - val_accuracy: 0.0324\n",
      "Epoch 4/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1881 - accuracy: 0.0312 - val_loss: 0.2500 - val_accuracy: 0.0018\n",
      "Epoch 5/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1875 - accuracy: 0.0126 - val_loss: 0.2501 - val_accuracy: 0.0342\n",
      "Epoch 6/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1874 - accuracy: 0.0354 - val_loss: 0.2503 - val_accuracy: 0.0450\n",
      "Epoch 7/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1872 - accuracy: 0.0348 - val_loss: 0.2503 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1870 - accuracy: 0.0144 - val_loss: 0.2497 - val_accuracy: 0.0450\n",
      "Epoch 9/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1870 - accuracy: 0.0312 - val_loss: 0.2502 - val_accuracy: 0.0486\n",
      "Epoch 10/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1870 - accuracy: 0.0204 - val_loss: 0.2496 - val_accuracy: 0.0486\n",
      "Epoch 11/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1868 - accuracy: 0.0408 - val_loss: 0.2497 - val_accuracy: 0.0324\n",
      "Epoch 12/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1868 - accuracy: 0.0318 - val_loss: 0.2494 - val_accuracy: 0.0324\n",
      "Epoch 13/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1867 - accuracy: 0.0396 - val_loss: 0.2488 - val_accuracy: 0.0827\n",
      "Epoch 14/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1866 - accuracy: 0.0462 - val_loss: 0.2496 - val_accuracy: 0.0288\n",
      "Epoch 15/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1865 - accuracy: 0.0210 - val_loss: 0.2492 - val_accuracy: 0.0270\n",
      "Epoch 16/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1865 - accuracy: 0.0390 - val_loss: 0.2493 - val_accuracy: 0.0270\n",
      "Epoch 17/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1864 - accuracy: 0.0252 - val_loss: 0.2493 - val_accuracy: 0.0486\n",
      "Epoch 18/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1864 - accuracy: 0.0300 - val_loss: 0.2489 - val_accuracy: 0.0486\n",
      "Epoch 19/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1863 - accuracy: 0.0330 - val_loss: 0.2489 - val_accuracy: 0.0270\n",
      "Epoch 20/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1862 - accuracy: 0.0270 - val_loss: 0.2480 - val_accuracy: 0.0468\n",
      "Epoch 21/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1862 - accuracy: 0.0360 - val_loss: 0.2487 - val_accuracy: 0.0342\n",
      "Epoch 22/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1861 - accuracy: 0.0264 - val_loss: 0.2487 - val_accuracy: 0.0216\n",
      "Epoch 23/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1860 - accuracy: 0.0276 - val_loss: 0.2486 - val_accuracy: 0.0198\n",
      "Epoch 24/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1859 - accuracy: 0.0312 - val_loss: 0.2483 - val_accuracy: 0.0773\n",
      "Epoch 25/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1858 - accuracy: 0.0547 - val_loss: 0.2476 - val_accuracy: 0.0504\n",
      "Epoch 26/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1857 - accuracy: 0.0474 - val_loss: 0.2477 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1857 - accuracy: 0.0360 - val_loss: 0.2478 - val_accuracy: 0.0342\n",
      "Epoch 28/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1856 - accuracy: 0.0336 - val_loss: 0.2483 - val_accuracy: 0.0737\n",
      "Epoch 29/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1855 - accuracy: 0.0643 - val_loss: 0.2484 - val_accuracy: 0.0540\n",
      "Epoch 30/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1854 - accuracy: 0.0474 - val_loss: 0.2478 - val_accuracy: 0.0647\n",
      "Epoch 31/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1853 - accuracy: 0.0432 - val_loss: 0.2474 - val_accuracy: 0.0396\n",
      "Epoch 32/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1853 - accuracy: 0.0426 - val_loss: 0.2476 - val_accuracy: 0.0450\n",
      "Epoch 33/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1852 - accuracy: 0.0505 - val_loss: 0.2469 - val_accuracy: 0.0522\n",
      "Epoch 34/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1851 - accuracy: 0.0571 - val_loss: 0.2474 - val_accuracy: 0.0234\n",
      "Epoch 35/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1850 - accuracy: 0.0607 - val_loss: 0.2470 - val_accuracy: 0.0540\n",
      "Epoch 36/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1849 - accuracy: 0.0517 - val_loss: 0.2469 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1848 - accuracy: 0.0384 - val_loss: 0.2471 - val_accuracy: 0.0018\n",
      "Epoch 38/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1848 - accuracy: 0.0390 - val_loss: 0.2470 - val_accuracy: 0.0612\n",
      "Epoch 39/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1847 - accuracy: 0.0456 - val_loss: 0.2467 - val_accuracy: 0.0755\n",
      "Epoch 40/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1845 - accuracy: 0.0553 - val_loss: 0.2460 - val_accuracy: 0.0036\n",
      "Epoch 41/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1845 - accuracy: 0.0613 - val_loss: 0.2463 - val_accuracy: 0.0647\n",
      "Epoch 42/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1844 - accuracy: 0.0781 - val_loss: 0.2457 - val_accuracy: 0.0144\n",
      "Epoch 43/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1843 - accuracy: 0.0817 - val_loss: 0.2462 - val_accuracy: 0.0072\n",
      "Epoch 44/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1842 - accuracy: 0.0324 - val_loss: 0.2454 - val_accuracy: 0.0180\n",
      "Epoch 45/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1841 - accuracy: 0.0673 - val_loss: 0.2456 - val_accuracy: 0.0719\n",
      "Epoch 46/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1840 - accuracy: 0.0667 - val_loss: 0.2460 - val_accuracy: 0.0540\n",
      "Epoch 47/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1839 - accuracy: 0.0565 - val_loss: 0.2449 - val_accuracy: 0.0306\n",
      "Epoch 48/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1837 - accuracy: 0.0793 - val_loss: 0.2451 - val_accuracy: 0.0450\n",
      "Epoch 49/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1836 - accuracy: 0.0691 - val_loss: 0.2453 - val_accuracy: 0.0665\n",
      "Epoch 50/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1835 - accuracy: 0.0679 - val_loss: 0.2449 - val_accuracy: 0.0432\n",
      "Epoch 51/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1833 - accuracy: 0.0619 - val_loss: 0.2448 - val_accuracy: 0.0054\n",
      "Epoch 52/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1833 - accuracy: 0.0547 - val_loss: 0.2444 - val_accuracy: 0.0360\n",
      "Epoch 53/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1831 - accuracy: 0.0979 - val_loss: 0.2441 - val_accuracy: 0.0683\n",
      "Epoch 54/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1830 - accuracy: 0.0757 - val_loss: 0.2441 - val_accuracy: 0.0270\n",
      "Epoch 55/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1829 - accuracy: 0.0691 - val_loss: 0.2443 - val_accuracy: 0.0719\n",
      "Epoch 56/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1827 - accuracy: 0.0919 - val_loss: 0.2442 - val_accuracy: 0.0252\n",
      "Epoch 57/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1826 - accuracy: 0.0769 - val_loss: 0.2440 - val_accuracy: 0.0018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1825 - accuracy: 0.0541 - val_loss: 0.2433 - val_accuracy: 0.0054\n",
      "Epoch 59/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1823 - accuracy: 0.0739 - val_loss: 0.2434 - val_accuracy: 0.0755\n",
      "Epoch 60/800\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.1823 - accuracy: 0.0742 - 0s 3ms/step - loss: 0.1822 - accuracy: 0.0883 - val_loss: 0.2430 - val_accuracy: 0.0468\n",
      "Epoch 61/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1820 - accuracy: 0.0853 - val_loss: 0.2428 - val_accuracy: 0.0360\n",
      "Epoch 62/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1818 - accuracy: 0.0841 - val_loss: 0.2429 - val_accuracy: 0.0270\n",
      "Epoch 63/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1817 - accuracy: 0.0679 - val_loss: 0.2428 - val_accuracy: 0.0468\n",
      "Epoch 64/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1814 - accuracy: 0.0835 - val_loss: 0.2427 - val_accuracy: 0.0647\n",
      "Epoch 65/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1813 - accuracy: 0.1003 - val_loss: 0.2417 - val_accuracy: 0.1151\n",
      "Epoch 66/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1810 - accuracy: 0.1147 - val_loss: 0.2420 - val_accuracy: 0.0468\n",
      "Epoch 67/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1809 - accuracy: 0.0997 - val_loss: 0.2411 - val_accuracy: 0.1025\n",
      "Epoch 68/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1806 - accuracy: 0.1255 - val_loss: 0.2410 - val_accuracy: 0.0234\n",
      "Epoch 69/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1805 - accuracy: 0.1075 - val_loss: 0.2409 - val_accuracy: 0.0594\n",
      "Epoch 70/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1802 - accuracy: 0.1069 - val_loss: 0.2406 - val_accuracy: 0.0576\n",
      "Epoch 71/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1800 - accuracy: 0.1087 - val_loss: 0.2401 - val_accuracy: 0.0737\n",
      "Epoch 72/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1797 - accuracy: 0.1261 - val_loss: 0.2401 - val_accuracy: 0.0809\n",
      "Epoch 73/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1795 - accuracy: 0.0931 - val_loss: 0.2391 - val_accuracy: 0.0468\n",
      "Epoch 74/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1792 - accuracy: 0.1123 - val_loss: 0.2385 - val_accuracy: 0.0701\n",
      "Epoch 75/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1789 - accuracy: 0.1171 - val_loss: 0.2388 - val_accuracy: 0.0755\n",
      "Epoch 76/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1786 - accuracy: 0.1291 - val_loss: 0.2381 - val_accuracy: 0.0522\n",
      "Epoch 77/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1783 - accuracy: 0.1237 - val_loss: 0.2375 - val_accuracy: 0.1295\n",
      "Epoch 78/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1779 - accuracy: 0.1273 - val_loss: 0.2373 - val_accuracy: 0.1187\n",
      "Epoch 79/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1776 - accuracy: 0.1285 - val_loss: 0.2362 - val_accuracy: 0.0737\n",
      "Epoch 80/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1772 - accuracy: 0.1081 - val_loss: 0.2358 - val_accuracy: 0.1295\n",
      "Epoch 81/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1768 - accuracy: 0.1357 - val_loss: 0.2356 - val_accuracy: 0.0917\n",
      "Epoch 82/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1765 - accuracy: 0.1321 - val_loss: 0.2350 - val_accuracy: 0.1115\n",
      "Epoch 83/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1760 - accuracy: 0.1399 - val_loss: 0.2336 - val_accuracy: 0.1331\n",
      "Epoch 84/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1756 - accuracy: 0.1514 - val_loss: 0.2340 - val_accuracy: 0.1367\n",
      "Epoch 85/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1752 - accuracy: 0.1483 - val_loss: 0.2330 - val_accuracy: 0.1169\n",
      "Epoch 86/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1748 - accuracy: 0.1483 - val_loss: 0.2323 - val_accuracy: 0.1565\n",
      "Epoch 87/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1743 - accuracy: 0.1544 - val_loss: 0.2315 - val_accuracy: 0.1223\n",
      "Epoch 88/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1739 - accuracy: 0.1610 - val_loss: 0.2310 - val_accuracy: 0.1385\n",
      "Epoch 89/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1735 - accuracy: 0.1520 - val_loss: 0.2306 - val_accuracy: 0.1367\n",
      "Epoch 90/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1730 - accuracy: 0.1598 - val_loss: 0.2298 - val_accuracy: 0.1547\n",
      "Epoch 91/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1725 - accuracy: 0.1688 - val_loss: 0.2293 - val_accuracy: 0.1439\n",
      "Epoch 92/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1720 - accuracy: 0.1604 - val_loss: 0.2289 - val_accuracy: 0.1745\n",
      "Epoch 93/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1715 - accuracy: 0.1886 - val_loss: 0.2286 - val_accuracy: 0.1565\n",
      "Epoch 94/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1711 - accuracy: 0.1562 - val_loss: 0.2269 - val_accuracy: 0.1061\n",
      "Epoch 95/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1705 - accuracy: 0.1562 - val_loss: 0.2269 - val_accuracy: 0.1025\n",
      "Epoch 96/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1700 - accuracy: 0.1477 - val_loss: 0.2258 - val_accuracy: 0.1529\n",
      "Epoch 97/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1695 - accuracy: 0.1634 - val_loss: 0.2250 - val_accuracy: 0.1835\n",
      "Epoch 98/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1689 - accuracy: 0.1778 - val_loss: 0.2236 - val_accuracy: 0.1817\n",
      "Epoch 99/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1685 - accuracy: 0.1700 - val_loss: 0.2233 - val_accuracy: 0.1709\n",
      "Epoch 100/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1679 - accuracy: 0.1850 - val_loss: 0.2237 - val_accuracy: 0.1007\n",
      "Epoch 101/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1674 - accuracy: 0.1634 - val_loss: 0.2223 - val_accuracy: 0.1799\n",
      "Epoch 102/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1669 - accuracy: 0.1754 - val_loss: 0.2216 - val_accuracy: 0.1511\n",
      "Epoch 103/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1664 - accuracy: 0.1778 - val_loss: 0.2211 - val_accuracy: 0.1727\n",
      "Epoch 104/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1659 - accuracy: 0.1724 - val_loss: 0.2201 - val_accuracy: 0.1781\n",
      "Epoch 105/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1653 - accuracy: 0.1898 - val_loss: 0.2200 - val_accuracy: 0.2014\n",
      "Epoch 106/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1648 - accuracy: 0.2048 - val_loss: 0.2191 - val_accuracy: 0.1619\n",
      "Epoch 107/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1643 - accuracy: 0.1970 - val_loss: 0.2178 - val_accuracy: 0.1745\n",
      "Epoch 108/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1638 - accuracy: 0.1916 - val_loss: 0.2173 - val_accuracy: 0.1781\n",
      "Epoch 109/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1632 - accuracy: 0.1838 - val_loss: 0.2165 - val_accuracy: 0.1691\n",
      "Epoch 110/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1627 - accuracy: 0.1874 - val_loss: 0.2155 - val_accuracy: 0.1439\n",
      "Epoch 111/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1622 - accuracy: 0.1976 - val_loss: 0.2145 - val_accuracy: 0.1888\n",
      "Epoch 112/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1616 - accuracy: 0.2000 - val_loss: 0.2146 - val_accuracy: 0.1439\n",
      "Epoch 113/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1611 - accuracy: 0.1880 - val_loss: 0.2135 - val_accuracy: 0.1835\n",
      "Epoch 114/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1606 - accuracy: 0.2006 - val_loss: 0.2123 - val_accuracy: 0.1906\n",
      "Epoch 115/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1600 - accuracy: 0.2042 - val_loss: 0.2121 - val_accuracy: 0.2050\n",
      "Epoch 116/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1595 - accuracy: 0.2054 - val_loss: 0.2105 - val_accuracy: 0.1942\n",
      "Epoch 117/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1590 - accuracy: 0.2036 - val_loss: 0.2097 - val_accuracy: 0.1871\n",
      "Epoch 118/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1583 - accuracy: 0.2060 - val_loss: 0.2103 - val_accuracy: 0.1835\n",
      "Epoch 119/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1579 - accuracy: 0.2048 - val_loss: 0.2090 - val_accuracy: 0.1709\n",
      "Epoch 120/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1573 - accuracy: 0.2072 - val_loss: 0.2091 - val_accuracy: 0.1924\n",
      "Epoch 121/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1568 - accuracy: 0.2024 - val_loss: 0.2081 - val_accuracy: 0.1745\n",
      "Epoch 122/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1563 - accuracy: 0.2030 - val_loss: 0.2085 - val_accuracy: 0.1763\n",
      "Epoch 123/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1558 - accuracy: 0.1952 - val_loss: 0.2063 - val_accuracy: 0.2284\n",
      "Epoch 124/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1552 - accuracy: 0.2120 - val_loss: 0.2059 - val_accuracy: 0.1906\n",
      "Epoch 125/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1548 - accuracy: 0.2054 - val_loss: 0.2049 - val_accuracy: 0.2086\n",
      "Epoch 126/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1542 - accuracy: 0.2108 - val_loss: 0.2045 - val_accuracy: 0.1781\n",
      "Epoch 127/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1537 - accuracy: 0.2096 - val_loss: 0.2044 - val_accuracy: 0.2050\n",
      "Epoch 128/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1532 - accuracy: 0.2174 - val_loss: 0.2039 - val_accuracy: 0.2176\n",
      "Epoch 129/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1527 - accuracy: 0.2180 - val_loss: 0.2029 - val_accuracy: 0.2248\n",
      "Epoch 130/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1523 - accuracy: 0.2204 - val_loss: 0.2019 - val_accuracy: 0.1871\n",
      "Epoch 131/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1517 - accuracy: 0.2252 - val_loss: 0.2010 - val_accuracy: 0.2176\n",
      "Epoch 132/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1512 - accuracy: 0.2276 - val_loss: 0.2003 - val_accuracy: 0.1978\n",
      "Epoch 133/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1508 - accuracy: 0.2114 - val_loss: 0.2000 - val_accuracy: 0.2032\n",
      "Epoch 134/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1503 - accuracy: 0.2234 - val_loss: 0.1996 - val_accuracy: 0.2104\n",
      "Epoch 135/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1499 - accuracy: 0.2234 - val_loss: 0.1985 - val_accuracy: 0.1888\n",
      "Epoch 136/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1493 - accuracy: 0.2174 - val_loss: 0.1982 - val_accuracy: 0.2356\n",
      "Epoch 137/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1489 - accuracy: 0.2336 - val_loss: 0.1991 - val_accuracy: 0.1906\n",
      "Epoch 138/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1484 - accuracy: 0.2306 - val_loss: 0.1987 - val_accuracy: 0.2104\n",
      "Epoch 139/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1481 - accuracy: 0.2210 - val_loss: 0.1966 - val_accuracy: 0.2140\n",
      "Epoch 140/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1474 - accuracy: 0.2366 - val_loss: 0.1958 - val_accuracy: 0.1942\n",
      "Epoch 141/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1471 - accuracy: 0.2246 - val_loss: 0.1954 - val_accuracy: 0.2122\n",
      "Epoch 142/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1465 - accuracy: 0.2264 - val_loss: 0.1947 - val_accuracy: 0.2392\n",
      "Epoch 143/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1462 - accuracy: 0.2408 - val_loss: 0.1943 - val_accuracy: 0.2122\n",
      "Epoch 144/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1457 - accuracy: 0.2312 - val_loss: 0.1948 - val_accuracy: 0.2050\n",
      "Epoch 145/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1452 - accuracy: 0.2252 - val_loss: 0.1932 - val_accuracy: 0.1888\n",
      "Epoch 146/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1448 - accuracy: 0.2342 - val_loss: 0.1934 - val_accuracy: 0.1871\n",
      "Epoch 147/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1444 - accuracy: 0.2276 - val_loss: 0.1912 - val_accuracy: 0.2068\n",
      "Epoch 148/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1439 - accuracy: 0.2324 - val_loss: 0.1919 - val_accuracy: 0.2176\n",
      "Epoch 149/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1436 - accuracy: 0.2246 - val_loss: 0.1917 - val_accuracy: 0.2086\n",
      "Epoch 150/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1430 - accuracy: 0.2498 - val_loss: 0.1933 - val_accuracy: 0.1817\n",
      "Epoch 151/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1426 - accuracy: 0.2396 - val_loss: 0.1917 - val_accuracy: 0.2554\n",
      "Epoch 152/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1422 - accuracy: 0.2462 - val_loss: 0.1895 - val_accuracy: 0.2104\n",
      "Epoch 153/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1416 - accuracy: 0.2426 - val_loss: 0.1889 - val_accuracy: 0.2158\n",
      "Epoch 154/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1412 - accuracy: 0.2462 - val_loss: 0.1908 - val_accuracy: 0.2104\n",
      "Epoch 155/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1410 - accuracy: 0.2432 - val_loss: 0.1878 - val_accuracy: 0.2356\n",
      "Epoch 156/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1403 - accuracy: 0.2438 - val_loss: 0.1875 - val_accuracy: 0.2068\n",
      "Epoch 157/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1399 - accuracy: 0.2384 - val_loss: 0.1924 - val_accuracy: 0.1942\n",
      "Epoch 158/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1397 - accuracy: 0.2396 - val_loss: 0.1870 - val_accuracy: 0.2482\n",
      "Epoch 159/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1391 - accuracy: 0.2547 - val_loss: 0.1878 - val_accuracy: 0.1853\n",
      "Epoch 160/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1386 - accuracy: 0.2498 - val_loss: 0.1856 - val_accuracy: 0.2320\n",
      "Epoch 161/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1381 - accuracy: 0.2595 - val_loss: 0.1851 - val_accuracy: 0.2158\n",
      "Epoch 162/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1377 - accuracy: 0.2456 - val_loss: 0.1842 - val_accuracy: 0.2194\n",
      "Epoch 163/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1371 - accuracy: 0.2468 - val_loss: 0.1840 - val_accuracy: 0.2014\n",
      "Epoch 164/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1368 - accuracy: 0.2288 - val_loss: 0.1829 - val_accuracy: 0.2374\n",
      "Epoch 165/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1364 - accuracy: 0.2541 - val_loss: 0.1828 - val_accuracy: 0.2068\n",
      "Epoch 166/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1359 - accuracy: 0.2492 - val_loss: 0.1816 - val_accuracy: 0.2410\n",
      "Epoch 167/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1354 - accuracy: 0.2541 - val_loss: 0.1815 - val_accuracy: 0.2194\n",
      "Epoch 168/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1350 - accuracy: 0.2511 - val_loss: 0.1810 - val_accuracy: 0.2050\n",
      "Epoch 169/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1344 - accuracy: 0.2517 - val_loss: 0.1800 - val_accuracy: 0.2194\n",
      "Epoch 170/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1341 - accuracy: 0.2523 - val_loss: 0.1800 - val_accuracy: 0.2572\n",
      "Epoch 171/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1336 - accuracy: 0.2625 - val_loss: 0.1802 - val_accuracy: 0.2446\n",
      "Epoch 172/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1332 - accuracy: 0.2505 - val_loss: 0.1790 - val_accuracy: 0.2446\n",
      "Epoch 173/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1328 - accuracy: 0.2517 - val_loss: 0.1796 - val_accuracy: 0.2644\n",
      "Epoch 174/800\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.1328 - accuracy: 0.24 - 0s 3ms/step - loss: 0.1324 - accuracy: 0.2589 - val_loss: 0.1785 - val_accuracy: 0.2338\n",
      "Epoch 175/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1318 - accuracy: 0.2655 - val_loss: 0.1767 - val_accuracy: 0.2680\n",
      "Epoch 176/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1313 - accuracy: 0.2655 - val_loss: 0.1771 - val_accuracy: 0.2050\n",
      "Epoch 177/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1309 - accuracy: 0.2595 - val_loss: 0.1761 - val_accuracy: 0.2266\n",
      "Epoch 178/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1305 - accuracy: 0.2607 - val_loss: 0.1752 - val_accuracy: 0.2716\n",
      "Epoch 179/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1300 - accuracy: 0.2613 - val_loss: 0.1737 - val_accuracy: 0.2626\n",
      "Epoch 180/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1296 - accuracy: 0.2715 - val_loss: 0.1731 - val_accuracy: 0.2500\n",
      "Epoch 181/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1291 - accuracy: 0.2589 - val_loss: 0.1729 - val_accuracy: 0.2446\n",
      "Epoch 182/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1287 - accuracy: 0.2649 - val_loss: 0.1728 - val_accuracy: 0.2626\n",
      "Epoch 183/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1283 - accuracy: 0.2631 - val_loss: 0.1731 - val_accuracy: 0.2536\n",
      "Epoch 184/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1279 - accuracy: 0.2679 - val_loss: 0.1734 - val_accuracy: 0.2554\n",
      "Epoch 185/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1275 - accuracy: 0.2625 - val_loss: 0.1709 - val_accuracy: 0.2590\n",
      "Epoch 186/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1269 - accuracy: 0.2703 - val_loss: 0.1714 - val_accuracy: 0.2374\n",
      "Epoch 187/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1266 - accuracy: 0.2577 - val_loss: 0.1706 - val_accuracy: 0.2608\n",
      "Epoch 188/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1262 - accuracy: 0.2547 - val_loss: 0.1704 - val_accuracy: 0.2518\n",
      "Epoch 189/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1258 - accuracy: 0.2733 - val_loss: 0.1705 - val_accuracy: 0.2428\n",
      "Epoch 190/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1255 - accuracy: 0.2643 - val_loss: 0.1696 - val_accuracy: 0.2572\n",
      "Epoch 191/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1250 - accuracy: 0.2667 - val_loss: 0.1689 - val_accuracy: 0.2590\n",
      "Epoch 192/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1246 - accuracy: 0.2661 - val_loss: 0.1698 - val_accuracy: 0.2896\n",
      "Epoch 193/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1242 - accuracy: 0.2697 - val_loss: 0.1687 - val_accuracy: 0.2338\n",
      "Epoch 194/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1238 - accuracy: 0.2649 - val_loss: 0.1754 - val_accuracy: 0.2068\n",
      "Epoch 195/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1239 - accuracy: 0.2745 - val_loss: 0.1673 - val_accuracy: 0.2626\n",
      "Epoch 196/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1231 - accuracy: 0.2637 - val_loss: 0.1661 - val_accuracy: 0.2626\n",
      "Epoch 197/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1226 - accuracy: 0.2655 - val_loss: 0.1664 - val_accuracy: 0.2770\n",
      "Epoch 198/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1223 - accuracy: 0.2739 - val_loss: 0.1668 - val_accuracy: 0.2986\n",
      "Epoch 199/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1220 - accuracy: 0.2727 - val_loss: 0.1645 - val_accuracy: 0.2842\n",
      "Epoch 200/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1217 - accuracy: 0.2739 - val_loss: 0.1655 - val_accuracy: 0.2698\n",
      "Epoch 201/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1214 - accuracy: 0.2673 - val_loss: 0.1658 - val_accuracy: 0.3112\n",
      "Epoch 202/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1210 - accuracy: 0.2733 - val_loss: 0.1647 - val_accuracy: 0.2536\n",
      "Epoch 203/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1206 - accuracy: 0.2757 - val_loss: 0.1696 - val_accuracy: 0.2284\n",
      "Epoch 204/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1205 - accuracy: 0.2793 - val_loss: 0.1642 - val_accuracy: 0.2554\n",
      "Epoch 205/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1200 - accuracy: 0.2781 - val_loss: 0.1630 - val_accuracy: 0.2554\n",
      "Epoch 206/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1196 - accuracy: 0.2763 - val_loss: 0.1624 - val_accuracy: 0.2842\n",
      "Epoch 207/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1192 - accuracy: 0.2835 - val_loss: 0.1639 - val_accuracy: 0.2608\n",
      "Epoch 208/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1191 - accuracy: 0.2721 - val_loss: 0.1620 - val_accuracy: 0.3058\n",
      "Epoch 209/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1187 - accuracy: 0.2739 - val_loss: 0.1648 - val_accuracy: 0.2428\n",
      "Epoch 210/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1185 - accuracy: 0.2739 - val_loss: 0.1619 - val_accuracy: 0.3327\n",
      "Epoch 211/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1181 - accuracy: 0.2739 - val_loss: 0.1615 - val_accuracy: 0.2968\n",
      "Epoch 212/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1177 - accuracy: 0.2787 - val_loss: 0.1602 - val_accuracy: 0.2914\n",
      "Epoch 213/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1175 - accuracy: 0.2733 - val_loss: 0.1622 - val_accuracy: 0.2842\n",
      "Epoch 214/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1172 - accuracy: 0.2787 - val_loss: 0.1598 - val_accuracy: 0.2950\n",
      "Epoch 215/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1168 - accuracy: 0.2775 - val_loss: 0.1613 - val_accuracy: 0.2716\n",
      "Epoch 216/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1167 - accuracy: 0.2811 - val_loss: 0.1593 - val_accuracy: 0.2842\n",
      "Epoch 217/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1163 - accuracy: 0.2763 - val_loss: 0.1584 - val_accuracy: 0.3076\n",
      "Epoch 218/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1160 - accuracy: 0.2811 - val_loss: 0.1579 - val_accuracy: 0.3004\n",
      "Epoch 219/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1158 - accuracy: 0.2829 - val_loss: 0.1581 - val_accuracy: 0.3040\n",
      "Epoch 220/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1154 - accuracy: 0.2781 - val_loss: 0.1648 - val_accuracy: 0.2824\n",
      "Epoch 221/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1156 - accuracy: 0.2865 - val_loss: 0.1584 - val_accuracy: 0.2860\n",
      "Epoch 222/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1149 - accuracy: 0.2841 - val_loss: 0.1578 - val_accuracy: 0.2752\n",
      "Epoch 223/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1145 - accuracy: 0.2805 - val_loss: 0.1595 - val_accuracy: 0.3183\n",
      "Epoch 224/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1144 - accuracy: 0.2919 - val_loss: 0.1567 - val_accuracy: 0.2842\n",
      "Epoch 225/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1141 - accuracy: 0.2913 - val_loss: 0.1574 - val_accuracy: 0.2986\n",
      "Epoch 226/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1138 - accuracy: 0.2877 - val_loss: 0.1565 - val_accuracy: 0.3219\n",
      "Epoch 227/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1136 - accuracy: 0.2841 - val_loss: 0.1560 - val_accuracy: 0.3004\n",
      "Epoch 228/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1134 - accuracy: 0.2817 - val_loss: 0.1649 - val_accuracy: 0.2122\n",
      "Epoch 229/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1135 - accuracy: 0.2859 - val_loss: 0.1590 - val_accuracy: 0.2734\n",
      "Epoch 230/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1130 - accuracy: 0.2823 - val_loss: 0.1557 - val_accuracy: 0.3183\n",
      "Epoch 231/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1125 - accuracy: 0.2811 - val_loss: 0.1561 - val_accuracy: 0.3076\n",
      "Epoch 232/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1123 - accuracy: 0.2871 - val_loss: 0.1558 - val_accuracy: 0.3112\n",
      "Epoch 233/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1121 - accuracy: 0.2823 - val_loss: 0.1649 - val_accuracy: 0.2950\n",
      "Epoch 234/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1121 - accuracy: 0.2859 - val_loss: 0.1550 - val_accuracy: 0.3094\n",
      "Epoch 235/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1115 - accuracy: 0.2811 - val_loss: 0.1548 - val_accuracy: 0.3022\n",
      "Epoch 236/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1114 - accuracy: 0.2913 - val_loss: 0.1555 - val_accuracy: 0.2932\n",
      "Epoch 237/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1111 - accuracy: 0.2823 - val_loss: 0.1538 - val_accuracy: 0.3112\n",
      "Epoch 238/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1108 - accuracy: 0.2811 - val_loss: 0.1608 - val_accuracy: 0.2932\n",
      "Epoch 239/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1110 - accuracy: 0.2805 - val_loss: 0.1541 - val_accuracy: 0.2770\n",
      "Epoch 240/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1103 - accuracy: 0.2907 - val_loss: 0.1544 - val_accuracy: 0.3165\n",
      "Epoch 241/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1101 - accuracy: 0.2883 - val_loss: 0.1526 - val_accuracy: 0.3058\n",
      "Epoch 242/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1099 - accuracy: 0.2901 - val_loss: 0.1545 - val_accuracy: 0.2806\n",
      "Epoch 243/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1097 - accuracy: 0.2829 - val_loss: 0.1522 - val_accuracy: 0.3094\n",
      "Epoch 244/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1094 - accuracy: 0.2841 - val_loss: 0.1537 - val_accuracy: 0.2932\n",
      "Epoch 245/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1095 - accuracy: 0.2835 - val_loss: 0.1525 - val_accuracy: 0.2860\n",
      "Epoch 246/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1090 - accuracy: 0.2943 - val_loss: 0.1545 - val_accuracy: 0.3183\n",
      "Epoch 247/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1088 - accuracy: 0.3033 - val_loss: 0.1526 - val_accuracy: 0.3112\n",
      "Epoch 248/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1084 - accuracy: 0.2955 - val_loss: 0.1528 - val_accuracy: 0.3201\n",
      "Epoch 249/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1084 - accuracy: 0.2907 - val_loss: 0.1592 - val_accuracy: 0.2554\n",
      "Epoch 250/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1085 - accuracy: 0.2949 - val_loss: 0.1493 - val_accuracy: 0.2968\n",
      "Epoch 251/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1078 - accuracy: 0.2937 - val_loss: 0.1519 - val_accuracy: 0.3183\n",
      "Epoch 252/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1078 - accuracy: 0.2895 - val_loss: 0.1531 - val_accuracy: 0.2824\n",
      "Epoch 253/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1074 - accuracy: 0.3021 - val_loss: 0.1507 - val_accuracy: 0.3165\n",
      "Epoch 254/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1070 - accuracy: 0.2991 - val_loss: 0.1509 - val_accuracy: 0.3201\n",
      "Epoch 255/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1069 - accuracy: 0.3009 - val_loss: 0.1491 - val_accuracy: 0.3058\n",
      "Epoch 256/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1068 - accuracy: 0.3015 - val_loss: 0.1501 - val_accuracy: 0.3183\n",
      "Epoch 257/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1065 - accuracy: 0.2985 - val_loss: 0.1524 - val_accuracy: 0.3112\n",
      "Epoch 258/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1064 - accuracy: 0.2943 - val_loss: 0.1499 - val_accuracy: 0.3129\n",
      "Epoch 259/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1061 - accuracy: 0.2865 - val_loss: 0.1507 - val_accuracy: 0.3094\n",
      "Epoch 260/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1059 - accuracy: 0.3015 - val_loss: 0.1509 - val_accuracy: 0.3273\n",
      "Epoch 261/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1058 - accuracy: 0.2979 - val_loss: 0.1483 - val_accuracy: 0.3076\n",
      "Epoch 262/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1055 - accuracy: 0.2937 - val_loss: 0.1492 - val_accuracy: 0.3219\n",
      "Epoch 263/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1052 - accuracy: 0.2949 - val_loss: 0.1477 - val_accuracy: 0.3417\n",
      "Epoch 264/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1049 - accuracy: 0.2973 - val_loss: 0.1476 - val_accuracy: 0.3255\n",
      "Epoch 265/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1047 - accuracy: 0.2985 - val_loss: 0.1479 - val_accuracy: 0.3219\n",
      "Epoch 266/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1046 - accuracy: 0.3057 - val_loss: 0.1477 - val_accuracy: 0.3183\n",
      "Epoch 267/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1043 - accuracy: 0.2949 - val_loss: 0.1462 - val_accuracy: 0.3309\n",
      "Epoch 268/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1041 - accuracy: 0.2955 - val_loss: 0.1463 - val_accuracy: 0.3147\n",
      "Epoch 269/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1039 - accuracy: 0.3027 - val_loss: 0.1476 - val_accuracy: 0.3040\n",
      "Epoch 270/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1036 - accuracy: 0.2961 - val_loss: 0.1456 - val_accuracy: 0.3219\n",
      "Epoch 271/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1034 - accuracy: 0.2973 - val_loss: 0.1477 - val_accuracy: 0.3273\n",
      "Epoch 272/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1032 - accuracy: 0.3129 - val_loss: 0.1460 - val_accuracy: 0.3291\n",
      "Epoch 273/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1031 - accuracy: 0.2931 - val_loss: 0.1449 - val_accuracy: 0.2860\n",
      "Epoch 274/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1028 - accuracy: 0.2901 - val_loss: 0.1460 - val_accuracy: 0.3237\n",
      "Epoch 275/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1025 - accuracy: 0.2943 - val_loss: 0.1457 - val_accuracy: 0.3147\n",
      "Epoch 276/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1025 - accuracy: 0.2955 - val_loss: 0.1597 - val_accuracy: 0.2626\n",
      "Epoch 277/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1027 - accuracy: 0.3003 - val_loss: 0.1446 - val_accuracy: 0.3291\n",
      "Epoch 278/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1019 - accuracy: 0.3021 - val_loss: 0.1446 - val_accuracy: 0.3237\n",
      "Epoch 279/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1017 - accuracy: 0.3015 - val_loss: 0.1441 - val_accuracy: 0.3040\n",
      "Epoch 280/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1014 - accuracy: 0.2961 - val_loss: 0.1475 - val_accuracy: 0.3183\n",
      "Epoch 281/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1014 - accuracy: 0.3081 - val_loss: 0.1450 - val_accuracy: 0.3525\n",
      "Epoch 282/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1010 - accuracy: 0.3069 - val_loss: 0.1447 - val_accuracy: 0.3471\n",
      "Epoch 283/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1008 - accuracy: 0.3015 - val_loss: 0.1438 - val_accuracy: 0.3076\n",
      "Epoch 284/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1007 - accuracy: 0.3021 - val_loss: 0.1431 - val_accuracy: 0.3201\n",
      "Epoch 285/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1003 - accuracy: 0.3087 - val_loss: 0.1449 - val_accuracy: 0.3183\n",
      "Epoch 286/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1002 - accuracy: 0.3033 - val_loss: 0.1429 - val_accuracy: 0.3363\n",
      "Epoch 287/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1000 - accuracy: 0.3063 - val_loss: 0.1437 - val_accuracy: 0.3165\n",
      "Epoch 288/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1000 - accuracy: 0.3027 - val_loss: 0.1448 - val_accuracy: 0.3255\n",
      "Epoch 289/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0996 - accuracy: 0.3147 - val_loss: 0.1431 - val_accuracy: 0.3129\n",
      "Epoch 290/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0994 - accuracy: 0.3045 - val_loss: 0.1462 - val_accuracy: 0.3291\n",
      "Epoch 291/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0993 - accuracy: 0.2973 - val_loss: 0.1422 - val_accuracy: 0.3237\n",
      "Epoch 292/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0989 - accuracy: 0.3075 - val_loss: 0.1499 - val_accuracy: 0.3579\n",
      "Epoch 293/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0991 - accuracy: 0.3015 - val_loss: 0.1417 - val_accuracy: 0.3094\n",
      "Epoch 294/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0984 - accuracy: 0.3105 - val_loss: 0.1425 - val_accuracy: 0.3327\n",
      "Epoch 295/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0984 - accuracy: 0.3015 - val_loss: 0.1409 - val_accuracy: 0.3345\n",
      "Epoch 296/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0980 - accuracy: 0.3057 - val_loss: 0.1417 - val_accuracy: 0.3129\n",
      "Epoch 297/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0980 - accuracy: 0.3069 - val_loss: 0.1414 - val_accuracy: 0.3147\n",
      "Epoch 298/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0978 - accuracy: 0.2997 - val_loss: 0.1406 - val_accuracy: 0.2932\n",
      "Epoch 299/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0975 - accuracy: 0.3027 - val_loss: 0.1405 - val_accuracy: 0.3237\n",
      "Epoch 300/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0973 - accuracy: 0.3039 - val_loss: 0.1405 - val_accuracy: 0.2986\n",
      "Epoch 301/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0971 - accuracy: 0.3033 - val_loss: 0.1593 - val_accuracy: 0.3094\n",
      "Epoch 302/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0979 - accuracy: 0.3027 - val_loss: 0.1424 - val_accuracy: 0.3201\n",
      "Epoch 303/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0968 - accuracy: 0.3021 - val_loss: 0.1420 - val_accuracy: 0.2950\n",
      "Epoch 304/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0966 - accuracy: 0.3087 - val_loss: 0.1407 - val_accuracy: 0.3112\n",
      "Epoch 305/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0964 - accuracy: 0.3159 - val_loss: 0.1400 - val_accuracy: 0.3147\n",
      "Epoch 306/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0962 - accuracy: 0.3111 - val_loss: 0.1398 - val_accuracy: 0.3147\n",
      "Epoch 307/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0960 - accuracy: 0.3117 - val_loss: 0.1397 - val_accuracy: 0.2896\n",
      "Epoch 308/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0958 - accuracy: 0.3051 - val_loss: 0.1394 - val_accuracy: 0.3453\n",
      "Epoch 309/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0958 - accuracy: 0.3099 - val_loss: 0.1526 - val_accuracy: 0.3183\n",
      "Epoch 310/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0961 - accuracy: 0.3165 - val_loss: 0.1392 - val_accuracy: 0.3219\n",
      "Epoch 311/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0954 - accuracy: 0.3057 - val_loss: 0.1410 - val_accuracy: 0.3327\n",
      "Epoch 312/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0953 - accuracy: 0.3123 - val_loss: 0.1401 - val_accuracy: 0.3058\n",
      "Epoch 313/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0949 - accuracy: 0.3087 - val_loss: 0.1383 - val_accuracy: 0.3201\n",
      "Epoch 314/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0946 - accuracy: 0.3171 - val_loss: 0.1389 - val_accuracy: 0.3273\n",
      "Epoch 315/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0945 - accuracy: 0.3147 - val_loss: 0.1383 - val_accuracy: 0.3219\n",
      "Epoch 316/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0943 - accuracy: 0.3075 - val_loss: 0.1433 - val_accuracy: 0.3543\n",
      "Epoch 317/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0944 - accuracy: 0.3141 - val_loss: 0.1373 - val_accuracy: 0.3183\n",
      "Epoch 318/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0941 - accuracy: 0.3069 - val_loss: 0.1553 - val_accuracy: 0.2464\n",
      "Epoch 319/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0946 - accuracy: 0.3069 - val_loss: 0.1400 - val_accuracy: 0.3094\n",
      "Epoch 320/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0937 - accuracy: 0.3093 - val_loss: 0.1381 - val_accuracy: 0.3327\n",
      "Epoch 321/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0934 - accuracy: 0.3201 - val_loss: 0.1374 - val_accuracy: 0.3147\n",
      "Epoch 322/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0932 - accuracy: 0.3093 - val_loss: 0.1372 - val_accuracy: 0.3255\n",
      "Epoch 323/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0931 - accuracy: 0.3093 - val_loss: 0.1377 - val_accuracy: 0.3435\n",
      "Epoch 324/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0930 - accuracy: 0.3141 - val_loss: 0.1363 - val_accuracy: 0.3129\n",
      "Epoch 325/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0927 - accuracy: 0.3165 - val_loss: 0.1375 - val_accuracy: 0.3112\n",
      "Epoch 326/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0927 - accuracy: 0.3075 - val_loss: 0.1375 - val_accuracy: 0.3165\n",
      "Epoch 327/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0924 - accuracy: 0.3111 - val_loss: 0.1512 - val_accuracy: 0.3112\n",
      "Epoch 328/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0929 - accuracy: 0.3147 - val_loss: 0.1454 - val_accuracy: 0.2914\n",
      "Epoch 329/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0925 - accuracy: 0.3135 - val_loss: 0.1362 - val_accuracy: 0.3147\n",
      "Epoch 330/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0920 - accuracy: 0.3105 - val_loss: 0.1362 - val_accuracy: 0.3094\n",
      "Epoch 331/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0918 - accuracy: 0.3117 - val_loss: 0.1346 - val_accuracy: 0.3201\n",
      "Epoch 332/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0916 - accuracy: 0.3153 - val_loss: 0.1370 - val_accuracy: 0.3291\n",
      "Epoch 333/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0916 - accuracy: 0.3093 - val_loss: 0.1366 - val_accuracy: 0.3040\n",
      "Epoch 334/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0913 - accuracy: 0.3105 - val_loss: 0.1341 - val_accuracy: 0.2986\n",
      "Epoch 335/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0911 - accuracy: 0.3093 - val_loss: 0.1348 - val_accuracy: 0.3058\n",
      "Epoch 336/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0909 - accuracy: 0.3099 - val_loss: 0.1423 - val_accuracy: 0.2230\n",
      "Epoch 337/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0913 - accuracy: 0.3003 - val_loss: 0.1346 - val_accuracy: 0.3129\n",
      "Epoch 338/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0905 - accuracy: 0.3201 - val_loss: 0.1357 - val_accuracy: 0.3309\n",
      "Epoch 339/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0906 - accuracy: 0.3093 - val_loss: 0.1355 - val_accuracy: 0.3183\n",
      "Epoch 340/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0903 - accuracy: 0.3153 - val_loss: 0.1366 - val_accuracy: 0.3579\n",
      "Epoch 341/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0903 - accuracy: 0.3171 - val_loss: 0.1354 - val_accuracy: 0.3040\n",
      "Epoch 342/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0900 - accuracy: 0.3177 - val_loss: 0.1331 - val_accuracy: 0.2950\n",
      "Epoch 343/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0897 - accuracy: 0.3129 - val_loss: 0.1349 - val_accuracy: 0.3237\n",
      "Epoch 344/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0897 - accuracy: 0.3129 - val_loss: 0.1400 - val_accuracy: 0.2968\n",
      "Epoch 345/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0899 - accuracy: 0.3141 - val_loss: 0.1340 - val_accuracy: 0.3273\n",
      "Epoch 346/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0894 - accuracy: 0.3165 - val_loss: 0.1366 - val_accuracy: 0.3651\n",
      "Epoch 347/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0894 - accuracy: 0.3111 - val_loss: 0.1342 - val_accuracy: 0.3147\n",
      "Epoch 348/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0892 - accuracy: 0.3135 - val_loss: 0.1320 - val_accuracy: 0.3129\n",
      "Epoch 349/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0888 - accuracy: 0.3105 - val_loss: 0.1347 - val_accuracy: 0.3040\n",
      "Epoch 350/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0889 - accuracy: 0.3123 - val_loss: 0.1329 - val_accuracy: 0.3040\n",
      "Epoch 351/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0885 - accuracy: 0.3195 - val_loss: 0.1329 - val_accuracy: 0.3309\n",
      "Epoch 352/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0885 - accuracy: 0.3159 - val_loss: 0.1334 - val_accuracy: 0.3255\n",
      "Epoch 353/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0882 - accuracy: 0.3213 - val_loss: 0.1337 - val_accuracy: 0.2968\n",
      "Epoch 354/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0882 - accuracy: 0.3081 - val_loss: 0.1327 - val_accuracy: 0.3165\n",
      "Epoch 355/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0880 - accuracy: 0.3099 - val_loss: 0.1345 - val_accuracy: 0.3309\n",
      "Epoch 356/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0879 - accuracy: 0.3135 - val_loss: 0.1337 - val_accuracy: 0.3237\n",
      "Epoch 357/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0877 - accuracy: 0.3201 - val_loss: 0.1336 - val_accuracy: 0.3183\n",
      "Epoch 358/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0875 - accuracy: 0.3273 - val_loss: 0.1371 - val_accuracy: 0.3309\n",
      "Epoch 359/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0878 - accuracy: 0.3111 - val_loss: 0.1338 - val_accuracy: 0.3112\n",
      "Epoch 360/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0873 - accuracy: 0.3141 - val_loss: 0.1326 - val_accuracy: 0.3165\n",
      "Epoch 361/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0873 - accuracy: 0.3135 - val_loss: 0.1326 - val_accuracy: 0.3183\n",
      "Epoch 362/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0872 - accuracy: 0.3141 - val_loss: 0.1315 - val_accuracy: 0.3219\n",
      "Epoch 363/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0869 - accuracy: 0.3159 - val_loss: 0.1375 - val_accuracy: 0.3183\n",
      "Epoch 364/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0870 - accuracy: 0.3213 - val_loss: 0.1378 - val_accuracy: 0.3237\n",
      "Epoch 365/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0868 - accuracy: 0.3201 - val_loss: 0.1327 - val_accuracy: 0.3183\n",
      "Epoch 366/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0867 - accuracy: 0.3213 - val_loss: 0.1316 - val_accuracy: 0.3094\n",
      "Epoch 367/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0865 - accuracy: 0.3129 - val_loss: 0.1314 - val_accuracy: 0.3183\n",
      "Epoch 368/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0864 - accuracy: 0.3135 - val_loss: 0.1309 - val_accuracy: 0.3022\n",
      "Epoch 369/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0860 - accuracy: 0.3153 - val_loss: 0.1335 - val_accuracy: 0.3381\n",
      "Epoch 370/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0862 - accuracy: 0.3165 - val_loss: 0.1325 - val_accuracy: 0.3183\n",
      "Epoch 371/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0860 - accuracy: 0.3129 - val_loss: 0.1310 - val_accuracy: 0.3291\n",
      "Epoch 372/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0859 - accuracy: 0.3243 - val_loss: 0.1336 - val_accuracy: 0.3165\n",
      "Epoch 373/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0858 - accuracy: 0.3177 - val_loss: 0.1320 - val_accuracy: 0.3237\n",
      "Epoch 374/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0856 - accuracy: 0.3213 - val_loss: 0.1332 - val_accuracy: 0.3129\n",
      "Epoch 375/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0855 - accuracy: 0.3213 - val_loss: 0.1299 - val_accuracy: 0.3165\n",
      "Epoch 376/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0853 - accuracy: 0.3123 - val_loss: 0.1318 - val_accuracy: 0.3219\n",
      "Epoch 377/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0851 - accuracy: 0.3249 - val_loss: 0.1310 - val_accuracy: 0.3058\n",
      "Epoch 378/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0852 - accuracy: 0.3225 - val_loss: 0.1299 - val_accuracy: 0.3255\n",
      "Epoch 379/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0849 - accuracy: 0.3207 - val_loss: 0.1299 - val_accuracy: 0.3112\n",
      "Epoch 380/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0850 - accuracy: 0.3159 - val_loss: 0.1318 - val_accuracy: 0.3129\n",
      "Epoch 381/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0848 - accuracy: 0.3165 - val_loss: 0.1323 - val_accuracy: 0.3040\n",
      "Epoch 382/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0848 - accuracy: 0.3213 - val_loss: 0.1361 - val_accuracy: 0.2644\n",
      "Epoch 383/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0849 - accuracy: 0.3129 - val_loss: 0.1305 - val_accuracy: 0.3201\n",
      "Epoch 384/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0844 - accuracy: 0.3159 - val_loss: 0.1306 - val_accuracy: 0.3219\n",
      "Epoch 385/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0844 - accuracy: 0.3123 - val_loss: 0.1302 - val_accuracy: 0.3543\n",
      "Epoch 386/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0841 - accuracy: 0.3237 - val_loss: 0.1289 - val_accuracy: 0.3147\n",
      "Epoch 387/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0841 - accuracy: 0.3129 - val_loss: 0.1299 - val_accuracy: 0.3094\n",
      "Epoch 388/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0842 - accuracy: 0.3219 - val_loss: 0.1288 - val_accuracy: 0.3201\n",
      "Epoch 389/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0838 - accuracy: 0.3183 - val_loss: 0.1358 - val_accuracy: 0.3183\n",
      "Epoch 390/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0841 - accuracy: 0.3195 - val_loss: 0.1287 - val_accuracy: 0.3058\n",
      "Epoch 391/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0837 - accuracy: 0.3207 - val_loss: 0.1302 - val_accuracy: 0.3094\n",
      "Epoch 392/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0837 - accuracy: 0.3189 - val_loss: 0.1294 - val_accuracy: 0.3219\n",
      "Epoch 393/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0835 - accuracy: 0.3171 - val_loss: 0.1335 - val_accuracy: 0.3201\n",
      "Epoch 394/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0836 - accuracy: 0.3207 - val_loss: 0.1286 - val_accuracy: 0.3165\n",
      "Epoch 395/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0832 - accuracy: 0.3189 - val_loss: 0.1300 - val_accuracy: 0.3147\n",
      "Epoch 396/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0831 - accuracy: 0.3225 - val_loss: 0.1286 - val_accuracy: 0.3291\n",
      "Epoch 397/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0829 - accuracy: 0.3267 - val_loss: 0.1290 - val_accuracy: 0.3129\n",
      "Epoch 398/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0830 - accuracy: 0.3087 - val_loss: 0.1290 - val_accuracy: 0.3381\n",
      "Epoch 399/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0830 - accuracy: 0.3195 - val_loss: 0.1289 - val_accuracy: 0.3094\n",
      "Epoch 400/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0827 - accuracy: 0.3207 - val_loss: 0.1297 - val_accuracy: 0.3183\n",
      "Epoch 401/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0828 - accuracy: 0.3213 - val_loss: 0.1295 - val_accuracy: 0.2896\n",
      "Epoch 402/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0827 - accuracy: 0.3201 - val_loss: 0.1300 - val_accuracy: 0.3255\n",
      "Epoch 403/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0824 - accuracy: 0.3201 - val_loss: 0.1280 - val_accuracy: 0.3255\n",
      "Epoch 404/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0824 - accuracy: 0.3195 - val_loss: 0.1283 - val_accuracy: 0.3543\n",
      "Epoch 405/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0823 - accuracy: 0.3147 - val_loss: 0.1314 - val_accuracy: 0.3201\n",
      "Epoch 406/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0825 - accuracy: 0.3267 - val_loss: 0.1285 - val_accuracy: 0.3327\n",
      "Epoch 407/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0822 - accuracy: 0.3207 - val_loss: 0.1284 - val_accuracy: 0.3183\n",
      "Epoch 408/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0821 - accuracy: 0.3147 - val_loss: 0.1287 - val_accuracy: 0.3147\n",
      "Epoch 409/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0821 - accuracy: 0.3201 - val_loss: 0.1288 - val_accuracy: 0.3076\n",
      "Epoch 410/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0821 - accuracy: 0.3207 - val_loss: 0.1281 - val_accuracy: 0.3597\n",
      "Epoch 411/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0817 - accuracy: 0.3225 - val_loss: 0.1293 - val_accuracy: 0.3453\n",
      "Epoch 412/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0817 - accuracy: 0.3225 - val_loss: 0.1264 - val_accuracy: 0.3076\n",
      "Epoch 413/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0814 - accuracy: 0.3219 - val_loss: 0.1270 - val_accuracy: 0.3471\n",
      "Epoch 414/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0814 - accuracy: 0.3183 - val_loss: 0.1268 - val_accuracy: 0.2968\n",
      "Epoch 415/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0816 - accuracy: 0.3081 - val_loss: 0.1281 - val_accuracy: 0.3094\n",
      "Epoch 416/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0816 - accuracy: 0.3159 - val_loss: 0.1282 - val_accuracy: 0.3273\n",
      "Epoch 417/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0813 - accuracy: 0.3189 - val_loss: 0.1282 - val_accuracy: 0.3165\n",
      "Epoch 418/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0811 - accuracy: 0.3147 - val_loss: 0.1264 - val_accuracy: 0.3273\n",
      "Epoch 419/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0809 - accuracy: 0.3231 - val_loss: 0.1281 - val_accuracy: 0.3129\n",
      "Epoch 420/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0811 - accuracy: 0.3225 - val_loss: 0.1269 - val_accuracy: 0.3435\n",
      "Epoch 421/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0808 - accuracy: 0.3297 - val_loss: 0.1295 - val_accuracy: 0.3363\n",
      "Epoch 422/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0808 - accuracy: 0.3267 - val_loss: 0.1288 - val_accuracy: 0.3309\n",
      "Epoch 423/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0807 - accuracy: 0.3153 - val_loss: 0.1258 - val_accuracy: 0.3147\n",
      "Epoch 424/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0808 - accuracy: 0.3135 - val_loss: 0.1269 - val_accuracy: 0.3345\n",
      "Epoch 425/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0804 - accuracy: 0.3189 - val_loss: 0.1273 - val_accuracy: 0.3327\n",
      "Epoch 426/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0804 - accuracy: 0.3291 - val_loss: 0.1256 - val_accuracy: 0.3399\n",
      "Epoch 427/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0804 - accuracy: 0.3189 - val_loss: 0.1266 - val_accuracy: 0.3471\n",
      "Epoch 428/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0802 - accuracy: 0.3273 - val_loss: 0.1271 - val_accuracy: 0.3165\n",
      "Epoch 429/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0801 - accuracy: 0.3207 - val_loss: 0.1348 - val_accuracy: 0.3183\n",
      "Epoch 430/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0805 - accuracy: 0.3159 - val_loss: 0.1280 - val_accuracy: 0.3219\n",
      "Epoch 431/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0799 - accuracy: 0.3273 - val_loss: 0.1296 - val_accuracy: 0.3291\n",
      "Epoch 432/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0803 - accuracy: 0.3213 - val_loss: 0.1274 - val_accuracy: 0.3309\n",
      "Epoch 433/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0798 - accuracy: 0.3255 - val_loss: 0.1369 - val_accuracy: 0.3040\n",
      "Epoch 434/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0801 - accuracy: 0.3213 - val_loss: 0.1373 - val_accuracy: 0.3615\n",
      "Epoch 435/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0802 - accuracy: 0.3165 - val_loss: 0.1318 - val_accuracy: 0.3183\n",
      "Epoch 436/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0797 - accuracy: 0.3237 - val_loss: 0.1263 - val_accuracy: 0.3040\n",
      "Epoch 437/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0794 - accuracy: 0.3159 - val_loss: 0.1283 - val_accuracy: 0.3147\n",
      "Epoch 438/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0793 - accuracy: 0.3213 - val_loss: 0.1429 - val_accuracy: 0.2914\n",
      "Epoch 439/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0801 - accuracy: 0.3267 - val_loss: 0.1331 - val_accuracy: 0.3201\n",
      "Epoch 440/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0795 - accuracy: 0.3225 - val_loss: 0.1265 - val_accuracy: 0.3327\n",
      "Epoch 441/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0790 - accuracy: 0.3171 - val_loss: 0.1298 - val_accuracy: 0.3345\n",
      "Epoch 442/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0792 - accuracy: 0.3201 - val_loss: 0.1268 - val_accuracy: 0.3129\n",
      "Epoch 443/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0790 - accuracy: 0.3213 - val_loss: 0.1273 - val_accuracy: 0.3129\n",
      "Epoch 444/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0790 - accuracy: 0.3213 - val_loss: 0.1311 - val_accuracy: 0.3129\n",
      "Epoch 445/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0789 - accuracy: 0.3129 - val_loss: 0.1284 - val_accuracy: 0.3525\n",
      "Epoch 446/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0787 - accuracy: 0.3183 - val_loss: 0.1270 - val_accuracy: 0.3022\n",
      "Epoch 447/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0788 - accuracy: 0.3171 - val_loss: 0.1362 - val_accuracy: 0.3147\n",
      "Epoch 448/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0789 - accuracy: 0.3123 - val_loss: 0.1549 - val_accuracy: 0.2608\n",
      "Epoch 449/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0799 - accuracy: 0.3087 - val_loss: 0.1265 - val_accuracy: 0.3363\n",
      "Epoch 450/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0785 - accuracy: 0.3243 - val_loss: 0.1264 - val_accuracy: 0.3201\n",
      "Epoch 451/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0782 - accuracy: 0.3207 - val_loss: 0.1250 - val_accuracy: 0.2878\n",
      "Epoch 452/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0783 - accuracy: 0.3177 - val_loss: 0.1236 - val_accuracy: 0.3273\n",
      "Epoch 453/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0781 - accuracy: 0.3177 - val_loss: 0.1251 - val_accuracy: 0.3435\n",
      "Epoch 454/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0780 - accuracy: 0.3261 - val_loss: 0.1302 - val_accuracy: 0.3219\n",
      "Epoch 455/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0782 - accuracy: 0.3255 - val_loss: 0.1253 - val_accuracy: 0.3417\n",
      "Epoch 456/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0779 - accuracy: 0.3189 - val_loss: 0.1291 - val_accuracy: 0.3363\n",
      "Epoch 457/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0780 - accuracy: 0.3213 - val_loss: 0.1278 - val_accuracy: 0.3201\n",
      "Epoch 458/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0783 - accuracy: 0.3147 - val_loss: 0.1258 - val_accuracy: 0.3273\n",
      "Epoch 459/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0777 - accuracy: 0.3207 - val_loss: 0.1272 - val_accuracy: 0.3112\n",
      "Epoch 460/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0778 - accuracy: 0.3153 - val_loss: 0.1244 - val_accuracy: 0.3435\n",
      "Epoch 461/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0775 - accuracy: 0.3183 - val_loss: 0.1259 - val_accuracy: 0.3129\n",
      "Epoch 462/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0774 - accuracy: 0.3183 - val_loss: 0.1270 - val_accuracy: 0.3273\n",
      "Epoch 463/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0775 - accuracy: 0.3243 - val_loss: 0.1243 - val_accuracy: 0.3345\n",
      "Epoch 464/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0773 - accuracy: 0.3165 - val_loss: 0.1414 - val_accuracy: 0.1888\n",
      "Epoch 465/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0782 - accuracy: 0.3099 - val_loss: 0.1309 - val_accuracy: 0.3219\n",
      "Epoch 466/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0776 - accuracy: 0.3267 - val_loss: 0.1240 - val_accuracy: 0.3327\n",
      "Epoch 467/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0770 - accuracy: 0.3243 - val_loss: 0.1262 - val_accuracy: 0.3147\n",
      "Epoch 468/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0771 - accuracy: 0.3243 - val_loss: 0.1242 - val_accuracy: 0.3417\n",
      "Epoch 469/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0767 - accuracy: 0.3183 - val_loss: 0.1242 - val_accuracy: 0.3363\n",
      "Epoch 470/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0768 - accuracy: 0.3207 - val_loss: 0.1243 - val_accuracy: 0.3309\n",
      "Epoch 471/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0766 - accuracy: 0.3237 - val_loss: 0.1311 - val_accuracy: 0.3705\n",
      "Epoch 472/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0770 - accuracy: 0.3267 - val_loss: 0.1242 - val_accuracy: 0.3201\n",
      "Epoch 473/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0765 - accuracy: 0.3213 - val_loss: 0.1245 - val_accuracy: 0.3381\n",
      "Epoch 474/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0765 - accuracy: 0.3171 - val_loss: 0.1241 - val_accuracy: 0.3255\n",
      "Epoch 475/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0763 - accuracy: 0.3159 - val_loss: 0.1247 - val_accuracy: 0.3489\n",
      "Epoch 476/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0763 - accuracy: 0.3237 - val_loss: 0.1354 - val_accuracy: 0.3201\n",
      "Epoch 477/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0766 - accuracy: 0.3285 - val_loss: 0.1235 - val_accuracy: 0.3255\n",
      "Epoch 478/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0760 - accuracy: 0.3171 - val_loss: 0.1242 - val_accuracy: 0.3471\n",
      "Epoch 479/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0760 - accuracy: 0.3273 - val_loss: 0.1225 - val_accuracy: 0.3112\n",
      "Epoch 480/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0760 - accuracy: 0.3213 - val_loss: 0.1250 - val_accuracy: 0.3076\n",
      "Epoch 481/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0758 - accuracy: 0.3183 - val_loss: 0.1229 - val_accuracy: 0.3579\n",
      "Epoch 482/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0757 - accuracy: 0.3189 - val_loss: 0.1264 - val_accuracy: 0.3399\n",
      "Epoch 483/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0758 - accuracy: 0.3267 - val_loss: 0.1239 - val_accuracy: 0.3255\n",
      "Epoch 484/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0755 - accuracy: 0.3195 - val_loss: 0.1294 - val_accuracy: 0.3076\n",
      "Epoch 485/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0758 - accuracy: 0.3231 - val_loss: 0.1278 - val_accuracy: 0.3058\n",
      "Epoch 486/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0758 - accuracy: 0.3213 - val_loss: 0.1242 - val_accuracy: 0.3363\n",
      "Epoch 487/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0754 - accuracy: 0.3201 - val_loss: 0.1250 - val_accuracy: 0.3345\n",
      "Epoch 488/800\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0762 - accuracy: 0.32 - 0s 4ms/step - loss: 0.0753 - accuracy: 0.3255 - val_loss: 0.1232 - val_accuracy: 0.3237\n",
      "Epoch 489/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0752 - accuracy: 0.3183 - val_loss: 0.1243 - val_accuracy: 0.3201\n",
      "Epoch 490/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0751 - accuracy: 0.3231 - val_loss: 0.1225 - val_accuracy: 0.3129\n",
      "Epoch 491/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0752 - accuracy: 0.3135 - val_loss: 0.1230 - val_accuracy: 0.3345\n",
      "Epoch 492/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0750 - accuracy: 0.3243 - val_loss: 0.1230 - val_accuracy: 0.3291\n",
      "Epoch 493/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0751 - accuracy: 0.3243 - val_loss: 0.1247 - val_accuracy: 0.3147\n",
      "Epoch 494/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0749 - accuracy: 0.3135 - val_loss: 0.1220 - val_accuracy: 0.3094\n",
      "Epoch 495/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0749 - accuracy: 0.3183 - val_loss: 0.1572 - val_accuracy: 0.3040\n",
      "Epoch 496/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0758 - accuracy: 0.3189 - val_loss: 0.1229 - val_accuracy: 0.3633\n",
      "Epoch 497/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0747 - accuracy: 0.3273 - val_loss: 0.1218 - val_accuracy: 0.3399\n",
      "Epoch 498/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0746 - accuracy: 0.3261 - val_loss: 0.1226 - val_accuracy: 0.3291\n",
      "Epoch 499/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0745 - accuracy: 0.3189 - val_loss: 0.1242 - val_accuracy: 0.3309\n",
      "Epoch 500/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0743 - accuracy: 0.3231 - val_loss: 0.1240 - val_accuracy: 0.3633\n",
      "Epoch 501/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0743 - accuracy: 0.3351 - val_loss: 0.1232 - val_accuracy: 0.3219\n",
      "Epoch 502/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0744 - accuracy: 0.3243 - val_loss: 0.1221 - val_accuracy: 0.3417\n",
      "Epoch 503/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0741 - accuracy: 0.3237 - val_loss: 0.1229 - val_accuracy: 0.3399\n",
      "Epoch 504/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0739 - accuracy: 0.3201 - val_loss: 0.1234 - val_accuracy: 0.3040\n",
      "Epoch 505/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0742 - accuracy: 0.3231 - val_loss: 0.1223 - val_accuracy: 0.3363\n",
      "Epoch 506/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0739 - accuracy: 0.3237 - val_loss: 0.1246 - val_accuracy: 0.3453\n",
      "Epoch 507/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0739 - accuracy: 0.3219 - val_loss: 0.1274 - val_accuracy: 0.3291\n",
      "Epoch 508/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0741 - accuracy: 0.3135 - val_loss: 0.1218 - val_accuracy: 0.3543\n",
      "Epoch 509/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0736 - accuracy: 0.3201 - val_loss: 0.1207 - val_accuracy: 0.3237\n",
      "Epoch 510/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0735 - accuracy: 0.3225 - val_loss: 0.1227 - val_accuracy: 0.3561\n",
      "Epoch 511/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0735 - accuracy: 0.3183 - val_loss: 0.1259 - val_accuracy: 0.3219\n",
      "Epoch 512/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0735 - accuracy: 0.3189 - val_loss: 0.1215 - val_accuracy: 0.3687\n",
      "Epoch 513/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0733 - accuracy: 0.3237 - val_loss: 0.1212 - val_accuracy: 0.3183\n",
      "Epoch 514/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0736 - accuracy: 0.3081 - val_loss: 0.1221 - val_accuracy: 0.3363\n",
      "Epoch 515/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0733 - accuracy: 0.3225 - val_loss: 0.1236 - val_accuracy: 0.3291\n",
      "Epoch 516/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0732 - accuracy: 0.3243 - val_loss: 0.1331 - val_accuracy: 0.3183\n",
      "Epoch 517/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0735 - accuracy: 0.3273 - val_loss: 0.1214 - val_accuracy: 0.3417\n",
      "Epoch 518/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0729 - accuracy: 0.3225 - val_loss: 0.1219 - val_accuracy: 0.3741\n",
      "Epoch 519/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0733 - accuracy: 0.3183 - val_loss: 0.1204 - val_accuracy: 0.3345\n",
      "Epoch 520/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0728 - accuracy: 0.3255 - val_loss: 0.1233 - val_accuracy: 0.3453\n",
      "Epoch 521/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0729 - accuracy: 0.3171 - val_loss: 0.1228 - val_accuracy: 0.3471\n",
      "Epoch 522/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0728 - accuracy: 0.3207 - val_loss: 0.1235 - val_accuracy: 0.3507\n",
      "Epoch 523/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0727 - accuracy: 0.3243 - val_loss: 0.1208 - val_accuracy: 0.3453\n",
      "Epoch 524/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0724 - accuracy: 0.3255 - val_loss: 0.1204 - val_accuracy: 0.3381\n",
      "Epoch 525/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0725 - accuracy: 0.3231 - val_loss: 0.1385 - val_accuracy: 0.3669\n",
      "Epoch 526/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0733 - accuracy: 0.3231 - val_loss: 0.1207 - val_accuracy: 0.3363\n",
      "Epoch 527/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0722 - accuracy: 0.3189 - val_loss: 0.1200 - val_accuracy: 0.3363\n",
      "Epoch 528/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0724 - accuracy: 0.3195 - val_loss: 0.1222 - val_accuracy: 0.3525\n",
      "Epoch 529/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0723 - accuracy: 0.3201 - val_loss: 0.1229 - val_accuracy: 0.3597\n",
      "Epoch 530/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0722 - accuracy: 0.3255 - val_loss: 0.1212 - val_accuracy: 0.3561\n",
      "Epoch 531/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0719 - accuracy: 0.3147 - val_loss: 0.1212 - val_accuracy: 0.3399\n",
      "Epoch 532/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0718 - accuracy: 0.3177 - val_loss: 0.1219 - val_accuracy: 0.3309\n",
      "Epoch 533/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0719 - accuracy: 0.3129 - val_loss: 0.1221 - val_accuracy: 0.3453\n",
      "Epoch 534/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0718 - accuracy: 0.3273 - val_loss: 0.1214 - val_accuracy: 0.3525\n",
      "Epoch 535/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0717 - accuracy: 0.3261 - val_loss: 0.1229 - val_accuracy: 0.3561\n",
      "Epoch 536/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0717 - accuracy: 0.3153 - val_loss: 0.1220 - val_accuracy: 0.3399\n",
      "Epoch 537/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0716 - accuracy: 0.3213 - val_loss: 0.1207 - val_accuracy: 0.3165\n",
      "Epoch 538/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0715 - accuracy: 0.3207 - val_loss: 0.1181 - val_accuracy: 0.3399\n",
      "Epoch 539/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0714 - accuracy: 0.3219 - val_loss: 0.1220 - val_accuracy: 0.3579\n",
      "Epoch 540/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0714 - accuracy: 0.3213 - val_loss: 0.1232 - val_accuracy: 0.3561\n",
      "Epoch 541/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0714 - accuracy: 0.3285 - val_loss: 0.1219 - val_accuracy: 0.3381\n",
      "Epoch 542/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0713 - accuracy: 0.3213 - val_loss: 0.1257 - val_accuracy: 0.3112\n",
      "Epoch 543/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0715 - accuracy: 0.3153 - val_loss: 0.1324 - val_accuracy: 0.2734\n",
      "Epoch 544/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0718 - accuracy: 0.3249 - val_loss: 0.1200 - val_accuracy: 0.3435\n",
      "Epoch 545/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0710 - accuracy: 0.3171 - val_loss: 0.1231 - val_accuracy: 0.3112\n",
      "Epoch 546/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0713 - accuracy: 0.3165 - val_loss: 0.1209 - val_accuracy: 0.3543\n",
      "Epoch 547/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0710 - accuracy: 0.3183 - val_loss: 0.1207 - val_accuracy: 0.3525\n",
      "Epoch 548/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0708 - accuracy: 0.3249 - val_loss: 0.1244 - val_accuracy: 0.3615\n",
      "Epoch 549/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0709 - accuracy: 0.3207 - val_loss: 0.1356 - val_accuracy: 0.3165\n",
      "Epoch 550/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0715 - accuracy: 0.3207 - val_loss: 0.1193 - val_accuracy: 0.3363\n",
      "Epoch 551/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0707 - accuracy: 0.3183 - val_loss: 0.1392 - val_accuracy: 0.2932\n",
      "Epoch 552/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0714 - accuracy: 0.3129 - val_loss: 0.1218 - val_accuracy: 0.3741\n",
      "Epoch 553/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0708 - accuracy: 0.3213 - val_loss: 0.1363 - val_accuracy: 0.3004\n",
      "Epoch 554/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0715 - accuracy: 0.3207 - val_loss: 0.1196 - val_accuracy: 0.3489\n",
      "Epoch 555/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0706 - accuracy: 0.3243 - val_loss: 0.1196 - val_accuracy: 0.3363\n",
      "Epoch 556/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0703 - accuracy: 0.3201 - val_loss: 0.1194 - val_accuracy: 0.3399\n",
      "Epoch 557/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0703 - accuracy: 0.3177 - val_loss: 0.1185 - val_accuracy: 0.3471\n",
      "Epoch 558/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0702 - accuracy: 0.3219 - val_loss: 0.1191 - val_accuracy: 0.3417\n",
      "Epoch 559/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0701 - accuracy: 0.3261 - val_loss: 0.1179 - val_accuracy: 0.3417\n",
      "Epoch 560/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0698 - accuracy: 0.3183 - val_loss: 0.1207 - val_accuracy: 0.3309\n",
      "Epoch 561/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0700 - accuracy: 0.3129 - val_loss: 0.1404 - val_accuracy: 0.3489\n",
      "Epoch 562/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0709 - accuracy: 0.3219 - val_loss: 0.1208 - val_accuracy: 0.3435\n",
      "Epoch 563/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0699 - accuracy: 0.3249 - val_loss: 0.1226 - val_accuracy: 0.3399\n",
      "Epoch 564/800\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0696 - accuracy: 0.30 - 0s 3ms/step - loss: 0.0701 - accuracy: 0.3105 - val_loss: 0.1185 - val_accuracy: 0.3417\n",
      "Epoch 565/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0695 - accuracy: 0.3195 - val_loss: 0.1251 - val_accuracy: 0.3381\n",
      "Epoch 566/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0700 - accuracy: 0.3213 - val_loss: 0.1380 - val_accuracy: 0.3004\n",
      "Epoch 567/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0706 - accuracy: 0.3153 - val_loss: 0.1185 - val_accuracy: 0.3561\n",
      "Epoch 568/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0694 - accuracy: 0.3225 - val_loss: 0.1199 - val_accuracy: 0.3597\n",
      "Epoch 569/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0694 - accuracy: 0.3177 - val_loss: 0.1217 - val_accuracy: 0.3543\n",
      "Epoch 570/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0695 - accuracy: 0.3291 - val_loss: 0.1373 - val_accuracy: 0.3651\n",
      "Epoch 571/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0703 - accuracy: 0.3165 - val_loss: 0.1237 - val_accuracy: 0.3453\n",
      "Epoch 572/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0694 - accuracy: 0.3237 - val_loss: 0.1173 - val_accuracy: 0.3525\n",
      "Epoch 573/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0691 - accuracy: 0.3189 - val_loss: 0.1183 - val_accuracy: 0.3525\n",
      "Epoch 574/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0690 - accuracy: 0.3171 - val_loss: 0.1188 - val_accuracy: 0.3525\n",
      "Epoch 575/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0690 - accuracy: 0.3207 - val_loss: 0.1391 - val_accuracy: 0.3543\n",
      "Epoch 576/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0700 - accuracy: 0.3171 - val_loss: 0.1180 - val_accuracy: 0.3669\n",
      "Epoch 577/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0688 - accuracy: 0.3177 - val_loss: 0.1192 - val_accuracy: 0.3417\n",
      "Epoch 578/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0689 - accuracy: 0.3195 - val_loss: 0.1190 - val_accuracy: 0.3399\n",
      "Epoch 579/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0686 - accuracy: 0.3087 - val_loss: 0.1182 - val_accuracy: 0.3399\n",
      "Epoch 580/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0686 - accuracy: 0.3195 - val_loss: 0.1163 - val_accuracy: 0.3561\n",
      "Epoch 581/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0686 - accuracy: 0.3195 - val_loss: 0.1193 - val_accuracy: 0.3453\n",
      "Epoch 582/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0687 - accuracy: 0.3237 - val_loss: 0.1180 - val_accuracy: 0.3489\n",
      "Epoch 583/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0684 - accuracy: 0.3243 - val_loss: 0.1184 - val_accuracy: 0.3489\n",
      "Epoch 584/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0683 - accuracy: 0.3213 - val_loss: 0.1282 - val_accuracy: 0.3363\n",
      "Epoch 585/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0688 - accuracy: 0.3249 - val_loss: 0.1173 - val_accuracy: 0.3525\n",
      "Epoch 586/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0682 - accuracy: 0.3231 - val_loss: 0.1216 - val_accuracy: 0.3669\n",
      "Epoch 587/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0682 - accuracy: 0.3231 - val_loss: 0.1179 - val_accuracy: 0.3651\n",
      "Epoch 588/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0682 - accuracy: 0.3261 - val_loss: 0.1203 - val_accuracy: 0.3471\n",
      "Epoch 589/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0681 - accuracy: 0.3201 - val_loss: 0.1190 - val_accuracy: 0.3561\n",
      "Epoch 590/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0680 - accuracy: 0.3189 - val_loss: 0.1283 - val_accuracy: 0.2806\n",
      "Epoch 591/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0688 - accuracy: 0.3189 - val_loss: 0.1177 - val_accuracy: 0.3561\n",
      "Epoch 592/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0679 - accuracy: 0.3225 - val_loss: 0.1173 - val_accuracy: 0.3543\n",
      "Epoch 593/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0679 - accuracy: 0.3225 - val_loss: 0.1208 - val_accuracy: 0.3489\n",
      "Epoch 594/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0681 - accuracy: 0.3243 - val_loss: 0.1167 - val_accuracy: 0.3615\n",
      "Epoch 595/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0678 - accuracy: 0.3261 - val_loss: 0.1252 - val_accuracy: 0.2950\n",
      "Epoch 596/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0687 - accuracy: 0.3111 - val_loss: 0.1184 - val_accuracy: 0.3543\n",
      "Epoch 597/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0677 - accuracy: 0.3225 - val_loss: 0.1381 - val_accuracy: 0.3579\n",
      "Epoch 598/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0684 - accuracy: 0.3267 - val_loss: 0.1199 - val_accuracy: 0.3507\n",
      "Epoch 599/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0676 - accuracy: 0.3231 - val_loss: 0.1155 - val_accuracy: 0.3471\n",
      "Epoch 600/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0675 - accuracy: 0.3165 - val_loss: 0.1180 - val_accuracy: 0.3507\n",
      "Epoch 601/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0673 - accuracy: 0.3261 - val_loss: 0.1297 - val_accuracy: 0.3741\n",
      "Epoch 602/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0681 - accuracy: 0.3261 - val_loss: 0.1181 - val_accuracy: 0.3201\n",
      "Epoch 603/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0674 - accuracy: 0.3171 - val_loss: 0.1169 - val_accuracy: 0.3471\n",
      "Epoch 604/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0671 - accuracy: 0.3171 - val_loss: 0.1187 - val_accuracy: 0.3345\n",
      "Epoch 605/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0672 - accuracy: 0.3195 - val_loss: 0.1182 - val_accuracy: 0.3327\n",
      "Epoch 606/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0671 - accuracy: 0.3213 - val_loss: 0.1188 - val_accuracy: 0.3399\n",
      "Epoch 607/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0670 - accuracy: 0.3165 - val_loss: 0.1203 - val_accuracy: 0.3327\n",
      "Epoch 608/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0673 - accuracy: 0.3201 - val_loss: 0.1230 - val_accuracy: 0.3561\n",
      "Epoch 609/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0672 - accuracy: 0.3279 - val_loss: 0.1245 - val_accuracy: 0.3489\n",
      "Epoch 610/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0673 - accuracy: 0.3207 - val_loss: 0.1190 - val_accuracy: 0.3255\n",
      "Epoch 611/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0672 - accuracy: 0.3123 - val_loss: 0.1155 - val_accuracy: 0.3525\n",
      "Epoch 612/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0667 - accuracy: 0.3153 - val_loss: 0.1273 - val_accuracy: 0.3435\n",
      "Epoch 613/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0672 - accuracy: 0.3249 - val_loss: 0.1175 - val_accuracy: 0.3507\n",
      "Epoch 614/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0666 - accuracy: 0.3225 - val_loss: 0.1174 - val_accuracy: 0.3597\n",
      "Epoch 615/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0666 - accuracy: 0.3153 - val_loss: 0.1244 - val_accuracy: 0.3040\n",
      "Epoch 616/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0673 - accuracy: 0.3171 - val_loss: 0.1144 - val_accuracy: 0.3543\n",
      "Epoch 617/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0666 - accuracy: 0.3201 - val_loss: 0.1218 - val_accuracy: 0.3525\n",
      "Epoch 618/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0665 - accuracy: 0.3273 - val_loss: 0.1157 - val_accuracy: 0.3399\n",
      "Epoch 619/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0665 - accuracy: 0.3153 - val_loss: 0.1194 - val_accuracy: 0.3435\n",
      "Epoch 620/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0663 - accuracy: 0.3225 - val_loss: 0.1179 - val_accuracy: 0.3489\n",
      "Epoch 621/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0664 - accuracy: 0.3159 - val_loss: 0.1161 - val_accuracy: 0.3579\n",
      "Epoch 622/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0659 - accuracy: 0.3243 - val_loss: 0.1243 - val_accuracy: 0.3201\n",
      "Epoch 623/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0665 - accuracy: 0.3177 - val_loss: 0.1179 - val_accuracy: 0.3579\n",
      "Epoch 624/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0661 - accuracy: 0.3201 - val_loss: 0.1226 - val_accuracy: 0.3363\n",
      "Epoch 625/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0663 - accuracy: 0.3195 - val_loss: 0.1189 - val_accuracy: 0.3579\n",
      "Epoch 626/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0660 - accuracy: 0.3153 - val_loss: 0.1174 - val_accuracy: 0.3669\n",
      "Epoch 627/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0657 - accuracy: 0.3201 - val_loss: 0.1163 - val_accuracy: 0.3417\n",
      "Epoch 628/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0657 - accuracy: 0.3159 - val_loss: 0.1169 - val_accuracy: 0.3615\n",
      "Epoch 629/800\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0649 - accuracy: 0.32 - 0s 4ms/step - loss: 0.0656 - accuracy: 0.3273 - val_loss: 0.1174 - val_accuracy: 0.3345\n",
      "Epoch 630/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0657 - accuracy: 0.3183 - val_loss: 0.1178 - val_accuracy: 0.3435\n",
      "Epoch 631/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0656 - accuracy: 0.3213 - val_loss: 0.1190 - val_accuracy: 0.3471\n",
      "Epoch 632/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0658 - accuracy: 0.3147 - val_loss: 0.1178 - val_accuracy: 0.3291\n",
      "Epoch 633/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0656 - accuracy: 0.3237 - val_loss: 0.1192 - val_accuracy: 0.3759\n",
      "Epoch 634/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0656 - accuracy: 0.3225 - val_loss: 0.1175 - val_accuracy: 0.3705\n",
      "Epoch 635/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0656 - accuracy: 0.3219 - val_loss: 0.1178 - val_accuracy: 0.3579\n",
      "Epoch 636/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0657 - accuracy: 0.3219 - val_loss: 0.1197 - val_accuracy: 0.3615\n",
      "Epoch 637/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0654 - accuracy: 0.3195 - val_loss: 0.1240 - val_accuracy: 0.3795\n",
      "Epoch 638/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0656 - accuracy: 0.3159 - val_loss: 0.1156 - val_accuracy: 0.3525\n",
      "Epoch 639/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0651 - accuracy: 0.3231 - val_loss: 0.1169 - val_accuracy: 0.3579\n",
      "Epoch 640/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0651 - accuracy: 0.3279 - val_loss: 0.1149 - val_accuracy: 0.3417\n",
      "Epoch 641/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0650 - accuracy: 0.3255 - val_loss: 0.1191 - val_accuracy: 0.3669\n",
      "Epoch 642/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0652 - accuracy: 0.3267 - val_loss: 0.1182 - val_accuracy: 0.3669\n",
      "Epoch 643/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0651 - accuracy: 0.3273 - val_loss: 0.1372 - val_accuracy: 0.3058\n",
      "Epoch 644/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0659 - accuracy: 0.3219 - val_loss: 0.1204 - val_accuracy: 0.3309\n",
      "Epoch 645/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0651 - accuracy: 0.3171 - val_loss: 0.1184 - val_accuracy: 0.3669\n",
      "Epoch 646/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0649 - accuracy: 0.3315 - val_loss: 0.1173 - val_accuracy: 0.3399\n",
      "Epoch 647/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0647 - accuracy: 0.3267 - val_loss: 0.1201 - val_accuracy: 0.3489\n",
      "Epoch 648/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0647 - accuracy: 0.3327 - val_loss: 0.1171 - val_accuracy: 0.3309\n",
      "Epoch 649/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0649 - accuracy: 0.3267 - val_loss: 0.1191 - val_accuracy: 0.3597\n",
      "Epoch 650/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0646 - accuracy: 0.3291 - val_loss: 0.1184 - val_accuracy: 0.3489\n",
      "Epoch 651/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0645 - accuracy: 0.3207 - val_loss: 0.1177 - val_accuracy: 0.3399\n",
      "Epoch 652/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0644 - accuracy: 0.3279 - val_loss: 0.1155 - val_accuracy: 0.3417\n",
      "Epoch 653/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0644 - accuracy: 0.3291 - val_loss: 0.1291 - val_accuracy: 0.3219\n",
      "Epoch 654/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0648 - accuracy: 0.3261 - val_loss: 0.1195 - val_accuracy: 0.3777\n",
      "Epoch 655/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0645 - accuracy: 0.3249 - val_loss: 0.1178 - val_accuracy: 0.3381\n",
      "Epoch 656/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0642 - accuracy: 0.3171 - val_loss: 0.1197 - val_accuracy: 0.3183\n",
      "Epoch 657/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0647 - accuracy: 0.3261 - val_loss: 0.1174 - val_accuracy: 0.3561\n",
      "Epoch 658/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0643 - accuracy: 0.3165 - val_loss: 0.1177 - val_accuracy: 0.3489\n",
      "Epoch 659/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0641 - accuracy: 0.3045 - val_loss: 0.1218 - val_accuracy: 0.3129\n",
      "Epoch 660/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0647 - accuracy: 0.3159 - val_loss: 0.1198 - val_accuracy: 0.3615\n",
      "Epoch 661/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0640 - accuracy: 0.3279 - val_loss: 0.1152 - val_accuracy: 0.3453\n",
      "Epoch 662/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0638 - accuracy: 0.3195 - val_loss: 0.1198 - val_accuracy: 0.3885\n",
      "Epoch 663/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0640 - accuracy: 0.3315 - val_loss: 0.1144 - val_accuracy: 0.3525\n",
      "Epoch 664/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0637 - accuracy: 0.3267 - val_loss: 0.1165 - val_accuracy: 0.3669\n",
      "Epoch 665/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0639 - accuracy: 0.3147 - val_loss: 0.1180 - val_accuracy: 0.3615\n",
      "Epoch 666/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0639 - accuracy: 0.3231 - val_loss: 0.1181 - val_accuracy: 0.3453\n",
      "Epoch 667/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0639 - accuracy: 0.3183 - val_loss: 0.1173 - val_accuracy: 0.3597\n",
      "Epoch 668/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0637 - accuracy: 0.3267 - val_loss: 0.1148 - val_accuracy: 0.3885\n",
      "Epoch 669/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0637 - accuracy: 0.3201 - val_loss: 0.1166 - val_accuracy: 0.3723\n",
      "Epoch 670/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0634 - accuracy: 0.3237 - val_loss: 0.1344 - val_accuracy: 0.2230\n",
      "Epoch 671/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0651 - accuracy: 0.3177 - val_loss: 0.1257 - val_accuracy: 0.3579\n",
      "Epoch 672/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0639 - accuracy: 0.3219 - val_loss: 0.1165 - val_accuracy: 0.3435\n",
      "Epoch 673/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0633 - accuracy: 0.3183 - val_loss: 0.1167 - val_accuracy: 0.3669\n",
      "Epoch 674/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0633 - accuracy: 0.3249 - val_loss: 0.1149 - val_accuracy: 0.3543\n",
      "Epoch 675/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0633 - accuracy: 0.3225 - val_loss: 0.1368 - val_accuracy: 0.2788\n",
      "Epoch 676/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0643 - accuracy: 0.3249 - val_loss: 0.1225 - val_accuracy: 0.3291\n",
      "Epoch 677/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0638 - accuracy: 0.3171 - val_loss: 0.1148 - val_accuracy: 0.3669\n",
      "Epoch 678/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0629 - accuracy: 0.3231 - val_loss: 0.1149 - val_accuracy: 0.3507\n",
      "Epoch 679/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0631 - accuracy: 0.3273 - val_loss: 0.1170 - val_accuracy: 0.3759\n",
      "Epoch 680/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0631 - accuracy: 0.3249 - val_loss: 0.1407 - val_accuracy: 0.3867\n",
      "Epoch 681/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0643 - accuracy: 0.3243 - val_loss: 0.1138 - val_accuracy: 0.3381\n",
      "Epoch 682/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0628 - accuracy: 0.3279 - val_loss: 0.1176 - val_accuracy: 0.3633\n",
      "Epoch 683/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0630 - accuracy: 0.3237 - val_loss: 0.1162 - val_accuracy: 0.3579\n",
      "Epoch 684/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0627 - accuracy: 0.3273 - val_loss: 0.1155 - val_accuracy: 0.3543\n",
      "Epoch 685/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0626 - accuracy: 0.3297 - val_loss: 0.1193 - val_accuracy: 0.3633\n",
      "Epoch 686/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0628 - accuracy: 0.3273 - val_loss: 0.1174 - val_accuracy: 0.3471\n",
      "Epoch 687/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0627 - accuracy: 0.3231 - val_loss: 0.1151 - val_accuracy: 0.3237\n",
      "Epoch 688/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0627 - accuracy: 0.3201 - val_loss: 0.1161 - val_accuracy: 0.3291\n",
      "Epoch 689/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0627 - accuracy: 0.3177 - val_loss: 0.1185 - val_accuracy: 0.3273\n",
      "Epoch 690/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0632 - accuracy: 0.3243 - val_loss: 0.1142 - val_accuracy: 0.3165\n",
      "Epoch 691/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0625 - accuracy: 0.3177 - val_loss: 0.1125 - val_accuracy: 0.3579\n",
      "Epoch 692/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0625 - accuracy: 0.3207 - val_loss: 0.1230 - val_accuracy: 0.3309\n",
      "Epoch 693/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0626 - accuracy: 0.3225 - val_loss: 0.1161 - val_accuracy: 0.3561\n",
      "Epoch 694/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0623 - accuracy: 0.3261 - val_loss: 0.1195 - val_accuracy: 0.3543\n",
      "Epoch 695/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0624 - accuracy: 0.3177 - val_loss: 0.1144 - val_accuracy: 0.3561\n",
      "Epoch 696/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0620 - accuracy: 0.3243 - val_loss: 0.1286 - val_accuracy: 0.2932\n",
      "Epoch 697/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0630 - accuracy: 0.3303 - val_loss: 0.1159 - val_accuracy: 0.3669\n",
      "Epoch 698/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0619 - accuracy: 0.3231 - val_loss: 0.1197 - val_accuracy: 0.3417\n",
      "Epoch 699/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0624 - accuracy: 0.3297 - val_loss: 0.1184 - val_accuracy: 0.3471\n",
      "Epoch 700/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0622 - accuracy: 0.3303 - val_loss: 0.1216 - val_accuracy: 0.3417\n",
      "Epoch 701/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0623 - accuracy: 0.3177 - val_loss: 0.1153 - val_accuracy: 0.3597\n",
      "Epoch 702/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0618 - accuracy: 0.3303 - val_loss: 0.1187 - val_accuracy: 0.3633\n",
      "Epoch 703/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0619 - accuracy: 0.3195 - val_loss: 0.1187 - val_accuracy: 0.3561\n",
      "Epoch 704/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0620 - accuracy: 0.3183 - val_loss: 0.1198 - val_accuracy: 0.3471\n",
      "Epoch 705/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0620 - accuracy: 0.3309 - val_loss: 0.1246 - val_accuracy: 0.3112\n",
      "Epoch 706/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0628 - accuracy: 0.3279 - val_loss: 0.1388 - val_accuracy: 0.3219\n",
      "Epoch 707/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0627 - accuracy: 0.3273 - val_loss: 0.1184 - val_accuracy: 0.3489\n",
      "Epoch 708/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0616 - accuracy: 0.3267 - val_loss: 0.1175 - val_accuracy: 0.3543\n",
      "Epoch 709/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0617 - accuracy: 0.3237 - val_loss: 0.1170 - val_accuracy: 0.3669\n",
      "Epoch 710/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0616 - accuracy: 0.3285 - val_loss: 0.1224 - val_accuracy: 0.3615\n",
      "Epoch 711/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0619 - accuracy: 0.3267 - val_loss: 0.1140 - val_accuracy: 0.3741\n",
      "Epoch 712/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0614 - accuracy: 0.3345 - val_loss: 0.1191 - val_accuracy: 0.3435\n",
      "Epoch 713/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0614 - accuracy: 0.3333 - val_loss: 0.1240 - val_accuracy: 0.2680\n",
      "Epoch 714/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0622 - accuracy: 0.3213 - val_loss: 0.1119 - val_accuracy: 0.3633\n",
      "Epoch 715/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0612 - accuracy: 0.3291 - val_loss: 0.1221 - val_accuracy: 0.3543\n",
      "Epoch 716/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0614 - accuracy: 0.3291 - val_loss: 0.1159 - val_accuracy: 0.3489\n",
      "Epoch 717/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0613 - accuracy: 0.3273 - val_loss: 0.1168 - val_accuracy: 0.3759\n",
      "Epoch 718/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0611 - accuracy: 0.3303 - val_loss: 0.1144 - val_accuracy: 0.3633\n",
      "Epoch 719/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0609 - accuracy: 0.3279 - val_loss: 0.1308 - val_accuracy: 0.3237\n",
      "Epoch 720/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0615 - accuracy: 0.3297 - val_loss: 0.1165 - val_accuracy: 0.3579\n",
      "Epoch 721/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0608 - accuracy: 0.3309 - val_loss: 0.1134 - val_accuracy: 0.3651\n",
      "Epoch 722/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0608 - accuracy: 0.3309 - val_loss: 0.1163 - val_accuracy: 0.3471\n",
      "Epoch 723/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0611 - accuracy: 0.3255 - val_loss: 0.1157 - val_accuracy: 0.3651\n",
      "Epoch 724/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0607 - accuracy: 0.3309 - val_loss: 0.1146 - val_accuracy: 0.3687\n",
      "Epoch 725/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0607 - accuracy: 0.3297 - val_loss: 0.1179 - val_accuracy: 0.3669\n",
      "Epoch 726/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0608 - accuracy: 0.3303 - val_loss: 0.1141 - val_accuracy: 0.3471\n",
      "Epoch 727/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0609 - accuracy: 0.3303 - val_loss: 0.1167 - val_accuracy: 0.3597\n",
      "Epoch 728/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0608 - accuracy: 0.3243 - val_loss: 0.1139 - val_accuracy: 0.3633\n",
      "Epoch 729/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0604 - accuracy: 0.3291 - val_loss: 0.1269 - val_accuracy: 0.3183\n",
      "Epoch 730/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0610 - accuracy: 0.3357 - val_loss: 0.1180 - val_accuracy: 0.3507\n",
      "Epoch 731/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0605 - accuracy: 0.3297 - val_loss: 0.1163 - val_accuracy: 0.3777\n",
      "Epoch 732/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0608 - accuracy: 0.3255 - val_loss: 0.1203 - val_accuracy: 0.3723\n",
      "Epoch 733/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0608 - accuracy: 0.3231 - val_loss: 0.1144 - val_accuracy: 0.3363\n",
      "Epoch 734/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0603 - accuracy: 0.3285 - val_loss: 0.1150 - val_accuracy: 0.2986\n",
      "Epoch 735/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0608 - accuracy: 0.3237 - val_loss: 0.1152 - val_accuracy: 0.3651\n",
      "Epoch 736/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0604 - accuracy: 0.3285 - val_loss: 0.1149 - val_accuracy: 0.3291\n",
      "Epoch 737/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0602 - accuracy: 0.3207 - val_loss: 0.1127 - val_accuracy: 0.3561\n",
      "Epoch 738/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0601 - accuracy: 0.3303 - val_loss: 0.1154 - val_accuracy: 0.3705\n",
      "Epoch 739/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0603 - accuracy: 0.3207 - val_loss: 0.1355 - val_accuracy: 0.2896\n",
      "Epoch 740/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0615 - accuracy: 0.3231 - val_loss: 0.1163 - val_accuracy: 0.3525\n",
      "Epoch 741/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0601 - accuracy: 0.3297 - val_loss: 0.1131 - val_accuracy: 0.3381\n",
      "Epoch 742/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0600 - accuracy: 0.3309 - val_loss: 0.1632 - val_accuracy: 0.3237\n",
      "Epoch 743/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0613 - accuracy: 0.3213 - val_loss: 0.1215 - val_accuracy: 0.3759\n",
      "Epoch 744/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0605 - accuracy: 0.3369 - val_loss: 0.1164 - val_accuracy: 0.3777\n",
      "Epoch 745/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0600 - accuracy: 0.3315 - val_loss: 0.1183 - val_accuracy: 0.3687\n",
      "Epoch 746/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0601 - accuracy: 0.3279 - val_loss: 0.1139 - val_accuracy: 0.3489\n",
      "Epoch 747/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0598 - accuracy: 0.3303 - val_loss: 0.1155 - val_accuracy: 0.3327\n",
      "Epoch 748/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0597 - accuracy: 0.3273 - val_loss: 0.1205 - val_accuracy: 0.3633\n",
      "Epoch 749/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0600 - accuracy: 0.3339 - val_loss: 0.1120 - val_accuracy: 0.3597\n",
      "Epoch 750/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0594 - accuracy: 0.3267 - val_loss: 0.1348 - val_accuracy: 0.4065\n",
      "Epoch 751/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0603 - accuracy: 0.3315 - val_loss: 0.1153 - val_accuracy: 0.3309\n",
      "Epoch 752/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0595 - accuracy: 0.3183 - val_loss: 0.1143 - val_accuracy: 0.3453\n",
      "Epoch 753/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0594 - accuracy: 0.3363 - val_loss: 0.1145 - val_accuracy: 0.3597\n",
      "Epoch 754/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0592 - accuracy: 0.3237 - val_loss: 0.1153 - val_accuracy: 0.3579\n",
      "Epoch 755/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0592 - accuracy: 0.3249 - val_loss: 0.1136 - val_accuracy: 0.3471\n",
      "Epoch 756/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0595 - accuracy: 0.3225 - val_loss: 0.1135 - val_accuracy: 0.3471\n",
      "Epoch 757/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0595 - accuracy: 0.3315 - val_loss: 0.1144 - val_accuracy: 0.3669\n",
      "Epoch 758/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0592 - accuracy: 0.3297 - val_loss: 0.1122 - val_accuracy: 0.3687\n",
      "Epoch 759/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0593 - accuracy: 0.3267 - val_loss: 0.1181 - val_accuracy: 0.3453\n",
      "Epoch 760/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0594 - accuracy: 0.3255 - val_loss: 0.1144 - val_accuracy: 0.3741\n",
      "Epoch 761/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0590 - accuracy: 0.3267 - val_loss: 0.1131 - val_accuracy: 0.3507\n",
      "Epoch 762/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0589 - accuracy: 0.3291 - val_loss: 0.1183 - val_accuracy: 0.3201\n",
      "Epoch 763/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0592 - accuracy: 0.3303 - val_loss: 0.1287 - val_accuracy: 0.2464\n",
      "Epoch 764/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0598 - accuracy: 0.3291 - val_loss: 0.1172 - val_accuracy: 0.3561\n",
      "Epoch 765/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0588 - accuracy: 0.3243 - val_loss: 0.1170 - val_accuracy: 0.3489\n",
      "Epoch 766/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0589 - accuracy: 0.3261 - val_loss: 0.1276 - val_accuracy: 0.3453\n",
      "Epoch 767/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0596 - accuracy: 0.3363 - val_loss: 0.1162 - val_accuracy: 0.3669\n",
      "Epoch 768/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0590 - accuracy: 0.3267 - val_loss: 0.1137 - val_accuracy: 0.3291\n",
      "Epoch 769/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0589 - accuracy: 0.3357 - val_loss: 0.1422 - val_accuracy: 0.3201\n",
      "Epoch 770/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0600 - accuracy: 0.3273 - val_loss: 0.1132 - val_accuracy: 0.3633\n",
      "Epoch 771/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0586 - accuracy: 0.3303 - val_loss: 0.1133 - val_accuracy: 0.3417\n",
      "Epoch 772/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0583 - accuracy: 0.3231 - val_loss: 0.1127 - val_accuracy: 0.3489\n",
      "Epoch 773/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0586 - accuracy: 0.3285 - val_loss: 0.1136 - val_accuracy: 0.3471\n",
      "Epoch 774/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0584 - accuracy: 0.3327 - val_loss: 0.1167 - val_accuracy: 0.3543\n",
      "Epoch 775/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0586 - accuracy: 0.3279 - val_loss: 0.1159 - val_accuracy: 0.3759\n",
      "Epoch 776/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0583 - accuracy: 0.3315 - val_loss: 0.1151 - val_accuracy: 0.3417\n",
      "Epoch 777/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0586 - accuracy: 0.3243 - val_loss: 0.1514 - val_accuracy: 0.4083\n",
      "Epoch 778/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0597 - accuracy: 0.3429 - val_loss: 0.1158 - val_accuracy: 0.3381\n",
      "Epoch 779/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0584 - accuracy: 0.3225 - val_loss: 0.1502 - val_accuracy: 0.3094\n",
      "Epoch 780/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0597 - accuracy: 0.3231 - val_loss: 0.1143 - val_accuracy: 0.3705\n",
      "Epoch 781/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0582 - accuracy: 0.3321 - val_loss: 0.1168 - val_accuracy: 0.3723\n",
      "Epoch 782/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0583 - accuracy: 0.3339 - val_loss: 0.1211 - val_accuracy: 0.3795\n",
      "Epoch 783/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0585 - accuracy: 0.3249 - val_loss: 0.1338 - val_accuracy: 0.2734\n",
      "Epoch 784/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0599 - accuracy: 0.3213 - val_loss: 0.1145 - val_accuracy: 0.3543\n",
      "Epoch 785/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0582 - accuracy: 0.3351 - val_loss: 0.1159 - val_accuracy: 0.3633\n",
      "Epoch 786/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0580 - accuracy: 0.3189 - val_loss: 0.1141 - val_accuracy: 0.3669\n",
      "Epoch 787/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0581 - accuracy: 0.3273 - val_loss: 0.1149 - val_accuracy: 0.3687\n",
      "Epoch 788/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0579 - accuracy: 0.3291 - val_loss: 0.1146 - val_accuracy: 0.3615\n",
      "Epoch 789/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0579 - accuracy: 0.3255 - val_loss: 0.1134 - val_accuracy: 0.3543\n",
      "Epoch 790/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0577 - accuracy: 0.3267 - val_loss: 0.1127 - val_accuracy: 0.3543\n",
      "Epoch 791/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0575 - accuracy: 0.3279 - val_loss: 0.1410 - val_accuracy: 0.3633\n",
      "Epoch 792/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0593 - accuracy: 0.3243 - val_loss: 0.1175 - val_accuracy: 0.3309\n",
      "Epoch 793/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0579 - accuracy: 0.3357 - val_loss: 0.1137 - val_accuracy: 0.3579\n",
      "Epoch 794/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0574 - accuracy: 0.3327 - val_loss: 0.1134 - val_accuracy: 0.3399\n",
      "Epoch 795/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0575 - accuracy: 0.3345 - val_loss: 0.1117 - val_accuracy: 0.3291\n",
      "Epoch 796/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0574 - accuracy: 0.3351 - val_loss: 0.1109 - val_accuracy: 0.3615\n",
      "Epoch 797/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0573 - accuracy: 0.3303 - val_loss: 0.1151 - val_accuracy: 0.3561\n",
      "Epoch 798/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0574 - accuracy: 0.3225 - val_loss: 0.1141 - val_accuracy: 0.3723\n",
      "Epoch 799/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0575 - accuracy: 0.3285 - val_loss: 0.1125 - val_accuracy: 0.3435\n",
      "Epoch 800/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0575 - accuracy: 0.3351 - val_loss: 0.1143 - val_accuracy: 0.3022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-32-f01cf9afd9ce>:131: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "  tprs.append(interp(mean_fpr, fpr, tpr))\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:806: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Epoch 1/800\n",
      "53/53 [==============================] - 1s 5ms/step - loss: 0.6672 - accuracy: 0.0156 - val_loss: 0.6329 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.4964 - accuracy: 0.0066 - val_loss: 0.3117 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.2044 - accuracy: 0.0078 - val_loss: 0.2534 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1881 - accuracy: 0.0198 - val_loss: 0.2506 - val_accuracy: 0.0360\n",
      "Epoch 5/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1869 - accuracy: 0.0282 - val_loss: 0.2502 - val_accuracy: 0.0396\n",
      "Epoch 6/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1864 - accuracy: 0.0270 - val_loss: 0.2500 - val_accuracy: 0.0594\n",
      "Epoch 7/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1862 - accuracy: 0.0330 - val_loss: 0.2500 - val_accuracy: 0.0360\n",
      "Epoch 8/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1860 - accuracy: 0.0336 - val_loss: 0.2497 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1860 - accuracy: 0.0090 - val_loss: 0.2501 - val_accuracy: 0.0288\n",
      "Epoch 10/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1859 - accuracy: 0.0240 - val_loss: 0.2498 - val_accuracy: 0.0198\n",
      "Epoch 11/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1858 - accuracy: 0.0300 - val_loss: 0.2494 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1857 - accuracy: 0.0366 - val_loss: 0.2493 - val_accuracy: 0.0216\n",
      "Epoch 13/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1857 - accuracy: 0.0168 - val_loss: 0.2491 - val_accuracy: 0.0522\n",
      "Epoch 14/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1856 - accuracy: 0.0492 - val_loss: 0.2488 - val_accuracy: 0.0234\n",
      "Epoch 15/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1856 - accuracy: 0.0330 - val_loss: 0.2481 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1855 - accuracy: 0.0204 - val_loss: 0.2478 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1855 - accuracy: 0.0348 - val_loss: 0.2484 - val_accuracy: 0.0288\n",
      "Epoch 18/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1854 - accuracy: 0.0264 - val_loss: 0.2483 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1854 - accuracy: 0.0240 - val_loss: 0.2488 - val_accuracy: 0.0396\n",
      "Epoch 20/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1853 - accuracy: 0.0426 - val_loss: 0.2487 - val_accuracy: 0.0486\n",
      "Epoch 21/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1853 - accuracy: 0.0390 - val_loss: 0.2482 - val_accuracy: 0.0234\n",
      "Epoch 22/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1852 - accuracy: 0.0252 - val_loss: 0.2483 - val_accuracy: 0.0665\n",
      "Epoch 23/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1852 - accuracy: 0.0348 - val_loss: 0.2483 - val_accuracy: 0.0396\n",
      "Epoch 24/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1851 - accuracy: 0.0456 - val_loss: 0.2481 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1851 - accuracy: 0.0144 - val_loss: 0.2484 - val_accuracy: 0.0504\n",
      "Epoch 26/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1850 - accuracy: 0.0414 - val_loss: 0.2483 - val_accuracy: 0.0540\n",
      "Epoch 27/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1850 - accuracy: 0.0468 - val_loss: 0.2478 - val_accuracy: 0.0252\n",
      "Epoch 28/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1849 - accuracy: 0.0486 - val_loss: 0.2475 - val_accuracy: 0.0504\n",
      "Epoch 29/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1849 - accuracy: 0.0547 - val_loss: 0.2477 - val_accuracy: 0.0342\n",
      "Epoch 30/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1849 - accuracy: 0.0354 - val_loss: 0.2472 - val_accuracy: 0.0558\n",
      "Epoch 31/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1847 - accuracy: 0.0559 - val_loss: 0.2477 - val_accuracy: 0.0198\n",
      "Epoch 32/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1847 - accuracy: 0.0384 - val_loss: 0.2477 - val_accuracy: 0.0504\n",
      "Epoch 33/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1846 - accuracy: 0.0432 - val_loss: 0.2472 - val_accuracy: 0.0288\n",
      "Epoch 34/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1845 - accuracy: 0.0312 - val_loss: 0.2472 - val_accuracy: 0.0486\n",
      "Epoch 35/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1845 - accuracy: 0.0637 - val_loss: 0.2478 - val_accuracy: 0.0791\n",
      "Epoch 36/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1844 - accuracy: 0.0631 - val_loss: 0.2473 - val_accuracy: 0.0360\n",
      "Epoch 37/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1844 - accuracy: 0.0468 - val_loss: 0.2466 - val_accuracy: 0.0540\n",
      "Epoch 38/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1843 - accuracy: 0.0486 - val_loss: 0.2469 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1843 - accuracy: 0.0294 - val_loss: 0.2467 - val_accuracy: 0.0432\n",
      "Epoch 40/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1842 - accuracy: 0.0498 - val_loss: 0.2468 - val_accuracy: 0.0540\n",
      "Epoch 41/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1842 - accuracy: 0.0529 - val_loss: 0.2465 - val_accuracy: 0.0396\n",
      "Epoch 42/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1841 - accuracy: 0.0505 - val_loss: 0.2468 - val_accuracy: 0.0360\n",
      "Epoch 43/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1840 - accuracy: 0.0571 - val_loss: 0.2468 - val_accuracy: 0.0827\n",
      "Epoch 44/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1840 - accuracy: 0.0601 - val_loss: 0.2467 - val_accuracy: 0.0234\n",
      "Epoch 45/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1839 - accuracy: 0.0378 - val_loss: 0.2463 - val_accuracy: 0.0486\n",
      "Epoch 46/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1839 - accuracy: 0.0432 - val_loss: 0.2467 - val_accuracy: 0.0018\n",
      "Epoch 47/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1838 - accuracy: 0.0486 - val_loss: 0.2465 - val_accuracy: 0.0755\n",
      "Epoch 48/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1837 - accuracy: 0.0643 - val_loss: 0.2467 - val_accuracy: 0.0468\n",
      "Epoch 49/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1837 - accuracy: 0.0529 - val_loss: 0.2463 - val_accuracy: 0.0827\n",
      "Epoch 50/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1836 - accuracy: 0.0793 - val_loss: 0.2460 - val_accuracy: 0.0522\n",
      "Epoch 51/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1835 - accuracy: 0.0571 - val_loss: 0.2459 - val_accuracy: 0.0378\n",
      "Epoch 52/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1835 - accuracy: 0.0565 - val_loss: 0.2459 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1834 - accuracy: 0.0396 - val_loss: 0.2458 - val_accuracy: 0.0414\n",
      "Epoch 54/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1833 - accuracy: 0.0486 - val_loss: 0.2453 - val_accuracy: 0.0378\n",
      "Epoch 55/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1832 - accuracy: 0.0438 - val_loss: 0.2458 - val_accuracy: 0.0504\n",
      "Epoch 56/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1831 - accuracy: 0.0619 - val_loss: 0.2453 - val_accuracy: 0.0252\n",
      "Epoch 57/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1830 - accuracy: 0.0366 - val_loss: 0.2446 - val_accuracy: 0.0144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1829 - accuracy: 0.0559 - val_loss: 0.2443 - val_accuracy: 0.0306\n",
      "Epoch 59/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1829 - accuracy: 0.0721 - val_loss: 0.2452 - val_accuracy: 0.0486\n",
      "Epoch 60/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1828 - accuracy: 0.0781 - val_loss: 0.2452 - val_accuracy: 0.0773\n",
      "Epoch 61/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1827 - accuracy: 0.0859 - val_loss: 0.2446 - val_accuracy: 0.0683\n",
      "Epoch 62/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1826 - accuracy: 0.0811 - val_loss: 0.2445 - val_accuracy: 0.0162\n",
      "Epoch 63/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1824 - accuracy: 0.0450 - val_loss: 0.2440 - val_accuracy: 0.0324\n",
      "Epoch 64/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1824 - accuracy: 0.0456 - val_loss: 0.2442 - val_accuracy: 0.0612\n",
      "Epoch 65/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1823 - accuracy: 0.0643 - val_loss: 0.2438 - val_accuracy: 0.1421\n",
      "Epoch 66/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1822 - accuracy: 0.1141 - val_loss: 0.2438 - val_accuracy: 0.0414\n",
      "Epoch 67/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1821 - accuracy: 0.0438 - val_loss: 0.2440 - val_accuracy: 0.0594\n",
      "Epoch 68/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1819 - accuracy: 0.0541 - val_loss: 0.2432 - val_accuracy: 0.0629\n",
      "Epoch 69/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1818 - accuracy: 0.1105 - val_loss: 0.2434 - val_accuracy: 0.0719\n",
      "Epoch 70/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1817 - accuracy: 0.0583 - val_loss: 0.2431 - val_accuracy: 0.0504\n",
      "Epoch 71/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1816 - accuracy: 0.0733 - val_loss: 0.2432 - val_accuracy: 0.0468\n",
      "Epoch 72/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1815 - accuracy: 0.0691 - val_loss: 0.2432 - val_accuracy: 0.0647\n",
      "Epoch 73/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1813 - accuracy: 0.0829 - val_loss: 0.2429 - val_accuracy: 0.1169\n",
      "Epoch 74/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1811 - accuracy: 0.1117 - val_loss: 0.2427 - val_accuracy: 0.0683\n",
      "Epoch 75/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1810 - accuracy: 0.0715 - val_loss: 0.2427 - val_accuracy: 0.0809\n",
      "Epoch 76/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1808 - accuracy: 0.0727 - val_loss: 0.2424 - val_accuracy: 0.0629\n",
      "Epoch 77/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1806 - accuracy: 0.0763 - val_loss: 0.2414 - val_accuracy: 0.1421\n",
      "Epoch 78/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1805 - accuracy: 0.0937 - val_loss: 0.2415 - val_accuracy: 0.0468\n",
      "Epoch 79/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1803 - accuracy: 0.0595 - val_loss: 0.2417 - val_accuracy: 0.0486\n",
      "Epoch 80/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1801 - accuracy: 0.0583 - val_loss: 0.2411 - val_accuracy: 0.0540\n",
      "Epoch 81/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1799 - accuracy: 0.0931 - val_loss: 0.2410 - val_accuracy: 0.1385\n",
      "Epoch 82/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1797 - accuracy: 0.1051 - val_loss: 0.2410 - val_accuracy: 0.0971\n",
      "Epoch 83/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1795 - accuracy: 0.0955 - val_loss: 0.2404 - val_accuracy: 0.0899\n",
      "Epoch 84/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1793 - accuracy: 0.0829 - val_loss: 0.2396 - val_accuracy: 0.1277\n",
      "Epoch 85/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1791 - accuracy: 0.1063 - val_loss: 0.2392 - val_accuracy: 0.0935\n",
      "Epoch 86/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1788 - accuracy: 0.0955 - val_loss: 0.2396 - val_accuracy: 0.0683\n",
      "Epoch 87/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1787 - accuracy: 0.0781 - val_loss: 0.2395 - val_accuracy: 0.0701\n",
      "Epoch 88/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1784 - accuracy: 0.1015 - val_loss: 0.2386 - val_accuracy: 0.1115\n",
      "Epoch 89/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1782 - accuracy: 0.1081 - val_loss: 0.2382 - val_accuracy: 0.0683\n",
      "Epoch 90/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1780 - accuracy: 0.0943 - val_loss: 0.2380 - val_accuracy: 0.0629\n",
      "Epoch 91/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1776 - accuracy: 0.0829 - val_loss: 0.2381 - val_accuracy: 0.1367\n",
      "Epoch 92/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1774 - accuracy: 0.1105 - val_loss: 0.2376 - val_accuracy: 0.0863\n",
      "Epoch 93/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1771 - accuracy: 0.1033 - val_loss: 0.2370 - val_accuracy: 0.1349\n",
      "Epoch 94/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1768 - accuracy: 0.1225 - val_loss: 0.2368 - val_accuracy: 0.0881\n",
      "Epoch 95/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1765 - accuracy: 0.1099 - val_loss: 0.2361 - val_accuracy: 0.0989\n",
      "Epoch 96/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1762 - accuracy: 0.1183 - val_loss: 0.2361 - val_accuracy: 0.1043\n",
      "Epoch 97/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1759 - accuracy: 0.1069 - val_loss: 0.2355 - val_accuracy: 0.0917\n",
      "Epoch 98/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1756 - accuracy: 0.1087 - val_loss: 0.2350 - val_accuracy: 0.0953\n",
      "Epoch 99/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1752 - accuracy: 0.1195 - val_loss: 0.2344 - val_accuracy: 0.1043\n",
      "Epoch 100/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1750 - accuracy: 0.1087 - val_loss: 0.2344 - val_accuracy: 0.0935\n",
      "Epoch 101/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1746 - accuracy: 0.1177 - val_loss: 0.2335 - val_accuracy: 0.1295\n",
      "Epoch 102/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1743 - accuracy: 0.1339 - val_loss: 0.2331 - val_accuracy: 0.1043\n",
      "Epoch 103/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1739 - accuracy: 0.1225 - val_loss: 0.2329 - val_accuracy: 0.1403\n",
      "Epoch 104/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1735 - accuracy: 0.1333 - val_loss: 0.2321 - val_accuracy: 0.1151\n",
      "Epoch 105/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1732 - accuracy: 0.1045 - val_loss: 0.2318 - val_accuracy: 0.1493\n",
      "Epoch 106/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1728 - accuracy: 0.1399 - val_loss: 0.2310 - val_accuracy: 0.2122\n",
      "Epoch 107/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1724 - accuracy: 0.1508 - val_loss: 0.2308 - val_accuracy: 0.1241\n",
      "Epoch 108/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1720 - accuracy: 0.1231 - val_loss: 0.2301 - val_accuracy: 0.1259\n",
      "Epoch 109/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1716 - accuracy: 0.1345 - val_loss: 0.2297 - val_accuracy: 0.1601\n",
      "Epoch 110/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1712 - accuracy: 0.1477 - val_loss: 0.2290 - val_accuracy: 0.1619\n",
      "Epoch 111/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1708 - accuracy: 0.1417 - val_loss: 0.2293 - val_accuracy: 0.1313\n",
      "Epoch 112/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1705 - accuracy: 0.1568 - val_loss: 0.2282 - val_accuracy: 0.1655\n",
      "Epoch 113/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1700 - accuracy: 0.1411 - val_loss: 0.2277 - val_accuracy: 0.1529\n",
      "Epoch 114/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1697 - accuracy: 0.1628 - val_loss: 0.2263 - val_accuracy: 0.1727\n",
      "Epoch 115/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1692 - accuracy: 0.1574 - val_loss: 0.2259 - val_accuracy: 0.1475\n",
      "Epoch 116/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1689 - accuracy: 0.1453 - val_loss: 0.2263 - val_accuracy: 0.1187\n",
      "Epoch 117/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1685 - accuracy: 0.1453 - val_loss: 0.2250 - val_accuracy: 0.1511\n",
      "Epoch 118/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1681 - accuracy: 0.1459 - val_loss: 0.2245 - val_accuracy: 0.1133\n",
      "Epoch 119/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1677 - accuracy: 0.1279 - val_loss: 0.2245 - val_accuracy: 0.1871\n",
      "Epoch 120/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1673 - accuracy: 0.1610 - val_loss: 0.2231 - val_accuracy: 0.1924\n",
      "Epoch 121/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1669 - accuracy: 0.1778 - val_loss: 0.2225 - val_accuracy: 0.1547\n",
      "Epoch 122/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1665 - accuracy: 0.1616 - val_loss: 0.2215 - val_accuracy: 0.1942\n",
      "Epoch 123/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1661 - accuracy: 0.1568 - val_loss: 0.2213 - val_accuracy: 0.1529\n",
      "Epoch 124/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1657 - accuracy: 0.1664 - val_loss: 0.2199 - val_accuracy: 0.1727\n",
      "Epoch 125/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1653 - accuracy: 0.1682 - val_loss: 0.2211 - val_accuracy: 0.1421\n",
      "Epoch 126/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1649 - accuracy: 0.1604 - val_loss: 0.2198 - val_accuracy: 0.1871\n",
      "Epoch 127/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1645 - accuracy: 0.1586 - val_loss: 0.2198 - val_accuracy: 0.1691\n",
      "Epoch 128/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1640 - accuracy: 0.1658 - val_loss: 0.2198 - val_accuracy: 0.2050\n",
      "Epoch 129/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1636 - accuracy: 0.1610 - val_loss: 0.2187 - val_accuracy: 0.1745\n",
      "Epoch 130/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1632 - accuracy: 0.1706 - val_loss: 0.2187 - val_accuracy: 0.1888\n",
      "Epoch 131/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1628 - accuracy: 0.1664 - val_loss: 0.2164 - val_accuracy: 0.1978\n",
      "Epoch 132/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1623 - accuracy: 0.1682 - val_loss: 0.2167 - val_accuracy: 0.1763\n",
      "Epoch 133/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1618 - accuracy: 0.1784 - val_loss: 0.2158 - val_accuracy: 0.1781\n",
      "Epoch 134/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1613 - accuracy: 0.1700 - val_loss: 0.2148 - val_accuracy: 0.1673\n",
      "Epoch 135/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1609 - accuracy: 0.1562 - val_loss: 0.2149 - val_accuracy: 0.2014\n",
      "Epoch 136/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1604 - accuracy: 0.1658 - val_loss: 0.2144 - val_accuracy: 0.2050\n",
      "Epoch 137/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1599 - accuracy: 0.1814 - val_loss: 0.2133 - val_accuracy: 0.1781\n",
      "Epoch 138/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1595 - accuracy: 0.1742 - val_loss: 0.2120 - val_accuracy: 0.2338\n",
      "Epoch 139/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1590 - accuracy: 0.1796 - val_loss: 0.2116 - val_accuracy: 0.1727\n",
      "Epoch 140/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1585 - accuracy: 0.1574 - val_loss: 0.2118 - val_accuracy: 0.1619\n",
      "Epoch 141/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1581 - accuracy: 0.1586 - val_loss: 0.2116 - val_accuracy: 0.1727\n",
      "Epoch 142/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1576 - accuracy: 0.1790 - val_loss: 0.2099 - val_accuracy: 0.2032\n",
      "Epoch 143/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1570 - accuracy: 0.1706 - val_loss: 0.2090 - val_accuracy: 0.1745\n",
      "Epoch 144/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1565 - accuracy: 0.1640 - val_loss: 0.2094 - val_accuracy: 0.1906\n",
      "Epoch 145/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1561 - accuracy: 0.1826 - val_loss: 0.2081 - val_accuracy: 0.1853\n",
      "Epoch 146/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1556 - accuracy: 0.1802 - val_loss: 0.2071 - val_accuracy: 0.1835\n",
      "Epoch 147/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1551 - accuracy: 0.1796 - val_loss: 0.2066 - val_accuracy: 0.2122\n",
      "Epoch 148/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1546 - accuracy: 0.1850 - val_loss: 0.2079 - val_accuracy: 0.2104\n",
      "Epoch 149/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1542 - accuracy: 0.1958 - val_loss: 0.2052 - val_accuracy: 0.2176\n",
      "Epoch 150/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1536 - accuracy: 0.1856 - val_loss: 0.2042 - val_accuracy: 0.2158\n",
      "Epoch 151/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1531 - accuracy: 0.1904 - val_loss: 0.2029 - val_accuracy: 0.1996\n",
      "Epoch 152/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1527 - accuracy: 0.1898 - val_loss: 0.2034 - val_accuracy: 0.2230\n",
      "Epoch 153/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1522 - accuracy: 0.1940 - val_loss: 0.2025 - val_accuracy: 0.2302\n",
      "Epoch 154/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1517 - accuracy: 0.1808 - val_loss: 0.2020 - val_accuracy: 0.2212\n",
      "Epoch 155/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1512 - accuracy: 0.1952 - val_loss: 0.2019 - val_accuracy: 0.2230\n",
      "Epoch 156/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1508 - accuracy: 0.2000 - val_loss: 0.2014 - val_accuracy: 0.2464\n",
      "Epoch 157/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1504 - accuracy: 0.1952 - val_loss: 0.2003 - val_accuracy: 0.2158\n",
      "Epoch 158/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1499 - accuracy: 0.1934 - val_loss: 0.2009 - val_accuracy: 0.2482\n",
      "Epoch 159/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1494 - accuracy: 0.1880 - val_loss: 0.2003 - val_accuracy: 0.2482\n",
      "Epoch 160/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1490 - accuracy: 0.1868 - val_loss: 0.1984 - val_accuracy: 0.1781\n",
      "Epoch 161/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1486 - accuracy: 0.2024 - val_loss: 0.1988 - val_accuracy: 0.2338\n",
      "Epoch 162/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1482 - accuracy: 0.2096 - val_loss: 0.1985 - val_accuracy: 0.2356\n",
      "Epoch 163/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1476 - accuracy: 0.2114 - val_loss: 0.1977 - val_accuracy: 0.2212\n",
      "Epoch 164/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1472 - accuracy: 0.1988 - val_loss: 0.1960 - val_accuracy: 0.2230\n",
      "Epoch 165/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1467 - accuracy: 0.1946 - val_loss: 0.1961 - val_accuracy: 0.2428\n",
      "Epoch 166/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1463 - accuracy: 0.2120 - val_loss: 0.1954 - val_accuracy: 0.2122\n",
      "Epoch 167/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1458 - accuracy: 0.2084 - val_loss: 0.1948 - val_accuracy: 0.2644\n",
      "Epoch 168/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1454 - accuracy: 0.2018 - val_loss: 0.1948 - val_accuracy: 0.2176\n",
      "Epoch 169/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1450 - accuracy: 0.2084 - val_loss: 0.1941 - val_accuracy: 0.2446\n",
      "Epoch 170/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1446 - accuracy: 0.2114 - val_loss: 0.1949 - val_accuracy: 0.2590\n",
      "Epoch 171/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1443 - accuracy: 0.2018 - val_loss: 0.1928 - val_accuracy: 0.2338\n",
      "Epoch 172/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1438 - accuracy: 0.2048 - val_loss: 0.1926 - val_accuracy: 0.2446\n",
      "Epoch 173/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1434 - accuracy: 0.2126 - val_loss: 0.1979 - val_accuracy: 0.2446\n",
      "Epoch 174/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1432 - accuracy: 0.2120 - val_loss: 0.1914 - val_accuracy: 0.2608\n",
      "Epoch 175/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1426 - accuracy: 0.2192 - val_loss: 0.1913 - val_accuracy: 0.2284\n",
      "Epoch 176/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1422 - accuracy: 0.2120 - val_loss: 0.1918 - val_accuracy: 0.2662\n",
      "Epoch 177/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1417 - accuracy: 0.2204 - val_loss: 0.1902 - val_accuracy: 0.2320\n",
      "Epoch 178/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1414 - accuracy: 0.2102 - val_loss: 0.1901 - val_accuracy: 0.2302\n",
      "Epoch 179/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1409 - accuracy: 0.2138 - val_loss: 0.1889 - val_accuracy: 0.2518\n",
      "Epoch 180/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1406 - accuracy: 0.2186 - val_loss: 0.1894 - val_accuracy: 0.2518\n",
      "Epoch 181/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1402 - accuracy: 0.2222 - val_loss: 0.1881 - val_accuracy: 0.2662\n",
      "Epoch 182/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1398 - accuracy: 0.2084 - val_loss: 0.1896 - val_accuracy: 0.2446\n",
      "Epoch 183/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1394 - accuracy: 0.2198 - val_loss: 0.1874 - val_accuracy: 0.2680\n",
      "Epoch 184/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1390 - accuracy: 0.2288 - val_loss: 0.1878 - val_accuracy: 0.2392\n",
      "Epoch 185/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1387 - accuracy: 0.2198 - val_loss: 0.1864 - val_accuracy: 0.2770\n",
      "Epoch 186/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1382 - accuracy: 0.2252 - val_loss: 0.1859 - val_accuracy: 0.2626\n",
      "Epoch 187/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1379 - accuracy: 0.2240 - val_loss: 0.1868 - val_accuracy: 0.2248\n",
      "Epoch 188/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1375 - accuracy: 0.2150 - val_loss: 0.1847 - val_accuracy: 0.2626\n",
      "Epoch 189/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1370 - accuracy: 0.2222 - val_loss: 0.1888 - val_accuracy: 0.2284\n",
      "Epoch 190/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1368 - accuracy: 0.2294 - val_loss: 0.1848 - val_accuracy: 0.2662\n",
      "Epoch 191/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1362 - accuracy: 0.2294 - val_loss: 0.1837 - val_accuracy: 0.2482\n",
      "Epoch 192/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1358 - accuracy: 0.2324 - val_loss: 0.1832 - val_accuracy: 0.2770\n",
      "Epoch 193/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1355 - accuracy: 0.2258 - val_loss: 0.1822 - val_accuracy: 0.2554\n",
      "Epoch 194/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1351 - accuracy: 0.2186 - val_loss: 0.1822 - val_accuracy: 0.2644\n",
      "Epoch 195/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1347 - accuracy: 0.2306 - val_loss: 0.1827 - val_accuracy: 0.2500\n",
      "Epoch 196/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1343 - accuracy: 0.2288 - val_loss: 0.1811 - val_accuracy: 0.2752\n",
      "Epoch 197/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1338 - accuracy: 0.2342 - val_loss: 0.1813 - val_accuracy: 0.2554\n",
      "Epoch 198/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1335 - accuracy: 0.2300 - val_loss: 0.1812 - val_accuracy: 0.2572\n",
      "Epoch 199/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1331 - accuracy: 0.2306 - val_loss: 0.1812 - val_accuracy: 0.2230\n",
      "Epoch 200/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1327 - accuracy: 0.2378 - val_loss: 0.1808 - val_accuracy: 0.2392\n",
      "Epoch 201/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1323 - accuracy: 0.2294 - val_loss: 0.1792 - val_accuracy: 0.2662\n",
      "Epoch 202/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1319 - accuracy: 0.2354 - val_loss: 0.1858 - val_accuracy: 0.2662\n",
      "Epoch 203/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1319 - accuracy: 0.2294 - val_loss: 0.1801 - val_accuracy: 0.2392\n",
      "Epoch 204/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1311 - accuracy: 0.2324 - val_loss: 0.1789 - val_accuracy: 0.2716\n",
      "Epoch 205/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1307 - accuracy: 0.2342 - val_loss: 0.1786 - val_accuracy: 0.2698\n",
      "Epoch 206/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1303 - accuracy: 0.2378 - val_loss: 0.1772 - val_accuracy: 0.2680\n",
      "Epoch 207/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1298 - accuracy: 0.2270 - val_loss: 0.1762 - val_accuracy: 0.2968\n",
      "Epoch 208/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1293 - accuracy: 0.2462 - val_loss: 0.1765 - val_accuracy: 0.2482\n",
      "Epoch 209/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1292 - accuracy: 0.2432 - val_loss: 0.1766 - val_accuracy: 0.2824\n",
      "Epoch 210/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1286 - accuracy: 0.2450 - val_loss: 0.1774 - val_accuracy: 0.2518\n",
      "Epoch 211/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1284 - accuracy: 0.2360 - val_loss: 0.1750 - val_accuracy: 0.2716\n",
      "Epoch 212/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1278 - accuracy: 0.2324 - val_loss: 0.1746 - val_accuracy: 0.2608\n",
      "Epoch 213/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1275 - accuracy: 0.2390 - val_loss: 0.1735 - val_accuracy: 0.2842\n",
      "Epoch 214/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1270 - accuracy: 0.2414 - val_loss: 0.1746 - val_accuracy: 0.2752\n",
      "Epoch 215/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1267 - accuracy: 0.2456 - val_loss: 0.1730 - val_accuracy: 0.3112\n",
      "Epoch 216/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1262 - accuracy: 0.2523 - val_loss: 0.1792 - val_accuracy: 0.2500\n",
      "Epoch 217/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1262 - accuracy: 0.2426 - val_loss: 0.1721 - val_accuracy: 0.2878\n",
      "Epoch 218/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1255 - accuracy: 0.2408 - val_loss: 0.1729 - val_accuracy: 0.2770\n",
      "Epoch 219/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1251 - accuracy: 0.2456 - val_loss: 0.1719 - val_accuracy: 0.2896\n",
      "Epoch 220/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1247 - accuracy: 0.2486 - val_loss: 0.1789 - val_accuracy: 0.2986\n",
      "Epoch 221/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1248 - accuracy: 0.2480 - val_loss: 0.1720 - val_accuracy: 0.2914\n",
      "Epoch 222/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1241 - accuracy: 0.2517 - val_loss: 0.1701 - val_accuracy: 0.2374\n",
      "Epoch 223/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1237 - accuracy: 0.2541 - val_loss: 0.1700 - val_accuracy: 0.2806\n",
      "Epoch 224/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1234 - accuracy: 0.2432 - val_loss: 0.1705 - val_accuracy: 0.3147\n",
      "Epoch 225/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1230 - accuracy: 0.2474 - val_loss: 0.1690 - val_accuracy: 0.2896\n",
      "Epoch 226/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1226 - accuracy: 0.2529 - val_loss: 0.1683 - val_accuracy: 0.2680\n",
      "Epoch 227/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1222 - accuracy: 0.2492 - val_loss: 0.1704 - val_accuracy: 0.2770\n",
      "Epoch 228/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1219 - accuracy: 0.2529 - val_loss: 0.1686 - val_accuracy: 0.3219\n",
      "Epoch 229/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1217 - accuracy: 0.2517 - val_loss: 0.1679 - val_accuracy: 0.2860\n",
      "Epoch 230/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1212 - accuracy: 0.2492 - val_loss: 0.1681 - val_accuracy: 0.2860\n",
      "Epoch 231/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1210 - accuracy: 0.2511 - val_loss: 0.1669 - val_accuracy: 0.3129\n",
      "Epoch 232/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1207 - accuracy: 0.2541 - val_loss: 0.1675 - val_accuracy: 0.3147\n",
      "Epoch 233/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1204 - accuracy: 0.2571 - val_loss: 0.1682 - val_accuracy: 0.3040\n",
      "Epoch 234/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1202 - accuracy: 0.2583 - val_loss: 0.1675 - val_accuracy: 0.2788\n",
      "Epoch 235/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1198 - accuracy: 0.2535 - val_loss: 0.1681 - val_accuracy: 0.3076\n",
      "Epoch 236/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1196 - accuracy: 0.2601 - val_loss: 0.1671 - val_accuracy: 0.2770\n",
      "Epoch 237/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1194 - accuracy: 0.2498 - val_loss: 0.1679 - val_accuracy: 0.3147\n",
      "Epoch 238/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1189 - accuracy: 0.2649 - val_loss: 0.1750 - val_accuracy: 0.2536\n",
      "Epoch 239/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1190 - accuracy: 0.2607 - val_loss: 0.1667 - val_accuracy: 0.3327\n",
      "Epoch 240/800\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1184 - accuracy: 0.2655 - val_loss: 0.1644 - val_accuracy: 0.3183\n",
      "Epoch 241/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1180 - accuracy: 0.2619 - val_loss: 0.1659 - val_accuracy: 0.3309\n",
      "Epoch 242/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1179 - accuracy: 0.2553 - val_loss: 0.1659 - val_accuracy: 0.3165\n",
      "Epoch 243/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1175 - accuracy: 0.2553 - val_loss: 0.1677 - val_accuracy: 0.3291\n",
      "Epoch 244/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1176 - accuracy: 0.2625 - val_loss: 0.1642 - val_accuracy: 0.2986\n",
      "Epoch 245/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1171 - accuracy: 0.2547 - val_loss: 0.1702 - val_accuracy: 0.3831\n",
      "Epoch 246/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1170 - accuracy: 0.2691 - val_loss: 0.1630 - val_accuracy: 0.3219\n",
      "Epoch 247/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1165 - accuracy: 0.2607 - val_loss: 0.1622 - val_accuracy: 0.3094\n",
      "Epoch 248/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1163 - accuracy: 0.2607 - val_loss: 0.1627 - val_accuracy: 0.2716\n",
      "Epoch 249/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1160 - accuracy: 0.2517 - val_loss: 0.1625 - val_accuracy: 0.2608\n",
      "Epoch 250/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1159 - accuracy: 0.2583 - val_loss: 0.1617 - val_accuracy: 0.3273\n",
      "Epoch 251/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1155 - accuracy: 0.2541 - val_loss: 0.1620 - val_accuracy: 0.3273\n",
      "Epoch 252/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1153 - accuracy: 0.2583 - val_loss: 0.1615 - val_accuracy: 0.3183\n",
      "Epoch 253/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1150 - accuracy: 0.2655 - val_loss: 0.1610 - val_accuracy: 0.3094\n",
      "Epoch 254/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1148 - accuracy: 0.2631 - val_loss: 0.1614 - val_accuracy: 0.3112\n",
      "Epoch 255/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1146 - accuracy: 0.2523 - val_loss: 0.1697 - val_accuracy: 0.2662\n",
      "Epoch 256/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1147 - accuracy: 0.2529 - val_loss: 0.1608 - val_accuracy: 0.3058\n",
      "Epoch 257/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1141 - accuracy: 0.2643 - val_loss: 0.1602 - val_accuracy: 0.3040\n",
      "Epoch 258/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1139 - accuracy: 0.2643 - val_loss: 0.1589 - val_accuracy: 0.3094\n",
      "Epoch 259/800\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.1131 - accuracy: 0.27 - 0s 4ms/step - loss: 0.1137 - accuracy: 0.2631 - val_loss: 0.1615 - val_accuracy: 0.2986\n",
      "Epoch 260/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1134 - accuracy: 0.2583 - val_loss: 0.1600 - val_accuracy: 0.2806\n",
      "Epoch 261/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1132 - accuracy: 0.2583 - val_loss: 0.1600 - val_accuracy: 0.3237\n",
      "Epoch 262/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1130 - accuracy: 0.2613 - val_loss: 0.1697 - val_accuracy: 0.2518\n",
      "Epoch 263/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1131 - accuracy: 0.2613 - val_loss: 0.1713 - val_accuracy: 0.2392\n",
      "Epoch 264/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1129 - accuracy: 0.2571 - val_loss: 0.1596 - val_accuracy: 0.3489\n",
      "Epoch 265/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1122 - accuracy: 0.2685 - val_loss: 0.1581 - val_accuracy: 0.2986\n",
      "Epoch 266/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1119 - accuracy: 0.2697 - val_loss: 0.1586 - val_accuracy: 0.3112\n",
      "Epoch 267/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1117 - accuracy: 0.2715 - val_loss: 0.1729 - val_accuracy: 0.2500\n",
      "Epoch 268/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1120 - accuracy: 0.2703 - val_loss: 0.1572 - val_accuracy: 0.3076\n",
      "Epoch 269/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1112 - accuracy: 0.2613 - val_loss: 0.1582 - val_accuracy: 0.3129\n",
      "Epoch 270/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1111 - accuracy: 0.2679 - val_loss: 0.1581 - val_accuracy: 0.2824\n",
      "Epoch 271/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1108 - accuracy: 0.2661 - val_loss: 0.1574 - val_accuracy: 0.3201\n",
      "Epoch 272/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1105 - accuracy: 0.2769 - val_loss: 0.1568 - val_accuracy: 0.3363\n",
      "Epoch 273/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1103 - accuracy: 0.2667 - val_loss: 0.1573 - val_accuracy: 0.3129\n",
      "Epoch 274/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1101 - accuracy: 0.2637 - val_loss: 0.1559 - val_accuracy: 0.3363\n",
      "Epoch 275/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1100 - accuracy: 0.2703 - val_loss: 0.1556 - val_accuracy: 0.3273\n",
      "Epoch 276/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1095 - accuracy: 0.2739 - val_loss: 0.1558 - val_accuracy: 0.2896\n",
      "Epoch 277/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1094 - accuracy: 0.2751 - val_loss: 0.1542 - val_accuracy: 0.2878\n",
      "Epoch 278/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1091 - accuracy: 0.2661 - val_loss: 0.1558 - val_accuracy: 0.3112\n",
      "Epoch 279/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1088 - accuracy: 0.2673 - val_loss: 0.1560 - val_accuracy: 0.3435\n",
      "Epoch 280/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1086 - accuracy: 0.2733 - val_loss: 0.1541 - val_accuracy: 0.2896\n",
      "Epoch 281/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1084 - accuracy: 0.2751 - val_loss: 0.1532 - val_accuracy: 0.2932\n",
      "Epoch 282/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1080 - accuracy: 0.2769 - val_loss: 0.1546 - val_accuracy: 0.2788\n",
      "Epoch 283/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1079 - accuracy: 0.2673 - val_loss: 0.1656 - val_accuracy: 0.2716\n",
      "Epoch 284/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1081 - accuracy: 0.2691 - val_loss: 0.1546 - val_accuracy: 0.3291\n",
      "Epoch 285/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1073 - accuracy: 0.2745 - val_loss: 0.1540 - val_accuracy: 0.3112\n",
      "Epoch 286/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1071 - accuracy: 0.2727 - val_loss: 0.1526 - val_accuracy: 0.3040\n",
      "Epoch 287/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1067 - accuracy: 0.2733 - val_loss: 0.1538 - val_accuracy: 0.3022\n",
      "Epoch 288/800\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.1071 - accuracy: 0.26 - 0s 3ms/step - loss: 0.1067 - accuracy: 0.2697 - val_loss: 0.1532 - val_accuracy: 0.3094\n",
      "Epoch 289/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1064 - accuracy: 0.2739 - val_loss: 0.1505 - val_accuracy: 0.2986\n",
      "Epoch 290/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1060 - accuracy: 0.2811 - val_loss: 0.1522 - val_accuracy: 0.3147\n",
      "Epoch 291/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1059 - accuracy: 0.2679 - val_loss: 0.1556 - val_accuracy: 0.3759\n",
      "Epoch 292/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1057 - accuracy: 0.2817 - val_loss: 0.1518 - val_accuracy: 0.3273\n",
      "Epoch 293/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1054 - accuracy: 0.2739 - val_loss: 0.1523 - val_accuracy: 0.3255\n",
      "Epoch 294/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1052 - accuracy: 0.2685 - val_loss: 0.1505 - val_accuracy: 0.3273\n",
      "Epoch 295/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1048 - accuracy: 0.2667 - val_loss: 0.1538 - val_accuracy: 0.3237\n",
      "Epoch 296/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1047 - accuracy: 0.2841 - val_loss: 0.1607 - val_accuracy: 0.3327\n",
      "Epoch 297/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1048 - accuracy: 0.2835 - val_loss: 0.1518 - val_accuracy: 0.3453\n",
      "Epoch 298/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1041 - accuracy: 0.2781 - val_loss: 0.1499 - val_accuracy: 0.2824\n",
      "Epoch 299/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1039 - accuracy: 0.2739 - val_loss: 0.1510 - val_accuracy: 0.3345\n",
      "Epoch 300/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1036 - accuracy: 0.2775 - val_loss: 0.1495 - val_accuracy: 0.2932\n",
      "Epoch 301/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1033 - accuracy: 0.2781 - val_loss: 0.1489 - val_accuracy: 0.3453\n",
      "Epoch 302/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1030 - accuracy: 0.2715 - val_loss: 0.1493 - val_accuracy: 0.3309\n",
      "Epoch 303/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1028 - accuracy: 0.2757 - val_loss: 0.1483 - val_accuracy: 0.3237\n",
      "Epoch 304/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1026 - accuracy: 0.2763 - val_loss: 0.1504 - val_accuracy: 0.3129\n",
      "Epoch 305/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1024 - accuracy: 0.2805 - val_loss: 0.1475 - val_accuracy: 0.2968\n",
      "Epoch 306/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1021 - accuracy: 0.2757 - val_loss: 0.1488 - val_accuracy: 0.3165\n",
      "Epoch 307/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1019 - accuracy: 0.2739 - val_loss: 0.1480 - val_accuracy: 0.3381\n",
      "Epoch 308/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1015 - accuracy: 0.2895 - val_loss: 0.1474 - val_accuracy: 0.3525\n",
      "Epoch 309/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.1013 - accuracy: 0.2787 - val_loss: 0.1518 - val_accuracy: 0.3345\n",
      "Epoch 310/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1015 - accuracy: 0.2787 - val_loss: 0.1472 - val_accuracy: 0.3399\n",
      "Epoch 311/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1009 - accuracy: 0.2877 - val_loss: 0.1475 - val_accuracy: 0.3183\n",
      "Epoch 312/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1006 - accuracy: 0.2751 - val_loss: 0.1477 - val_accuracy: 0.3201\n",
      "Epoch 313/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1006 - accuracy: 0.2853 - val_loss: 0.1473 - val_accuracy: 0.3040\n",
      "Epoch 314/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1004 - accuracy: 0.2775 - val_loss: 0.1471 - val_accuracy: 0.3543\n",
      "Epoch 315/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1000 - accuracy: 0.2859 - val_loss: 0.1483 - val_accuracy: 0.3309\n",
      "Epoch 316/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.1000 - accuracy: 0.2853 - val_loss: 0.1464 - val_accuracy: 0.3453\n",
      "Epoch 317/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0996 - accuracy: 0.2829 - val_loss: 0.1513 - val_accuracy: 0.3525\n",
      "Epoch 318/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0997 - accuracy: 0.2895 - val_loss: 0.1461 - val_accuracy: 0.3004\n",
      "Epoch 319/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0992 - accuracy: 0.2811 - val_loss: 0.1450 - val_accuracy: 0.3291\n",
      "Epoch 320/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0990 - accuracy: 0.2817 - val_loss: 0.1441 - val_accuracy: 0.3381\n",
      "Epoch 321/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0988 - accuracy: 0.2799 - val_loss: 0.1454 - val_accuracy: 0.3417\n",
      "Epoch 322/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0986 - accuracy: 0.2811 - val_loss: 0.1591 - val_accuracy: 0.2986\n",
      "Epoch 323/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0990 - accuracy: 0.2793 - val_loss: 0.1453 - val_accuracy: 0.3291\n",
      "Epoch 324/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0982 - accuracy: 0.2877 - val_loss: 0.1448 - val_accuracy: 0.3309\n",
      "Epoch 325/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0980 - accuracy: 0.2865 - val_loss: 0.1472 - val_accuracy: 0.3291\n",
      "Epoch 326/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0979 - accuracy: 0.2835 - val_loss: 0.1430 - val_accuracy: 0.3237\n",
      "Epoch 327/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0976 - accuracy: 0.2793 - val_loss: 0.1434 - val_accuracy: 0.3327\n",
      "Epoch 328/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0974 - accuracy: 0.2889 - val_loss: 0.1647 - val_accuracy: 0.2626\n",
      "Epoch 329/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0981 - accuracy: 0.2871 - val_loss: 0.1442 - val_accuracy: 0.3309\n",
      "Epoch 330/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0971 - accuracy: 0.2877 - val_loss: 0.1420 - val_accuracy: 0.3058\n",
      "Epoch 331/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0969 - accuracy: 0.2853 - val_loss: 0.1440 - val_accuracy: 0.3381\n",
      "Epoch 332/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0967 - accuracy: 0.2829 - val_loss: 0.1440 - val_accuracy: 0.3345\n",
      "Epoch 333/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0965 - accuracy: 0.2907 - val_loss: 0.1416 - val_accuracy: 0.3201\n",
      "Epoch 334/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0964 - accuracy: 0.2841 - val_loss: 0.1445 - val_accuracy: 0.3309\n",
      "Epoch 335/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0963 - accuracy: 0.2889 - val_loss: 0.1463 - val_accuracy: 0.2536\n",
      "Epoch 336/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0963 - accuracy: 0.2805 - val_loss: 0.1434 - val_accuracy: 0.3273\n",
      "Epoch 337/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0958 - accuracy: 0.2841 - val_loss: 0.1426 - val_accuracy: 0.3543\n",
      "Epoch 338/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0956 - accuracy: 0.2937 - val_loss: 0.1445 - val_accuracy: 0.3687\n",
      "Epoch 339/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0954 - accuracy: 0.2937 - val_loss: 0.1409 - val_accuracy: 0.3327\n",
      "Epoch 340/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0953 - accuracy: 0.2937 - val_loss: 0.1421 - val_accuracy: 0.3273\n",
      "Epoch 341/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0953 - accuracy: 0.2847 - val_loss: 0.1415 - val_accuracy: 0.3201\n",
      "Epoch 342/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0951 - accuracy: 0.2811 - val_loss: 0.1414 - val_accuracy: 0.3561\n",
      "Epoch 343/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0948 - accuracy: 0.2865 - val_loss: 0.1416 - val_accuracy: 0.3363\n",
      "Epoch 344/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0947 - accuracy: 0.2871 - val_loss: 0.1413 - val_accuracy: 0.3345\n",
      "Epoch 345/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0945 - accuracy: 0.2817 - val_loss: 0.1401 - val_accuracy: 0.3489\n",
      "Epoch 346/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0943 - accuracy: 0.2847 - val_loss: 0.1410 - val_accuracy: 0.3327\n",
      "Epoch 347/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0942 - accuracy: 0.2829 - val_loss: 0.1441 - val_accuracy: 0.3363\n",
      "Epoch 348/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0943 - accuracy: 0.2907 - val_loss: 0.1439 - val_accuracy: 0.2842\n",
      "Epoch 349/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0944 - accuracy: 0.2781 - val_loss: 0.1407 - val_accuracy: 0.3381\n",
      "Epoch 350/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0939 - accuracy: 0.2799 - val_loss: 0.1517 - val_accuracy: 0.2518\n",
      "Epoch 351/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0945 - accuracy: 0.2835 - val_loss: 0.1409 - val_accuracy: 0.3471\n",
      "Epoch 352/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0936 - accuracy: 0.2901 - val_loss: 0.1400 - val_accuracy: 0.3561\n",
      "Epoch 353/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0933 - accuracy: 0.2955 - val_loss: 0.1402 - val_accuracy: 0.3309\n",
      "Epoch 354/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0933 - accuracy: 0.2895 - val_loss: 0.1399 - val_accuracy: 0.3309\n",
      "Epoch 355/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0932 - accuracy: 0.2901 - val_loss: 0.1413 - val_accuracy: 0.3255\n",
      "Epoch 356/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0930 - accuracy: 0.2955 - val_loss: 0.1402 - val_accuracy: 0.3435\n",
      "Epoch 357/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0929 - accuracy: 0.2871 - val_loss: 0.1416 - val_accuracy: 0.3525\n",
      "Epoch 358/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0928 - accuracy: 0.3003 - val_loss: 0.1412 - val_accuracy: 0.3507\n",
      "Epoch 359/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0925 - accuracy: 0.2919 - val_loss: 0.1405 - val_accuracy: 0.3435\n",
      "Epoch 360/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0925 - accuracy: 0.2907 - val_loss: 0.1402 - val_accuracy: 0.3363\n",
      "Epoch 361/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0925 - accuracy: 0.2961 - val_loss: 0.1395 - val_accuracy: 0.3399\n",
      "Epoch 362/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0921 - accuracy: 0.2907 - val_loss: 0.1395 - val_accuracy: 0.3219\n",
      "Epoch 363/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0922 - accuracy: 0.2889 - val_loss: 0.1404 - val_accuracy: 0.3795\n",
      "Epoch 364/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0919 - accuracy: 0.2913 - val_loss: 0.1395 - val_accuracy: 0.3489\n",
      "Epoch 365/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0918 - accuracy: 0.2991 - val_loss: 0.1394 - val_accuracy: 0.3759\n",
      "Epoch 366/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0916 - accuracy: 0.2913 - val_loss: 0.1476 - val_accuracy: 0.3921\n",
      "Epoch 367/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0919 - accuracy: 0.3009 - val_loss: 0.1396 - val_accuracy: 0.3525\n",
      "Epoch 368/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0915 - accuracy: 0.2919 - val_loss: 0.1390 - val_accuracy: 0.3543\n",
      "Epoch 369/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0914 - accuracy: 0.2937 - val_loss: 0.1386 - val_accuracy: 0.3165\n",
      "Epoch 370/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0913 - accuracy: 0.2943 - val_loss: 0.1384 - val_accuracy: 0.3237\n",
      "Epoch 371/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0912 - accuracy: 0.2961 - val_loss: 0.1398 - val_accuracy: 0.3741\n",
      "Epoch 372/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0912 - accuracy: 0.2907 - val_loss: 0.1503 - val_accuracy: 0.3939\n",
      "Epoch 373/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0914 - accuracy: 0.2997 - val_loss: 0.1389 - val_accuracy: 0.3561\n",
      "Epoch 374/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0908 - accuracy: 0.2889 - val_loss: 0.1379 - val_accuracy: 0.3633\n",
      "Epoch 375/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0907 - accuracy: 0.2925 - val_loss: 0.1375 - val_accuracy: 0.3471\n",
      "Epoch 376/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0905 - accuracy: 0.2949 - val_loss: 0.1385 - val_accuracy: 0.3777\n",
      "Epoch 377/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0904 - accuracy: 0.2955 - val_loss: 0.1383 - val_accuracy: 0.3381\n",
      "Epoch 378/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0904 - accuracy: 0.2925 - val_loss: 0.1387 - val_accuracy: 0.3489\n",
      "Epoch 379/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0903 - accuracy: 0.2895 - val_loss: 0.1367 - val_accuracy: 0.3489\n",
      "Epoch 380/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0901 - accuracy: 0.2979 - val_loss: 0.1379 - val_accuracy: 0.3579\n",
      "Epoch 381/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0901 - accuracy: 0.2871 - val_loss: 0.1375 - val_accuracy: 0.3453\n",
      "Epoch 382/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0900 - accuracy: 0.2931 - val_loss: 0.1401 - val_accuracy: 0.3471\n",
      "Epoch 383/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0900 - accuracy: 0.2895 - val_loss: 0.1366 - val_accuracy: 0.3525\n",
      "Epoch 384/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0897 - accuracy: 0.2919 - val_loss: 0.1371 - val_accuracy: 0.3507\n",
      "Epoch 385/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0896 - accuracy: 0.2931 - val_loss: 0.1385 - val_accuracy: 0.3525\n",
      "Epoch 386/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0895 - accuracy: 0.2943 - val_loss: 0.1376 - val_accuracy: 0.3417\n",
      "Epoch 387/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0895 - accuracy: 0.2949 - val_loss: 0.1397 - val_accuracy: 0.3345\n",
      "Epoch 388/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0895 - accuracy: 0.2925 - val_loss: 0.1378 - val_accuracy: 0.3381\n",
      "Epoch 389/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0893 - accuracy: 0.2997 - val_loss: 0.1395 - val_accuracy: 0.3597\n",
      "Epoch 390/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0893 - accuracy: 0.2943 - val_loss: 0.1407 - val_accuracy: 0.3759\n",
      "Epoch 391/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0891 - accuracy: 0.3057 - val_loss: 0.1367 - val_accuracy: 0.3435\n",
      "Epoch 392/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0891 - accuracy: 0.2973 - val_loss: 0.1368 - val_accuracy: 0.3435\n",
      "Epoch 393/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0888 - accuracy: 0.2925 - val_loss: 0.1375 - val_accuracy: 0.3417\n",
      "Epoch 394/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0888 - accuracy: 0.2937 - val_loss: 0.1382 - val_accuracy: 0.3291\n",
      "Epoch 395/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0887 - accuracy: 0.2961 - val_loss: 0.1389 - val_accuracy: 0.3561\n",
      "Epoch 396/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0887 - accuracy: 0.2961 - val_loss: 0.1406 - val_accuracy: 0.3417\n",
      "Epoch 397/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0887 - accuracy: 0.2961 - val_loss: 0.1376 - val_accuracy: 0.3435\n",
      "Epoch 398/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0885 - accuracy: 0.2973 - val_loss: 0.1372 - val_accuracy: 0.3651\n",
      "Epoch 399/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0882 - accuracy: 0.3039 - val_loss: 0.1385 - val_accuracy: 0.3759\n",
      "Epoch 400/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0883 - accuracy: 0.3051 - val_loss: 0.1372 - val_accuracy: 0.3741\n",
      "Epoch 401/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0881 - accuracy: 0.3021 - val_loss: 0.1358 - val_accuracy: 0.3507\n",
      "Epoch 402/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0878 - accuracy: 0.2931 - val_loss: 0.1370 - val_accuracy: 0.3633\n",
      "Epoch 403/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0877 - accuracy: 0.3015 - val_loss: 0.1361 - val_accuracy: 0.3471\n",
      "Epoch 404/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0878 - accuracy: 0.2943 - val_loss: 0.1446 - val_accuracy: 0.3615\n",
      "Epoch 405/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0881 - accuracy: 0.2973 - val_loss: 0.1358 - val_accuracy: 0.3615\n",
      "Epoch 406/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0875 - accuracy: 0.2961 - val_loss: 0.1408 - val_accuracy: 0.3561\n",
      "Epoch 407/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0878 - accuracy: 0.2931 - val_loss: 0.1377 - val_accuracy: 0.3165\n",
      "Epoch 408/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0874 - accuracy: 0.2925 - val_loss: 0.1354 - val_accuracy: 0.3525\n",
      "Epoch 409/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0873 - accuracy: 0.2997 - val_loss: 0.1366 - val_accuracy: 0.3165\n",
      "Epoch 410/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0874 - accuracy: 0.2889 - val_loss: 0.1361 - val_accuracy: 0.3471\n",
      "Epoch 411/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0870 - accuracy: 0.2973 - val_loss: 0.1362 - val_accuracy: 0.3669\n",
      "Epoch 412/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0870 - accuracy: 0.2961 - val_loss: 0.1386 - val_accuracy: 0.3651\n",
      "Epoch 413/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0870 - accuracy: 0.3009 - val_loss: 0.1367 - val_accuracy: 0.3651\n",
      "Epoch 414/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0869 - accuracy: 0.3033 - val_loss: 0.1407 - val_accuracy: 0.3615\n",
      "Epoch 415/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0870 - accuracy: 0.3009 - val_loss: 0.1586 - val_accuracy: 0.3885\n",
      "Epoch 416/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0877 - accuracy: 0.3009 - val_loss: 0.1380 - val_accuracy: 0.3381\n",
      "Epoch 417/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0866 - accuracy: 0.3009 - val_loss: 0.1375 - val_accuracy: 0.3777\n",
      "Epoch 418/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0865 - accuracy: 0.2967 - val_loss: 0.1376 - val_accuracy: 0.3345\n",
      "Epoch 419/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0866 - accuracy: 0.2967 - val_loss: 0.1390 - val_accuracy: 0.3633\n",
      "Epoch 420/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0865 - accuracy: 0.3051 - val_loss: 0.1366 - val_accuracy: 0.3327\n",
      "Epoch 421/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0862 - accuracy: 0.2931 - val_loss: 0.1353 - val_accuracy: 0.3525\n",
      "Epoch 422/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0861 - accuracy: 0.2991 - val_loss: 0.1388 - val_accuracy: 0.3345\n",
      "Epoch 423/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0860 - accuracy: 0.2991 - val_loss: 0.1361 - val_accuracy: 0.3525\n",
      "Epoch 424/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0859 - accuracy: 0.3039 - val_loss: 0.1354 - val_accuracy: 0.3489\n",
      "Epoch 425/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0857 - accuracy: 0.3027 - val_loss: 0.1370 - val_accuracy: 0.3435\n",
      "Epoch 426/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0858 - accuracy: 0.2949 - val_loss: 0.1400 - val_accuracy: 0.3615\n",
      "Epoch 427/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0859 - accuracy: 0.2985 - val_loss: 0.1358 - val_accuracy: 0.3651\n",
      "Epoch 428/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0855 - accuracy: 0.2973 - val_loss: 0.1576 - val_accuracy: 0.3435\n",
      "Epoch 429/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0865 - accuracy: 0.2955 - val_loss: 0.1518 - val_accuracy: 0.2986\n",
      "Epoch 430/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0862 - accuracy: 0.2961 - val_loss: 0.1358 - val_accuracy: 0.3507\n",
      "Epoch 431/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0853 - accuracy: 0.2997 - val_loss: 0.1348 - val_accuracy: 0.3489\n",
      "Epoch 432/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0852 - accuracy: 0.2985 - val_loss: 0.1353 - val_accuracy: 0.3453\n",
      "Epoch 433/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0850 - accuracy: 0.3081 - val_loss: 0.1344 - val_accuracy: 0.3435\n",
      "Epoch 434/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0849 - accuracy: 0.3027 - val_loss: 0.1339 - val_accuracy: 0.3165\n",
      "Epoch 435/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0850 - accuracy: 0.3003 - val_loss: 0.1336 - val_accuracy: 0.3633\n",
      "Epoch 436/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0848 - accuracy: 0.2955 - val_loss: 0.1356 - val_accuracy: 0.3309\n",
      "Epoch 437/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0849 - accuracy: 0.3045 - val_loss: 0.1343 - val_accuracy: 0.3417\n",
      "Epoch 438/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0847 - accuracy: 0.2991 - val_loss: 0.1343 - val_accuracy: 0.3291\n",
      "Epoch 439/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0847 - accuracy: 0.2919 - val_loss: 0.1361 - val_accuracy: 0.3345\n",
      "Epoch 440/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0847 - accuracy: 0.2967 - val_loss: 0.1351 - val_accuracy: 0.3417\n",
      "Epoch 441/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0844 - accuracy: 0.2961 - val_loss: 0.1339 - val_accuracy: 0.3705\n",
      "Epoch 442/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0842 - accuracy: 0.3009 - val_loss: 0.1536 - val_accuracy: 0.3705\n",
      "Epoch 443/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0852 - accuracy: 0.3069 - val_loss: 0.1331 - val_accuracy: 0.3489\n",
      "Epoch 444/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0840 - accuracy: 0.2961 - val_loss: 0.1348 - val_accuracy: 0.3597\n",
      "Epoch 445/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0840 - accuracy: 0.2985 - val_loss: 0.1332 - val_accuracy: 0.3435\n",
      "Epoch 446/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0840 - accuracy: 0.2997 - val_loss: 0.1389 - val_accuracy: 0.3489\n",
      "Epoch 447/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0840 - accuracy: 0.2961 - val_loss: 0.1337 - val_accuracy: 0.3561\n",
      "Epoch 448/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0836 - accuracy: 0.2973 - val_loss: 0.1341 - val_accuracy: 0.3651\n",
      "Epoch 449/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0836 - accuracy: 0.3075 - val_loss: 0.1363 - val_accuracy: 0.3759\n",
      "Epoch 450/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0837 - accuracy: 0.3033 - val_loss: 0.1353 - val_accuracy: 0.3453\n",
      "Epoch 451/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0835 - accuracy: 0.2985 - val_loss: 0.1329 - val_accuracy: 0.3633\n",
      "Epoch 452/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0834 - accuracy: 0.3075 - val_loss: 0.1331 - val_accuracy: 0.3471\n",
      "Epoch 453/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0835 - accuracy: 0.2967 - val_loss: 0.1334 - val_accuracy: 0.3561\n",
      "Epoch 454/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0833 - accuracy: 0.2907 - val_loss: 0.1337 - val_accuracy: 0.3525\n",
      "Epoch 455/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0832 - accuracy: 0.3039 - val_loss: 0.1342 - val_accuracy: 0.3094\n",
      "Epoch 456/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0833 - accuracy: 0.2967 - val_loss: 0.1322 - val_accuracy: 0.3201\n",
      "Epoch 457/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0829 - accuracy: 0.3027 - val_loss: 0.1314 - val_accuracy: 0.3291\n",
      "Epoch 458/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0828 - accuracy: 0.2973 - val_loss: 0.1327 - val_accuracy: 0.3633\n",
      "Epoch 459/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0827 - accuracy: 0.3009 - val_loss: 0.1468 - val_accuracy: 0.2608\n",
      "Epoch 460/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0835 - accuracy: 0.3021 - val_loss: 0.1339 - val_accuracy: 0.3309\n",
      "Epoch 461/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0826 - accuracy: 0.2997 - val_loss: 0.1335 - val_accuracy: 0.3543\n",
      "Epoch 462/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0825 - accuracy: 0.2997 - val_loss: 0.1380 - val_accuracy: 0.2896\n",
      "Epoch 463/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0827 - accuracy: 0.2949 - val_loss: 0.1382 - val_accuracy: 0.3094\n",
      "Epoch 464/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0826 - accuracy: 0.3027 - val_loss: 0.1346 - val_accuracy: 0.3489\n",
      "Epoch 465/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0824 - accuracy: 0.2961 - val_loss: 0.1400 - val_accuracy: 0.3813\n",
      "Epoch 466/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0825 - accuracy: 0.3051 - val_loss: 0.1342 - val_accuracy: 0.3561\n",
      "Epoch 467/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0823 - accuracy: 0.3057 - val_loss: 0.1322 - val_accuracy: 0.3183\n",
      "Epoch 468/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0821 - accuracy: 0.2961 - val_loss: 0.1403 - val_accuracy: 0.3399\n",
      "Epoch 469/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0824 - accuracy: 0.3009 - val_loss: 0.1327 - val_accuracy: 0.3543\n",
      "Epoch 470/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0819 - accuracy: 0.2973 - val_loss: 0.1379 - val_accuracy: 0.3867\n",
      "Epoch 471/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0821 - accuracy: 0.2955 - val_loss: 0.1344 - val_accuracy: 0.3525\n",
      "Epoch 472/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0817 - accuracy: 0.2997 - val_loss: 0.1302 - val_accuracy: 0.3471\n",
      "Epoch 473/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0817 - accuracy: 0.3033 - val_loss: 0.1389 - val_accuracy: 0.3022\n",
      "Epoch 474/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0819 - accuracy: 0.3009 - val_loss: 0.1311 - val_accuracy: 0.3507\n",
      "Epoch 475/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0815 - accuracy: 0.3003 - val_loss: 0.1326 - val_accuracy: 0.3453\n",
      "Epoch 476/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0814 - accuracy: 0.2997 - val_loss: 0.1333 - val_accuracy: 0.3579\n",
      "Epoch 477/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0813 - accuracy: 0.3003 - val_loss: 0.1329 - val_accuracy: 0.3363\n",
      "Epoch 478/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0812 - accuracy: 0.2985 - val_loss: 0.1325 - val_accuracy: 0.3741\n",
      "Epoch 479/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0811 - accuracy: 0.3039 - val_loss: 0.1325 - val_accuracy: 0.3561\n",
      "Epoch 480/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0810 - accuracy: 0.3069 - val_loss: 0.1346 - val_accuracy: 0.3381\n",
      "Epoch 481/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0812 - accuracy: 0.2979 - val_loss: 0.1414 - val_accuracy: 0.3849\n",
      "Epoch 482/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0813 - accuracy: 0.2991 - val_loss: 0.1329 - val_accuracy: 0.3561\n",
      "Epoch 483/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0808 - accuracy: 0.3003 - val_loss: 0.1337 - val_accuracy: 0.3633\n",
      "Epoch 484/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0808 - accuracy: 0.3015 - val_loss: 0.1387 - val_accuracy: 0.3795\n",
      "Epoch 485/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0810 - accuracy: 0.3009 - val_loss: 0.1315 - val_accuracy: 0.3561\n",
      "Epoch 486/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0807 - accuracy: 0.2991 - val_loss: 0.1341 - val_accuracy: 0.3273\n",
      "Epoch 487/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0807 - accuracy: 0.2979 - val_loss: 0.1341 - val_accuracy: 0.3579\n",
      "Epoch 488/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0805 - accuracy: 0.2997 - val_loss: 0.1318 - val_accuracy: 0.3543\n",
      "Epoch 489/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0802 - accuracy: 0.2961 - val_loss: 0.1319 - val_accuracy: 0.3525\n",
      "Epoch 490/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0803 - accuracy: 0.2979 - val_loss: 0.1323 - val_accuracy: 0.3633\n",
      "Epoch 491/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0802 - accuracy: 0.3015 - val_loss: 0.1319 - val_accuracy: 0.3471\n",
      "Epoch 492/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0803 - accuracy: 0.3033 - val_loss: 0.1314 - val_accuracy: 0.3309\n",
      "Epoch 493/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0802 - accuracy: 0.3009 - val_loss: 0.1332 - val_accuracy: 0.3561\n",
      "Epoch 494/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0801 - accuracy: 0.3003 - val_loss: 0.1342 - val_accuracy: 0.3489\n",
      "Epoch 495/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0801 - accuracy: 0.3063 - val_loss: 0.1340 - val_accuracy: 0.3615\n",
      "Epoch 496/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0801 - accuracy: 0.2937 - val_loss: 0.1332 - val_accuracy: 0.3345\n",
      "Epoch 497/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0801 - accuracy: 0.2973 - val_loss: 0.1321 - val_accuracy: 0.3669\n",
      "Epoch 498/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0797 - accuracy: 0.3045 - val_loss: 0.1312 - val_accuracy: 0.3219\n",
      "Epoch 499/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0797 - accuracy: 0.2979 - val_loss: 0.1320 - val_accuracy: 0.3435\n",
      "Epoch 500/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0799 - accuracy: 0.2943 - val_loss: 0.1336 - val_accuracy: 0.3291\n",
      "Epoch 501/800\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0797 - accuracy: 0.30 - 0s 3ms/step - loss: 0.0796 - accuracy: 0.3033 - val_loss: 0.1307 - val_accuracy: 0.3489\n",
      "Epoch 502/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0793 - accuracy: 0.2973 - val_loss: 0.1316 - val_accuracy: 0.3309\n",
      "Epoch 503/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0793 - accuracy: 0.3075 - val_loss: 0.1387 - val_accuracy: 0.3867\n",
      "Epoch 504/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0797 - accuracy: 0.2967 - val_loss: 0.1322 - val_accuracy: 0.3507\n",
      "Epoch 505/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0792 - accuracy: 0.2991 - val_loss: 0.1302 - val_accuracy: 0.3345\n",
      "Epoch 506/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0791 - accuracy: 0.2985 - val_loss: 0.1349 - val_accuracy: 0.3633\n",
      "Epoch 507/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0794 - accuracy: 0.3081 - val_loss: 0.1314 - val_accuracy: 0.3615\n",
      "Epoch 508/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0789 - accuracy: 0.3045 - val_loss: 0.1320 - val_accuracy: 0.3597\n",
      "Epoch 509/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0789 - accuracy: 0.3057 - val_loss: 0.1488 - val_accuracy: 0.3165\n",
      "Epoch 510/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0798 - accuracy: 0.2997 - val_loss: 0.1328 - val_accuracy: 0.3453\n",
      "Epoch 511/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0788 - accuracy: 0.3003 - val_loss: 0.1295 - val_accuracy: 0.3453\n",
      "Epoch 512/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0787 - accuracy: 0.3021 - val_loss: 0.1315 - val_accuracy: 0.3705\n",
      "Epoch 513/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0787 - accuracy: 0.3039 - val_loss: 0.1348 - val_accuracy: 0.3669\n",
      "Epoch 514/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0788 - accuracy: 0.3033 - val_loss: 0.1363 - val_accuracy: 0.3885\n",
      "Epoch 515/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0788 - accuracy: 0.3063 - val_loss: 0.1302 - val_accuracy: 0.3525\n",
      "Epoch 516/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0786 - accuracy: 0.2979 - val_loss: 0.1328 - val_accuracy: 0.3471\n",
      "Epoch 517/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0784 - accuracy: 0.3027 - val_loss: 0.1354 - val_accuracy: 0.3201\n",
      "Epoch 518/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0788 - accuracy: 0.2967 - val_loss: 0.1318 - val_accuracy: 0.3489\n",
      "Epoch 519/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0782 - accuracy: 0.2979 - val_loss: 0.1313 - val_accuracy: 0.3363\n",
      "Epoch 520/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0780 - accuracy: 0.3045 - val_loss: 0.1321 - val_accuracy: 0.3399\n",
      "Epoch 521/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0780 - accuracy: 0.3015 - val_loss: 0.1303 - val_accuracy: 0.3561\n",
      "Epoch 522/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0781 - accuracy: 0.2997 - val_loss: 0.1352 - val_accuracy: 0.3561\n",
      "Epoch 523/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0784 - accuracy: 0.3039 - val_loss: 0.1333 - val_accuracy: 0.3363\n",
      "Epoch 524/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0781 - accuracy: 0.2997 - val_loss: 0.1306 - val_accuracy: 0.3435\n",
      "Epoch 525/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0778 - accuracy: 0.2967 - val_loss: 0.1305 - val_accuracy: 0.3417\n",
      "Epoch 526/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0777 - accuracy: 0.2985 - val_loss: 0.1323 - val_accuracy: 0.3561\n",
      "Epoch 527/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0776 - accuracy: 0.3039 - val_loss: 0.1302 - val_accuracy: 0.3471\n",
      "Epoch 528/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0775 - accuracy: 0.3033 - val_loss: 0.1295 - val_accuracy: 0.3399\n",
      "Epoch 529/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0776 - accuracy: 0.3027 - val_loss: 0.1400 - val_accuracy: 0.2968\n",
      "Epoch 530/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0782 - accuracy: 0.3075 - val_loss: 0.1537 - val_accuracy: 0.3363\n",
      "Epoch 531/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0785 - accuracy: 0.2991 - val_loss: 0.1291 - val_accuracy: 0.3525\n",
      "Epoch 532/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0772 - accuracy: 0.3027 - val_loss: 0.1450 - val_accuracy: 0.3112\n",
      "Epoch 533/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0782 - accuracy: 0.2937 - val_loss: 0.1315 - val_accuracy: 0.3453\n",
      "Epoch 534/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0773 - accuracy: 0.2973 - val_loss: 0.1297 - val_accuracy: 0.3381\n",
      "Epoch 535/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0772 - accuracy: 0.2979 - val_loss: 0.1292 - val_accuracy: 0.3345\n",
      "Epoch 536/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0772 - accuracy: 0.2949 - val_loss: 0.1307 - val_accuracy: 0.3615\n",
      "Epoch 537/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0770 - accuracy: 0.3027 - val_loss: 0.1300 - val_accuracy: 0.3381\n",
      "Epoch 538/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0769 - accuracy: 0.3003 - val_loss: 0.1292 - val_accuracy: 0.3543\n",
      "Epoch 539/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0768 - accuracy: 0.3003 - val_loss: 0.1337 - val_accuracy: 0.3004\n",
      "Epoch 540/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0774 - accuracy: 0.2997 - val_loss: 0.1311 - val_accuracy: 0.3615\n",
      "Epoch 541/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0768 - accuracy: 0.3003 - val_loss: 0.1285 - val_accuracy: 0.3471\n",
      "Epoch 542/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0769 - accuracy: 0.2979 - val_loss: 0.1294 - val_accuracy: 0.3435\n",
      "Epoch 543/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0766 - accuracy: 0.2991 - val_loss: 0.1292 - val_accuracy: 0.3615\n",
      "Epoch 544/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0765 - accuracy: 0.3057 - val_loss: 0.1324 - val_accuracy: 0.3219\n",
      "Epoch 545/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0767 - accuracy: 0.2925 - val_loss: 0.1374 - val_accuracy: 0.3112\n",
      "Epoch 546/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0768 - accuracy: 0.3003 - val_loss: 0.1507 - val_accuracy: 0.2842\n",
      "Epoch 547/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0775 - accuracy: 0.2913 - val_loss: 0.1288 - val_accuracy: 0.3381\n",
      "Epoch 548/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0763 - accuracy: 0.2955 - val_loss: 0.1305 - val_accuracy: 0.3363\n",
      "Epoch 549/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0763 - accuracy: 0.2979 - val_loss: 0.1306 - val_accuracy: 0.3201\n",
      "Epoch 550/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0766 - accuracy: 0.2907 - val_loss: 0.1285 - val_accuracy: 0.3597\n",
      "Epoch 551/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0761 - accuracy: 0.2997 - val_loss: 0.1278 - val_accuracy: 0.3435\n",
      "Epoch 552/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0762 - accuracy: 0.2991 - val_loss: 0.1326 - val_accuracy: 0.3651\n",
      "Epoch 553/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0761 - accuracy: 0.2937 - val_loss: 0.1317 - val_accuracy: 0.3471\n",
      "Epoch 554/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0762 - accuracy: 0.2955 - val_loss: 0.1279 - val_accuracy: 0.3507\n",
      "Epoch 555/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0759 - accuracy: 0.2985 - val_loss: 0.1285 - val_accuracy: 0.3417\n",
      "Epoch 556/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0759 - accuracy: 0.2979 - val_loss: 0.1297 - val_accuracy: 0.3525\n",
      "Epoch 557/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0759 - accuracy: 0.3051 - val_loss: 0.1271 - val_accuracy: 0.3399\n",
      "Epoch 558/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0756 - accuracy: 0.2949 - val_loss: 0.1333 - val_accuracy: 0.3543\n",
      "Epoch 559/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0759 - accuracy: 0.2943 - val_loss: 0.1293 - val_accuracy: 0.3309\n",
      "Epoch 560/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0756 - accuracy: 0.2961 - val_loss: 0.1281 - val_accuracy: 0.3399\n",
      "Epoch 561/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0755 - accuracy: 0.2979 - val_loss: 0.1295 - val_accuracy: 0.3507\n",
      "Epoch 562/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0756 - accuracy: 0.2973 - val_loss: 0.1548 - val_accuracy: 0.3381\n",
      "Epoch 563/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0766 - accuracy: 0.3075 - val_loss: 0.1300 - val_accuracy: 0.3219\n",
      "Epoch 564/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0756 - accuracy: 0.2907 - val_loss: 0.1282 - val_accuracy: 0.3615\n",
      "Epoch 565/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0755 - accuracy: 0.2925 - val_loss: 0.1307 - val_accuracy: 0.3345\n",
      "Epoch 566/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0753 - accuracy: 0.2985 - val_loss: 0.1288 - val_accuracy: 0.3381\n",
      "Epoch 567/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0753 - accuracy: 0.3027 - val_loss: 0.1318 - val_accuracy: 0.3112\n",
      "Epoch 568/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0753 - accuracy: 0.2907 - val_loss: 0.1357 - val_accuracy: 0.3381\n",
      "Epoch 569/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0756 - accuracy: 0.2901 - val_loss: 0.1326 - val_accuracy: 0.3237\n",
      "Epoch 570/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0752 - accuracy: 0.2973 - val_loss: 0.1292 - val_accuracy: 0.3507\n",
      "Epoch 571/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0749 - accuracy: 0.2955 - val_loss: 0.1298 - val_accuracy: 0.3435\n",
      "Epoch 572/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0749 - accuracy: 0.3033 - val_loss: 0.1270 - val_accuracy: 0.3399\n",
      "Epoch 573/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0748 - accuracy: 0.2937 - val_loss: 0.1291 - val_accuracy: 0.3507\n",
      "Epoch 574/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0747 - accuracy: 0.2955 - val_loss: 0.1299 - val_accuracy: 0.3399\n",
      "Epoch 575/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0747 - accuracy: 0.2991 - val_loss: 0.1288 - val_accuracy: 0.3597\n",
      "Epoch 576/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0747 - accuracy: 0.3081 - val_loss: 0.1302 - val_accuracy: 0.3291\n",
      "Epoch 577/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0746 - accuracy: 0.3057 - val_loss: 0.1359 - val_accuracy: 0.3597\n",
      "Epoch 578/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0750 - accuracy: 0.3003 - val_loss: 0.1318 - val_accuracy: 0.3309\n",
      "Epoch 579/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0747 - accuracy: 0.2895 - val_loss: 0.1276 - val_accuracy: 0.3489\n",
      "Epoch 580/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0743 - accuracy: 0.2991 - val_loss: 0.1308 - val_accuracy: 0.3597\n",
      "Epoch 581/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0745 - accuracy: 0.3069 - val_loss: 0.1282 - val_accuracy: 0.3435\n",
      "Epoch 582/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0743 - accuracy: 0.2925 - val_loss: 0.1352 - val_accuracy: 0.3705\n",
      "Epoch 583/800\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0745 - accuracy: 0.31 - 0s 4ms/step - loss: 0.0747 - accuracy: 0.3057 - val_loss: 0.1322 - val_accuracy: 0.3291\n",
      "Epoch 584/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0745 - accuracy: 0.2889 - val_loss: 0.1266 - val_accuracy: 0.3327\n",
      "Epoch 585/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0740 - accuracy: 0.2967 - val_loss: 0.1292 - val_accuracy: 0.3453\n",
      "Epoch 586/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0740 - accuracy: 0.3027 - val_loss: 0.1424 - val_accuracy: 0.2716\n",
      "Epoch 587/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0747 - accuracy: 0.2883 - val_loss: 0.1292 - val_accuracy: 0.3381\n",
      "Epoch 588/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0741 - accuracy: 0.2961 - val_loss: 0.1280 - val_accuracy: 0.3471\n",
      "Epoch 589/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0739 - accuracy: 0.2973 - val_loss: 0.1334 - val_accuracy: 0.3669\n",
      "Epoch 590/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0742 - accuracy: 0.3051 - val_loss: 0.1278 - val_accuracy: 0.3381\n",
      "Epoch 591/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0739 - accuracy: 0.2985 - val_loss: 0.1422 - val_accuracy: 0.2878\n",
      "Epoch 592/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0744 - accuracy: 0.2979 - val_loss: 0.1294 - val_accuracy: 0.3219\n",
      "Epoch 593/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0737 - accuracy: 0.2991 - val_loss: 0.1303 - val_accuracy: 0.3273\n",
      "Epoch 594/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0741 - accuracy: 0.2961 - val_loss: 0.1295 - val_accuracy: 0.3561\n",
      "Epoch 595/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0739 - accuracy: 0.2889 - val_loss: 0.1271 - val_accuracy: 0.3453\n",
      "Epoch 596/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0736 - accuracy: 0.2889 - val_loss: 0.1275 - val_accuracy: 0.3381\n",
      "Epoch 597/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0735 - accuracy: 0.2961 - val_loss: 0.1280 - val_accuracy: 0.3597\n",
      "Epoch 598/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0735 - accuracy: 0.3021 - val_loss: 0.1291 - val_accuracy: 0.3453\n",
      "Epoch 599/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0734 - accuracy: 0.3033 - val_loss: 0.1293 - val_accuracy: 0.2824\n",
      "Epoch 600/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0736 - accuracy: 0.2895 - val_loss: 0.1314 - val_accuracy: 0.3399\n",
      "Epoch 601/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0734 - accuracy: 0.3003 - val_loss: 0.1268 - val_accuracy: 0.3561\n",
      "Epoch 602/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0732 - accuracy: 0.2925 - val_loss: 0.1279 - val_accuracy: 0.3687\n",
      "Epoch 603/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0731 - accuracy: 0.3015 - val_loss: 0.1292 - val_accuracy: 0.3453\n",
      "Epoch 604/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0732 - accuracy: 0.2997 - val_loss: 0.1325 - val_accuracy: 0.3489\n",
      "Epoch 605/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0731 - accuracy: 0.3003 - val_loss: 0.1292 - val_accuracy: 0.3651\n",
      "Epoch 606/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0730 - accuracy: 0.3027 - val_loss: 0.1287 - val_accuracy: 0.3579\n",
      "Epoch 607/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0729 - accuracy: 0.3015 - val_loss: 0.1267 - val_accuracy: 0.3435\n",
      "Epoch 608/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0728 - accuracy: 0.3003 - val_loss: 0.1270 - val_accuracy: 0.3363\n",
      "Epoch 609/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0728 - accuracy: 0.3027 - val_loss: 0.1283 - val_accuracy: 0.3453\n",
      "Epoch 610/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0727 - accuracy: 0.3051 - val_loss: 0.1271 - val_accuracy: 0.3309\n",
      "Epoch 611/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0728 - accuracy: 0.2961 - val_loss: 0.1279 - val_accuracy: 0.3723\n",
      "Epoch 612/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0729 - accuracy: 0.2967 - val_loss: 0.1383 - val_accuracy: 0.3255\n",
      "Epoch 613/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0732 - accuracy: 0.2907 - val_loss: 0.1361 - val_accuracy: 0.3633\n",
      "Epoch 614/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0730 - accuracy: 0.3021 - val_loss: 0.1276 - val_accuracy: 0.3543\n",
      "Epoch 615/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0726 - accuracy: 0.2973 - val_loss: 0.1282 - val_accuracy: 0.3615\n",
      "Epoch 616/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0724 - accuracy: 0.2991 - val_loss: 0.1268 - val_accuracy: 0.3507\n",
      "Epoch 617/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0725 - accuracy: 0.2985 - val_loss: 0.1264 - val_accuracy: 0.3309\n",
      "Epoch 618/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0724 - accuracy: 0.2907 - val_loss: 0.1263 - val_accuracy: 0.3615\n",
      "Epoch 619/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0722 - accuracy: 0.3081 - val_loss: 0.1282 - val_accuracy: 0.3399\n",
      "Epoch 620/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0721 - accuracy: 0.3015 - val_loss: 0.1418 - val_accuracy: 0.3417\n",
      "Epoch 621/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0729 - accuracy: 0.3075 - val_loss: 0.1275 - val_accuracy: 0.3435\n",
      "Epoch 622/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0721 - accuracy: 0.2949 - val_loss: 0.1266 - val_accuracy: 0.3489\n",
      "Epoch 623/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0720 - accuracy: 0.2991 - val_loss: 0.1279 - val_accuracy: 0.3759\n",
      "Epoch 624/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0721 - accuracy: 0.3003 - val_loss: 0.1289 - val_accuracy: 0.3489\n",
      "Epoch 625/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0720 - accuracy: 0.2997 - val_loss: 0.1410 - val_accuracy: 0.3040\n",
      "Epoch 626/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0727 - accuracy: 0.2967 - val_loss: 0.1323 - val_accuracy: 0.3309\n",
      "Epoch 627/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0721 - accuracy: 0.2967 - val_loss: 0.1265 - val_accuracy: 0.3453\n",
      "Epoch 628/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0718 - accuracy: 0.2973 - val_loss: 0.1310 - val_accuracy: 0.3417\n",
      "Epoch 629/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0719 - accuracy: 0.3069 - val_loss: 0.1254 - val_accuracy: 0.3399\n",
      "Epoch 630/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0716 - accuracy: 0.2991 - val_loss: 0.1269 - val_accuracy: 0.3651\n",
      "Epoch 631/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0716 - accuracy: 0.3039 - val_loss: 0.1280 - val_accuracy: 0.3363\n",
      "Epoch 632/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0716 - accuracy: 0.2967 - val_loss: 0.1258 - val_accuracy: 0.3597\n",
      "Epoch 633/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0713 - accuracy: 0.3039 - val_loss: 0.1273 - val_accuracy: 0.3561\n",
      "Epoch 634/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0715 - accuracy: 0.2961 - val_loss: 0.1248 - val_accuracy: 0.3597\n",
      "Epoch 635/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0714 - accuracy: 0.2991 - val_loss: 0.1280 - val_accuracy: 0.3561\n",
      "Epoch 636/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0713 - accuracy: 0.3015 - val_loss: 0.1278 - val_accuracy: 0.3435\n",
      "Epoch 637/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0713 - accuracy: 0.2955 - val_loss: 0.1272 - val_accuracy: 0.3597\n",
      "Epoch 638/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0714 - accuracy: 0.2937 - val_loss: 0.1273 - val_accuracy: 0.3417\n",
      "Epoch 639/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0711 - accuracy: 0.2967 - val_loss: 0.1268 - val_accuracy: 0.3651\n",
      "Epoch 640/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0710 - accuracy: 0.3015 - val_loss: 0.1296 - val_accuracy: 0.2968\n",
      "Epoch 641/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0715 - accuracy: 0.2973 - val_loss: 0.1248 - val_accuracy: 0.3399\n",
      "Epoch 642/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0711 - accuracy: 0.2943 - val_loss: 0.1259 - val_accuracy: 0.3471\n",
      "Epoch 643/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0710 - accuracy: 0.3003 - val_loss: 0.1275 - val_accuracy: 0.3453\n",
      "Epoch 644/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0711 - accuracy: 0.2907 - val_loss: 0.1270 - val_accuracy: 0.3507\n",
      "Epoch 645/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0709 - accuracy: 0.2997 - val_loss: 0.1245 - val_accuracy: 0.3435\n",
      "Epoch 646/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0708 - accuracy: 0.3039 - val_loss: 0.1263 - val_accuracy: 0.3489\n",
      "Epoch 647/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0706 - accuracy: 0.3033 - val_loss: 0.1306 - val_accuracy: 0.3849\n",
      "Epoch 648/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0709 - accuracy: 0.3009 - val_loss: 0.1277 - val_accuracy: 0.3525\n",
      "Epoch 649/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0707 - accuracy: 0.2949 - val_loss: 0.1275 - val_accuracy: 0.3561\n",
      "Epoch 650/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0706 - accuracy: 0.3021 - val_loss: 0.1295 - val_accuracy: 0.3345\n",
      "Epoch 651/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0707 - accuracy: 0.3057 - val_loss: 0.1284 - val_accuracy: 0.3561\n",
      "Epoch 652/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0707 - accuracy: 0.3123 - val_loss: 0.1281 - val_accuracy: 0.3597\n",
      "Epoch 653/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0705 - accuracy: 0.3015 - val_loss: 0.1297 - val_accuracy: 0.3849\n",
      "Epoch 654/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0706 - accuracy: 0.3057 - val_loss: 0.1265 - val_accuracy: 0.3489\n",
      "Epoch 655/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0704 - accuracy: 0.2997 - val_loss: 0.1254 - val_accuracy: 0.3507\n",
      "Epoch 656/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0704 - accuracy: 0.2967 - val_loss: 0.1287 - val_accuracy: 0.3615\n",
      "Epoch 657/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0703 - accuracy: 0.3081 - val_loss: 0.1278 - val_accuracy: 0.3633\n",
      "Epoch 658/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0704 - accuracy: 0.3027 - val_loss: 0.1254 - val_accuracy: 0.3507\n",
      "Epoch 659/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0701 - accuracy: 0.3039 - val_loss: 0.1263 - val_accuracy: 0.3417\n",
      "Epoch 660/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0699 - accuracy: 0.3009 - val_loss: 0.1278 - val_accuracy: 0.3543\n",
      "Epoch 661/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0701 - accuracy: 0.2985 - val_loss: 0.1272 - val_accuracy: 0.3579\n",
      "Epoch 662/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0699 - accuracy: 0.2997 - val_loss: 0.1260 - val_accuracy: 0.3489\n",
      "Epoch 663/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0698 - accuracy: 0.2991 - val_loss: 0.1244 - val_accuracy: 0.3094\n",
      "Epoch 664/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0698 - accuracy: 0.3051 - val_loss: 0.1275 - val_accuracy: 0.3561\n",
      "Epoch 665/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0697 - accuracy: 0.3021 - val_loss: 0.1340 - val_accuracy: 0.3813\n",
      "Epoch 666/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0701 - accuracy: 0.3021 - val_loss: 0.1278 - val_accuracy: 0.3507\n",
      "Epoch 667/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0698 - accuracy: 0.3021 - val_loss: 0.1289 - val_accuracy: 0.3705\n",
      "Epoch 668/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0696 - accuracy: 0.3021 - val_loss: 0.1260 - val_accuracy: 0.3471\n",
      "Epoch 669/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0694 - accuracy: 0.3015 - val_loss: 0.1255 - val_accuracy: 0.3543\n",
      "Epoch 670/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0694 - accuracy: 0.2961 - val_loss: 0.1270 - val_accuracy: 0.3543\n",
      "Epoch 671/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0695 - accuracy: 0.3045 - val_loss: 0.1275 - val_accuracy: 0.3741\n",
      "Epoch 672/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0696 - accuracy: 0.3039 - val_loss: 0.1272 - val_accuracy: 0.3471\n",
      "Epoch 673/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0694 - accuracy: 0.3087 - val_loss: 0.1244 - val_accuracy: 0.3525\n",
      "Epoch 674/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0692 - accuracy: 0.3003 - val_loss: 0.1256 - val_accuracy: 0.3633\n",
      "Epoch 675/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0693 - accuracy: 0.3075 - val_loss: 0.1283 - val_accuracy: 0.3435\n",
      "Epoch 676/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0693 - accuracy: 0.3027 - val_loss: 0.1252 - val_accuracy: 0.3435\n",
      "Epoch 677/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0691 - accuracy: 0.3057 - val_loss: 0.1291 - val_accuracy: 0.3561\n",
      "Epoch 678/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0693 - accuracy: 0.2985 - val_loss: 0.1297 - val_accuracy: 0.3597\n",
      "Epoch 679/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0692 - accuracy: 0.3009 - val_loss: 0.1256 - val_accuracy: 0.3435\n",
      "Epoch 680/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0690 - accuracy: 0.2979 - val_loss: 0.1245 - val_accuracy: 0.3615\n",
      "Epoch 681/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0692 - accuracy: 0.2919 - val_loss: 0.1289 - val_accuracy: 0.3507\n",
      "Epoch 682/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0693 - accuracy: 0.2949 - val_loss: 0.1292 - val_accuracy: 0.3579\n",
      "Epoch 683/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0689 - accuracy: 0.3003 - val_loss: 0.1242 - val_accuracy: 0.3507\n",
      "Epoch 684/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0689 - accuracy: 0.2997 - val_loss: 0.1274 - val_accuracy: 0.3579\n",
      "Epoch 685/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0687 - accuracy: 0.3039 - val_loss: 0.1270 - val_accuracy: 0.3669\n",
      "Epoch 686/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0689 - accuracy: 0.3033 - val_loss: 0.1277 - val_accuracy: 0.3741\n",
      "Epoch 687/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0689 - accuracy: 0.3021 - val_loss: 0.1240 - val_accuracy: 0.3579\n",
      "Epoch 688/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0685 - accuracy: 0.2961 - val_loss: 0.1260 - val_accuracy: 0.3435\n",
      "Epoch 689/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0684 - accuracy: 0.2997 - val_loss: 0.1353 - val_accuracy: 0.2950\n",
      "Epoch 690/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0698 - accuracy: 0.2985 - val_loss: 0.1273 - val_accuracy: 0.3705\n",
      "Epoch 691/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0684 - accuracy: 0.3021 - val_loss: 0.1253 - val_accuracy: 0.3723\n",
      "Epoch 692/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0684 - accuracy: 0.3081 - val_loss: 0.1295 - val_accuracy: 0.3615\n",
      "Epoch 693/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0687 - accuracy: 0.3021 - val_loss: 0.1262 - val_accuracy: 0.3615\n",
      "Epoch 694/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0682 - accuracy: 0.3021 - val_loss: 0.1236 - val_accuracy: 0.3633\n",
      "Epoch 695/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0682 - accuracy: 0.2997 - val_loss: 0.1300 - val_accuracy: 0.3957\n",
      "Epoch 696/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0684 - accuracy: 0.3039 - val_loss: 0.1314 - val_accuracy: 0.3705\n",
      "Epoch 697/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0684 - accuracy: 0.2991 - val_loss: 0.1264 - val_accuracy: 0.3525\n",
      "Epoch 698/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0682 - accuracy: 0.3045 - val_loss: 0.1239 - val_accuracy: 0.3579\n",
      "Epoch 699/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0679 - accuracy: 0.3033 - val_loss: 0.1249 - val_accuracy: 0.3345\n",
      "Epoch 700/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0681 - accuracy: 0.3033 - val_loss: 0.1495 - val_accuracy: 0.3201\n",
      "Epoch 701/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0696 - accuracy: 0.2925 - val_loss: 0.1263 - val_accuracy: 0.3579\n",
      "Epoch 702/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0678 - accuracy: 0.2943 - val_loss: 0.1267 - val_accuracy: 0.3507\n",
      "Epoch 703/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0678 - accuracy: 0.2931 - val_loss: 0.1262 - val_accuracy: 0.3633\n",
      "Epoch 704/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0676 - accuracy: 0.2979 - val_loss: 0.1352 - val_accuracy: 0.3058\n",
      "Epoch 705/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0684 - accuracy: 0.3075 - val_loss: 0.1270 - val_accuracy: 0.3453\n",
      "Epoch 706/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0678 - accuracy: 0.3027 - val_loss: 0.1255 - val_accuracy: 0.3543\n",
      "Epoch 707/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0675 - accuracy: 0.2985 - val_loss: 0.1248 - val_accuracy: 0.3381\n",
      "Epoch 708/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0674 - accuracy: 0.2991 - val_loss: 0.1266 - val_accuracy: 0.3525\n",
      "Epoch 709/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0675 - accuracy: 0.2991 - val_loss: 0.1245 - val_accuracy: 0.3579\n",
      "Epoch 710/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0674 - accuracy: 0.3003 - val_loss: 0.1265 - val_accuracy: 0.3471\n",
      "Epoch 711/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0674 - accuracy: 0.2985 - val_loss: 0.1282 - val_accuracy: 0.3561\n",
      "Epoch 712/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0674 - accuracy: 0.2985 - val_loss: 0.1245 - val_accuracy: 0.3471\n",
      "Epoch 713/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0673 - accuracy: 0.3039 - val_loss: 0.1254 - val_accuracy: 0.3597\n",
      "Epoch 714/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0673 - accuracy: 0.2973 - val_loss: 0.1239 - val_accuracy: 0.3687\n",
      "Epoch 715/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0670 - accuracy: 0.2979 - val_loss: 0.1327 - val_accuracy: 0.3633\n",
      "Epoch 716/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0673 - accuracy: 0.3105 - val_loss: 0.1286 - val_accuracy: 0.3597\n",
      "Epoch 717/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0671 - accuracy: 0.3009 - val_loss: 0.1243 - val_accuracy: 0.3543\n",
      "Epoch 718/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0669 - accuracy: 0.3027 - val_loss: 0.1263 - val_accuracy: 0.3597\n",
      "Epoch 719/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0671 - accuracy: 0.2985 - val_loss: 0.1271 - val_accuracy: 0.3921\n",
      "Epoch 720/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0672 - accuracy: 0.3039 - val_loss: 0.1277 - val_accuracy: 0.3381\n",
      "Epoch 721/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0669 - accuracy: 0.3021 - val_loss: 0.1232 - val_accuracy: 0.3597\n",
      "Epoch 722/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0669 - accuracy: 0.3069 - val_loss: 0.1347 - val_accuracy: 0.3759\n",
      "Epoch 723/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0672 - accuracy: 0.3033 - val_loss: 0.1253 - val_accuracy: 0.3507\n",
      "Epoch 724/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0666 - accuracy: 0.2997 - val_loss: 0.1249 - val_accuracy: 0.3687\n",
      "Epoch 725/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0668 - accuracy: 0.2985 - val_loss: 0.1310 - val_accuracy: 0.2986\n",
      "Epoch 726/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0670 - accuracy: 0.3003 - val_loss: 0.1223 - val_accuracy: 0.3309\n",
      "Epoch 727/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0668 - accuracy: 0.2991 - val_loss: 0.1217 - val_accuracy: 0.3579\n",
      "Epoch 728/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0665 - accuracy: 0.2901 - val_loss: 0.1271 - val_accuracy: 0.3813\n",
      "Epoch 729/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0666 - accuracy: 0.3045 - val_loss: 0.1232 - val_accuracy: 0.3435\n",
      "Epoch 730/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0664 - accuracy: 0.2967 - val_loss: 0.1281 - val_accuracy: 0.3273\n",
      "Epoch 731/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0665 - accuracy: 0.2997 - val_loss: 0.1227 - val_accuracy: 0.3471\n",
      "Epoch 732/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0662 - accuracy: 0.3003 - val_loss: 0.1215 - val_accuracy: 0.3543\n",
      "Epoch 733/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0661 - accuracy: 0.3051 - val_loss: 0.1253 - val_accuracy: 0.3543\n",
      "Epoch 734/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0661 - accuracy: 0.2997 - val_loss: 0.1231 - val_accuracy: 0.3435\n",
      "Epoch 735/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0661 - accuracy: 0.3045 - val_loss: 0.1215 - val_accuracy: 0.3579\n",
      "Epoch 736/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0661 - accuracy: 0.3003 - val_loss: 0.1254 - val_accuracy: 0.3741\n",
      "Epoch 737/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0659 - accuracy: 0.3009 - val_loss: 0.1273 - val_accuracy: 0.3705\n",
      "Epoch 738/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0661 - accuracy: 0.3129 - val_loss: 0.1241 - val_accuracy: 0.3489\n",
      "Epoch 739/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0657 - accuracy: 0.2955 - val_loss: 0.1250 - val_accuracy: 0.3831\n",
      "Epoch 740/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0659 - accuracy: 0.3009 - val_loss: 0.1234 - val_accuracy: 0.3741\n",
      "Epoch 741/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0659 - accuracy: 0.2985 - val_loss: 0.1239 - val_accuracy: 0.3777\n",
      "Epoch 742/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0657 - accuracy: 0.3051 - val_loss: 0.1215 - val_accuracy: 0.3597\n",
      "Epoch 743/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0657 - accuracy: 0.3021 - val_loss: 0.1236 - val_accuracy: 0.3705\n",
      "Epoch 744/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0657 - accuracy: 0.3009 - val_loss: 0.1595 - val_accuracy: 0.3489\n",
      "Epoch 745/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0671 - accuracy: 0.2943 - val_loss: 0.1254 - val_accuracy: 0.3327\n",
      "Epoch 746/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0658 - accuracy: 0.3123 - val_loss: 0.1231 - val_accuracy: 0.3669\n",
      "Epoch 747/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0654 - accuracy: 0.3009 - val_loss: 0.1243 - val_accuracy: 0.3525\n",
      "Epoch 748/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0654 - accuracy: 0.3033 - val_loss: 0.1230 - val_accuracy: 0.3687\n",
      "Epoch 749/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0654 - accuracy: 0.3027 - val_loss: 0.1205 - val_accuracy: 0.3201\n",
      "Epoch 750/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0654 - accuracy: 0.3009 - val_loss: 0.1251 - val_accuracy: 0.3507\n",
      "Epoch 751/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0652 - accuracy: 0.3009 - val_loss: 0.1234 - val_accuracy: 0.3633\n",
      "Epoch 752/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0651 - accuracy: 0.3069 - val_loss: 0.1243 - val_accuracy: 0.3417\n",
      "Epoch 753/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0653 - accuracy: 0.3081 - val_loss: 0.1217 - val_accuracy: 0.3525\n",
      "Epoch 754/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0651 - accuracy: 0.2955 - val_loss: 0.1251 - val_accuracy: 0.3543\n",
      "Epoch 755/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0652 - accuracy: 0.3051 - val_loss: 0.1264 - val_accuracy: 0.3705\n",
      "Epoch 756/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0652 - accuracy: 0.3009 - val_loss: 0.1227 - val_accuracy: 0.3597\n",
      "Epoch 757/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0651 - accuracy: 0.3039 - val_loss: 0.1274 - val_accuracy: 0.3381\n",
      "Epoch 758/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0651 - accuracy: 0.2967 - val_loss: 0.1269 - val_accuracy: 0.3579\n",
      "Epoch 759/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0651 - accuracy: 0.2991 - val_loss: 0.1243 - val_accuracy: 0.3381\n",
      "Epoch 760/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0649 - accuracy: 0.3003 - val_loss: 0.1395 - val_accuracy: 0.2302\n",
      "Epoch 761/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0660 - accuracy: 0.3033 - val_loss: 0.1241 - val_accuracy: 0.3741\n",
      "Epoch 762/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0648 - accuracy: 0.2979 - val_loss: 0.1257 - val_accuracy: 0.3489\n",
      "Epoch 763/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0650 - accuracy: 0.2967 - val_loss: 0.1251 - val_accuracy: 0.3525\n",
      "Epoch 764/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0647 - accuracy: 0.3027 - val_loss: 0.1232 - val_accuracy: 0.3741\n",
      "Epoch 765/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0649 - accuracy: 0.3009 - val_loss: 0.1241 - val_accuracy: 0.3201\n",
      "Epoch 766/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0648 - accuracy: 0.3069 - val_loss: 0.1232 - val_accuracy: 0.3471\n",
      "Epoch 767/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0649 - accuracy: 0.3015 - val_loss: 0.1258 - val_accuracy: 0.3705\n",
      "Epoch 768/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0644 - accuracy: 0.2991 - val_loss: 0.1273 - val_accuracy: 0.3543\n",
      "Epoch 769/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0645 - accuracy: 0.3063 - val_loss: 0.1222 - val_accuracy: 0.3543\n",
      "Epoch 770/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0643 - accuracy: 0.3069 - val_loss: 0.1205 - val_accuracy: 0.3453\n",
      "Epoch 771/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0642 - accuracy: 0.3081 - val_loss: 0.1250 - val_accuracy: 0.3417\n",
      "Epoch 772/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0642 - accuracy: 0.3045 - val_loss: 0.1232 - val_accuracy: 0.3651\n",
      "Epoch 773/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0642 - accuracy: 0.3111 - val_loss: 0.1205 - val_accuracy: 0.3201\n",
      "Epoch 774/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0641 - accuracy: 0.3003 - val_loss: 0.1325 - val_accuracy: 0.3112\n",
      "Epoch 775/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0649 - accuracy: 0.2943 - val_loss: 0.1279 - val_accuracy: 0.3561\n",
      "Epoch 776/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0642 - accuracy: 0.3057 - val_loss: 0.1266 - val_accuracy: 0.3327\n",
      "Epoch 777/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0643 - accuracy: 0.3039 - val_loss: 0.1223 - val_accuracy: 0.3561\n",
      "Epoch 778/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0639 - accuracy: 0.3063 - val_loss: 0.1224 - val_accuracy: 0.3453\n",
      "Epoch 779/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0641 - accuracy: 0.3111 - val_loss: 0.1237 - val_accuracy: 0.3597\n",
      "Epoch 780/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0638 - accuracy: 0.2991 - val_loss: 0.1240 - val_accuracy: 0.3687\n",
      "Epoch 781/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0638 - accuracy: 0.3051 - val_loss: 0.1242 - val_accuracy: 0.3453\n",
      "Epoch 782/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0638 - accuracy: 0.2967 - val_loss: 0.1224 - val_accuracy: 0.3615\n",
      "Epoch 783/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0637 - accuracy: 0.3093 - val_loss: 0.1251 - val_accuracy: 0.3561\n",
      "Epoch 784/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0637 - accuracy: 0.3039 - val_loss: 0.1309 - val_accuracy: 0.3363\n",
      "Epoch 785/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0638 - accuracy: 0.3027 - val_loss: 0.1509 - val_accuracy: 0.4191\n",
      "Epoch 786/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0648 - accuracy: 0.3093 - val_loss: 0.1224 - val_accuracy: 0.3435\n",
      "Epoch 787/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0635 - accuracy: 0.2973 - val_loss: 0.1251 - val_accuracy: 0.3597\n",
      "Epoch 788/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0633 - accuracy: 0.3141 - val_loss: 0.1194 - val_accuracy: 0.3273\n",
      "Epoch 789/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0635 - accuracy: 0.2949 - val_loss: 0.1223 - val_accuracy: 0.3579\n",
      "Epoch 790/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0631 - accuracy: 0.2997 - val_loss: 0.1243 - val_accuracy: 0.3579\n",
      "Epoch 791/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0631 - accuracy: 0.3027 - val_loss: 0.1215 - val_accuracy: 0.3579\n",
      "Epoch 792/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0630 - accuracy: 0.3015 - val_loss: 0.1212 - val_accuracy: 0.3687\n",
      "Epoch 793/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0632 - accuracy: 0.3051 - val_loss: 0.1345 - val_accuracy: 0.3399\n",
      "Epoch 794/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0636 - accuracy: 0.3051 - val_loss: 0.1221 - val_accuracy: 0.3741\n",
      "Epoch 795/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0630 - accuracy: 0.3153 - val_loss: 0.1256 - val_accuracy: 0.3345\n",
      "Epoch 796/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0633 - accuracy: 0.3075 - val_loss: 0.1241 - val_accuracy: 0.3201\n",
      "Epoch 797/800\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0629 - accuracy: 0.3039 - val_loss: 0.1264 - val_accuracy: 0.3471\n",
      "Epoch 798/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0630 - accuracy: 0.3069 - val_loss: 0.1248 - val_accuracy: 0.3471\n",
      "Epoch 799/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0629 - accuracy: 0.2997 - val_loss: 0.1239 - val_accuracy: 0.3741\n",
      "Epoch 800/800\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0629 - accuracy: 0.3033 - val_loss: 0.1208 - val_accuracy: 0.3471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-32-f01cf9afd9ce>:131: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "  tprs.append(interp(mean_fpr, fpr, tpr))\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:806: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[555, 0]\n",
      "[0, 0]\n",
      "both b and c are zero\n",
      "[553, 2]\n",
      "[0, 0]\n",
      "[514, 11]\n",
      "[27, 3]\n",
      "[501, 24]\n",
      "[2, 28]\n",
      "[516, 8]\n",
      "[30, 1]\n",
      "[521, 3]\n",
      "[2, 29]\n",
      "[534, 6]\n",
      "[14, 1]\n",
      "[518, 22]\n",
      "[4, 11]\n",
      "[540, 5]\n",
      "[10, 0]\n",
      "[521, 24]\n",
      "[5, 5]\n",
      "[505, 25]\n",
      "[16, 9]\n",
      "[518, 12]\n",
      "[10, 15]\n",
      "[505, 25]\n",
      "[14, 11]\n",
      "[519, 11]\n",
      "[6, 19]\n",
      "[530, 7]\n",
      "[14, 4]\n",
      "[521, 16]\n",
      "[10, 8]\n",
      "[525, 7]\n",
      "[19, 4]\n",
      "[512, 20]\n",
      "[13, 10]\n",
      "[532, 11]\n",
      "[11, 1]\n",
      "[523, 20]\n",
      "[7, 5]\n",
      "[521, 10]\n",
      "[10, 14]\n",
      "[516, 15]\n",
      "[8, 16]\n",
      "[512, 11]\n",
      "[11, 21]\n",
      "[510, 13]\n",
      "[11, 21]\n",
      "[528, 11]\n",
      "[15, 1]\n",
      "[517, 22]\n",
      "[7, 9]\n",
      "[536, 12]\n",
      "[7, 0]\n",
      "[529, 19]\n",
      "[6, 1]\n",
      "[528, 12]\n",
      "[11, 4]\n",
      "[520, 20]\n",
      "[8, 7]\n",
      "[510, 23]\n",
      "[13, 9]\n",
      "[519, 14]\n",
      "[1, 21]\n",
      "[498, 21]\n",
      "[22, 14]\n",
      "[509, 10]\n",
      "[15, 21]\n",
      "[534, 5]\n",
      "[11, 5]\n",
      "[529, 10]\n",
      "[5, 11]\n",
      "[532, 5]\n",
      "[13, 5]\n",
      "[522, 15]\n",
      "[4, 14]\n",
      "[531, 6]\n",
      "[13, 5]\n",
      "[517, 20]\n",
      "[12, 6]\n",
      "[517, 13]\n",
      "[6, 19]\n",
      "[513, 17]\n",
      "[6, 19]\n",
      "[515, 13]\n",
      "[14, 13]\n",
      "[514, 14]\n",
      "[16, 11]\n",
      "[539, 3]\n",
      "[10, 3]\n",
      "[523, 19]\n",
      "[2, 11]\n",
      "[543, 1]\n",
      "[9, 2]\n",
      "[527, 17]\n",
      "[3, 8]\n",
      "[542, 4]\n",
      "[8, 1]\n",
      "[529, 17]\n",
      "[4, 5]\n",
      "[535, 3]\n",
      "[12, 5]\n",
      "[525, 13]\n",
      "[7, 10]\n",
      "[467, 25]\n",
      "[33, 30]\n",
      "[477, 15]\n",
      "[35, 28]\n",
      "[515, 5]\n",
      "[1, 34]\n",
      "[507, 13]\n",
      "[2, 33]\n",
      "[504, 23]\n",
      "[19, 9]\n",
      "[515, 12]\n",
      "[6, 22]\n",
      "[522, 9]\n",
      "[9, 15]\n",
      "[521, 10]\n",
      "[6, 18]\n",
      "[530, 6]\n",
      "[14, 5]\n",
      "[523, 13]\n",
      "[10, 9]\n",
      "[529, 10]\n",
      "[15, 1]\n",
      "[518, 21]\n",
      "[6, 10]\n",
      "[526, 8]\n",
      "[17, 4]\n",
      "[514, 20]\n",
      "[11, 10]\n",
      "[524, 9]\n",
      "[17, 5]\n",
      "[519, 14]\n",
      "[7, 15]\n",
      "[535, 8]\n",
      "[9, 3]\n",
      "[527, 16]\n",
      "[3, 9]\n",
      "[536, 4]\n",
      "[12, 3]\n",
      "[528, 12]\n",
      "[5, 10]\n",
      "[540, 2]\n",
      "[11, 2]\n",
      "[520, 22]\n",
      "[3, 10]\n",
      "[526, 9]\n",
      "[16, 4]\n",
      "[520, 15]\n",
      "[7, 13]\n",
      "[533, 7]\n",
      "[9, 6]\n",
      "[526, 14]\n",
      "[6, 9]\n",
      "[537, 7]\n",
      "[9, 2]\n",
      "[520, 24]\n",
      "[3, 8]\n",
      "[543, 0]\n",
      "[10, 2]\n",
      "[528, 15]\n",
      "[3, 9]\n",
      "[532, 6]\n",
      "[15, 2]\n",
      "[523, 15]\n",
      "[9, 8]\n",
      "[530, 9]\n",
      "[11, 5]\n",
      "[527, 12]\n",
      "[8, 8]\n",
      "[522, 9]\n",
      "[11, 13]\n",
      "[514, 17]\n",
      "[6, 18]\n",
      "[533, 10]\n",
      "[9, 3]\n",
      "[520, 23]\n",
      "[5, 7]\n",
      "[541, 1]\n",
      "[11, 2]\n",
      "[524, 18]\n",
      "[2, 11]\n",
      "[530, 10]\n",
      "[12, 3]\n",
      "[533, 7]\n",
      "[5, 10]\n",
      "[501, 26]\n",
      "[22, 6]\n",
      "[520, 7]\n",
      "[3, 25]\n",
      "[527, 1]\n",
      "[4, 23]\n",
      "[523, 5]\n",
      "[6, 21]\n",
      "[492, 32]\n",
      "[18, 13]\n",
      "[510, 14]\n",
      "[7, 24]\n",
      "[519, 12]\n",
      "[5, 19]\n",
      "[517, 14]\n",
      "[5, 19]\n",
      "------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds of MWPM:\n",
      "> Accuracy: 0.9691681186052723 (+- 0.0008711251039806928)\n",
      "> F1: 0.7114186702930166(+- 0.007889767856740008)\n",
      "> Time: 27.989618380000003 (+- 1.4371095852678075)\n",
      "##############################################################################\n",
      "Average scores for all folds of PLUT:\n",
      "> Accuracy: 0.948868085182089 (+- 0.0006419884522527193)\n",
      "> F1: 0.24029941167832755(+- 0.007819888278286813)\n",
      "> Time: 0.012786200000000001 (+- 0.004797366867772362)\n",
      "##############################################################################\n",
      "Average scores for all folds of NN:\n",
      "> Accuracy: 0.9652622703109364 (+- 0.0011593787306987562)\n",
      "> F1: 0.6415165807556502(+- 0.0110558613157407)\n",
      "> Time: 0.10571185999999999 (+- 0.024252465976275478)\n",
      "##############################################################################\n",
      "> AUC for class : 0.7436823104693141 (+- 0.0)\n",
      "X^2 for MWPM and NN: 0\n",
      "X^2 for PLUT and NN: 0.5\n",
      "> AUC for class X00: 0.7162600625141373 (+- 0.06143420037631798)\n",
      "X^2 for MWPM and NN: 7.605263157894737\n",
      "X^2 for PLUT and NN: 16.96153846153846\n",
      "> AUC for class X01: 0.9282289896335693 (+- 0.01904596037698549)\n",
      "X^2 for MWPM and NN: 13.921052631578947\n",
      "X^2 for PLUT and NN: 0.0\n",
      "> AUC for class X02: 0.9888527050655019 (+- 0.006019676884747292)\n",
      "X^2 for MWPM and NN: 4.05\n",
      "X^2 for PLUT and NN: 11.115384615384615\n",
      "> AUC for class X03: 0.9898481052332114 (+- 0.00671432592833772)\n",
      "X^2 for MWPM and NN: 2.4\n",
      "X^2 for PLUT and NN: 11.172413793103448\n",
      "> AUC for class X04: 0.9415128920595419 (+- 0.016143977203744424)\n",
      "X^2 for MWPM and NN: 1.5609756097560976\n",
      "X^2 for PLUT and NN: 0.045454545454545456\n",
      "> AUC for class X10: 0.9360389425031561 (+- 0.013126093048856247)\n",
      "X^2 for MWPM and NN: 2.5641025641025643\n",
      "X^2 for PLUT and NN: 0.9411764705882353\n",
      "> AUC for class X11: 0.9751851286039794 (+- 0.008908369605124552)\n",
      "X^2 for MWPM and NN: 3.0476190476190474\n",
      "X^2 for PLUT and NN: 0.9615384615384616\n",
      "> AUC for class X12: 0.983133171706101 (+- 0.0026219293543650285)\n",
      "X^2 for MWPM and NN: 6.5\n",
      "X^2 for PLUT and NN: 1.0909090909090908\n",
      "> AUC for class X13: 0.983954639096439 (+- 0.006753559245885509)\n",
      "X^2 for MWPM and NN: 0.045454545454545456\n",
      "X^2 for PLUT and NN: 5.333333333333333\n",
      "> AUC for class X14: 0.9394826553046183 (+- 0.013572927673578993)\n",
      "X^2 for MWPM and NN: 0.05\n",
      "X^2 for PLUT and NN: 1.565217391304348\n",
      "> AUC for class X20: 0.9397595774827447 (+- 0.011873361106792976)\n",
      "X^2 for MWPM and NN: 0.045454545454545456\n",
      "X^2 for PLUT and NN: 0.041666666666666664\n",
      "> AUC for class X21: 0.9812145670210815 (+- 0.006352029511077764)\n",
      "X^2 for MWPM and NN: 0.9615384615384616\n",
      "X^2 for PLUT and NN: 6.758620689655173\n",
      "> AUC for class X22: 0.9857204034543969 (+- 0.007522719423685517)\n",
      "X^2 for MWPM and NN: 0.8421052631578947\n",
      "X^2 for PLUT and NN: 5.76\n",
      "> AUC for class X23: 0.9841999148855114 (+- 0.002585988980265476)\n",
      "X^2 for MWPM and NN: 0.0\n",
      "X^2 for PLUT and NN: 4.321428571428571\n",
      "> AUC for class X24: 0.9392780176219432 (+- 0.023996698688459668)\n",
      "X^2 for MWPM and NN: 2.25\n",
      "X^2 for PLUT and NN: 9.6\n",
      "> AUC for class X30: 0.9393298601077973 (+- 0.014444360811233709)\n",
      "X^2 for MWPM and NN: 0.09302325581395349\n",
      "X^2 for PLUT and NN: 1.44\n",
      "> AUC for class X31: 0.9780464021231765 (+- 0.013022646972223322)\n",
      "X^2 for MWPM and NN: 3.0625\n",
      "X^2 for PLUT and NN: 1.0666666666666667\n",
      "> AUC for class X32: 0.9805082301705783 (+- 0.011514296220151729)\n",
      "X^2 for MWPM and NN: 4.5\n",
      "X^2 for PLUT and NN: 5.2631578947368425\n",
      "> AUC for class X33: 0.9824049493265429 (+- 0.008088119764792179)\n",
      "X^2 for MWPM and NN: 3.3684210526315788\n",
      "X^2 for PLUT and NN: 1.53125\n",
      "> AUC for class X34: 0.9352712017474687 (+- 0.009487559754476732)\n",
      "X^2 for MWPM and NN: 1.894736842105263\n",
      "X^2 for PLUT and NN: 4.3478260869565215\n",
      "> AUC for class X40: 0.9210350951020487 (+- 0.043067605471681876)\n",
      "X^2 for MWPM and NN: 0.14814814814814814\n",
      "X^2 for PLUT and NN: 0.3\n",
      "> AUC for class X41: 0.9839335777443887 (+- 0.006319956114533357)\n",
      "X^2 for MWPM and NN: 4.923076923076923\n",
      "X^2 for PLUT and NN: 12.19047619047619\n",
      "> AUC for class X42: 0.9900971287430602 (+- 0.009735738698660346)\n",
      "X^2 for MWPM and NN: 8.1\n",
      "X^2 for PLUT and NN: 8.45\n",
      "> AUC for class X43: 0.9904840814051366 (+- 0.0035165050994769927)\n",
      "X^2 for MWPM and NN: 2.0833333333333335\n",
      "X^2 for PLUT and NN: 6.857142857142857\n",
      "> AUC for class X44: 0.9709580275123206 (+- 0.011475924916451425)\n",
      "X^2 for MWPM and NN: 6.666666666666667\n",
      "X^2 for PLUT and NN: 1.25\n",
      "> AUC for class Z00: 0.9160914405834116 (+- 0.015403892065637217)\n",
      "X^2 for MWPM and NN: 1.396551724137931\n",
      "X^2 for PLUT and NN: 8.82\n",
      "> AUC for class Z01: 0.9248515109244622 (+- 0.019520790201823617)\n",
      "X^2 for MWPM and NN: 1.5\n",
      "X^2 for PLUT and NN: 6.666666666666667\n",
      "> AUC for class Z02: 0.9314892903895352 (+- 0.024743695612587588)\n",
      "X^2 for MWPM and NN: 0.21428571428571427\n",
      "X^2 for PLUT and NN: 1.3888888888888888\n",
      "> AUC for class Z03: 0.923713218456748 (+- 0.013621804326904583)\n",
      "X^2 for MWPM and NN: 0.05555555555555555\n",
      "X^2 for PLUT and NN: 0.5625\n",
      "> AUC for class Z04: 0.9657068483455677 (+- 0.015466749875719853)\n",
      "X^2 for MWPM and NN: 4.05\n",
      "X^2 for PLUT and NN: 0.17391304347826086\n",
      "> AUC for class Z10: 0.9905040537139227 (+- 0.005564959008014672)\n",
      "X^2 for MWPM and NN: 1.44\n",
      "X^2 for PLUT and NN: 7.2592592592592595\n",
      "> AUC for class Z11: 0.9718313639345448 (+- 0.007176406105658129)\n",
      "X^2 for MWPM and NN: 4.0\n",
      "X^2 for PLUT and NN: 2.064516129032258\n",
      "> AUC for class Z12: 0.9792632736297675 (+- 0.013938755772020106)\n",
      "X^2 for MWPM and NN: 3.1153846153846154\n",
      "X^2 for PLUT and NN: 1.7142857142857142\n",
      "> AUC for class Z13: 0.9796690823085132 (+- 0.013824232690964658)\n",
      "X^2 for MWPM and NN: 0.23529411764705882\n",
      "X^2 for PLUT and NN: 7.578947368421052\n",
      "> AUC for class Z14: 0.9825960602156453 (+- 0.013468197866062058)\n",
      "X^2 for MWPM and NN: 5.0625\n",
      "X^2 for PLUT and NN: 2.1176470588235294\n",
      "> AUC for class Z20: 0.988071531386943 (+- 0.006635579027290153)\n",
      "X^2 for MWPM and NN: 7.6923076923076925\n",
      "X^2 for PLUT and NN: 12.96\n",
      "> AUC for class Z21: 0.9860558088582921 (+- 0.0040307278030589175)\n",
      "X^2 for MWPM and NN: 2.56\n",
      "X^2 for PLUT and NN: 2.227272727272727\n",
      "> AUC for class Z22: 0.9851520969918065 (+- 0.0025169854685645706)\n",
      "X^2 for MWPM and NN: 0.5625\n",
      "X^2 for PLUT and NN: 2.45\n",
      "> AUC for class Z23: 0.9851709565186304 (+- 0.003734014394813754)\n",
      "X^2 for MWPM and NN: 0.5625\n",
      "X^2 for PLUT and NN: 14.814814814814815\n",
      "> AUC for class Z24: 0.98863084997685 (+- 0.005996951111774807)\n",
      "X^2 for MWPM and NN: 12.1\n",
      "X^2 for PLUT and NN: 6.722222222222222\n",
      "> AUC for class Z30: 0.9784976634215639 (+- 0.011173876434172158)\n",
      "X^2 for MWPM and NN: 4.761904761904762\n",
      "X^2 for PLUT and NN: 1.0416666666666667\n",
      "> AUC for class Z31: 0.9833056782067215 (+- 0.006485718380107338)\n",
      "X^2 for MWPM and NN: 0.45\n",
      "X^2 for PLUT and NN: 0.45\n",
      "> AUC for class Z32: 0.9776885958722368 (+- 0.0070623571906714335)\n",
      "X^2 for MWPM and NN: 0.45\n",
      "X^2 for PLUT and NN: 4.3478260869565215\n",
      "> AUC for class Z33: 0.9835785685620249 (+- 0.007432608193884995)\n",
      "X^2 for MWPM and NN: 0.0\n",
      "X^2 for PLUT and NN: 10.321428571428571\n",
      "> AUC for class Z34: 0.9845037892315714 (+- 0.007227811781038365)\n",
      "X^2 for MWPM and NN: 10.083333333333334\n",
      "X^2 for PLUT and NN: 11.25\n",
      "> AUC for class Z40: 0.9582266181125701 (+- 0.012998655124629626)\n",
      "X^2 for MWPM and NN: 0.4090909090909091\n",
      "X^2 for PLUT and NN: 0.08333333333333333\n",
      "> AUC for class Z41: 0.9297861316327005 (+- 0.01824590199312008)\n",
      "X^2 for MWPM and NN: 0.1875\n",
      "X^2 for PLUT and NN: 0.9\n",
      "> AUC for class Z42: 0.9234844776483195 (+- 0.037463046723650376)\n",
      "X^2 for MWPM and NN: 3.2\n",
      "X^2 for PLUT and NN: 0.36363636363636365\n",
      "> AUC for class Z43: 0.9227435315517921 (+- 0.027569571120780498)\n",
      "X^2 for MWPM and NN: 3.38\n",
      "X^2 for PLUT and NN: 1.7142857142857142\n",
      "> AUC for class Z44: 0.9354472725863037 (+- 0.019358791732531067)\n",
      "X^2 for MWPM and NN: 2.1176470588235294\n",
      "X^2 for PLUT and NN: 3.3684210526315788\n",
      "###################################################################################\n",
      "TOTAL F1 NN: [0.6417744916820701, 0.6472019464720195, 0.6497953107554895, 0.6487076811066618, 0.6201034737620104]\n",
      "TOTAL F1 PLUT: [0.2548505506030414, 0.24166666666666667, 0.23725286160249737, 0.23397435897435898, 0.23375262054507337]\n",
      "TOTAL F1 MWPM: [0.7173476222371065, 0.7168814007267922, 0.6976897689768977, 0.7073416024136776, 0.7178329571106095]\n",
      "TOTAL ACC NN: [0.9658273381294952, 0.9641406111994335, 0.9667549902844021, 0.9659070835541417, 0.9636813283872091]\n",
      "TOTAL ACC PLUT: [0.9498871491042413, 0.9485603250309097, 0.948207030559968, 0.9493375728669807, 0.9483483483483449]\n",
      "TOTAL ACC MWPM: [0.9702355762448841, 0.9697226638403087, 0.9676382264617533, 0.9691573926868026, 0.9690867337926126]\n",
      "TOTAL TIME NN: [0.0999791, 0.107711, 0.1505963, 0.0897593, 0.0805136]\n",
      "TOTAL TIME PLUT: [0.0156224, 0.0119675, 0.0061435, 0.0201751, 0.0100225]\n",
      "TOTAL TIME MWPM: [26.808406900000026, 29.384822299999964, 27.58337440000001, 29.929795499999994, 26.241692799999996]\n",
      "------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "inputs = x_d5.copy()\n",
    "inputs = np.array(inputs)\n",
    "inputs = np.stack(inputs)\n",
    "targets = np.array(y_d5)\n",
    "#targets = np.stack(targets)\n",
    "targets = np.array([np.array(xi) for xi in targets])\n",
    "\n",
    "\n",
    "n_classes = 51\n",
    "\n",
    "acc_per_fold = []\n",
    "f1_per_fold = []\n",
    "time_per_fold = []\n",
    "\n",
    "acc_per_fold_plut = []\n",
    "f1_per_fold_plut = []\n",
    "time_per_fold_plut = []\n",
    "\n",
    "acc_per_fold_mwpm = []\n",
    "f1_per_fold_mwpm = []\n",
    "time_per_fold_mwpm = []\n",
    "\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "aucs_classes = {}\n",
    "for i in mlb.classes_:\n",
    "    aucs_classes[i] = []\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "for i, (train, test) in enumerate(kfold.split(inputs, targets)):\n",
    "    i_train = inputs[train].copy()\n",
    "    i_test = inputs[test].copy()\n",
    "    x_test_d5 = i_test[:,:2]\n",
    "    inputs_train = i_train[:,3:]\n",
    "    inputs_test = i_test[:,3:]\n",
    "    \n",
    "    ####################################################################################################\n",
    "    #test MWPM decoder for this fold\n",
    "    #labels = targets[train], features = inputs[train]\n",
    "    #x_test_d5 = translate_to_graph(testData_d5_MWPM, targets[test], mlb)\n",
    "    decoding_d5, time_mwpm = do_new_decoding(x_test_d5, 5, 0)\n",
    "    decoding_d5['combine'] = decoding_d5[[0, 1]].values.tolist()\n",
    "    decoding_d5['combine'].apply(lambda x: x[0].extend(x[1]))\n",
    "    decoding_d5 = np.array(decoding_d5[0])\n",
    "                                              \n",
    "    time_per_fold_mwpm.append(time_mwpm)\n",
    "                                              \n",
    "    pred_mwpm = mlb.transform(decoding_d5)\n",
    "    if fold_no < 5:\n",
    "        acc = partial_accuracy(targets[test], pred_mwpm)\n",
    "    else:\n",
    "        acc, contingency_mwpm = partial_accuracy_and_contingency(targets[test], pred_mwpm, mlb)\n",
    "        \n",
    "    acc_per_fold_mwpm.append(acc)\n",
    "    f1_per_fold_mwpm.append(f1_score(targets[test], pred_mwpm, average='micro'))\n",
    "    \n",
    "    #####################################################################################################\n",
    "    #test the plut decoder for this fold\n",
    "    \n",
    "    lookup_d5 = lookup_decoder(5)\n",
    "    \n",
    "    lookup_d5 = train_plut(lookup_d5, inputs_train, targets[train])\n",
    "    \n",
    "    start = time.time_ns()\n",
    "    pred_plut_d5 = test_plut(lookup_d5, inputs_test)\n",
    "    end = time.time_ns() \n",
    "    time_per_fold_plut.append((end - start)/ (10 ** 9))\n",
    "    \n",
    "    if fold_no < 5:\n",
    "        acc = partial_accuracy(targets[test], pred_plut_d5)\n",
    "    else:\n",
    "        acc, contingency_plut = partial_accuracy_and_contingency(targets[test], pred_plut_d5, mlb)\n",
    "        \n",
    "    acc_per_fold_plut.append(acc)\n",
    "    f1_per_fold_plut.append(f1_score(targets[test], pred_plut_d5, average='micro'))\n",
    "\n",
    "    #####################################################################################################\n",
    "    #Test the NN decoder for this fold\n",
    "    \n",
    "    model_d5 = compile_FFNN_model_DepthFive(5)\n",
    "    # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    # Fit data to model\n",
    "    \n",
    "    inputs_train = np.asarray(inputs_train).astype(np.int) #CHANGE (added)\n",
    "    targets[train] = np.asarray(targets[train]).astype(np.int) #CHANGE (added)\n",
    "    \n",
    "    history = model_d5.fit(\n",
    "    inputs_train,\n",
    "    targets[train],\n",
    "    validation_split=.25,\n",
    "    epochs = 800\n",
    ")\n",
    "   # Generate generalization metrics\n",
    "\n",
    "    inputs_test = np.asarray(inputs_test).astype(np.int) #CHANGE (added)\n",
    "    targets[test] = np.asarray(targets[test]).astype(np.int) #CHANGE (added)\n",
    "    scores = model_d5.evaluate(inputs_test, targets[test], verbose=0)\n",
    "    \n",
    "    start = time.time_ns()\n",
    "    predictions_d5 = model_d5.predict(inputs_test)\n",
    "    end = time.time_ns() \n",
    "    time_per_fold.append((end - start)/ (10 ** 9))\n",
    "    \n",
    "    #threshold based on previous tests with train_test_split in hyperparameter tuning\n",
    "    pred=predictions_d5.copy() #change here\n",
    "    pred[pred>=.4]=1 \n",
    "    pred[pred<.4]=0\n",
    "    \n",
    "    if fold_no < 5:\n",
    "        acc = scores[1]\n",
    "    else:\n",
    "        acc, contingency_nn = partial_accuracy_and_contingency(targets[test], pred, mlb)\n",
    "\n",
    "    acc_per_fold.append(acc)\n",
    "    f1_per_fold.append(f1_score(targets[test], pred, average='micro'))\n",
    "\n",
    "    \n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(targets[test].ravel(), predictions_d5.ravel())\n",
    "    tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "    tprs[-1][0] = 0.0\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "    plt.plot(fpr, tpr, lw=1, alpha=0.3, label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
    "    \n",
    "    #get the AUCs of each class, used to get average AUC of each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(targets[test][:, i], predictions_d5[:, i]) \n",
    "        aucs_classes[mlb.classes_[i]].append(auc(fpr[i], tpr[i]))\n",
    "        \n",
    "        \n",
    "#########################################################################################################\n",
    "#compute McNemar's statistic on results of last fold\n",
    "\n",
    "mcnemar_results_mwpm = {}\n",
    "mcnemar_results_plut = {}\n",
    "\n",
    "for class_ in mlb.classes_:\n",
    "    \n",
    "    #compute the x^2 for NN and MWPM\n",
    "    mcnemar_results_mwpm[class_]=contingency_table_and_t(contingency_nn[class_], contingency_mwpm[class_])[1]    \n",
    "    #comput the x^2 for NN and PLUT\n",
    "    mcnemar_results_plut[class_] = contingency_table_and_t(contingency_nn[class_], contingency_plut[class_])[1]\n",
    "        \n",
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds of MWPM:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold_mwpm)} (+- {np.std(acc_per_fold_mwpm)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold_mwpm)}(+- {np.std(f1_per_fold_mwpm)})')\n",
    "print(f'> Time: {np.mean(time_per_fold_mwpm)} (+- {np.std(time_per_fold_mwpm)})')\n",
    "print(\"##############################################################################\")\n",
    "print('Average scores for all folds of PLUT:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold_plut)} (+- {np.std(acc_per_fold_plut)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold_plut)}(+- {np.std(f1_per_fold_plut)})')\n",
    "print(f'> Time: {np.mean(time_per_fold_plut)} (+- {np.std(time_per_fold_plut)})')\n",
    "print(\"##############################################################################\")\n",
    "print('Average scores for all folds of NN:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold)}(+- {np.std(f1_per_fold)})')\n",
    "print(f'> Time: {np.mean(time_per_fold)} (+- {np.std(time_per_fold)})')\n",
    "print(\"##############################################################################\")\n",
    "for key in aucs_classes:\n",
    "    cleanedList = [x for x in aucs_classes[key] if str(x) != 'nan']\n",
    "    print(f'> AUC for class {key}: {np.mean(cleanedList)} (+- {np.std(cleanedList)})') #this has to be a for loop\n",
    "    print(\"X^2 for MWPM and NN: \" + str(mcnemar_results_mwpm[key]))  \n",
    "    print(\"X^2 for PLUT and NN: \" + str(mcnemar_results_plut[key]))\n",
    "    \n",
    "print(\"###################################################################################\")\n",
    "print(\"TOTAL F1 NN: \" + str(f1_per_fold))\n",
    "print(\"TOTAL F1 PLUT: \" + str(f1_per_fold_plut))\n",
    "print(\"TOTAL F1 MWPM: \" + str(f1_per_fold_mwpm))\n",
    "print(\"TOTAL ACC NN: \" + str(acc_per_fold))\n",
    "print(\"TOTAL ACC PLUT: \" + str(acc_per_fold_plut))\n",
    "print(\"TOTAL ACC MWPM: \" + str(acc_per_fold_mwpm))\n",
    "print(\"TOTAL TIME NN: \" + str(time_per_fold))\n",
    "print(\"TOTAL TIME PLUT: \" + str(time_per_fold_plut))\n",
    "print(\"TOTAL TIME MWPM: \" + str(time_per_fold_mwpm))\n",
    "print('------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "         label='Chance', alpha=.8)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "         label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "         lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                 label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "plt.xlim([-0.01, 1.01])\n",
    "plt.ylim([-0.01, 1.01])\n",
    "plt.xlabel('False Positive Rate',fontsize=18)\n",
    "plt.ylabel('True Positive Rate',fontsize=18)\n",
    "plt.title('Cross-Validation ROC of Depth 5 NN',fontsize=18)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Epoch 1/150\n",
      "7941/7941 [==============================] - 41s 5ms/step - loss: 0.1617 - accuracy: 0.0542 - val_loss: 0.1857 - val_accuracy: 0.1692\n",
      "Epoch 2/150\n",
      "7941/7941 [==============================] - 41s 5ms/step - loss: 0.1399 - accuracy: 0.1539 - val_loss: 0.1478 - val_accuracy: 0.2982\n",
      "Epoch 3/150\n",
      "7941/7941 [==============================] - 41s 5ms/step - loss: 0.1092 - accuracy: 0.2230 - val_loss: 0.1194 - val_accuracy: 0.3197\n",
      "Epoch 4/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0909 - accuracy: 0.2345 - val_loss: 0.1037 - val_accuracy: 0.3242\n",
      "Epoch 5/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0798 - accuracy: 0.2371 - val_loss: 0.0952 - val_accuracy: 0.3254\n",
      "Epoch 6/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0727 - accuracy: 0.2345 - val_loss: 0.0892 - val_accuracy: 0.3173\n",
      "Epoch 7/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0676 - accuracy: 0.2315 - val_loss: 0.0847 - val_accuracy: 0.3108\n",
      "Epoch 8/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0637 - accuracy: 0.2295 - val_loss: 0.0811 - val_accuracy: 0.3170\n",
      "Epoch 9/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0607 - accuracy: 0.2307 - val_loss: 0.0779 - val_accuracy: 0.3144\n",
      "Epoch 10/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0582 - accuracy: 0.2309 - val_loss: 0.0755 - val_accuracy: 0.3161\n",
      "Epoch 11/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0561 - accuracy: 0.2314 - val_loss: 0.0732 - val_accuracy: 0.3285\n",
      "Epoch 12/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0543 - accuracy: 0.2321 - val_loss: 0.0715 - val_accuracy: 0.3183\n",
      "Epoch 13/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0527 - accuracy: 0.2322 - val_loss: 0.0689 - val_accuracy: 0.3295\n",
      "Epoch 14/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0513 - accuracy: 0.2329 - val_loss: 0.0674 - val_accuracy: 0.3306\n",
      "Epoch 15/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0501 - accuracy: 0.2341 - val_loss: 0.0657 - val_accuracy: 0.3295\n",
      "Epoch 16/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0491 - accuracy: 0.2339 - val_loss: 0.0649 - val_accuracy: 0.3254\n",
      "Epoch 17/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0482 - accuracy: 0.2344 - val_loss: 0.0633 - val_accuracy: 0.3167\n",
      "Epoch 18/150\n",
      "7941/7941 [==============================] - 44s 6ms/step - loss: 0.0474 - accuracy: 0.2341 - val_loss: 0.0627 - val_accuracy: 0.3176\n",
      "Epoch 19/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0467 - accuracy: 0.2346 - val_loss: 0.0619 - val_accuracy: 0.3164\n",
      "Epoch 20/150\n",
      "7941/7941 [==============================] - 45s 6ms/step - loss: 0.0461 - accuracy: 0.2348 - val_loss: 0.0607 - val_accuracy: 0.3215\n",
      "Epoch 21/150\n",
      "7941/7941 [==============================] - 44s 6ms/step - loss: 0.0455 - accuracy: 0.2358 - val_loss: 0.0601 - val_accuracy: 0.3125\n",
      "Epoch 22/150\n",
      "7941/7941 [==============================] - 44s 5ms/step - loss: 0.0450 - accuracy: 0.2346 - val_loss: 0.0600 - val_accuracy: 0.3153\n",
      "Epoch 23/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0445 - accuracy: 0.2349 - val_loss: 0.0589 - val_accuracy: 0.3175\n",
      "Epoch 24/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0440 - accuracy: 0.2348 - val_loss: 0.0592 - val_accuracy: 0.3247\n",
      "Epoch 25/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0436 - accuracy: 0.2350 - val_loss: 0.0578 - val_accuracy: 0.3120\n",
      "Epoch 26/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0432 - accuracy: 0.2349 - val_loss: 0.0570 - val_accuracy: 0.3041\n",
      "Epoch 27/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0428 - accuracy: 0.2339 - val_loss: 0.0565 - val_accuracy: 0.3150\n",
      "Epoch 28/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0424 - accuracy: 0.2341 - val_loss: 0.0566 - val_accuracy: 0.3273\n",
      "Epoch 29/150\n",
      "7941/7941 [==============================] - 44s 5ms/step - loss: 0.0420 - accuracy: 0.2343 - val_loss: 0.0561 - val_accuracy: 0.3259\n",
      "Epoch 30/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0417 - accuracy: 0.2338 - val_loss: 0.0553 - val_accuracy: 0.3192\n",
      "Epoch 31/150\n",
      "7941/7941 [==============================] - 44s 5ms/step - loss: 0.0414 - accuracy: 0.2344 - val_loss: 0.0543 - val_accuracy: 0.3147\n",
      "Epoch 32/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0411 - accuracy: 0.2332 - val_loss: 0.0550 - val_accuracy: 0.3329\n",
      "Epoch 33/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0408 - accuracy: 0.2342 - val_loss: 0.0541 - val_accuracy: 0.3026\n",
      "Epoch 34/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0405 - accuracy: 0.2345 - val_loss: 0.0539 - val_accuracy: 0.3236\n",
      "Epoch 35/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0402 - accuracy: 0.2335 - val_loss: 0.0534 - val_accuracy: 0.3206\n",
      "Epoch 36/150\n",
      "7941/7941 [==============================] - 46s 6ms/step - loss: 0.0400 - accuracy: 0.2342 - val_loss: 0.0541 - val_accuracy: 0.3204\n",
      "Epoch 37/150\n",
      "7941/7941 [==============================] - 45s 6ms/step - loss: 0.0397 - accuracy: 0.2337 - val_loss: 0.0538 - val_accuracy: 0.3357\n",
      "Epoch 38/150\n",
      "7941/7941 [==============================] - 46s 6ms/step - loss: 0.0395 - accuracy: 0.2340 - val_loss: 0.0530 - val_accuracy: 0.3215\n",
      "Epoch 39/150\n",
      "7941/7941 [==============================] - 46s 6ms/step - loss: 0.0393 - accuracy: 0.2332 - val_loss: 0.0529 - val_accuracy: 0.3354\n",
      "Epoch 40/150\n",
      "7941/7941 [==============================] - 45s 6ms/step - loss: 0.0390 - accuracy: 0.2334 - val_loss: 0.0526 - val_accuracy: 0.3218\n",
      "Epoch 41/150\n",
      "7941/7941 [==============================] - 46s 6ms/step - loss: 0.0388 - accuracy: 0.2333 - val_loss: 0.0522 - val_accuracy: 0.3131\n",
      "Epoch 42/150\n",
      "7941/7941 [==============================] - 46s 6ms/step - loss: 0.0386 - accuracy: 0.2334 - val_loss: 0.0523 - val_accuracy: 0.3263\n",
      "Epoch 43/150\n",
      "7941/7941 [==============================] - 45s 6ms/step - loss: 0.0384 - accuracy: 0.2332 - val_loss: 0.0523 - val_accuracy: 0.3520\n",
      "Epoch 44/150\n",
      "7941/7941 [==============================] - 45s 6ms/step - loss: 0.0382 - accuracy: 0.2333 - val_loss: 0.0514 - val_accuracy: 0.3285\n",
      "Epoch 45/150\n",
      "7941/7941 [==============================] - 46s 6ms/step - loss: 0.0380 - accuracy: 0.2339 - val_loss: 0.0515 - val_accuracy: 0.3251\n",
      "Epoch 46/150\n",
      "7941/7941 [==============================] - 46s 6ms/step - loss: 0.0379 - accuracy: 0.2326 - val_loss: 0.0517 - val_accuracy: 0.3124\n",
      "Epoch 47/150\n",
      "7941/7941 [==============================] - 45s 6ms/step - loss: 0.0377 - accuracy: 0.2328 - val_loss: 0.0513 - val_accuracy: 0.3046\n",
      "Epoch 48/150\n",
      "7941/7941 [==============================] - 45s 6ms/step - loss: 0.0375 - accuracy: 0.2331 - val_loss: 0.0507 - val_accuracy: 0.3180\n",
      "Epoch 49/150\n",
      "7941/7941 [==============================] - 45s 6ms/step - loss: 0.0374 - accuracy: 0.2331 - val_loss: 0.0503 - val_accuracy: 0.3051\n",
      "Epoch 50/150\n",
      "7941/7941 [==============================] - 45s 6ms/step - loss: 0.0372 - accuracy: 0.2329 - val_loss: 0.0509 - val_accuracy: 0.3347\n",
      "Epoch 51/150\n",
      "7941/7941 [==============================] - 45s 6ms/step - loss: 0.0370 - accuracy: 0.2327 - val_loss: 0.0502 - val_accuracy: 0.2966\n",
      "Epoch 52/150\n",
      "7941/7941 [==============================] - 45s 6ms/step - loss: 0.0369 - accuracy: 0.2327 - val_loss: 0.0498 - val_accuracy: 0.3115\n",
      "Epoch 53/150\n",
      "7941/7941 [==============================] - 45s 6ms/step - loss: 0.0367 - accuracy: 0.2325 - val_loss: 0.0496 - val_accuracy: 0.3184\n",
      "Epoch 54/150\n",
      "7941/7941 [==============================] - 45s 6ms/step - loss: 0.0366 - accuracy: 0.2329 - val_loss: 0.0498 - val_accuracy: 0.3350\n",
      "Epoch 55/150\n",
      "7941/7941 [==============================] - 45s 6ms/step - loss: 0.0364 - accuracy: 0.2322 - val_loss: 0.0505 - val_accuracy: 0.3116\n",
      "Epoch 56/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0363 - accuracy: 0.2324 - val_loss: 0.0492 - val_accuracy: 0.3066\n",
      "Epoch 57/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0362 - accuracy: 0.2317 - val_loss: 0.0491 - val_accuracy: 0.2887\n",
      "Epoch 58/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0360 - accuracy: 0.2326 - val_loss: 0.0493 - val_accuracy: 0.3063\n",
      "Epoch 59/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0359 - accuracy: 0.2324 - val_loss: 0.0496 - val_accuracy: 0.3034\n",
      "Epoch 60/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0358 - accuracy: 0.2314 - val_loss: 0.0492 - val_accuracy: 0.3074\n",
      "Epoch 61/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0357 - accuracy: 0.2309 - val_loss: 0.0490 - val_accuracy: 0.3240\n",
      "Epoch 62/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0356 - accuracy: 0.2317 - val_loss: 0.0483 - val_accuracy: 0.3131\n",
      "Epoch 63/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0354 - accuracy: 0.2311 - val_loss: 0.0489 - val_accuracy: 0.2952\n",
      "Epoch 64/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0353 - accuracy: 0.2314 - val_loss: 0.0480 - val_accuracy: 0.3125\n",
      "Epoch 65/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0352 - accuracy: 0.2314 - val_loss: 0.0484 - val_accuracy: 0.2978\n",
      "Epoch 66/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0351 - accuracy: 0.2305 - val_loss: 0.0478 - val_accuracy: 0.3120\n",
      "Epoch 67/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0350 - accuracy: 0.2310 - val_loss: 0.0473 - val_accuracy: 0.3081\n",
      "Epoch 68/150\n",
      "7941/7941 [==============================] - 44s 5ms/step - loss: 0.0349 - accuracy: 0.2309 - val_loss: 0.0480 - val_accuracy: 0.3313\n",
      "Epoch 69/150\n",
      "7941/7941 [==============================] - 46s 6ms/step - loss: 0.0348 - accuracy: 0.2301 - val_loss: 0.0478 - val_accuracy: 0.3107\n",
      "Epoch 70/150\n",
      "7941/7941 [==============================] - 46s 6ms/step - loss: 0.0347 - accuracy: 0.2310 - val_loss: 0.0479 - val_accuracy: 0.2966\n",
      "Epoch 71/150\n",
      "7941/7941 [==============================] - 46s 6ms/step - loss: 0.0346 - accuracy: 0.2309 - val_loss: 0.0472 - val_accuracy: 0.3096\n",
      "Epoch 72/150\n",
      "7941/7941 [==============================] - 46s 6ms/step - loss: 0.0345 - accuracy: 0.2304 - val_loss: 0.0477 - val_accuracy: 0.3086\n",
      "Epoch 73/150\n",
      "7941/7941 [==============================] - 45s 6ms/step - loss: 0.0344 - accuracy: 0.2315 - val_loss: 0.0472 - val_accuracy: 0.3234\n",
      "Epoch 74/150\n",
      "7941/7941 [==============================] - 46s 6ms/step - loss: 0.0343 - accuracy: 0.2307 - val_loss: 0.0470 - val_accuracy: 0.3110\n",
      "Epoch 75/150\n",
      "7941/7941 [==============================] - 46s 6ms/step - loss: 0.0342 - accuracy: 0.2309 - val_loss: 0.0475 - val_accuracy: 0.2948\n",
      "Epoch 76/150\n",
      "7941/7941 [==============================] - 46s 6ms/step - loss: 0.0341 - accuracy: 0.2310 - val_loss: 0.0471 - val_accuracy: 0.3018\n",
      "Epoch 77/150\n",
      "7941/7941 [==============================] - 46s 6ms/step - loss: 0.0340 - accuracy: 0.2302 - val_loss: 0.0468 - val_accuracy: 0.3110\n",
      "Epoch 78/150\n",
      "7941/7941 [==============================] - 46s 6ms/step - loss: 0.0339 - accuracy: 0.2310 - val_loss: 0.0470 - val_accuracy: 0.3115\n",
      "Epoch 79/150\n",
      "7941/7941 [==============================] - 46s 6ms/step - loss: 0.0338 - accuracy: 0.2308 - val_loss: 0.0467 - val_accuracy: 0.3093\n",
      "Epoch 80/150\n",
      "7941/7941 [==============================] - 46s 6ms/step - loss: 0.0338 - accuracy: 0.2310 - val_loss: 0.0461 - val_accuracy: 0.3019\n",
      "Epoch 81/150\n",
      "7941/7941 [==============================] - 46s 6ms/step - loss: 0.0337 - accuracy: 0.2301 - val_loss: 0.0464 - val_accuracy: 0.3194\n",
      "Epoch 82/150\n",
      "7941/7941 [==============================] - 46s 6ms/step - loss: 0.0336 - accuracy: 0.2313 - val_loss: 0.0467 - val_accuracy: 0.3077\n",
      "Epoch 83/150\n",
      "7941/7941 [==============================] - 46s 6ms/step - loss: 0.0335 - accuracy: 0.2310 - val_loss: 0.0468 - val_accuracy: 0.3164\n",
      "Epoch 84/150\n",
      "7941/7941 [==============================] - 45s 6ms/step - loss: 0.0334 - accuracy: 0.2314 - val_loss: 0.0465 - val_accuracy: 0.3217\n",
      "Epoch 85/150\n",
      "7941/7941 [==============================] - 46s 6ms/step - loss: 0.0334 - accuracy: 0.2300 - val_loss: 0.0462 - val_accuracy: 0.3146\n",
      "Epoch 86/150\n",
      "7941/7941 [==============================] - 46s 6ms/step - loss: 0.0333 - accuracy: 0.2305 - val_loss: 0.0467 - val_accuracy: 0.3281\n",
      "Epoch 87/150\n",
      "7941/7941 [==============================] - 46s 6ms/step - loss: 0.0332 - accuracy: 0.2312 - val_loss: 0.0457 - val_accuracy: 0.3151\n",
      "Epoch 88/150\n",
      "7941/7941 [==============================] - 46s 6ms/step - loss: 0.0331 - accuracy: 0.2314 - val_loss: 0.0458 - val_accuracy: 0.3128\n",
      "Epoch 89/150\n",
      "7941/7941 [==============================] - 45s 6ms/step - loss: 0.0331 - accuracy: 0.2306 - val_loss: 0.0458 - val_accuracy: 0.3040\n",
      "Epoch 90/150\n",
      "7941/7941 [==============================] - 45s 6ms/step - loss: 0.0330 - accuracy: 0.2316 - val_loss: 0.0457 - val_accuracy: 0.3133\n",
      "Epoch 91/150\n",
      "7941/7941 [==============================] - 46s 6ms/step - loss: 0.0329 - accuracy: 0.2305 - val_loss: 0.0462 - val_accuracy: 0.3099\n",
      "Epoch 92/150\n",
      "7941/7941 [==============================] - 45s 6ms/step - loss: 0.0329 - accuracy: 0.2315 - val_loss: 0.0456 - val_accuracy: 0.3194\n",
      "Epoch 93/150\n",
      "7941/7941 [==============================] - 45s 6ms/step - loss: 0.0328 - accuracy: 0.2313 - val_loss: 0.0456 - val_accuracy: 0.2938\n",
      "Epoch 94/150\n",
      "7941/7941 [==============================] - 45s 6ms/step - loss: 0.0327 - accuracy: 0.2323 - val_loss: 0.0447 - val_accuracy: 0.2961\n",
      "Epoch 95/150\n",
      "7941/7941 [==============================] - 45s 6ms/step - loss: 0.0326 - accuracy: 0.2308 - val_loss: 0.0458 - val_accuracy: 0.3158\n",
      "Epoch 96/150\n",
      "7941/7941 [==============================] - 45s 6ms/step - loss: 0.0326 - accuracy: 0.2317 - val_loss: 0.0452 - val_accuracy: 0.3063\n",
      "Epoch 97/150\n",
      "7941/7941 [==============================] - 45s 6ms/step - loss: 0.0325 - accuracy: 0.2307 - val_loss: 0.0453 - val_accuracy: 0.3235\n",
      "Epoch 98/150\n",
      "7941/7941 [==============================] - 46s 6ms/step - loss: 0.0325 - accuracy: 0.2319 - val_loss: 0.0462 - val_accuracy: 0.3156\n",
      "Epoch 99/150\n",
      "7941/7941 [==============================] - 46s 6ms/step - loss: 0.0324 - accuracy: 0.2306 - val_loss: 0.0456 - val_accuracy: 0.3129\n",
      "Epoch 100/150\n",
      "7941/7941 [==============================] - 47s 6ms/step - loss: 0.0323 - accuracy: 0.2319 - val_loss: 0.0452 - val_accuracy: 0.3115\n",
      "Epoch 101/150\n",
      "7941/7941 [==============================] - 46s 6ms/step - loss: 0.0323 - accuracy: 0.2317 - val_loss: 0.0445 - val_accuracy: 0.3252\n",
      "Epoch 102/150\n",
      "7941/7941 [==============================] - 45s 6ms/step - loss: 0.0322 - accuracy: 0.2325 - val_loss: 0.0447 - val_accuracy: 0.3055\n",
      "Epoch 103/150\n",
      "7941/7941 [==============================] - 45s 6ms/step - loss: 0.0321 - accuracy: 0.2321 - val_loss: 0.0448 - val_accuracy: 0.3032\n",
      "Epoch 104/150\n",
      "7941/7941 [==============================] - 46s 6ms/step - loss: 0.0321 - accuracy: 0.2323 - val_loss: 0.0450 - val_accuracy: 0.3032\n",
      "Epoch 105/150\n",
      "7941/7941 [==============================] - 45s 6ms/step - loss: 0.0320 - accuracy: 0.2323 - val_loss: 0.0440 - val_accuracy: 0.3061\n",
      "Epoch 106/150\n",
      "7941/7941 [==============================] - 45s 6ms/step - loss: 0.0320 - accuracy: 0.2327 - val_loss: 0.0446 - val_accuracy: 0.3079\n",
      "Epoch 107/150\n",
      "7941/7941 [==============================] - 45s 6ms/step - loss: 0.0319 - accuracy: 0.2315 - val_loss: 0.0458 - val_accuracy: 0.3080\n",
      "Epoch 108/150\n",
      "7941/7941 [==============================] - 46s 6ms/step - loss: 0.0319 - accuracy: 0.2327 - val_loss: 0.0447 - val_accuracy: 0.3110\n",
      "Epoch 109/150\n",
      "7941/7941 [==============================] - 45s 6ms/step - loss: 0.0318 - accuracy: 0.2322 - val_loss: 0.0439 - val_accuracy: 0.3066\n",
      "Epoch 110/150\n",
      "7941/7941 [==============================] - 45s 6ms/step - loss: 0.0317 - accuracy: 0.2330 - val_loss: 0.0448 - val_accuracy: 0.3164\n",
      "Epoch 111/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0317 - accuracy: 0.2322 - val_loss: 0.0443 - val_accuracy: 0.2984\n",
      "Epoch 112/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0316 - accuracy: 0.2334 - val_loss: 0.0448 - val_accuracy: 0.3173\n",
      "Epoch 113/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0316 - accuracy: 0.2325 - val_loss: 0.0442 - val_accuracy: 0.3122\n",
      "Epoch 114/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0315 - accuracy: 0.2336 - val_loss: 0.0439 - val_accuracy: 0.3242\n",
      "Epoch 115/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0315 - accuracy: 0.2338 - val_loss: 0.0449 - val_accuracy: 0.3155\n",
      "Epoch 116/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0314 - accuracy: 0.2334 - val_loss: 0.0438 - val_accuracy: 0.3023\n",
      "Epoch 117/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0314 - accuracy: 0.2338 - val_loss: 0.0443 - val_accuracy: 0.3016\n",
      "Epoch 118/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0313 - accuracy: 0.2330 - val_loss: 0.0439 - val_accuracy: 0.3139\n",
      "Epoch 119/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0313 - accuracy: 0.2336 - val_loss: 0.0442 - val_accuracy: 0.3064\n",
      "Epoch 120/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0312 - accuracy: 0.2336 - val_loss: 0.0445 - val_accuracy: 0.3314\n",
      "Epoch 121/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0312 - accuracy: 0.2338 - val_loss: 0.0441 - val_accuracy: 0.3152\n",
      "Epoch 122/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0311 - accuracy: 0.2339 - val_loss: 0.0438 - val_accuracy: 0.3123\n",
      "Epoch 123/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0311 - accuracy: 0.2339 - val_loss: 0.0437 - val_accuracy: 0.3170\n",
      "Epoch 124/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0310 - accuracy: 0.2338 - val_loss: 0.0436 - val_accuracy: 0.3189\n",
      "Epoch 125/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0310 - accuracy: 0.2341 - val_loss: 0.0438 - val_accuracy: 0.3019\n",
      "Epoch 126/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0309 - accuracy: 0.2349 - val_loss: 0.0442 - val_accuracy: 0.3322\n",
      "Epoch 127/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0309 - accuracy: 0.2340 - val_loss: 0.0440 - val_accuracy: 0.3307\n",
      "Epoch 128/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0308 - accuracy: 0.2340 - val_loss: 0.0441 - val_accuracy: 0.3258\n",
      "Epoch 129/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0308 - accuracy: 0.2355 - val_loss: 0.0430 - val_accuracy: 0.3012\n",
      "Epoch 130/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0308 - accuracy: 0.2345 - val_loss: 0.0430 - val_accuracy: 0.3238\n",
      "Epoch 131/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0307 - accuracy: 0.2344 - val_loss: 0.0436 - val_accuracy: 0.3221\n",
      "Epoch 132/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0307 - accuracy: 0.2348 - val_loss: 0.0436 - val_accuracy: 0.3371\n",
      "Epoch 133/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0306 - accuracy: 0.2349 - val_loss: 0.0438 - val_accuracy: 0.3273\n",
      "Epoch 134/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0306 - accuracy: 0.2354 - val_loss: 0.0433 - val_accuracy: 0.3047\n",
      "Epoch 135/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0305 - accuracy: 0.2353 - val_loss: 0.0431 - val_accuracy: 0.3204\n",
      "Epoch 136/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0305 - accuracy: 0.2347 - val_loss: 0.0439 - val_accuracy: 0.3265\n",
      "Epoch 137/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0304 - accuracy: 0.2368 - val_loss: 0.0433 - val_accuracy: 0.3461\n",
      "Epoch 138/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0304 - accuracy: 0.2346 - val_loss: 0.0436 - val_accuracy: 0.3268\n",
      "Epoch 139/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0304 - accuracy: 0.2355 - val_loss: 0.0434 - val_accuracy: 0.3320\n",
      "Epoch 140/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0303 - accuracy: 0.2365 - val_loss: 0.0428 - val_accuracy: 0.3292\n",
      "Epoch 141/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0303 - accuracy: 0.2364 - val_loss: 0.0430 - val_accuracy: 0.3157\n",
      "Epoch 142/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0302 - accuracy: 0.2351 - val_loss: 0.0432 - val_accuracy: 0.3173\n",
      "Epoch 143/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0302 - accuracy: 0.2360 - val_loss: 0.0435 - val_accuracy: 0.3292\n",
      "Epoch 144/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0302 - accuracy: 0.2362 - val_loss: 0.0434 - val_accuracy: 0.3263\n",
      "Epoch 145/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0301 - accuracy: 0.2359 - val_loss: 0.0428 - val_accuracy: 0.3235\n",
      "Epoch 146/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0301 - accuracy: 0.2362 - val_loss: 0.0429 - val_accuracy: 0.3247\n",
      "Epoch 147/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0300 - accuracy: 0.2358 - val_loss: 0.0430 - val_accuracy: 0.3315\n",
      "Epoch 148/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0300 - accuracy: 0.2370 - val_loss: 0.0429 - val_accuracy: 0.3232\n",
      "Epoch 149/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0300 - accuracy: 0.2370 - val_loss: 0.0431 - val_accuracy: 0.3265\n",
      "Epoch 150/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0299 - accuracy: 0.2364 - val_loss: 0.0425 - val_accuracy: 0.3157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-30-b4a9dc4678bb>:145: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "  tprs.append(interp(mean_fpr, fpr, tpr))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Epoch 1/150\n",
      "7941/7941 [==============================] - 34s 4ms/step - loss: 0.1614 - accuracy: 0.0560 - val_loss: 0.1857 - val_accuracy: 0.1391\n",
      "Epoch 2/150\n",
      "7941/7941 [==============================] - 34s 4ms/step - loss: 0.1375 - accuracy: 0.1544 - val_loss: 0.1429 - val_accuracy: 0.2897\n",
      "Epoch 3/150\n",
      "7941/7941 [==============================] - 34s 4ms/step - loss: 0.1054 - accuracy: 0.2025 - val_loss: 0.1136 - val_accuracy: 0.3094\n",
      "Epoch 4/150\n",
      "7941/7941 [==============================] - 34s 4ms/step - loss: 0.0869 - accuracy: 0.2227 - val_loss: 0.1005 - val_accuracy: 0.3332\n",
      "Epoch 5/150\n",
      "7941/7941 [==============================] - 34s 4ms/step - loss: 0.0763 - accuracy: 0.2316 - val_loss: 0.0923 - val_accuracy: 0.3208\n",
      "Epoch 6/150\n",
      "7941/7941 [==============================] - 35s 4ms/step - loss: 0.0699 - accuracy: 0.2330 - val_loss: 0.0869 - val_accuracy: 0.3257\n",
      "Epoch 7/150\n",
      "7941/7941 [==============================] - 35s 4ms/step - loss: 0.0654 - accuracy: 0.2313 - val_loss: 0.0831 - val_accuracy: 0.3372\n",
      "Epoch 8/150\n",
      "7941/7941 [==============================] - 35s 4ms/step - loss: 0.0617 - accuracy: 0.2309 - val_loss: 0.0794 - val_accuracy: 0.3414\n",
      "Epoch 9/150\n",
      "7941/7941 [==============================] - 35s 4ms/step - loss: 0.0586 - accuracy: 0.2321 - val_loss: 0.0762 - val_accuracy: 0.3397\n",
      "Epoch 10/150\n",
      "7941/7941 [==============================] - 35s 4ms/step - loss: 0.0562 - accuracy: 0.2322 - val_loss: 0.0734 - val_accuracy: 0.3379\n",
      "Epoch 11/150\n",
      "7941/7941 [==============================] - 35s 4ms/step - loss: 0.0543 - accuracy: 0.2332 - val_loss: 0.0712 - val_accuracy: 0.3359\n",
      "Epoch 12/150\n",
      "7941/7941 [==============================] - 35s 4ms/step - loss: 0.0527 - accuracy: 0.2336 - val_loss: 0.0695 - val_accuracy: 0.3443\n",
      "Epoch 13/150\n",
      "7941/7941 [==============================] - 35s 4ms/step - loss: 0.0513 - accuracy: 0.2346 - val_loss: 0.0679 - val_accuracy: 0.3342\n",
      "Epoch 14/150\n",
      "7941/7941 [==============================] - 35s 4ms/step - loss: 0.0501 - accuracy: 0.2344 - val_loss: 0.0665 - val_accuracy: 0.3422\n",
      "Epoch 15/150\n",
      "7941/7941 [==============================] - 35s 4ms/step - loss: 0.0490 - accuracy: 0.2358 - val_loss: 0.0652 - val_accuracy: 0.3417\n",
      "Epoch 16/150\n",
      "7941/7941 [==============================] - 35s 4ms/step - loss: 0.0480 - accuracy: 0.2356 - val_loss: 0.0641 - val_accuracy: 0.3475\n",
      "Epoch 17/150\n",
      "7941/7941 [==============================] - 35s 4ms/step - loss: 0.0472 - accuracy: 0.2360 - val_loss: 0.0625 - val_accuracy: 0.3414\n",
      "Epoch 18/150\n",
      "7941/7941 [==============================] - 35s 4ms/step - loss: 0.0465 - accuracy: 0.2369 - val_loss: 0.0618 - val_accuracy: 0.3383\n",
      "Epoch 19/150\n",
      "7941/7941 [==============================] - 36s 4ms/step - loss: 0.0458 - accuracy: 0.2360 - val_loss: 0.0606 - val_accuracy: 0.3275\n",
      "Epoch 20/150\n",
      "7941/7941 [==============================] - 35s 4ms/step - loss: 0.0452 - accuracy: 0.2358 - val_loss: 0.0604 - val_accuracy: 0.3332\n",
      "Epoch 21/150\n",
      "7941/7941 [==============================] - 35s 4ms/step - loss: 0.0446 - accuracy: 0.2362 - val_loss: 0.0595 - val_accuracy: 0.3174\n",
      "Epoch 22/150\n",
      "7941/7941 [==============================] - 35s 4ms/step - loss: 0.0441 - accuracy: 0.2361 - val_loss: 0.0585 - val_accuracy: 0.3294\n",
      "Epoch 23/150\n",
      "7941/7941 [==============================] - 35s 4ms/step - loss: 0.0436 - accuracy: 0.2363 - val_loss: 0.0582 - val_accuracy: 0.3320\n",
      "Epoch 24/150\n",
      "7941/7941 [==============================] - 36s 4ms/step - loss: 0.0432 - accuracy: 0.2355 - val_loss: 0.0573 - val_accuracy: 0.3308\n",
      "Epoch 25/150\n",
      "7941/7941 [==============================] - 36s 4ms/step - loss: 0.0428 - accuracy: 0.2366 - val_loss: 0.0566 - val_accuracy: 0.3323\n",
      "Epoch 26/150\n",
      "7941/7941 [==============================] - 37s 5ms/step - loss: 0.0423 - accuracy: 0.2361 - val_loss: 0.0563 - val_accuracy: 0.3342\n",
      "Epoch 27/150\n",
      "7941/7941 [==============================] - 35s 4ms/step - loss: 0.0420 - accuracy: 0.2358 - val_loss: 0.0562 - val_accuracy: 0.3342\n",
      "Epoch 28/150\n",
      "7941/7941 [==============================] - 36s 4ms/step - loss: 0.0416 - accuracy: 0.2360 - val_loss: 0.0560 - val_accuracy: 0.3281\n",
      "Epoch 29/150\n",
      "7941/7941 [==============================] - 36s 5ms/step - loss: 0.0413 - accuracy: 0.2352 - val_loss: 0.0549 - val_accuracy: 0.3170\n",
      "Epoch 30/150\n",
      "7941/7941 [==============================] - 36s 4ms/step - loss: 0.0409 - accuracy: 0.2356 - val_loss: 0.0543 - val_accuracy: 0.3236\n",
      "Epoch 31/150\n",
      "7941/7941 [==============================] - 36s 4ms/step - loss: 0.0406 - accuracy: 0.2355 - val_loss: 0.0539 - val_accuracy: 0.3175\n",
      "Epoch 32/150\n",
      "7941/7941 [==============================] - 35s 4ms/step - loss: 0.0403 - accuracy: 0.2352 - val_loss: 0.0537 - val_accuracy: 0.3238\n",
      "Epoch 33/150\n",
      "7941/7941 [==============================] - 36s 4ms/step - loss: 0.0400 - accuracy: 0.2343 - val_loss: 0.0536 - val_accuracy: 0.3247\n",
      "Epoch 34/150\n",
      "7941/7941 [==============================] - 35s 4ms/step - loss: 0.0397 - accuracy: 0.2348 - val_loss: 0.0537 - val_accuracy: 0.3192\n",
      "Epoch 35/150\n",
      "7941/7941 [==============================] - 35s 4ms/step - loss: 0.0395 - accuracy: 0.2349 - val_loss: 0.0522 - val_accuracy: 0.3229\n",
      "Epoch 36/150\n",
      "7941/7941 [==============================] - 36s 5ms/step - loss: 0.0392 - accuracy: 0.2337 - val_loss: 0.0533 - val_accuracy: 0.3398\n",
      "Epoch 37/150\n",
      "7941/7941 [==============================] - 36s 5ms/step - loss: 0.0390 - accuracy: 0.2338 - val_loss: 0.0523 - val_accuracy: 0.3350\n",
      "Epoch 38/150\n",
      "7941/7941 [==============================] - 36s 4ms/step - loss: 0.0387 - accuracy: 0.2337 - val_loss: 0.0522 - val_accuracy: 0.3332\n",
      "Epoch 39/150\n",
      "7941/7941 [==============================] - 41s 5ms/step - loss: 0.0385 - accuracy: 0.2331 - val_loss: 0.0526 - val_accuracy: 0.3250\n",
      "Epoch 40/150\n",
      "7941/7941 [==============================] - 46s 6ms/step - loss: 0.0383 - accuracy: 0.2339 - val_loss: 0.0517 - val_accuracy: 0.3396\n",
      "Epoch 41/150\n",
      "7941/7941 [==============================] - 46s 6ms/step - loss: 0.0381 - accuracy: 0.2332 - val_loss: 0.0513 - val_accuracy: 0.3411\n",
      "Epoch 42/150\n",
      "7941/7941 [==============================] - 46s 6ms/step - loss: 0.0379 - accuracy: 0.2332 - val_loss: 0.0509 - val_accuracy: 0.3223\n",
      "Epoch 43/150\n",
      "7941/7941 [==============================] - 46s 6ms/step - loss: 0.0377 - accuracy: 0.2318 - val_loss: 0.0517 - val_accuracy: 0.3243\n",
      "Epoch 44/150\n",
      "7941/7941 [==============================] - 46s 6ms/step - loss: 0.0375 - accuracy: 0.2327 - val_loss: 0.0508 - val_accuracy: 0.3302\n",
      "Epoch 45/150\n",
      "7941/7941 [==============================] - 46s 6ms/step - loss: 0.0373 - accuracy: 0.2328 - val_loss: 0.0509 - val_accuracy: 0.3217\n",
      "Epoch 46/150\n",
      "7941/7941 [==============================] - 46s 6ms/step - loss: 0.0371 - accuracy: 0.2327 - val_loss: 0.0498 - val_accuracy: 0.3242\n",
      "Epoch 47/150\n",
      "7941/7941 [==============================] - 46s 6ms/step - loss: 0.0370 - accuracy: 0.2310 - val_loss: 0.0508 - val_accuracy: 0.3265\n",
      "Epoch 48/150\n",
      "7941/7941 [==============================] - 46s 6ms/step - loss: 0.0368 - accuracy: 0.2320 - val_loss: 0.0503 - val_accuracy: 0.3294\n",
      "Epoch 49/150\n",
      "7941/7941 [==============================] - 46s 6ms/step - loss: 0.0366 - accuracy: 0.2311 - val_loss: 0.0497 - val_accuracy: 0.3384\n",
      "Epoch 50/150\n",
      "7941/7941 [==============================] - 45s 6ms/step - loss: 0.0365 - accuracy: 0.2320 - val_loss: 0.0497 - val_accuracy: 0.3189\n",
      "Epoch 51/150\n",
      "7941/7941 [==============================] - 46s 6ms/step - loss: 0.0363 - accuracy: 0.2313 - val_loss: 0.0496 - val_accuracy: 0.3227\n",
      "Epoch 52/150\n",
      "7941/7941 [==============================] - 46s 6ms/step - loss: 0.0362 - accuracy: 0.2311 - val_loss: 0.0489 - val_accuracy: 0.3267\n",
      "Epoch 53/150\n",
      "7941/7941 [==============================] - 46s 6ms/step - loss: 0.0360 - accuracy: 0.2305 - val_loss: 0.0493 - val_accuracy: 0.3333\n",
      "Epoch 54/150\n",
      "7941/7941 [==============================] - 46s 6ms/step - loss: 0.0359 - accuracy: 0.2300 - val_loss: 0.0499 - val_accuracy: 0.3285\n",
      "Epoch 55/150\n",
      "7941/7941 [==============================] - 46s 6ms/step - loss: 0.0357 - accuracy: 0.2298 - val_loss: 0.0494 - val_accuracy: 0.3498\n",
      "Epoch 56/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0356 - accuracy: 0.2294 - val_loss: 0.0485 - val_accuracy: 0.3158\n",
      "Epoch 57/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0355 - accuracy: 0.2291 - val_loss: 0.0486 - val_accuracy: 0.3185\n",
      "Epoch 58/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0354 - accuracy: 0.2289 - val_loss: 0.0479 - val_accuracy: 0.3264\n",
      "Epoch 59/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0352 - accuracy: 0.2282 - val_loss: 0.0485 - val_accuracy: 0.3396\n",
      "Epoch 60/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0351 - accuracy: 0.2287 - val_loss: 0.0490 - val_accuracy: 0.3372\n",
      "Epoch 61/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0350 - accuracy: 0.2286 - val_loss: 0.0481 - val_accuracy: 0.3302\n",
      "Epoch 62/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0349 - accuracy: 0.2270 - val_loss: 0.0480 - val_accuracy: 0.3251\n",
      "Epoch 63/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0348 - accuracy: 0.2282 - val_loss: 0.0476 - val_accuracy: 0.3128\n",
      "Epoch 64/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0346 - accuracy: 0.2271 - val_loss: 0.0479 - val_accuracy: 0.3364\n",
      "Epoch 65/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0345 - accuracy: 0.2265 - val_loss: 0.0480 - val_accuracy: 0.3206\n",
      "Epoch 66/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0344 - accuracy: 0.2272 - val_loss: 0.0474 - val_accuracy: 0.3092\n",
      "Epoch 67/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0343 - accuracy: 0.2264 - val_loss: 0.0471 - val_accuracy: 0.2989\n",
      "Epoch 68/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0342 - accuracy: 0.2266 - val_loss: 0.0473 - val_accuracy: 0.3087\n",
      "Epoch 69/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0341 - accuracy: 0.2254 - val_loss: 0.0474 - val_accuracy: 0.3318\n",
      "Epoch 70/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0340 - accuracy: 0.2253 - val_loss: 0.0471 - val_accuracy: 0.3057\n",
      "Epoch 71/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0339 - accuracy: 0.2257 - val_loss: 0.0473 - val_accuracy: 0.3180\n",
      "Epoch 72/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0338 - accuracy: 0.2250 - val_loss: 0.0468 - val_accuracy: 0.3209\n",
      "Epoch 73/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0337 - accuracy: 0.2245 - val_loss: 0.0468 - val_accuracy: 0.3274\n",
      "Epoch 74/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0337 - accuracy: 0.2256 - val_loss: 0.0469 - val_accuracy: 0.3241\n",
      "Epoch 75/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0336 - accuracy: 0.2256 - val_loss: 0.0467 - val_accuracy: 0.3145\n",
      "Epoch 76/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0335 - accuracy: 0.2241 - val_loss: 0.0464 - val_accuracy: 0.3238\n",
      "Epoch 77/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0334 - accuracy: 0.2239 - val_loss: 0.0463 - val_accuracy: 0.3028\n",
      "Epoch 78/150\n",
      "7941/7941 [==============================] - 44s 5ms/step - loss: 0.0333 - accuracy: 0.2244 - val_loss: 0.0462 - val_accuracy: 0.3076\n",
      "Epoch 79/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0332 - accuracy: 0.2236 - val_loss: 0.0466 - val_accuracy: 0.3140\n",
      "Epoch 80/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0332 - accuracy: 0.2243 - val_loss: 0.0464 - val_accuracy: 0.3136\n",
      "Epoch 81/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0331 - accuracy: 0.2228 - val_loss: 0.0462 - val_accuracy: 0.3113\n",
      "Epoch 82/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0330 - accuracy: 0.2236 - val_loss: 0.0467 - val_accuracy: 0.3062\n",
      "Epoch 83/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0329 - accuracy: 0.2234 - val_loss: 0.0455 - val_accuracy: 0.3199\n",
      "Epoch 84/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0328 - accuracy: 0.2238 - val_loss: 0.0457 - val_accuracy: 0.3076\n",
      "Epoch 85/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0328 - accuracy: 0.2233 - val_loss: 0.0465 - val_accuracy: 0.3200\n",
      "Epoch 86/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0327 - accuracy: 0.2225 - val_loss: 0.0458 - val_accuracy: 0.3139\n",
      "Epoch 87/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0326 - accuracy: 0.2228 - val_loss: 0.0455 - val_accuracy: 0.3147\n",
      "Epoch 88/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0325 - accuracy: 0.2234 - val_loss: 0.0452 - val_accuracy: 0.3097\n",
      "Epoch 89/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0325 - accuracy: 0.2229 - val_loss: 0.0457 - val_accuracy: 0.3067\n",
      "Epoch 90/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0324 - accuracy: 0.2234 - val_loss: 0.0455 - val_accuracy: 0.3356\n",
      "Epoch 91/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0323 - accuracy: 0.2230 - val_loss: 0.0453 - val_accuracy: 0.3059\n",
      "Epoch 92/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0323 - accuracy: 0.2219 - val_loss: 0.0457 - val_accuracy: 0.3214\n",
      "Epoch 93/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0322 - accuracy: 0.2230 - val_loss: 0.0455 - val_accuracy: 0.3159\n",
      "Epoch 94/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0322 - accuracy: 0.2228 - val_loss: 0.0456 - val_accuracy: 0.3137\n",
      "Epoch 95/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0321 - accuracy: 0.2233 - val_loss: 0.0447 - val_accuracy: 0.3034\n",
      "Epoch 96/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0320 - accuracy: 0.2228 - val_loss: 0.0458 - val_accuracy: 0.3213\n",
      "Epoch 97/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0320 - accuracy: 0.2227 - val_loss: 0.0455 - val_accuracy: 0.3165\n",
      "Epoch 98/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0319 - accuracy: 0.2236 - val_loss: 0.0452 - val_accuracy: 0.3077\n",
      "Epoch 99/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0318 - accuracy: 0.2227 - val_loss: 0.0455 - val_accuracy: 0.3162\n",
      "Epoch 100/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0318 - accuracy: 0.2217 - val_loss: 0.0446 - val_accuracy: 0.3175\n",
      "Epoch 101/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0317 - accuracy: 0.2230 - val_loss: 0.0451 - val_accuracy: 0.3175\n",
      "Epoch 102/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0316 - accuracy: 0.2231 - val_loss: 0.0439 - val_accuracy: 0.3136\n",
      "Epoch 103/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0316 - accuracy: 0.2223 - val_loss: 0.0448 - val_accuracy: 0.3133\n",
      "Epoch 104/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0315 - accuracy: 0.2221 - val_loss: 0.0444 - val_accuracy: 0.3168\n",
      "Epoch 105/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0315 - accuracy: 0.2221 - val_loss: 0.0447 - val_accuracy: 0.2999\n",
      "Epoch 106/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0314 - accuracy: 0.2217 - val_loss: 0.0442 - val_accuracy: 0.3088\n",
      "Epoch 107/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0314 - accuracy: 0.2227 - val_loss: 0.0440 - val_accuracy: 0.2989\n",
      "Epoch 108/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0313 - accuracy: 0.2227 - val_loss: 0.0451 - val_accuracy: 0.3512\n",
      "Epoch 109/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0313 - accuracy: 0.2231 - val_loss: 0.0445 - val_accuracy: 0.3247\n",
      "Epoch 110/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0312 - accuracy: 0.2224 - val_loss: 0.0443 - val_accuracy: 0.3206\n",
      "Epoch 111/150\n",
      "7941/7941 [==============================] - 44s 6ms/step - loss: 0.0312 - accuracy: 0.2222 - val_loss: 0.0443 - val_accuracy: 0.3143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/150\n",
      "7941/7941 [==============================] - 44s 6ms/step - loss: 0.0311 - accuracy: 0.2238 - val_loss: 0.0442 - val_accuracy: 0.3399\n",
      "Epoch 113/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0311 - accuracy: 0.2234 - val_loss: 0.0439 - val_accuracy: 0.3249\n",
      "Epoch 114/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0310 - accuracy: 0.2221 - val_loss: 0.0439 - val_accuracy: 0.2976\n",
      "Epoch 115/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0310 - accuracy: 0.2242 - val_loss: 0.0440 - val_accuracy: 0.3124\n",
      "Epoch 116/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0309 - accuracy: 0.2238 - val_loss: 0.0440 - val_accuracy: 0.3149\n",
      "Epoch 117/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0309 - accuracy: 0.2234 - val_loss: 0.0441 - val_accuracy: 0.3221\n",
      "Epoch 118/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0308 - accuracy: 0.2229 - val_loss: 0.0449 - val_accuracy: 0.3171\n",
      "Epoch 119/150\n",
      "7941/7941 [==============================] - 47s 6ms/step - loss: 0.0308 - accuracy: 0.2237 - val_loss: 0.0441 - val_accuracy: 0.3236\n",
      "Epoch 120/150\n",
      "7941/7941 [==============================] - 47s 6ms/step - loss: 0.0307 - accuracy: 0.2241 - val_loss: 0.0437 - val_accuracy: 0.3150\n",
      "Epoch 121/150\n",
      "7941/7941 [==============================] - 47s 6ms/step - loss: 0.0307 - accuracy: 0.2231 - val_loss: 0.0438 - val_accuracy: 0.3150\n",
      "Epoch 122/150\n",
      "7941/7941 [==============================] - 47s 6ms/step - loss: 0.0306 - accuracy: 0.2249 - val_loss: 0.0444 - val_accuracy: 0.3178\n",
      "Epoch 123/150\n",
      "7941/7941 [==============================] - 47s 6ms/step - loss: 0.0306 - accuracy: 0.2241 - val_loss: 0.0434 - val_accuracy: 0.3150\n",
      "Epoch 124/150\n",
      "7941/7941 [==============================] - 47s 6ms/step - loss: 0.0305 - accuracy: 0.2234 - val_loss: 0.0442 - val_accuracy: 0.3068\n",
      "Epoch 125/150\n",
      "7941/7941 [==============================] - 47s 6ms/step - loss: 0.0305 - accuracy: 0.2232 - val_loss: 0.0442 - val_accuracy: 0.3234\n",
      "Epoch 126/150\n",
      "7941/7941 [==============================] - 47s 6ms/step - loss: 0.0304 - accuracy: 0.2243 - val_loss: 0.0442 - val_accuracy: 0.3276\n",
      "Epoch 127/150\n",
      "7941/7941 [==============================] - 47s 6ms/step - loss: 0.0304 - accuracy: 0.2242 - val_loss: 0.0439 - val_accuracy: 0.3183\n",
      "Epoch 128/150\n",
      "7941/7941 [==============================] - 47s 6ms/step - loss: 0.0303 - accuracy: 0.2245 - val_loss: 0.0437 - val_accuracy: 0.3274\n",
      "Epoch 129/150\n",
      "7941/7941 [==============================] - 47s 6ms/step - loss: 0.0303 - accuracy: 0.2240 - val_loss: 0.0435 - val_accuracy: 0.3204\n",
      "Epoch 130/150\n",
      "7941/7941 [==============================] - 47s 6ms/step - loss: 0.0303 - accuracy: 0.2251 - val_loss: 0.0436 - val_accuracy: 0.3188\n",
      "Epoch 131/150\n",
      "7941/7941 [==============================] - 47s 6ms/step - loss: 0.0302 - accuracy: 0.2251 - val_loss: 0.0432 - val_accuracy: 0.3313\n",
      "Epoch 132/150\n",
      "7941/7941 [==============================] - 47s 6ms/step - loss: 0.0302 - accuracy: 0.2267 - val_loss: 0.0445 - val_accuracy: 0.3509\n",
      "Epoch 133/150\n",
      "7941/7941 [==============================] - 47s 6ms/step - loss: 0.0301 - accuracy: 0.2250 - val_loss: 0.0436 - val_accuracy: 0.3432\n",
      "Epoch 134/150\n",
      "7941/7941 [==============================] - 47s 6ms/step - loss: 0.0301 - accuracy: 0.2247 - val_loss: 0.0436 - val_accuracy: 0.3301\n",
      "Epoch 135/150\n",
      "7941/7941 [==============================] - 47s 6ms/step - loss: 0.0301 - accuracy: 0.2256 - val_loss: 0.0431 - val_accuracy: 0.3309\n",
      "Epoch 136/150\n",
      "7941/7941 [==============================] - 47s 6ms/step - loss: 0.0300 - accuracy: 0.2254 - val_loss: 0.0427 - val_accuracy: 0.3250\n",
      "Epoch 137/150\n",
      "7941/7941 [==============================] - 47s 6ms/step - loss: 0.0300 - accuracy: 0.2253 - val_loss: 0.0436 - val_accuracy: 0.3183\n",
      "Epoch 138/150\n",
      "7941/7941 [==============================] - 46s 6ms/step - loss: 0.0299 - accuracy: 0.2254 - val_loss: 0.0425 - val_accuracy: 0.3088\n",
      "Epoch 139/150\n",
      "7941/7941 [==============================] - 46s 6ms/step - loss: 0.0299 - accuracy: 0.2257 - val_loss: 0.0428 - val_accuracy: 0.3119\n",
      "Epoch 140/150\n",
      "7941/7941 [==============================] - 45s 6ms/step - loss: 0.0299 - accuracy: 0.2246 - val_loss: 0.0432 - val_accuracy: 0.3281\n",
      "Epoch 141/150\n",
      "7941/7941 [==============================] - 45s 6ms/step - loss: 0.0298 - accuracy: 0.2262 - val_loss: 0.0426 - val_accuracy: 0.3213\n",
      "Epoch 142/150\n",
      "7941/7941 [==============================] - 45s 6ms/step - loss: 0.0298 - accuracy: 0.2256 - val_loss: 0.0434 - val_accuracy: 0.3117\n",
      "Epoch 143/150\n",
      "7941/7941 [==============================] - 45s 6ms/step - loss: 0.0297 - accuracy: 0.2266 - val_loss: 0.0431 - val_accuracy: 0.3520\n",
      "Epoch 144/150\n",
      "7941/7941 [==============================] - 46s 6ms/step - loss: 0.0297 - accuracy: 0.2264 - val_loss: 0.0432 - val_accuracy: 0.3392\n",
      "Epoch 145/150\n",
      "7941/7941 [==============================] - 45s 6ms/step - loss: 0.0297 - accuracy: 0.2253 - val_loss: 0.0428 - val_accuracy: 0.3145\n",
      "Epoch 146/150\n",
      "7941/7941 [==============================] - 45s 6ms/step - loss: 0.0296 - accuracy: 0.2259 - val_loss: 0.0429 - val_accuracy: 0.3436\n",
      "Epoch 147/150\n",
      "7941/7941 [==============================] - 46s 6ms/step - loss: 0.0296 - accuracy: 0.2257 - val_loss: 0.0430 - val_accuracy: 0.3267\n",
      "Epoch 148/150\n",
      "7941/7941 [==============================] - 45s 6ms/step - loss: 0.0296 - accuracy: 0.2264 - val_loss: 0.0428 - val_accuracy: 0.3179\n",
      "Epoch 149/150\n",
      "7941/7941 [==============================] - 45s 6ms/step - loss: 0.0295 - accuracy: 0.2257 - val_loss: 0.0425 - val_accuracy: 0.3116\n",
      "Epoch 150/150\n",
      "7941/7941 [==============================] - 45s 6ms/step - loss: 0.0295 - accuracy: 0.2267 - val_loss: 0.0427 - val_accuracy: 0.3174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-30-b4a9dc4678bb>:145: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "  tprs.append(interp(mean_fpr, fpr, tpr))\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:806: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Epoch 1/150\n",
      "7941/7941 [==============================] - 40s 5ms/step - loss: 0.1618 - accuracy: 0.0498 - val_loss: 0.1863 - val_accuracy: 0.1152\n",
      "Epoch 2/150\n",
      "7941/7941 [==============================] - 40s 5ms/step - loss: 0.1393 - accuracy: 0.1522 - val_loss: 0.1436 - val_accuracy: 0.3128\n",
      "Epoch 3/150\n",
      "7941/7941 [==============================] - 40s 5ms/step - loss: 0.1052 - accuracy: 0.2299 - val_loss: 0.1140 - val_accuracy: 0.3430\n",
      "Epoch 4/150\n",
      "7941/7941 [==============================] - 41s 5ms/step - loss: 0.0878 - accuracy: 0.2400 - val_loss: 0.1006 - val_accuracy: 0.3355\n",
      "Epoch 5/150\n",
      "7941/7941 [==============================] - 41s 5ms/step - loss: 0.0774 - accuracy: 0.2418 - val_loss: 0.0930 - val_accuracy: 0.3378\n",
      "Epoch 6/150\n",
      "7941/7941 [==============================] - 41s 5ms/step - loss: 0.0706 - accuracy: 0.2422 - val_loss: 0.0866 - val_accuracy: 0.3471\n",
      "Epoch 7/150\n",
      "7941/7941 [==============================] - 41s 5ms/step - loss: 0.0655 - accuracy: 0.2430 - val_loss: 0.0822 - val_accuracy: 0.3434\n",
      "Epoch 8/150\n",
      "7941/7941 [==============================] - 41s 5ms/step - loss: 0.0616 - accuracy: 0.2433 - val_loss: 0.0788 - val_accuracy: 0.3398\n",
      "Epoch 9/150\n",
      "7941/7941 [==============================] - 41s 5ms/step - loss: 0.0587 - accuracy: 0.2406 - val_loss: 0.0761 - val_accuracy: 0.3460\n",
      "Epoch 10/150\n",
      "7941/7941 [==============================] - 41s 5ms/step - loss: 0.0563 - accuracy: 0.2408 - val_loss: 0.0736 - val_accuracy: 0.3539\n",
      "Epoch 11/150\n",
      "7941/7941 [==============================] - 41s 5ms/step - loss: 0.0545 - accuracy: 0.2391 - val_loss: 0.0712 - val_accuracy: 0.3505\n",
      "Epoch 12/150\n",
      "7941/7941 [==============================] - 41s 5ms/step - loss: 0.0529 - accuracy: 0.2384 - val_loss: 0.0690 - val_accuracy: 0.3386\n",
      "Epoch 13/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0514 - accuracy: 0.2371 - val_loss: 0.0675 - val_accuracy: 0.3444\n",
      "Epoch 14/150\n",
      "7941/7941 [==============================] - 41s 5ms/step - loss: 0.0502 - accuracy: 0.2352 - val_loss: 0.0665 - val_accuracy: 0.3542\n",
      "Epoch 15/150\n",
      "7941/7941 [==============================] - 41s 5ms/step - loss: 0.0491 - accuracy: 0.2346 - val_loss: 0.0645 - val_accuracy: 0.3509\n",
      "Epoch 16/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0482 - accuracy: 0.2335 - val_loss: 0.0635 - val_accuracy: 0.3498\n",
      "Epoch 17/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0474 - accuracy: 0.2335 - val_loss: 0.0631 - val_accuracy: 0.3502\n",
      "Epoch 18/150\n",
      "7941/7941 [==============================] - 41s 5ms/step - loss: 0.0466 - accuracy: 0.2333 - val_loss: 0.0618 - val_accuracy: 0.3349\n",
      "Epoch 19/150\n",
      "7941/7941 [==============================] - 41s 5ms/step - loss: 0.0460 - accuracy: 0.2322 - val_loss: 0.0615 - val_accuracy: 0.3500\n",
      "Epoch 20/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0454 - accuracy: 0.2321 - val_loss: 0.0602 - val_accuracy: 0.3202\n",
      "Epoch 21/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0448 - accuracy: 0.2326 - val_loss: 0.0588 - val_accuracy: 0.3334\n",
      "Epoch 22/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0443 - accuracy: 0.2319 - val_loss: 0.0581 - val_accuracy: 0.3349\n",
      "Epoch 23/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0438 - accuracy: 0.2325 - val_loss: 0.0576 - val_accuracy: 0.3301\n",
      "Epoch 24/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0434 - accuracy: 0.2330 - val_loss: 0.0575 - val_accuracy: 0.3513\n",
      "Epoch 25/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0430 - accuracy: 0.2324 - val_loss: 0.0568 - val_accuracy: 0.3285\n",
      "Epoch 26/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0426 - accuracy: 0.2330 - val_loss: 0.0564 - val_accuracy: 0.3234\n",
      "Epoch 27/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0422 - accuracy: 0.2328 - val_loss: 0.0556 - val_accuracy: 0.3301\n",
      "Epoch 28/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0419 - accuracy: 0.2329 - val_loss: 0.0558 - val_accuracy: 0.3400\n",
      "Epoch 29/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0415 - accuracy: 0.2329 - val_loss: 0.0551 - val_accuracy: 0.3338\n",
      "Epoch 30/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0412 - accuracy: 0.2336 - val_loss: 0.0545 - val_accuracy: 0.3131\n",
      "Epoch 31/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0409 - accuracy: 0.2341 - val_loss: 0.0542 - val_accuracy: 0.3311\n",
      "Epoch 32/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0406 - accuracy: 0.2338 - val_loss: 0.0539 - val_accuracy: 0.3279\n",
      "Epoch 33/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0403 - accuracy: 0.2334 - val_loss: 0.0539 - val_accuracy: 0.3270\n",
      "Epoch 34/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0400 - accuracy: 0.2346 - val_loss: 0.0530 - val_accuracy: 0.3166\n",
      "Epoch 35/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0398 - accuracy: 0.2338 - val_loss: 0.0534 - val_accuracy: 0.3330\n",
      "Epoch 36/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0395 - accuracy: 0.2331 - val_loss: 0.0524 - val_accuracy: 0.3284\n",
      "Epoch 37/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0393 - accuracy: 0.2333 - val_loss: 0.0523 - val_accuracy: 0.3170\n",
      "Epoch 38/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0391 - accuracy: 0.2335 - val_loss: 0.0519 - val_accuracy: 0.3321\n",
      "Epoch 39/150\n",
      "7941/7941 [==============================] - 44s 5ms/step - loss: 0.0389 - accuracy: 0.2341 - val_loss: 0.0515 - val_accuracy: 0.3198\n",
      "Epoch 40/150\n",
      "7941/7941 [==============================] - 45s 6ms/step - loss: 0.0386 - accuracy: 0.2337 - val_loss: 0.0509 - val_accuracy: 0.3182\n",
      "Epoch 41/150\n",
      "7941/7941 [==============================] - 44s 6ms/step - loss: 0.0384 - accuracy: 0.2327 - val_loss: 0.0510 - val_accuracy: 0.3043\n",
      "Epoch 42/150\n",
      "7941/7941 [==============================] - 44s 6ms/step - loss: 0.0382 - accuracy: 0.2329 - val_loss: 0.0511 - val_accuracy: 0.3251\n",
      "Epoch 43/150\n",
      "7941/7941 [==============================] - 44s 6ms/step - loss: 0.0381 - accuracy: 0.2330 - val_loss: 0.0508 - val_accuracy: 0.3067\n",
      "Epoch 44/150\n",
      "7941/7941 [==============================] - 44s 6ms/step - loss: 0.0379 - accuracy: 0.2322 - val_loss: 0.0508 - val_accuracy: 0.3208\n",
      "Epoch 45/150\n",
      "7941/7941 [==============================] - 45s 6ms/step - loss: 0.0377 - accuracy: 0.2326 - val_loss: 0.0503 - val_accuracy: 0.3041\n",
      "Epoch 46/150\n",
      "7941/7941 [==============================] - 44s 6ms/step - loss: 0.0375 - accuracy: 0.2324 - val_loss: 0.0501 - val_accuracy: 0.3220\n",
      "Epoch 47/150\n",
      "7941/7941 [==============================] - 44s 6ms/step - loss: 0.0373 - accuracy: 0.2317 - val_loss: 0.0498 - val_accuracy: 0.3259\n",
      "Epoch 48/150\n",
      "7941/7941 [==============================] - 45s 6ms/step - loss: 0.0372 - accuracy: 0.2322 - val_loss: 0.0502 - val_accuracy: 0.3210\n",
      "Epoch 49/150\n",
      "7941/7941 [==============================] - 44s 6ms/step - loss: 0.0370 - accuracy: 0.2302 - val_loss: 0.0496 - val_accuracy: 0.3185\n",
      "Epoch 50/150\n",
      "7941/7941 [==============================] - 44s 6ms/step - loss: 0.0369 - accuracy: 0.2314 - val_loss: 0.0492 - val_accuracy: 0.3172\n",
      "Epoch 51/150\n",
      "7941/7941 [==============================] - 44s 6ms/step - loss: 0.0367 - accuracy: 0.2307 - val_loss: 0.0492 - val_accuracy: 0.3229\n",
      "Epoch 52/150\n",
      "7941/7941 [==============================] - 44s 6ms/step - loss: 0.0366 - accuracy: 0.2302 - val_loss: 0.0490 - val_accuracy: 0.3186\n",
      "Epoch 53/150\n",
      "7941/7941 [==============================] - 44s 6ms/step - loss: 0.0364 - accuracy: 0.2301 - val_loss: 0.0491 - val_accuracy: 0.3117\n",
      "Epoch 54/150\n",
      "7941/7941 [==============================] - 44s 6ms/step - loss: 0.0363 - accuracy: 0.2301 - val_loss: 0.0488 - val_accuracy: 0.3060\n",
      "Epoch 55/150\n",
      "7941/7941 [==============================] - 44s 6ms/step - loss: 0.0361 - accuracy: 0.2294 - val_loss: 0.0483 - val_accuracy: 0.3228\n",
      "Epoch 56/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0360 - accuracy: 0.2295 - val_loss: 0.0493 - val_accuracy: 0.3043\n",
      "Epoch 57/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0359 - accuracy: 0.2291 - val_loss: 0.0484 - val_accuracy: 0.3251\n",
      "Epoch 58/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0357 - accuracy: 0.2283 - val_loss: 0.0481 - val_accuracy: 0.3284\n",
      "Epoch 59/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0356 - accuracy: 0.2280 - val_loss: 0.0483 - val_accuracy: 0.3186\n",
      "Epoch 60/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0355 - accuracy: 0.2282 - val_loss: 0.0476 - val_accuracy: 0.3128\n",
      "Epoch 61/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0354 - accuracy: 0.2281 - val_loss: 0.0480 - val_accuracy: 0.3134\n",
      "Epoch 62/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0353 - accuracy: 0.2266 - val_loss: 0.0474 - val_accuracy: 0.3285\n",
      "Epoch 63/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0351 - accuracy: 0.2282 - val_loss: 0.0475 - val_accuracy: 0.3214\n",
      "Epoch 64/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0350 - accuracy: 0.2269 - val_loss: 0.0475 - val_accuracy: 0.3252\n",
      "Epoch 65/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0349 - accuracy: 0.2270 - val_loss: 0.0469 - val_accuracy: 0.3014\n",
      "Epoch 66/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0348 - accuracy: 0.2271 - val_loss: 0.0473 - val_accuracy: 0.3069\n",
      "Epoch 67/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0347 - accuracy: 0.2272 - val_loss: 0.0473 - val_accuracy: 0.3278\n",
      "Epoch 68/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0346 - accuracy: 0.2261 - val_loss: 0.0469 - val_accuracy: 0.3110\n",
      "Epoch 69/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0345 - accuracy: 0.2259 - val_loss: 0.0472 - val_accuracy: 0.3024\n",
      "Epoch 70/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0344 - accuracy: 0.2255 - val_loss: 0.0465 - val_accuracy: 0.3232\n",
      "Epoch 71/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0343 - accuracy: 0.2252 - val_loss: 0.0464 - val_accuracy: 0.3038\n",
      "Epoch 72/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0342 - accuracy: 0.2259 - val_loss: 0.0466 - val_accuracy: 0.3123\n",
      "Epoch 73/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0341 - accuracy: 0.2248 - val_loss: 0.0466 - val_accuracy: 0.3144\n",
      "Epoch 74/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0341 - accuracy: 0.2249 - val_loss: 0.0466 - val_accuracy: 0.3084\n",
      "Epoch 75/150\n",
      "7941/7941 [==============================] - 44s 5ms/step - loss: 0.0340 - accuracy: 0.2253 - val_loss: 0.0469 - val_accuracy: 0.3033\n",
      "Epoch 76/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0339 - accuracy: 0.2250 - val_loss: 0.0465 - val_accuracy: 0.3166\n",
      "Epoch 77/150\n",
      "7941/7941 [==============================] - 47s 6ms/step - loss: 0.0338 - accuracy: 0.2263 - val_loss: 0.0462 - val_accuracy: 0.3110\n",
      "Epoch 78/150\n",
      "7941/7941 [==============================] - 45s 6ms/step - loss: 0.0337 - accuracy: 0.2240 - val_loss: 0.0455 - val_accuracy: 0.2936\n",
      "Epoch 79/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0336 - accuracy: 0.2252 - val_loss: 0.0458 - val_accuracy: 0.3007\n",
      "Epoch 80/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0335 - accuracy: 0.2242 - val_loss: 0.0468 - val_accuracy: 0.3103\n",
      "Epoch 81/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0335 - accuracy: 0.2253 - val_loss: 0.0460 - val_accuracy: 0.2951\n",
      "Epoch 82/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0334 - accuracy: 0.2237 - val_loss: 0.0464 - val_accuracy: 0.3241\n",
      "Epoch 83/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0333 - accuracy: 0.2248 - val_loss: 0.0463 - val_accuracy: 0.3100\n",
      "Epoch 84/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0332 - accuracy: 0.2240 - val_loss: 0.0458 - val_accuracy: 0.2862\n",
      "Epoch 85/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0331 - accuracy: 0.2238 - val_loss: 0.0452 - val_accuracy: 0.2942\n",
      "Epoch 86/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0331 - accuracy: 0.2238 - val_loss: 0.0457 - val_accuracy: 0.3103\n",
      "Epoch 87/150\n",
      "7941/7941 [==============================] - 44s 6ms/step - loss: 0.0330 - accuracy: 0.2245 - val_loss: 0.0449 - val_accuracy: 0.3179\n",
      "Epoch 88/150\n",
      "7941/7941 [==============================] - 45s 6ms/step - loss: 0.0329 - accuracy: 0.2240 - val_loss: 0.0461 - val_accuracy: 0.3011\n",
      "Epoch 89/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0329 - accuracy: 0.2231 - val_loss: 0.0450 - val_accuracy: 0.3041\n",
      "Epoch 90/150\n",
      "7941/7941 [==============================] - 44s 6ms/step - loss: 0.0328 - accuracy: 0.2242 - val_loss: 0.0447 - val_accuracy: 0.2870\n",
      "Epoch 91/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0327 - accuracy: 0.2241 - val_loss: 0.0454 - val_accuracy: 0.3111\n",
      "Epoch 92/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0327 - accuracy: 0.2233 - val_loss: 0.0450 - val_accuracy: 0.3014\n",
      "Epoch 93/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0326 - accuracy: 0.2233 - val_loss: 0.0449 - val_accuracy: 0.3033\n",
      "Epoch 94/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0325 - accuracy: 0.2229 - val_loss: 0.0451 - val_accuracy: 0.3041\n",
      "Epoch 95/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0325 - accuracy: 0.2239 - val_loss: 0.0449 - val_accuracy: 0.3005\n",
      "Epoch 96/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0324 - accuracy: 0.2238 - val_loss: 0.0449 - val_accuracy: 0.3121\n",
      "Epoch 97/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0323 - accuracy: 0.2239 - val_loss: 0.0448 - val_accuracy: 0.3172\n",
      "Epoch 98/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0323 - accuracy: 0.2223 - val_loss: 0.0450 - val_accuracy: 0.3020\n",
      "Epoch 99/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0322 - accuracy: 0.2240 - val_loss: 0.0449 - val_accuracy: 0.2990\n",
      "Epoch 100/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0321 - accuracy: 0.2235 - val_loss: 0.0442 - val_accuracy: 0.3027\n",
      "Epoch 101/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0321 - accuracy: 0.2232 - val_loss: 0.0450 - val_accuracy: 0.3149\n",
      "Epoch 102/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0320 - accuracy: 0.2231 - val_loss: 0.0446 - val_accuracy: 0.2872\n",
      "Epoch 103/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0320 - accuracy: 0.2232 - val_loss: 0.0439 - val_accuracy: 0.3095\n",
      "Epoch 104/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0319 - accuracy: 0.2231 - val_loss: 0.0445 - val_accuracy: 0.3092\n",
      "Epoch 105/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0319 - accuracy: 0.2236 - val_loss: 0.0440 - val_accuracy: 0.2900\n",
      "Epoch 106/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0318 - accuracy: 0.2230 - val_loss: 0.0446 - val_accuracy: 0.3191\n",
      "Epoch 107/150\n",
      "7941/7941 [==============================] - 45s 6ms/step - loss: 0.0317 - accuracy: 0.2243 - val_loss: 0.0439 - val_accuracy: 0.3125\n",
      "Epoch 108/150\n",
      "7941/7941 [==============================] - 44s 6ms/step - loss: 0.0317 - accuracy: 0.2231 - val_loss: 0.0437 - val_accuracy: 0.3027\n",
      "Epoch 109/150\n",
      "7941/7941 [==============================] - 44s 6ms/step - loss: 0.0316 - accuracy: 0.2235 - val_loss: 0.0441 - val_accuracy: 0.3065\n",
      "Epoch 110/150\n",
      "7941/7941 [==============================] - 44s 6ms/step - loss: 0.0316 - accuracy: 0.2234 - val_loss: 0.0441 - val_accuracy: 0.2998\n",
      "Epoch 111/150\n",
      "7941/7941 [==============================] - 46s 6ms/step - loss: 0.0315 - accuracy: 0.2227 - val_loss: 0.0438 - val_accuracy: 0.3136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0315 - accuracy: 0.2230 - val_loss: 0.0438 - val_accuracy: 0.2926\n",
      "Epoch 113/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0314 - accuracy: 0.2235 - val_loss: 0.0437 - val_accuracy: 0.2942\n",
      "Epoch 114/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0314 - accuracy: 0.2232 - val_loss: 0.0429 - val_accuracy: 0.2943\n",
      "Epoch 115/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0313 - accuracy: 0.2241 - val_loss: 0.0441 - val_accuracy: 0.2956\n",
      "Epoch 116/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0313 - accuracy: 0.2235 - val_loss: 0.0439 - val_accuracy: 0.3071\n",
      "Epoch 117/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0312 - accuracy: 0.2238 - val_loss: 0.0440 - val_accuracy: 0.3046\n",
      "Epoch 118/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0312 - accuracy: 0.2247 - val_loss: 0.0439 - val_accuracy: 0.3159\n",
      "Epoch 119/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0311 - accuracy: 0.2240 - val_loss: 0.0438 - val_accuracy: 0.3060\n",
      "Epoch 120/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0311 - accuracy: 0.2238 - val_loss: 0.0439 - val_accuracy: 0.3127\n",
      "Epoch 121/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0310 - accuracy: 0.2246 - val_loss: 0.0433 - val_accuracy: 0.3107\n",
      "Epoch 122/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0310 - accuracy: 0.2240 - val_loss: 0.0433 - val_accuracy: 0.3122\n",
      "Epoch 123/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0309 - accuracy: 0.2240 - val_loss: 0.0436 - val_accuracy: 0.3098\n",
      "Epoch 124/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0309 - accuracy: 0.2248 - val_loss: 0.0436 - val_accuracy: 0.2976\n",
      "Epoch 125/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0308 - accuracy: 0.2246 - val_loss: 0.0432 - val_accuracy: 0.3110\n",
      "Epoch 126/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0308 - accuracy: 0.2247 - val_loss: 0.0431 - val_accuracy: 0.3277\n",
      "Epoch 127/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0307 - accuracy: 0.2245 - val_loss: 0.0430 - val_accuracy: 0.3076\n",
      "Epoch 128/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0307 - accuracy: 0.2247 - val_loss: 0.0431 - val_accuracy: 0.3232\n",
      "Epoch 129/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0306 - accuracy: 0.2242 - val_loss: 0.0427 - val_accuracy: 0.2879\n",
      "Epoch 130/150\n",
      "7941/7941 [==============================] - 48s 6ms/step - loss: 0.0306 - accuracy: 0.2250 - val_loss: 0.0430 - val_accuracy: 0.3016\n",
      "Epoch 131/150\n",
      "7941/7941 [==============================] - 42s 5ms/step - loss: 0.0305 - accuracy: 0.2249 - val_loss: 0.0426 - val_accuracy: 0.3111\n",
      "Epoch 132/150\n",
      "7941/7941 [==============================] - 36s 5ms/step - loss: 0.0305 - accuracy: 0.2250 - val_loss: 0.0430 - val_accuracy: 0.3261\n",
      "Epoch 133/150\n",
      "7941/7941 [==============================] - 36s 4ms/step - loss: 0.0305 - accuracy: 0.2256 - val_loss: 0.0434 - val_accuracy: 0.3140\n",
      "Epoch 134/150\n",
      "7941/7941 [==============================] - 36s 5ms/step - loss: 0.0304 - accuracy: 0.2251 - val_loss: 0.0430 - val_accuracy: 0.3017\n",
      "Epoch 135/150\n",
      "7941/7941 [==============================] - 35s 4ms/step - loss: 0.0304 - accuracy: 0.2258 - val_loss: 0.0427 - val_accuracy: 0.3056\n",
      "Epoch 136/150\n",
      "7941/7941 [==============================] - 36s 4ms/step - loss: 0.0303 - accuracy: 0.2261 - val_loss: 0.0431 - val_accuracy: 0.3288\n",
      "Epoch 137/150\n",
      "7941/7941 [==============================] - 36s 4ms/step - loss: 0.0303 - accuracy: 0.2251 - val_loss: 0.0429 - val_accuracy: 0.3270\n",
      "Epoch 138/150\n",
      "7941/7941 [==============================] - 37s 5ms/step - loss: 0.0303 - accuracy: 0.2249 - val_loss: 0.0430 - val_accuracy: 0.3096\n",
      "Epoch 139/150\n",
      "7941/7941 [==============================] - 36s 5ms/step - loss: 0.0302 - accuracy: 0.2266 - val_loss: 0.0430 - val_accuracy: 0.3100\n",
      "Epoch 140/150\n",
      "7941/7941 [==============================] - 36s 4ms/step - loss: 0.0302 - accuracy: 0.2252 - val_loss: 0.0427 - val_accuracy: 0.3071\n",
      "Epoch 141/150\n",
      "7941/7941 [==============================] - 36s 4ms/step - loss: 0.0301 - accuracy: 0.2258 - val_loss: 0.0429 - val_accuracy: 0.3148\n",
      "Epoch 142/150\n",
      "7941/7941 [==============================] - 35s 4ms/step - loss: 0.0301 - accuracy: 0.2261 - val_loss: 0.0425 - val_accuracy: 0.3068\n",
      "Epoch 143/150\n",
      "7941/7941 [==============================] - 36s 4ms/step - loss: 0.0300 - accuracy: 0.2268 - val_loss: 0.0430 - val_accuracy: 0.3304\n",
      "Epoch 144/150\n",
      "7941/7941 [==============================] - 36s 5ms/step - loss: 0.0300 - accuracy: 0.2266 - val_loss: 0.0426 - val_accuracy: 0.3082\n",
      "Epoch 145/150\n",
      "7941/7941 [==============================] - 37s 5ms/step - loss: 0.0300 - accuracy: 0.2263 - val_loss: 0.0427 - val_accuracy: 0.3013\n",
      "Epoch 146/150\n",
      "7941/7941 [==============================] - 35s 4ms/step - loss: 0.0299 - accuracy: 0.2259 - val_loss: 0.0425 - val_accuracy: 0.3051\n",
      "Epoch 147/150\n",
      "7941/7941 [==============================] - 36s 5ms/step - loss: 0.0299 - accuracy: 0.2259 - val_loss: 0.0421 - val_accuracy: 0.3221\n",
      "Epoch 148/150\n",
      "7941/7941 [==============================] - 44s 5ms/step - loss: 0.0299 - accuracy: 0.2261 - val_loss: 0.0416 - val_accuracy: 0.3029\n",
      "Epoch 149/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0298 - accuracy: 0.2274 - val_loss: 0.0422 - val_accuracy: 0.3147\n",
      "Epoch 150/150\n",
      "7941/7941 [==============================] - 43s 5ms/step - loss: 0.0298 - accuracy: 0.2262 - val_loss: 0.0422 - val_accuracy: 0.3029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-30-b4a9dc4678bb>:145: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "  tprs.append(interp(mean_fpr, fpr, tpr))\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:806: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  warnings.warn(\"No positive samples in y_true, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20000, 0]\n",
      "[0, 0]\n",
      "both b and c are zero\n",
      "[20000, 0]\n",
      "[0, 0]\n",
      "both b and c are zero\n",
      "[18599, 472]\n",
      "[826, 103]\n",
      "[18703, 368]\n",
      "[123, 806]\n",
      "[19360, 292]\n",
      "[194, 154]\n",
      "[19067, 585]\n",
      "[115, 233]\n",
      "[19719, 154]\n",
      "[58, 69]\n",
      "[19216, 657]\n",
      "[52, 75]\n",
      "[19772, 124]\n",
      "[42, 62]\n",
      "[19267, 629]\n",
      "[34, 70]\n",
      "[19688, 207]\n",
      "[62, 43]\n",
      "[19239, 656]\n",
      "[29, 76]\n",
      "[19592, 294]\n",
      "[57, 57]\n",
      "[19254, 632]\n",
      "[59, 55]\n",
      "[18139, 1306]\n",
      "[360, 195]\n",
      "[18948, 497]\n",
      "[106, 449]\n",
      "[18468, 977]\n",
      "[367, 188]\n",
      "[19006, 439]\n",
      "[91, 464]\n",
      "[19430, 331]\n",
      "[129, 110]\n",
      "[19097, 664]\n",
      "[85, 154]\n",
      "[19598, 224]\n",
      "[67, 111]\n",
      "[19222, 600]\n",
      "[51, 127]\n",
      "[19665, 195]\n",
      "[59, 81]\n",
      "[19254, 606]\n",
      "[36, 104]\n",
      "[19657, 202]\n",
      "[68, 73]\n",
      "[19218, 641]\n",
      "[68, 73]\n",
      "[19354, 467]\n",
      "[89, 90]\n",
      "[19224, 597]\n",
      "[56, 123]\n",
      "[18870, 551]\n",
      "[323, 256]\n",
      "[18852, 569]\n",
      "[220, 359]\n",
      "[19058, 402]\n",
      "[168, 372]\n",
      "[19019, 441]\n",
      "[131, 409]\n",
      "[19289, 531]\n",
      "[93, 87]\n",
      "[19190, 630]\n",
      "[47, 133]\n",
      "[19634, 225]\n",
      "[67, 74]\n",
      "[19255, 604]\n",
      "[48, 93]\n",
      "[19638, 236]\n",
      "[64, 62]\n",
      "[19250, 624]\n",
      "[53, 73]\n",
      "[19544, 305]\n",
      "[68, 83]\n",
      "[19204, 645]\n",
      "[78, 73]\n",
      "[19499, 318]\n",
      "[83, 100]\n",
      "[19246, 571]\n",
      "[56, 127]\n",
      "[18532, 907]\n",
      "[255, 306]\n",
      "[18947, 492]\n",
      "[169, 392]\n",
      "[18411, 1061]\n",
      "[324, 204]\n",
      "[19003, 469]\n",
      "[88, 440]\n",
      "[19370, 449]\n",
      "[94, 87]\n",
      "[19168, 651]\n",
      "[74, 107]\n",
      "[19574, 289]\n",
      "[61, 76]\n",
      "[19191, 672]\n",
      "[48, 89]\n",
      "[19593, 267]\n",
      "[74, 66]\n",
      "[19260, 600]\n",
      "[57, 83]\n",
      "[19593, 253]\n",
      "[78, 76]\n",
      "[19217, 629]\n",
      "[62, 92]\n",
      "[19393, 413]\n",
      "[105, 89]\n",
      "[19142, 664]\n",
      "[64, 130]\n",
      "[19066, 428]\n",
      "[115, 391]\n",
      "[19004, 490]\n",
      "[95, 411]\n",
      "[18940, 517]\n",
      "[241, 302]\n",
      "[18937, 520]\n",
      "[188, 355]\n",
      "[19282, 539]\n",
      "[81, 98]\n",
      "[19173, 648]\n",
      "[52, 127]\n",
      "[19586, 250]\n",
      "[87, 77]\n",
      "[19222, 614]\n",
      "[65, 99]\n",
      "[19607, 270]\n",
      "[61, 62]\n",
      "[19252, 625]\n",
      "[32, 91]\n",
      "[19540, 320]\n",
      "[75, 65]\n",
      "[19252, 608]\n",
      "[68, 72]\n",
      "[19523, 315]\n",
      "[86, 76]\n",
      "[19223, 615]\n",
      "[71, 91]\n",
      "[18470, 1022]\n",
      "[290, 218]\n",
      "[19015, 477]\n",
      "[109, 399]\n",
      "[18482, 930]\n",
      "[311, 277]\n",
      "[18946, 466]\n",
      "[151, 437]\n",
      "[19452, 365]\n",
      "[88, 95]\n",
      "[19182, 635]\n",
      "[60, 123]\n",
      "[19582, 238]\n",
      "[76, 104]\n",
      "[19187, 633]\n",
      "[84, 96]\n",
      "[19655, 191]\n",
      "[65, 89]\n",
      "[19233, 613]\n",
      "[38, 116]\n",
      "[19656, 210]\n",
      "[65, 69]\n",
      "[19242, 624]\n",
      "[42, 92]\n",
      "[19487, 342]\n",
      "[71, 100]\n",
      "[19151, 678]\n",
      "[88, 83]\n",
      "[19004, 444]\n",
      "[160, 392]\n",
      "[18963, 485]\n",
      "[134, 418]\n",
      "[18941, 489]\n",
      "[236, 334]\n",
      "[18904, 526]\n",
      "[181, 389]\n",
      "[19698, 179]\n",
      "[38, 85]\n",
      "[19285, 592]\n",
      "[60, 63]\n",
      "[19815, 71]\n",
      "[50, 64]\n",
      "[19261, 625]\n",
      "[59, 55]\n",
      "[19779, 105]\n",
      "[36, 80]\n",
      "[19249, 635]\n",
      "[57, 59]\n",
      "[19762, 129]\n",
      "[45, 64]\n",
      "[19285, 606]\n",
      "[17, 92]\n",
      "[19732, 134]\n",
      "[52, 82]\n",
      "[19244, 622]\n",
      "[48, 86]\n",
      "[19548, 179]\n",
      "[98, 175]\n",
      "[19197, 530]\n",
      "[121, 152]\n",
      "[18217, 1086]\n",
      "[324, 373]\n",
      "[18925, 378]\n",
      "[250, 447]\n",
      "[18984, 403]\n",
      "[164, 449]\n",
      "[18870, 517]\n",
      "[139, 474]\n",
      "[18489, 994]\n",
      "[269, 248]\n",
      "[18992, 491]\n",
      "[106, 411]\n",
      "[19101, 367]\n",
      "[92, 440]\n",
      "[19021, 447]\n",
      "[88, 444]\n",
      "[18445, 1013]\n",
      "[304, 238]\n",
      "[19053, 405]\n",
      "[67, 475]\n",
      "[19022, 448]\n",
      "[143, 387]\n",
      "[18985, 485]\n",
      "[129, 401]\n",
      "[19547, 165]\n",
      "[150, 138]\n",
      "[19190, 522]\n",
      "[76, 212]\n",
      "[19552, 303]\n",
      "[87, 58]\n",
      "[19286, 569]\n",
      "[37, 108]\n",
      "[19318, 459]\n",
      "[116, 107]\n",
      "[19137, 640]\n",
      "[96, 127]\n",
      "[19497, 311]\n",
      "[93, 99]\n",
      "[19250, 558]\n",
      "[64, 128]\n",
      "[19385, 423]\n",
      "[105, 87]\n",
      "[19221, 587]\n",
      "[60, 132]\n",
      "[19417, 403]\n",
      "[77, 103]\n",
      "[19199, 621]\n",
      "[46, 134]\n",
      "[19371, 442]\n",
      "[98, 89]\n",
      "[19207, 606]\n",
      "[68, 119]\n",
      "[19706, 135]\n",
      "[93, 66]\n",
      "[19293, 548]\n",
      "[28, 131]\n",
      "[19720, 147]\n",
      "[71, 62]\n",
      "[19286, 581]\n",
      "[57, 76]\n",
      "[19634, 189]\n",
      "[98, 79]\n",
      "[19224, 599]\n",
      "[41, 136]\n",
      "[19526, 307]\n",
      "[88, 79]\n",
      "[19264, 569]\n",
      "[55, 112]\n",
      "[19518, 323]\n",
      "[100, 59]\n",
      "[19232, 609]\n",
      "[44, 115]\n",
      "[19567, 260]\n",
      "[94, 79]\n",
      "[19290, 537]\n",
      "[63, 110]\n",
      "[19653, 165]\n",
      "[97, 85]\n",
      "[19251, 567]\n",
      "[44, 138]\n",
      "[19709, 178]\n",
      "[60, 53]\n",
      "[19275, 612]\n",
      "[49, 64]\n",
      "[19697, 177]\n",
      "[77, 49]\n",
      "[19244, 630]\n",
      "[32, 94]\n",
      "[19631, 220]\n",
      "[63, 86]\n",
      "[19253, 598]\n",
      "[38, 111]\n",
      "[19588, 214]\n",
      "[89, 109]\n",
      "[19219, 583]\n",
      "[41, 157]\n",
      "[19634, 193]\n",
      "[74, 99]\n",
      "[19249, 578]\n",
      "[70, 103]\n",
      "[19629, 192]\n",
      "[86, 93]\n",
      "[19246, 575]\n",
      "[48, 131]\n",
      "[19615, 196]\n",
      "[91, 98]\n",
      "[19183, 628]\n",
      "[59, 130]\n",
      "[19801, 66]\n",
      "[56, 77]\n",
      "[19242, 625]\n",
      "[42, 91]\n",
      "[19702, 174]\n",
      "[55, 69]\n",
      "[19270, 606]\n",
      "[41, 83]\n",
      "[19585, 247]\n",
      "[86, 82]\n",
      "[19206, 626]\n",
      "[53, 115]\n",
      "[19580, 246]\n",
      "[93, 81]\n",
      "[19252, 574]\n",
      "[44, 130]\n",
      "[19521, 294]\n",
      "[107, 78]\n",
      "[19254, 561]\n",
      "[48, 137]\n",
      "[19588, 241]\n",
      "[95, 76]\n",
      "[19261, 568]\n",
      "[61, 110]\n",
      "[19588, 220]\n",
      "[88, 104]\n",
      "[19234, 574]\n",
      "[85, 107]\n",
      "[19756, 123]\n",
      "[68, 53]\n",
      "[19300, 579]\n",
      "[26, 95]\n",
      "[19451, 400]\n",
      "[81, 68]\n",
      "[19229, 622]\n",
      "[66, 83]\n",
      "[19602, 233]\n",
      "[68, 97]\n",
      "[19200, 635]\n",
      "[39, 126]\n",
      "[19399, 422]\n",
      "[89, 90]\n",
      "[19201, 620]\n",
      "[84, 95]\n",
      "[19511, 282]\n",
      "[91, 116]\n",
      "[19208, 585]\n",
      "[52, 155]\n",
      "[19366, 422]\n",
      "[122, 90]\n",
      "[19203, 585]\n",
      "[46, 166]\n",
      "[19477, 334]\n",
      "[101, 88]\n",
      "[19197, 614]\n",
      "[57, 132]\n",
      "[19652, 184]\n",
      "[61, 103]\n",
      "[19235, 601]\n",
      "[76, 88]\n",
      "[19249, 476]\n",
      "[137, 138]\n",
      "[19196, 529]\n",
      "[109, 166]\n",
      "[18486, 1037]\n",
      "[293, 184]\n",
      "[19115, 408]\n",
      "[64, 413]\n",
      "[19143, 349]\n",
      "[62, 446]\n",
      "[19070, 422]\n",
      "[74, 434]\n",
      "[18484, 1008]\n",
      "[297, 211]\n",
      "[19033, 459]\n",
      "[65, 443]\n",
      "[19163, 272]\n",
      "[24, 541]\n",
      "[19075, 360]\n",
      "[42, 523]\n",
      "[18408, 1024]\n",
      "[383, 185]\n",
      "[19050, 382]\n",
      "[73, 495]\n",
      "[19015, 441]\n",
      "[227, 317]\n",
      "[19001, 455]\n",
      "[160, 384]\n",
      "------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds of MWPM:\n",
      "> Accuracy: 0.9740824915825103 (+- 8.282554537914421e-05)\n",
      "> F1: 0.6989849544406607(+- 0.0009174128445085054)\n",
      "> Time: 4218.66922253332 (+- 221.63304957963783)\n",
      "Average scores for all folds of PLUT:\n",
      "> Accuracy: 0.9621580984189365 (+- 3.5066351972436276e-05)\n",
      "> F1: 0.2176246818043047(+- 0.001673345547472553)\n",
      "> Time: 6.342273366666667 (+- 0.14778046902723277)\n",
      "Average scores for all folds of NN:\n",
      "> Accuracy: 0.4946452601203979 (+- 0.34796370527292)\n",
      "> F1: 0.8229599436086684(+- 0.0007771803250872642)\n",
      "> Time: 7.805722133333333 (+- 0.1140205485253817)\n",
      "> AUC for class : 0.9988635894045208 (+- 0.0)\n",
      "X^2 for MWPM and NN: 0\n",
      "X^2 for PLUT and NN: 0\n",
      "> AUC for class X00: 0.8789961486278308 (+- 0.0030417187306681433)\n",
      "X^2 for MWPM and NN: 97.09167950693374\n",
      "X^2 for PLUT and NN: 121.25458248472505\n",
      "> AUC for class X01: 0.9864504118411137 (+- 0.0003234492845580759)\n",
      "X^2 for MWPM and NN: 19.36008230452675\n",
      "X^2 for PLUT and NN: 314.23\n",
      "> AUC for class X02: 0.9984977482636282 (+- 6.78016048958364e-05)\n",
      "X^2 for MWPM and NN: 42.570754716981135\n",
      "X^2 for PLUT and NN: 514.5500705218618\n",
      "> AUC for class X03: 0.9985962679565442 (+- 0.0001054458491984315)\n",
      "X^2 for MWPM and NN: 39.524096385542165\n",
      "X^2 for PLUT and NN: 532.1809954751131\n",
      "> AUC for class X04: 0.9987720354438089 (+- 0.00012158210139239372)\n",
      "X^2 for MWPM and NN: 77.08550185873607\n",
      "X^2 for PLUT and NN: 572.0817518248175\n",
      "> AUC for class X05: 0.9981659121686431 (+- 5.4157497216102624e-05)\n",
      "X^2 for MWPM and NN: 158.67806267806267\n",
      "X^2 for PLUT and NN: 473.493487698987\n",
      "> AUC for class X06: 0.981075095352696 (+- 0.001110859353968728)\n",
      "X^2 for MWPM and NN: 536.0294117647059\n",
      "X^2 for PLUT and NN: 252.23880597014926\n",
      "> AUC for class X10: 0.9795467524292424 (+- 0.0003884672485206659)\n",
      "X^2 for MWPM and NN: 275.953125\n",
      "X^2 for PLUT and NN: 227.1867924528302\n",
      "> AUC for class X11: 0.9957953787029462 (+- 4.33138788006227e-05)\n",
      "X^2 for MWPM and NN: 87.82826086956521\n",
      "X^2 for PLUT and NN: 446.0400534045394\n",
      "> AUC for class X12: 0.9972141275024358 (+- 0.00015205015300324718)\n",
      "X^2 for MWPM and NN: 83.62886597938144\n",
      "X^2 for PLUT and NN: 461.2964669738863\n",
      "> AUC for class X13: 0.997828066773625 (+- 7.292630661289782e-05)\n",
      "X^2 for MWPM and NN: 71.75196850393701\n",
      "X^2 for PLUT and NN: 504.3006230529595\n",
      "> AUC for class X14: 0.9979720403415775 (+- 0.00015585505687968316)\n",
      "X^2 for MWPM and NN: 65.51481481481481\n",
      "X^2 for PLUT and NN: 461.4724964739069\n",
      "> AUC for class X15: 0.9970951288704221 (+- 0.00027346312514147546)\n",
      "X^2 for MWPM and NN: 255.62769784172662\n",
      "X^2 for PLUT and NN: 446.5543644716692\n",
      "> AUC for class X16: 0.9806539117374676 (+- 0.0008851634087738802)\n",
      "X^2 for MWPM and NN: 58.95766590389016\n",
      "X^2 for PLUT and NN: 153.49049429657794\n",
      "> AUC for class X20: 0.9794097518721746 (+- 7.545170178154495e-05)\n",
      "X^2 for MWPM and NN: 95.24385964912281\n",
      "X^2 for PLUT and NN: 166.92482517482517\n",
      "> AUC for class X21: 0.9968905497804609 (+- 0.00019677779322432116)\n",
      "X^2 for MWPM and NN: 306.0400641025641\n",
      "X^2 for PLUT and NN: 500.33087149187594\n",
      "> AUC for class X22: 0.9977636788864451 (+- 0.00010824482011430005)\n",
      "X^2 for MWPM and NN: 84.41438356164383\n",
      "X^2 for PLUT and NN: 472.430981595092\n",
      "> AUC for class X23: 0.9976679571136492 (+- 9.148088364848186e-05)\n",
      "X^2 for MWPM and NN: 97.47\n",
      "X^2 for PLUT and NN: 479.91137370753324\n",
      "> AUC for class X24: 0.997612175473093 (+- 0.00016721329735676713)\n",
      "X^2 for MWPM and NN: 149.3190348525469\n",
      "X^2 for PLUT and NN: 443.0926694329184\n",
      "> AUC for class X25: 0.9971002645309844 (+- 0.0002050421196790154)\n",
      "X^2 for MWPM and NN: 136.54862842892769\n",
      "X^2 for PLUT and NN: 421.3652312599681\n",
      "> AUC for class X26: 0.9813029518151751 (+- 0.00039757391583092224)\n",
      "X^2 for MWPM and NN: 364.7168674698795\n",
      "X^2 for PLUT and NN: 156.85930408472012\n",
      "> AUC for class X30: 0.9814148882387923 (+- 0.0007095165206524134)\n",
      "X^2 for MWPM and NN: 391.1162454873646\n",
      "X^2 for PLUT and NN: 259.245960502693\n",
      "> AUC for class X31: 0.997067942332461 (+- 4.7431806408320627e-05)\n",
      "X^2 for MWPM and NN: 230.78453038674033\n",
      "X^2 for PLUT and NN: 457.62206896551726\n",
      "> AUC for class X32: 0.9977972727420715 (+- 0.00019631015587020986)\n",
      "X^2 for MWPM and NN: 147.22571428571428\n",
      "X^2 for PLUT and NN: 539.0680555555556\n",
      "> AUC for class X33: 0.9978772392085985 (+- 7.159545048376285e-05)\n",
      "X^2 for MWPM and NN: 108.10557184750733\n",
      "X^2 for PLUT and NN: 447.12937595129375\n",
      "> AUC for class X34: 0.997635586067308 (+- 1.7923023195387896e-05)\n",
      "X^2 for MWPM and NN: 91.46827794561933\n",
      "X^2 for PLUT and NN: 463.61215629522434\n",
      "> AUC for class X35: 0.9967871747426141 (+- 5.509580738599767e-05)\n",
      "X^2 for MWPM and NN: 181.94787644787644\n",
      "X^2 for PLUT and NN: 492.8585164835165\n",
      "> AUC for class X36: 0.9798485363571388 (+- 0.0008842658139232793)\n",
      "X^2 for MWPM and NN: 179.2707182320442\n",
      "X^2 for PLUT and NN: 265.36068376068374\n",
      "> AUC for class X40: 0.9815274558537549 (+- 0.0007373416709950826)\n",
      "X^2 for MWPM and NN: 99.76912928759894\n",
      "X^2 for PLUT and NN: 154.74717514124293\n",
      "> AUC for class X41: 0.9969815604523355 (+- 0.0001838018269196324)\n",
      "X^2 for MWPM and NN: 336.85322580645163\n",
      "X^2 for PLUT and NN: 505.75\n",
      "> AUC for class X42: 0.9976421598201402 (+- 0.00018574870430101032)\n",
      "X^2 for MWPM and NN: 77.8753709198813\n",
      "X^2 for PLUT and NN: 442.2739322533137\n",
      "> AUC for class X43: 0.9977628634257076 (+- 0.00014171739501795983)\n",
      "X^2 for MWPM and NN: 130.70694864048338\n",
      "X^2 for PLUT and NN: 533.4307458143074\n",
      "> AUC for class X44: 0.9976882629120559 (+- 0.00013801467790164785)\n",
      "X^2 for MWPM and NN: 150.7240506329114\n",
      "X^2 for PLUT and NN: 429.7647928994083\n",
      "> AUC for class X45: 0.9967379670662684 (+- 0.0002837715590937481)\n",
      "X^2 for MWPM and NN: 129.6359102244389\n",
      "X^2 for PLUT and NN: 429.8090379008746\n",
      "> AUC for class X46: 0.9802727719010155 (+- 0.000813878322939442)\n",
      "X^2 for MWPM and NN: 407.2873475609756\n",
      "X^2 for PLUT and NN: 229.84470989761093\n",
      "> AUC for class X50: 0.9806240249811903 (+- 0.0001437992591175234)\n",
      "X^2 for MWPM and NN: 307.7550362610798\n",
      "X^2 for PLUT and NN: 159.79902755267423\n",
      "> AUC for class X51: 0.9969710535753326 (+- 5.59960468832995e-05)\n",
      "X^2 for MWPM and NN: 168.158940397351\n",
      "X^2 for PLUT and NN: 474.0661870503597\n",
      "> AUC for class X52: 0.9979313901977953 (+- 0.00010514879302344332)\n",
      "X^2 for MWPM and NN: 82.55095541401273\n",
      "X^2 for PLUT and NN: 418.83403068340306\n",
      "> AUC for class X53: 0.9979518383114018 (+- 0.00012444317785785696)\n",
      "X^2 for MWPM and NN: 61.03515625\n",
      "X^2 for PLUT and NN: 506.10752688172045\n",
      "> AUC for class X54: 0.9977965673424863 (+- 5.8027208688186556e-05)\n",
      "X^2 for MWPM and NN: 75.40363636363637\n",
      "X^2 for PLUT and NN: 506.84834834834834\n",
      "> AUC for class X55: 0.9974363541489968 (+- 5.833038372653479e-05)\n",
      "X^2 for MWPM and NN: 176.5133171912833\n",
      "X^2 for PLUT and NN: 452.8994778067885\n",
      "> AUC for class X56: 0.9806821017246232 (+- 0.0008965616541855835)\n",
      "X^2 for MWPM and NN: 132.5976821192053\n",
      "X^2 for PLUT and NN: 197.89983844911146\n",
      "> AUC for class X60: 0.9810076409895444 (+- 0.00020499349334078854)\n",
      "X^2 for MWPM and NN: 87.59172413793104\n",
      "X^2 for PLUT and NN: 167.37765205091938\n",
      "> AUC for class X61: 0.9981887380438556 (+- 4.462588348604703e-05)\n",
      "X^2 for MWPM and NN: 90.3225806451613\n",
      "X^2 for PLUT and NN: 432.45552147239266\n",
      "> AUC for class X62: 0.9987367133341617 (+- 0.00017797078382520212)\n",
      "X^2 for MWPM and NN: 3.3057851239669422\n",
      "X^2 for PLUT and NN: 466.703216374269\n",
      "> AUC for class X63: 0.9986279028475118 (+- 4.277339271595316e-05)\n",
      "X^2 for MWPM and NN: 32.794326241134755\n",
      "X^2 for PLUT and NN: 481.1112716763006\n",
      "> AUC for class X64: 0.9986185377846847 (+- 0.0002516361779702352)\n",
      "X^2 for MWPM and NN: 39.5919540229885\n",
      "X^2 for PLUT and NN: 554.9662921348314\n",
      "> AUC for class X65: 0.9975380084306537 (+- 7.771206740837903e-05)\n",
      "X^2 for MWPM and NN: 35.274193548387096\n",
      "X^2 for PLUT and NN: 490.04328358208954\n",
      "> AUC for class X66: 0.9933451566128491 (+- 0.00039438659583328694)\n",
      "X^2 for MWPM and NN: 23.104693140794225\n",
      "X^2 for PLUT and NN: 255.70506912442397\n",
      "> AUC for class Z00: 0.9740433317629854 (+- 0.0004604497284215058)\n",
      "X^2 for MWPM and NN: 410.7241134751773\n",
      "X^2 for PLUT and NN: 25.68312101910828\n",
      "> AUC for class Z01: 0.9751974356797071 (+- 0.0007919576450551559)\n",
      "X^2 for MWPM and NN: 99.90123456790124\n",
      "X^2 for PLUT and NN: 216.66006097560975\n",
      "> AUC for class Z02: 0.9773095552341532 (+- 0.0011463439412756312)\n",
      "X^2 for MWPM and NN: 415.02454473475854\n",
      "X^2 for PLUT and NN: 246.99497487437185\n",
      "> AUC for class Z03: 0.976886942095034 (+- 0.0005052416653261195)\n",
      "X^2 for MWPM and NN: 163.56427015250546\n",
      "X^2 for PLUT and NN: 239.5588785046729\n",
      "> AUC for class Z04: 0.9769547613654349 (+- 0.0022564331086549557)\n",
      "X^2 for MWPM and NN: 380.6104783599089\n",
      "X^2 for PLUT and NN: 240.61228813559322\n",
      "> AUC for class Z05: 0.9771443713551845 (+- 0.001536554999924842)\n",
      "X^2 for MWPM and NN: 156.37225042301185\n",
      "X^2 for PLUT and NN: 205.25244299674267\n",
      "> AUC for class Z06: 0.9921382261460842 (+- 0.0001338703779422453)\n",
      "X^2 for MWPM and NN: 0.6222222222222222\n",
      "X^2 for PLUT and NN: 331.1454849498328\n",
      "> AUC for class Z10: 0.9977176166556624 (+- 0.00017251135846302841)\n",
      "X^2 for MWPM and NN: 118.52564102564102\n",
      "X^2 for PLUT and NN: 465.2821782178218\n",
      "> AUC for class Z11: 0.9953257740152348 (+- 0.00025402041593499563)\n",
      "X^2 for MWPM and NN: 203.41565217391303\n",
      "X^2 for PLUT and NN: 400.61005434782606\n",
      "> AUC for class Z12: 0.9958527365088624 (+- 0.00024146490829019893)\n",
      "X^2 for MWPM and NN: 116.5569306930693\n",
      "X^2 for PLUT and NN: 390.7540192926045\n",
      "> AUC for class Z13: 0.9958527029632226 (+- 0.00015225004262702313)\n",
      "X^2 for MWPM and NN: 190.32007575757575\n",
      "X^2 for PLUT and NN: 427.629057187017\n",
      "> AUC for class Z14: 0.9956137244965539 (+- 6.983189698720638e-05)\n",
      "X^2 for MWPM and NN: 220.05208333333334\n",
      "X^2 for PLUT and NN: 493.9670164917541\n",
      "> AUC for class Z15: 0.9966158246534821 (+- 5.949619334408003e-05)\n",
      "X^2 for MWPM and NN: 217.86851851851853\n",
      "X^2 for PLUT and NN: 427.84718100890206\n",
      "> AUC for class Z16: 0.9970664628731702 (+- 0.00011536449002722308)\n",
      "X^2 for MWPM and NN: 7.37280701754386\n",
      "X^2 for PLUT and NN: 467.640625\n",
      "> AUC for class Z20: 0.9982738077243983 (+- 8.727757631915115e-05)\n",
      "X^2 for MWPM and NN: 25.80275229357798\n",
      "X^2 for PLUT and NN: 428.72884012539186\n",
      "> AUC for class Z21: 0.9965917403809484 (+- 8.706494140822933e-05)\n",
      "X^2 for MWPM and NN: 28.222996515679444\n",
      "X^2 for PLUT and NN: 484.7640625\n",
      "> AUC for class Z22: 0.9963592164191102 (+- 8.250612630131816e-05)\n",
      "X^2 for MWPM and NN: 120.3139240506329\n",
      "X^2 for PLUT and NN: 421.7451923076923\n",
      "> AUC for class Z23: 0.9964148356286638 (+- 7.787157380654321e-05)\n",
      "X^2 for MWPM and NN: 116.51063829787235\n",
      "X^2 for PLUT and NN: 487.13016845329247\n",
      "> AUC for class Z24: 0.9964532357340007 (+- 0.000270712922731806)\n",
      "X^2 for MWPM and NN: 76.90677966101696\n",
      "X^2 for PLUT and NN: 372.88166666666666\n",
      "> AUC for class Z25: 0.9966868022625572 (+- 0.00016117620611570641)\n",
      "X^2 for MWPM and NN: 17.133587786259543\n",
      "X^2 for PLUT and NN: 445.96399345335516\n",
      "> AUC for class Z26: 0.9981478639168673 (+- 0.000157788090308679)\n",
      "X^2 for MWPM and NN: 57.516806722689076\n",
      "X^2 for PLUT and NN: 477.82753403933435\n",
      "> AUC for class Z30: 0.9983768741030544 (+- 0.0003444015896507351)\n",
      "X^2 for MWPM and NN: 38.58661417322835\n",
      "X^2 for PLUT and NN: 538.3821752265861\n",
      "> AUC for class Z31: 0.9972305652978547 (+- 0.00014015116643065217)\n",
      "X^2 for MWPM and NN: 85.99293286219081\n",
      "X^2 for PLUT and NN: 491.32232704402514\n",
      "> AUC for class Z32: 0.9968161768231864 (+- 0.00026595615843885955)\n",
      "X^2 for MWPM and NN: 50.745874587458744\n",
      "X^2 for PLUT and NN: 469.0400641025641\n",
      "> AUC for class Z33: 0.9966398561301224 (+- 0.00020696566205535068)\n",
      "X^2 for MWPM and NN: 52.149812734082396\n",
      "X^2 for PLUT and NN: 396.68055555555554\n",
      "> AUC for class Z34: 0.9966164010163885 (+- 0.000122035037741405)\n",
      "X^2 for MWPM and NN: 39.65827338129496\n",
      "X^2 for PLUT and NN: 444.1027287319422\n",
      "> AUC for class Z35: 0.9966283576606321 (+- 0.00021100802813891006)\n",
      "X^2 for MWPM and NN: 37.68641114982579\n",
      "X^2 for PLUT and NN: 469.61280931586606\n",
      "> AUC for class Z36: 0.9981064306081424 (+- 0.00010197452440883539)\n",
      "X^2 for MWPM and NN: 0.6639344262295082\n",
      "X^2 for PLUT and NN: 507.832083958021\n",
      "> AUC for class Z40: 0.9979901709482518 (+- 2.1709976529077543e-05)\n",
      "X^2 for MWPM and NN: 60.80349344978166\n",
      "X^2 for PLUT and NN: 491.64760432766616\n",
      "> AUC for class Z41: 0.9965663805836221 (+- 9.363234025126239e-05)\n",
      "X^2 for MWPM and NN: 76.87687687687688\n",
      "X^2 for PLUT and NN: 481.86156111929307\n",
      "> AUC for class Z42: 0.9966572312575507 (+- 0.00027057834986168476)\n",
      "X^2 for MWPM and NN: 68.15339233038348\n",
      "X^2 for PLUT and NN: 452.81715210355986\n",
      "> AUC for class Z43: 0.9965982529290446 (+- 0.0001713359313264622)\n",
      "X^2 for MWPM and NN: 86.27431421446384\n",
      "X^2 for PLUT and NN: 430.44991789819375\n",
      "> AUC for class Z44: 0.9960919672794067 (+- 0.0001333393404548728)\n",
      "X^2 for MWPM and NN: 62.57440476190476\n",
      "X^2 for PLUT and NN: 407.0524642289348\n",
      "> AUC for class Z45: 0.9965029683687957 (+- 0.00026913865992667286)\n",
      "X^2 for MWPM and NN: 55.717532467532465\n",
      "X^2 for PLUT and NN: 361.3717754172989\n",
      "> AUC for class Z46: 0.9982080911273618 (+- 7.212285254443227e-05)\n",
      "X^2 for MWPM and NN: 15.267015706806284\n",
      "X^2 for PLUT and NN: 503.6429752066116\n",
      "> AUC for class Z50: 0.9971408003407833 (+- 0.00019267930615820511)\n",
      "X^2 for MWPM and NN: 210.23700623700623\n",
      "X^2 for PLUT and NN: 447.7107558139535\n",
      "> AUC for class Z51: 0.9961320687988003 (+- 0.00032445808936427945)\n",
      "X^2 for MWPM and NN: 89.35548172757476\n",
      "X^2 for PLUT and NN: 525.259643916914\n",
      "> AUC for class Z52: 0.9958979396452369 (+- 9.193232270406796e-05)\n",
      "X^2 for MWPM and NN: 215.70254403131116\n",
      "X^2 for PLUT and NN: 406.56960227272725\n",
      "> AUC for class Z53: 0.9958076426507981 (+- 0.00018797614608635294)\n",
      "X^2 for MWPM and NN: 96.7828418230563\n",
      "X^2 for PLUT and NN: 444.3076923076923\n",
      "> AUC for class Z54: 0.9956232821039336 (+- 0.0003949489900409349)\n",
      "X^2 for MWPM and NN: 164.34007352941177\n",
      "X^2 for PLUT and NN: 458.7068145800317\n",
      "> AUC for class Z55: 0.9957071931229073 (+- 0.000223540362586437)\n",
      "X^2 for MWPM and NN: 123.73333333333333\n",
      "X^2 for PLUT and NN: 460.70938897168406\n",
      "> AUC for class Z56: 0.9977577554757283 (+- 0.00018539708881205581)\n",
      "X^2 for MWPM and NN: 60.751020408163264\n",
      "X^2 for PLUT and NN: 405.5775480059084\n",
      "> AUC for class Z60: 0.9925694852883927 (+- 0.00047657207670821723)\n",
      "X^2 for MWPM and NN: 186.36867862969004\n",
      "X^2 for PLUT and NN: 275.1739811912226\n",
      "> AUC for class Z61: 0.9768205331752431 (+- 0.00022701104050526776)\n",
      "X^2 for MWPM and NN: 415.07443609022556\n",
      "X^2 for PLUT and NN: 249.25635593220338\n",
      "> AUC for class Z62: 0.9774346921314802 (+- 0.0004282626318825816)\n",
      "X^2 for MWPM and NN: 199.01703163017032\n",
      "X^2 for PLUT and NN: 242.76008064516128\n",
      "> AUC for class Z63: 0.9768756323380452 (+- 0.0011336255628560232)\n",
      "X^2 for MWPM and NN: 386.2835249042146\n",
      "X^2 for PLUT and NN: 294.75\n",
      "> AUC for class Z64: 0.9767222708770826 (+- 0.0006210365621922647)\n",
      "X^2 for MWPM and NN: 206.11148648648648\n",
      "X^2 for PLUT and NN: 249.9726368159204\n",
      "> AUC for class Z65: 0.9770719130129478 (+- 0.0008612136355333857)\n",
      "X^2 for MWPM and NN: 291.11584932480457\n",
      "X^2 for PLUT and NN: 208.4923076923077\n",
      "> AUC for class Z66: 0.9774864825783367 (+- 0.00023697781418664435)\n",
      "X^2 for MWPM and NN: 67.91766467065868\n",
      "X^2 for PLUT and NN: 140.54634146341462\n",
      "###################################################################################\n",
      "TOTAL F1 NN: [0.8219852454141684, 0.8238871636628853, 0.8230074217489513]\n",
      "TOTAL F1 PLUT: [0.2169584612196157, 0.21599126201010793, 0.21992432218319038]\n",
      "TOTAL F1 MWPM: [0.6988408381631742, 0.6979403692322128, 0.7001736559265952]\n",
      "TOTAL ACC NN: [0.25385788083076477, 0.24337486922740936, 0.9867030303030194]\n",
      "TOTAL ACC PLUT: [0.9621499240949071, 0.9621198257074743, 0.9622045454544281]\n",
      "TOTAL ACC MWPM: [0.9740717171717349, 0.9739868686868911, 0.9741888888889043]\n",
      "TOTAL TIME NN: [7.7991714, 7.9485283, 7.6694667]\n",
      "TOTAL TIME PLUT: [6.5494794, 6.2150516, 6.2622891]\n",
      "TOTAL TIME MWPM: [4528.202375699958, 4021.1974551999974, 4106.607836700004]\n",
      "------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "kfold = KFold(n_splits=3, shuffle=True)\n",
    "inputs = x_d7.copy()\n",
    "inputs = np.array(inputs)\n",
    "inputs = np.stack(inputs)\n",
    "targets = np.array(y_d7)\n",
    "targets = np.array([np.array(xi) for xi in targets])\n",
    "\n",
    "n_classes = 99\n",
    "\n",
    "acc_per_fold = []\n",
    "f1_per_fold = []\n",
    "time_per_fold = []\n",
    "\n",
    "acc_per_fold_plut = []\n",
    "f1_per_fold_plut = []\n",
    "time_per_fold_plut = []\n",
    "\n",
    "acc_per_fold_mwpm = []\n",
    "f1_per_fold_mwpm = []\n",
    "time_per_fold_mwpm = []\n",
    "\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "aucs_classes = {}\n",
    "for i in mlb_d7.classes_:\n",
    "    aucs_classes[i] = []\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "for i, (train, test) in enumerate(kfold.split(inputs, targets)):\n",
    "    i_train = inputs[train].copy()\n",
    "    i_test = inputs[test].copy()\n",
    "    t_test = targets[test].copy()\n",
    "    x_test_d7 = i_test[:,:2]\n",
    "    inputs_train = i_train[:,3:]\n",
    "    inputs_test = i_test[:,3:]\n",
    "    indices = np.random.choice(inputs[test].shape[0], 20000, replace=False)\n",
    "    x_test_d7 = x_test_d7[indices]\n",
    "    inputs_test_2 = inputs_test[indices]\n",
    "    targets_test_2 = targets[test][indices]\n",
    "    ####################################################################################################\n",
    "    #test MWPM decoder for this fold\n",
    "    #labels = targets[train], features = inputs[train]\n",
    "   # x_test_d7 = translate_to_graph(testData_d7_MWPM, targs, mlb_d7)\n",
    "    decoding_d7, time_mwpm = do_new_decoding(x_test_d7, 7, 0)\n",
    "    decoding_d7['combine'] = decoding_d7[[0, 1]].values.tolist()\n",
    "    decoding_d7['combine'].apply(lambda x: x[0].extend(x[1]))\n",
    "    decoding_d7 = np.array(decoding_d7[0])\n",
    "                                              \n",
    "    time_per_fold_mwpm.append(time_mwpm)\n",
    "                                              \n",
    "    pred_mwpm = mlb_d7.transform(decoding_d7)\n",
    "\n",
    "    if fold_no < 3:\n",
    "        acc = partial_accuracy(targets_test_2, pred_mwpm)\n",
    "    else:\n",
    "        acc, contingency_mwpm = partial_accuracy_and_contingency(targets_test_2, pred_mwpm, mlb_d7)\n",
    "        \n",
    "    acc_per_fold_mwpm.append(acc)\n",
    "    f1_per_fold_mwpm.append(f1_score(targets_test_2, pred_mwpm, average='micro'))\n",
    "\n",
    "    #####################################################################################################\n",
    "    #test the plut decoder for this fold\n",
    "    \n",
    "    lookup_d7 = lookup_decoder(7)\n",
    "    \n",
    "    lookup_d7 = train_plut(lookup_d7, inputs_train, targets[train])\n",
    "    \n",
    "    start = time.time_ns()\n",
    "    pred_plut_d7 = test_plut(lookup_d7, inputs_test)\n",
    "    end = time.time_ns() \n",
    "    time_per_fold_plut.append((end - start)/ (10 ** 9))\n",
    "    \n",
    "    if fold_no < 3:\n",
    "        acc = partial_accuracy(targets[test], pred_plut_d7)\n",
    "        f1 = f1_score(targets[test], pred_plut_d7, average='micro')\n",
    "    else:\n",
    "        pred_plut_d7 = test_plut(lookup_d7, inputs_test_2)\n",
    "        f1 = f1_score(targets_test_2, pred_plut_d7, average='micro')\n",
    "        acc, contingency_plut = partial_accuracy_and_contingency(targets_test_2, pred_plut_d7, mlb_d7)\n",
    "        \n",
    "    acc_per_fold_plut.append(acc)\n",
    "    f1_per_fold_plut.append(f1)\n",
    "    \n",
    "    #####################################################################################################\n",
    "    #Test the NN decoder for this fold\n",
    "    \n",
    "    model_d7 = compile_FFNN_model_DepthSeven(7)\n",
    "\n",
    "    # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    # Fit data to model\n",
    "    \n",
    "    inputs_train = np.asarray(inputs_train).astype(np.int) #CHANGE (added)\n",
    "    targets[train] = np.asarray(targets[train]).astype(np.int) #CHANGE (added)\n",
    "    \n",
    "    history = model_d7.fit(\n",
    "        x=inputs_train ,\n",
    "        y=targets[train],\n",
    "        validation_split=.25,\n",
    "        epochs= 150)\n",
    "    \n",
    "   # Generate generalization metrics\n",
    "    \n",
    "    inputs_test = np.asarray(inputs_test).astype(np.int) #CHANGE (added)\n",
    "    targets[test] = np.asarray(targets[test]).astype(np.int) #CHANGE (added)\n",
    "    \n",
    "    scores = model_d7.evaluate(inputs_test, targets[test], verbose=0)\n",
    "    \n",
    "    start = time.time_ns()\n",
    "    predictions_d7 = model_d7.predict(inputs_test)\n",
    "    end = time.time_ns() \n",
    "    time_per_fold.append((end - start)/ (10 ** 9))\n",
    "    \n",
    "    #threshold based on previous tests with train_test_split in hyperparameter tuning\n",
    "    pred=predictions_d7.copy() #change here\n",
    "    pred[pred>=.5]=1 \n",
    "    pred[pred<.5]=0\n",
    "     \n",
    "    if fold_no < 3:\n",
    "        acc = scores[1]\n",
    "        f1 = f1_score(targets[test], pred, average='micro')\n",
    "    else:\n",
    "        inputs_test_2 = np.asarray(inputs_test_2).astype(np.int) #CHANGE (added)\n",
    "        pred = model_d7.predict(inputs_test_2)\n",
    "        pred[pred>=.5]=1 \n",
    "        pred[pred<.5]=0\n",
    "        acc, contingency_nn = partial_accuracy_and_contingency(targets_test_2, pred, mlb_d7)\n",
    "        f1 = f1_score(targets_test_2, pred, average='micro')\n",
    " \n",
    "    acc_per_fold.append(acc)\n",
    "    f1_per_fold.append(f1)\n",
    "\n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(targets[test].ravel(), predictions_d7.ravel())\n",
    "    tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "    tprs[-1][0] = 0.0\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "    plt.plot(fpr, tpr, lw=1, alpha=0.3, label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
    "    \n",
    "    #get the AUCs of each class, used to get average AUC of each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(targets[test][:, i], predictions_d7[:, i]) \n",
    "        aucs_classes[mlb_d7.classes_[i]].append(auc(fpr[i], tpr[i]))\n",
    "\n",
    "#########################################################################################################\n",
    "#compute McNemar's statistic on results of last fold\n",
    "\n",
    "mcnemar_results_mwpm = {}\n",
    "mcnemar_results_plut = {}\n",
    "\n",
    "for class_ in mlb_d7.classes_:\n",
    "    \n",
    "    #compute the x^2 for NN and MWPM\n",
    "    mcnemar_results_mwpm[class_]=contingency_table_and_t(contingency_nn[class_], contingency_mwpm[class_])[1]    \n",
    "    #comput the x^2 for NN and PLUT\n",
    "    mcnemar_results_plut[class_] = contingency_table_and_t(contingency_nn[class_], contingency_plut[class_])[1]\n",
    "        \n",
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds of MWPM:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold_mwpm)} (+- {np.std(acc_per_fold_mwpm)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold_mwpm)}(+- {np.std(f1_per_fold_mwpm)})')\n",
    "print(f'> Time: {np.mean(time_per_fold_mwpm)} (+- {np.std(time_per_fold_mwpm)})')\n",
    "print('Average scores for all folds of PLUT:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold_plut)} (+- {np.std(acc_per_fold_plut)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold_plut)}(+- {np.std(f1_per_fold_plut)})')\n",
    "print(f'> Time: {np.mean(time_per_fold_plut)} (+- {np.std(time_per_fold_plut)})')\n",
    "print('Average scores for all folds of NN:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold)}(+- {np.std(f1_per_fold)})')\n",
    "print(f'> Time: {np.mean(time_per_fold)} (+- {np.std(time_per_fold)})')\n",
    "for key in aucs_classes:\n",
    "    cleanedList = [x for x in aucs_classes[key] if str(x) != 'nan']\n",
    "    print(f'> AUC for class {key}: {np.mean(cleanedList)} (+- {np.std(cleanedList)})') #this has to be a for loop\n",
    "    print(\"X^2 for MWPM and NN: \" + str(mcnemar_results_mwpm[key]))  \n",
    "    print(\"X^2 for PLUT and NN: \" + str(mcnemar_results_plut[key]))\n",
    "print(\"###################################################################################\")\n",
    "print(\"TOTAL F1 NN: \" + str(f1_per_fold))\n",
    "print(\"TOTAL F1 PLUT: \" + str(f1_per_fold_plut))\n",
    "print(\"TOTAL F1 MWPM: \" + str(f1_per_fold_mwpm))\n",
    "print(\"TOTAL ACC NN: \" + str(acc_per_fold))\n",
    "print(\"TOTAL ACC PLUT: \" + str(acc_per_fold_plut))\n",
    "print(\"TOTAL ACC MWPM: \" + str(acc_per_fold_mwpm))\n",
    "print(\"TOTAL TIME NN: \" + str(time_per_fold))\n",
    "print(\"TOTAL TIME PLUT: \" + str(time_per_fold_plut))\n",
    "print(\"TOTAL TIME MWPM: \" + str(time_per_fold_mwpm))\n",
    "print('------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "         label='Chance', alpha=.8)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "         label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "         lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                 label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "plt.xlim([-0.01, 1.01])\n",
    "plt.ylim([-0.01, 1.01])\n",
    "plt.xlabel('False Positive Rate',fontsize=18)\n",
    "plt.ylabel('True Positive Rate',fontsize=18)\n",
    "plt.title('Cross-Validation ROC of Depth 7 NN',fontsize=18)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TOTAL F1 NN: \" + str(f1_per_fold))\n",
    "print(\"TOTAL F1 PLUT: \" + str(f1_per_fold_plut))\n",
    "print(\"TOTAL F1 MWPM: \" + str(f1_per_fold_mwpm))\n",
    "print(\"TOTAL ACC NN: \" + str(acc_per_fold))\n",
    "print(\"TOTAL ACC PLUT: \" + str(acc_per_fold_plut))\n",
    "print(\"TOTAL ACC MWPM: \" + str(acc_per_fold_mwpm))\n",
    "print(\"TOTAL TIME NN: \" + str(time_per_fold))\n",
    "print(\"TOTAL TIME PLUT: \" + str(time_per_fold_plut))\n",
    "print(\"TOTAL TIME MWPM: \" + str(time_per_fold_mwpm))\n",
    "print('-----------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test MWPM decoder for this fold\n",
    "#labels = targets[train], features = inputs[train]\n",
    "# x_test_d7 = translate_to_graph(testData_d7_MWPM, targs, mlb_d7)\n",
    "\"\"\"\n",
    "decoding_d7, time_mwpm = do_new_decoding(x_test_d7, 7, 0)\n",
    "decoding_d7['combine'] = decoding_d7[[0, 1]].values.tolist()\n",
    "decoding_d7['combine'].apply(lambda x: x[0].extend(x[1]))\n",
    "decoding_d7 = np.array(decoding_d7[0])\n",
    "\n",
    "time_per_fold_mwpm.append(time_mwpm)\n",
    "\n",
    "pred_mwpm = mlb_d7.transform(decoding_d7)\n",
    "\n",
    "if fold_no < 3:\n",
    "    acc = partial_accuracy(targets_test_2, pred_mwpm)\n",
    "else:\n",
    "    acc, contingency_mwpm = partial_accuracy_and_contingency(targets_test_2, pred_mwpm, mlb_d7)\"\"\"\n",
    "\n",
    "#acc_per_fold_mwpm.append(acc)\n",
    "#f1_per_fold_mwpm.append(f1_score(targets_test_2, pred_mwpm, average='micro'))\n",
    "\n",
    "#####################################################################################################\n",
    "#test the plut decoder for this fold\n",
    "\n",
    "#lookup_d7 = lookup_decoder(7)\n",
    "\n",
    "#lookup_d7 = train_plut(lookup_d7, inputs_train, targets[train])\n",
    "\n",
    "#start = time.time_ns()\n",
    "#pred_plut_d7 = test_plut(lookup_d7, inputs_test)\n",
    "#end = time.time_ns() \n",
    "#time_per_fold_plut.append((end - start)/ (10 ** 9))\n",
    "\n",
    "pred_plut_d7 = test_plut(lookup_d7, inputs_test_2)\n",
    "#f1 = f1_score(targets_test_2, pred_plut_d7, average='micro')\n",
    "acc, contingency_plut = partial_accuracy_and_contingency(targets_test_2, pred_plut_d7, mlb_d7)\n",
    "\n",
    "#acc_per_fold_plut.append(acc)\n",
    "#f1_per_fold_plut.append(f1)\n",
    "\n",
    "#####################################################################################################\n",
    "#Test the NN decoder for this fold\n",
    "\"\"\"\n",
    "model_d7 = compile_FFNN_model_DepthSeven(7)\n",
    "\n",
    "# Generate a print\n",
    "print('------------------------------------------------------------------------')\n",
    "print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "# Fit data to model\n",
    "history = model_d7.fit(\n",
    "    x=inputs_train ,\n",
    "    y=targets[train],\n",
    "    validation_split=.25,\n",
    "    epochs= 150)\"\"\"\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(targets[test].ravel(), predictions_d7.ravel())\n",
    "tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "tprs[-1][0] = 0.0\n",
    "roc_auc = auc(fpr, tpr)\n",
    "aucs.append(roc_auc)\n",
    "plt.plot(fpr, tpr, lw=1, alpha=0.3, label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
    "\n",
    "#get the AUCs of each class, used to get average AUC of each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(targets[test][:, i], predictions_d7[:, i]) \n",
    "    aucs_classes[mlb_d7.classes_[i]].append(auc(fpr[i], tpr[i]))\n",
    "\n",
    "#########################################################################################################\n",
    "#compute McNemar's statistic on results of last fold\n",
    "\n",
    "mcnemar_results_mwpm = {}\n",
    "mcnemar_results_plut = {}\n",
    "\n",
    "for class_ in mlb_d7.classes_:\n",
    "    \n",
    "    #compute the x^2 for NN and MWPM\n",
    "    mcnemar_results_mwpm[class_]=contingency_table_and_t(contingency_nn[class_], contingency_mwpm[class_])[1]    \n",
    "    #comput the x^2 for NN and PLUT\n",
    "    mcnemar_results_plut[class_] = contingency_table_and_t(contingency_nn[class_], contingency_plut[class_])[1]\n",
    "        \n",
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds of MWPM:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold_mwpm)} (+- {np.std(acc_per_fold_mwpm)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold_mwpm)}(+- {np.std(f1_per_fold_mwpm)})')\n",
    "print(f'> Time: {np.mean(time_per_fold_mwpm)} (+- {np.std(time_per_fold_mwpm)})')\n",
    "print('Average scores for all folds of PLUT:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold_plut)} (+- {np.std(acc_per_fold_plut)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold_plut)}(+- {np.std(f1_per_fold_plut)})')\n",
    "print(f'> Time: {np.mean(time_per_fold_plut)} (+- {np.std(time_per_fold_plut)})')\n",
    "print('Average scores for all folds of NN:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold)}(+- {np.std(f1_per_fold)})')\n",
    "print(f'> Time: {np.mean(time_per_fold)} (+- {np.std(time_per_fold)})')\n",
    "for key in aucs_classes:\n",
    "    cleanedList = [x for x in aucs_classes[key] if str(x) != 'nan']\n",
    "    print(f'> AUC for class {key}: {np.mean(cleanedList)} (+- {np.std(cleanedList)})') #this has to be a for loop\n",
    "    print(\"X^2 for MWPM and NN: \" + str(mcnemar_results_mwpm[key]))  \n",
    "    print(\"X^2 for PLUT and NN: \" + str(mcnemar_results_plut[key]))\n",
    "print(\"###################################################################################\")\n",
    "print(\"TOTAL F1 NN: \" + str(f1_per_fold))\n",
    "print(\"TOTAL F1 PLUT: \" + str(f1_per_fold_plut))\n",
    "print(\"TOTAL F1 MWPM: \" + str(f1_per_fold_mwpm))\n",
    "print(\"TOTAL ACC NN: \" + str(acc_per_fold))\n",
    "print(\"TOTAL ACC PLUT: \" + str(acc_per_fold_plut))\n",
    "print(\"TOTAL ACC MWPM: \" + str(acc_per_fold_mwpm))\n",
    "print(\"TOTAL TIME NN: \" + str(time_per_fold))\n",
    "print(\"TOTAL TIME PLUT: \" + str(time_per_fold_plut))\n",
    "print(\"TOTAL TIME MWPM: \" + str(time_per_fold_mwpm))\n",
    "print('------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "         label='Chance', alpha=.8)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "         label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "         lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                 label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "plt.xlim([-0.01, 1.01])\n",
    "plt.ylim([-0.01, 1.01])\n",
    "plt.xlabel('False Positive Rate',fontsize=18)\n",
    "plt.ylabel('True Positive Rate',fontsize=18)\n",
    "plt.title('Cross-Validation ROC of Depth 7 NN',fontsize=18)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test MWPM decoder for this fold\n",
    "#labels = targets[train], features = inputs[train]\n",
    "# x_test_d7 = translate_to_graph(testData_d7_MWPM, targs, mlb_d7)\n",
    "\"\"\"\n",
    "decoding_d7, time_mwpm = do_new_decoding(x_test_d7, 7, 0)\n",
    "decoding_d7['combine'] = decoding_d7[[0, 1]].values.tolist()\n",
    "decoding_d7['combine'].apply(lambda x: x[0].extend(x[1]))\n",
    "decoding_d7 = np.array(decoding_d7[0])\n",
    "\n",
    "time_per_fold_mwpm.append(time_mwpm)\n",
    "\n",
    "pred_mwpm = mlb_d7.transform(decoding_d7)\n",
    "\n",
    "if fold_no < 3:\n",
    "    acc = partial_accuracy(targets_test_2, pred_mwpm)\n",
    "else:\n",
    "    acc, contingency_mwpm = partial_accuracy_and_contingency(targets_test_2, pred_mwpm, mlb_d7)\"\"\"\n",
    "\n",
    "#acc_per_fold_mwpm.append(acc)\n",
    "#f1_per_fold_mwpm.append(f1_score(targets_test_2, pred_mwpm, average='micro'))\n",
    "\n",
    "#####################################################################################################\n",
    "#test the plut decoder for this fold\n",
    "\n",
    "#lookup_d7 = lookup_decoder(7)\n",
    "\n",
    "#lookup_d7 = train_plut(lookup_d7, inputs_train, targets[train])\n",
    "\n",
    "#start = time.time_ns()\n",
    "#pred_plut_d7 = test_plut(lookup_d7, inputs_test)\n",
    "#end = time.time_ns() \n",
    "#time_per_fold_plut.append((end - start)/ (10 ** 9))\n",
    "\n",
    "if fold_no < 3:\n",
    "    acc = partial_accuracy(targets[test], pred_plut_d7)\n",
    "    f1 = f1_score(targets[test], pred_plut_d7, average='micro')\n",
    "else:\n",
    "    pred_plut_d7 = test_plut(lookup_d7, inputs_test_2)\n",
    "    #f1 = f1_score(targets_test_2, pred_plut_d7, average='micro')\n",
    "    acc, contingency_plut = partial_accuracy_and_contingency(targets_test_2, pred_plut_d7, mlb_d7)\n",
    "\n",
    "#acc_per_fold_plut.append(acc)\n",
    "#f1_per_fold_plut.append(f1)\n",
    "\n",
    "#####################################################################################################\n",
    "#Test the NN decoder for this fold\n",
    "\"\"\"\n",
    "model_d7 = compile_FFNN_model_DepthSeven(7)\n",
    "\n",
    "# Generate a print\n",
    "print('------------------------------------------------------------------------')\n",
    "print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "# Fit data to model\n",
    "history = model_d7.fit(\n",
    "    x=inputs_train ,\n",
    "    y=targets[train],\n",
    "    validation_split=.25,\n",
    "    epochs= 150)\"\"\"\n",
    "\n",
    "pred = model_d7.predict(inputs_test_2)\n",
    "pred[pred>=.5]=1 \n",
    "pred[pred<.5]=0\n",
    "acc, contingency_nn = partial_accuracy_and_contingency(targets_test_2, pred, mlb_d7)\n",
    "f1 = f1_score(targets_test_2, pred, average='micro')\n",
    "\n",
    "#acc_per_fold.append(acc)\n",
    "f1_per_fold.append(f1)\n",
    "\n",
    "# Increase fold number\n",
    "fold_no = fold_no + 1\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(targets[test].ravel(), predictions_d7.ravel())\n",
    "tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "tprs[-1][0] = 0.0\n",
    "roc_auc = auc(fpr, tpr)\n",
    "aucs.append(roc_auc)\n",
    "plt.plot(fpr, tpr, lw=1, alpha=0.3, label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
    "\n",
    "#get the AUCs of each class, used to get average AUC of each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(targets[test][:, i], predictions_d7[:, i]) \n",
    "    aucs_classes[mlb_d7.classes_[i]].append(auc(fpr[i], tpr[i]))\n",
    "\n",
    "#########################################################################################################\n",
    "#compute McNemar's statistic on results of last fold\n",
    "\n",
    "mcnemar_results_mwpm = {}\n",
    "mcnemar_results_plut = {}\n",
    "\n",
    "for class_ in mlb_d7.classes_:\n",
    "    \n",
    "    #compute the x^2 for NN and MWPM\n",
    "    mcnemar_results_mwpm[class_]=contingency_table_and_t(contingency_nn[class_], contingency_mwpm[class_])[1]    \n",
    "    #comput the x^2 for NN and PLUT\n",
    "    mcnemar_results_plut[class_] = contingency_table_and_t(contingency_nn[class_], contingency_plut[class_])[1]\n",
    "        \n",
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds of MWPM:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold_mwpm)} (+- {np.std(acc_per_fold_mwpm)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold_mwpm)}(+- {np.std(f1_per_fold_mwpm)})')\n",
    "print(f'> Time: {np.mean(time_per_fold_mwpm)} (+- {np.std(time_per_fold_mwpm)})')\n",
    "print('Average scores for all folds of PLUT:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold_plut)} (+- {np.std(acc_per_fold_plut)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold_plut)}(+- {np.std(f1_per_fold_plut)})')\n",
    "print(f'> Time: {np.mean(time_per_fold_plut)} (+- {np.std(time_per_fold_plut)})')\n",
    "print('Average scores for all folds of NN:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> F1: {np.mean(f1_per_fold)}(+- {np.std(f1_per_fold)})')\n",
    "print(f'> Time: {np.mean(time_per_fold)} (+- {np.std(time_per_fold)})')\n",
    "for key in aucs_classes:\n",
    "    cleanedList = [x for x in aucs_classes[key] if str(x) != 'nan']\n",
    "    print(f'> AUC for class {key}: {np.mean(cleanedList)} (+- {np.std(cleanedList)})') #this has to be a for loop\n",
    "    print(\"X^2 for MWPM and NN: \" + str(mcnemar_results_mwpm[key]))  \n",
    "    print(\"X^2 for PLUT and NN: \" + str(mcnemar_results_plut[key]))\n",
    "print(\"###################################################################################\")\n",
    "print(\"TOTAL F1 NN: \" + str(f1_per_fold))\n",
    "print(\"TOTAL F1 PLUT: \" + str(f1_per_fold_plut))\n",
    "print(\"TOTAL F1 MWPM: \" + str(f1_per_fold_mwpm))\n",
    "print(\"TOTAL ACC NN: \" + str(acc_per_fold))\n",
    "print(\"TOTAL ACC PLUT: \" + str(acc_per_fold_plut))\n",
    "print(\"TOTAL ACC MWPM: \" + str(acc_per_fold_mwpm))\n",
    "print(\"TOTAL TIME NN: \" + str(time_per_fold))\n",
    "print(\"TOTAL TIME PLUT: \" + str(time_per_fold_plut))\n",
    "print(\"TOTAL TIME MWPM: \" + str(time_per_fold_mwpm))\n",
    "print('------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "         label='Chance', alpha=.8)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "         label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "         lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                 label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "plt.xlim([-0.01, 1.01])\n",
    "plt.ylim([-0.01, 1.01])\n",
    "plt.xlabel('False Positive Rate',fontsize=18)\n",
    "plt.ylabel('True Positive Rate',fontsize=18)\n",
    "plt.title('Cross-Validation ROC of Depth 7 NN',fontsize=18)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "model_d5 = compile_FFNN_model_DepthFive(5)\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "print(\"Fit model on training data\")\n",
    "history = model_d5.fit(\n",
    "    x=x_train_d5.values,\n",
    "    y=Y_train_d5,\n",
    "    validation_split=.25,\n",
    "    epochs = 500\n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Make a quick epoch vs. loss plot\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.grid()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Depth 5 Model Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (MSE)')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Make a quick epoch vs. accuracy plot\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.grid()\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Depth 5 Model Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# predict\n",
    "from sklearn.metrics import precision_score,accuracy_score, f1_score, recall_score, hamming_loss\n",
    "\n",
    "start = time.time()\n",
    "predictions_d5 = model_d5.predict(x_test_d5.values)\n",
    "end = time.time()\n",
    "print(\"Time: \" + str(end - start))\n",
    "\n",
    "pred=predictions_d5.copy()\n",
    "\n",
    "thresholds=[0.1, 0.2, 0.3, .4, .5, .6, .7, .8, .9]\n",
    "for val in thresholds:\n",
    "    pred=predictions_d5.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "    \n",
    "  \n",
    "    precision = precision_score(Y_test_d5, pred, average='micro')\n",
    "    recall = recall_score(Y_test_d5, pred, average='micro')\n",
    "    f1 = f1_score(Y_test_d5, pred, average='micro')\n",
    "   \n",
    "    print(\"Micro-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "    print(\"Accuracy = \",accuracy_score(Y_test_d5, pred))\n",
    "    print(\"Accuracy = \",partial_accuracy(Y_test_d5, pred))\n",
    "    print(\"\\n\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "pred=predictions_d5.copy()\n",
    "from sklearn.metrics import multilabel_confusion_matrix, classification_report\n",
    "#look at confusion matrix to see what got misclassified    \n",
    "pred[pred>=.5]=1\n",
    "pred[pred<.5]=0\n",
    "multilabel_confusion_matrix(Y_test_d5, pred)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#look at classifcation report to see what got mislabeled\n",
    "print(classification_report(Y_test_d5, pred))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_d7 = compile_FFNN_model_DepthSeven(7)\n",
    "print(\"Fit model on training data\")\n",
    "history = model_d7.fit(\n",
    "    x=x_train_d7.values,\n",
    "    y=Y_train_d7,\n",
    "    validation_split=.25,\n",
    "    epochs= 150\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a quick epoch vs. loss plot\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.grid()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Depth 7 Model Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (MSE)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a quick epoch vs. accuracy plot\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.grid()\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Depth 7 Model Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\"\"\n",
    "# predict\n",
    "from sklearn.metrics import precision_score,accuracy_score, f1_score, recall_score, hamming_loss\n",
    "\n",
    "start = time.time()\n",
    "predictions_d7 = model_d7.predict(x_test_d7.values)\n",
    "end = time.time()\n",
    "print(\"Time: \" + str(end - start))\n",
    "\n",
    "pred=predictions_d7.copy()\n",
    "\n",
    "\n",
    "thresholds=[0.1, 0.2, 0.3, .4, .5, .6, .7, .8, .9]\n",
    "for val in thresholds:\n",
    "    pred=predictions_d7.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "    \n",
    "  \n",
    "    precision = precision_score(Y_test_d7, pred, average='micro')\n",
    "    recall = recall_score(Y_test_d7, pred, average='micro')\n",
    "    f1 = f1_score(Y_test_d7, pred, average='micro')\n",
    "   \n",
    "    print(\"Micro-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "    print(\"Accuracy = \",accuracy_score(Y_test_d7, pred))\n",
    "    print(\"Accuracy = \",partial_accuracy(Y_test_d7, pred))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "predictions_d7 = model_d7.predict(x_test_d7.values)\n",
    "pred=predictions_d7.copy()\n",
    "\n",
    "n_classes = 99\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(Y_test_d7[:, i], pred[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(Y_test_d7.ravel(), pred.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "    \n",
    "# Plot of a ROC curve for a specific class\n",
    "for i in range(n_classes):\n",
    "    plt.figure()\n",
    "    plt.plot(fpr[i], tpr[i], label='ROC curve (area = %0.2f)' % roc_auc[i])\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure()\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Some extension of Receiver operating characteristic to multi-class')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "x_train_d3.to_csv(\"x_train_d3_01.csv\")\n",
    "x_test_d3.to_csv(\"x_test_d3_01.csv\")\n",
    "pd.DataFrame(Y_train_d3).to_csv(\"Y_train_d3_01.csv\")\n",
    "pd.DataFrame(Y_test_d3).to_csv(\"Y_test_d3_01.csv\")\n",
    "\n",
    "x_train_d5.to_csv(\"x_train_d5_01.csv\")\n",
    "x_test_d5.to_csv(\"x_test_d5_01.csv\")\n",
    "pd.DataFrame(Y_train_d5).to_csv(\"Y_train_d5_01.csv\")\n",
    "pd.DataFrame(Y_test_d5).to_csv(\"Y_test_d5_01.csv\")\n",
    "\n",
    "x_train_d7.to_csv(\"x_train_d7_01.csv\")\n",
    "x_test_d7.to_csv(\"x_test_d7_01.csv\")\n",
    "pd.DataFrame(Y_train_d7).to_csv(\"Y_train_d7_01.csv\")\n",
    "pd.DataFrame(Y_test_d7).to_csv(\"Y_test_d7_01.csv\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "model_d7.save(\"model_d7_01.h5\")\n",
    "model_d5.save(\"model_d5_01.h5\")\n",
    "model.save(\"model_d3_01.h5\")\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
